
var documents = [{
    "id": 0,
    "url": "http://localhost:4000/404.html",
    "title": "404",
    "body": "404 Page does not exist!Please use the search bar at the top or visit our homepage! "
    }, {
    "id": 1,
    "url": "http://localhost:4000/about",
    "title": "About the author",
    "body": "Dalong has many years experience in enterprise/digital applications architecture and operating skills based on Cloud. He helps enterprises leverage advanced technologies for digital transformation and innovation to achieve strategic goals. His key areas of industry expertise are Retail, Automotive, Energy, Pharmaceutical and Health Care. His major project experiences are: - Lead Watson for Oncology SaaS Product localization program in China with AI treatment plan integration in more than 100 hospitals - Lead Digital innovations with AI, robotic, Auto Driving, Sustainability and carbon neutral solutions on cloud - Lead development of large-scale e-commerce websites, mobile applications and WeChat ecosystems, managing more than 100 IT team members with Agile/ Scrum methodology and tools - Retail industry customer experience improvement, CRM &amp; Loyalty Program product design and development - Client engagement, application architecture design and migration on Hybrid/Public Cloud.  Documentation: Contact the author dalong_co@hotmail. com. Questions or bug reports?: Head over to our Github repository! Buy me a coffeeThank you for your support! Your donation helps me to maintain and improve Dalong. work . "
    }, {
    "id": 2,
    "url": "http://localhost:4000/categories",
    "title": "Categories",
    "body": ""
    }, {
    "id": 3,
    "url": "http://localhost:4000/",
    "title": "Home",
    "body": "      Featured:                                                                                                                                                                                                                       Designing and Implementing a Microsoft Azure AI Solution                                                 1 2 3 4 5                                              :               随着微软收购了 OpenAI, ChatGPT 的能力也融入到了 微软的 Azure 中. 我将带领大家了解 Microsoft Azure AI 不同服务, 包括 Cognitive Services, Computer Vision, Custom Vision, Video indexer, Natural Language Processing (NLP),. . . :                                                                                                                                                                       Dalong                                30 May 2023                                                                                                                                                                                                                                                                                                                                          Prepare for Google CLoud Platform Professional Architect Certification                                                 1 2 3 4 5                                              :               我一直以来都是谷歌的忠实粉丝, 从 08 年开始就开始使用谷歌的各种产品和服务. 因为项目需要, 我花了 2 个多月的时间筹备谷歌专业认证架构师的认证并一次性通过了考试. 这里我把一些学习心得总结了下来跟大家一起分享. 如果你也在准备这门考试, 看这篇文章就足够了. :                                                                                                                                                                       Dalong                                23 Feb 2023                                                                                                                                                                                                                            All Stories:                                                                                                      How to use Jekyll develop your own Blog                         1 2 3 4 5                      :       博客可以让一个人表达他/她的想法，每个人都渴望拥有他/她的博客。截至今天，创建博客非常简单。WordPress，jekyll，hugo等开源软件可以帮助您创建和设置博客。在这篇文章中，我将带领你了解 Jekyll 以及如何在 Github 中免费托管它。:                                                                               Dalong                21 Jul 2023                                                                                                                                     Preparing for ITIL Certification                         1 2 3 4 5                      :       如果您正在考虑 2023 年的 ITIL 认证，您会在这里找到很多有用的信息和指导。ITIL认证是全球公认的IT服务管理（ITSM）标准。这些认证为个人和组织提供了与ITIL最佳实践相关的能力和知识的明确指示。 让我带你一起探索 ITIL 的理念和如何获得这个认证. :                                                                               Dalong                24 Jun 2023                                                                                                                                     Designing and Implementing a Microsoft Azure AI Solution                         1 2 3 4 5                      :       随着微软收购了 OpenAI, ChatGPT 的能力也融入到了 微软的 Azure 中. 我将带领大家了解 Microsoft Azure AI 不同服务, 包括 Cognitive Services, Computer Vision, Custom Vision, Video indexer, Natural Language Processing (NLP), Speech Service, Translate Language, OpenAI. . . :                                                                               Dalong                30 May 2023                                                                                                                                     Use ChatGPT in Python                         1 2 3 4 5                      :       ChatGPT API 是一个应用程序编程接口（API），由 OpenAI 提供，使开发人员能够将强大的 ChatGPT 模型无缝集成到他们的应用程序中。 这种大型语言模型(LLM)拥有能够理解自然语言并生成类似人类的回应。要在 Python 中使用 ChatGPT API，必须遵循一系列一般步骤。 我将带您了解如何在应用程序中使用 ChatGPT API Python，从在 OpenAl 网站注册到 Python 编程语言。:                                                                               Dalong                09 May 2023                                                                                                                                     Prepare TOGAF, The Open Group Architecture Framework Certification                         1 2 3 4 5                      :       TOGAF® 基本上是一套清晰的组织发展规则和实践，旨在帮助组织。它指导企业如何在单一战略中创建、解释、分析和利用不同的要素。该框架还使用了明确定义的术语。这样可以简化部门之间的沟通，并大大减少浪费错误的可能性。就让我带领你揭开他的面纱。:                                                                               Dalong                05 Apr 2023                                                                                                                                     Metaverse research                         1 2 3 4 5                      :       “元宇宙（Metaverse）是一个虚拟时空间的集合，由一系列的增强现实（AR）、虚拟现实（VR）和互联网（Internet）所组成。表示“超越宇宙”的概念：一个平行于现实世界运行的虚拟空间。随着 Apple Vision Pro 的发布, 接下来的元宇宙生态和应用场景将会更加的丰富. 让我们一起来了解元宇宙和它的生态吧. :                                                                               Dalong                21 Mar 2023                                               &laquo; Prev       1        2        3        4        5      Next &raquo; "
    }, {
    "id": 4,
    "url": "http://localhost:4000/robots.txt",
    "title": "",
    "body": "      Sitemap: {{ “sitemap. xml”   absolute_url }}   "
    }, {
    "id": 5,
    "url": "http://localhost:4000/page2/",
    "title": "Home",
    "body": "{% if page. url == “/” %}       Featured:       {% for post in site. posts %}    {% if post. featured == true %}      {% include featuredbox. html %}    {% endif %}  {% endfor %}  {% endif %}       All Stories:         {% for post in paginator. posts %}    {% include postbox. html %}    {% endfor %}    {% include pagination. html %}"
    }, {
    "id": 6,
    "url": "http://localhost:4000/page3/",
    "title": "Home",
    "body": "{% if page. url == “/” %}       Featured:       {% for post in site. posts %}    {% if post. featured == true %}      {% include featuredbox. html %}    {% endif %}  {% endfor %}  {% endif %}       All Stories:         {% for post in paginator. posts %}    {% include postbox. html %}    {% endfor %}    {% include pagination. html %}"
    }, {
    "id": 7,
    "url": "http://localhost:4000/page4/",
    "title": "Home",
    "body": "{% if page. url == “/” %}       Featured:       {% for post in site. posts %}    {% if post. featured == true %}      {% include featuredbox. html %}    {% endif %}  {% endfor %}  {% endif %}       All Stories:         {% for post in paginator. posts %}    {% include postbox. html %}    {% endfor %}    {% include pagination. html %}"
    }, {
    "id": 8,
    "url": "http://localhost:4000/page5/",
    "title": "Home",
    "body": "{% if page. url == “/” %}       Featured:       {% for post in site. posts %}    {% if post. featured == true %}      {% include featuredbox. html %}    {% endif %}  {% endfor %}  {% endif %}       All Stories:         {% for post in paginator. posts %}    {% include postbox. html %}    {% endfor %}    {% include pagination. html %}"
    }, {
    "id": 9,
    "url": "http://localhost:4000/Jekyll-Static-Website-Development/",
    "title": "How to use Jekyll develop your own Blog",
    "body": "2023/07/21 - 博客可以让一个人表达他/她的想法，每个人都渴望拥有他/她的博客。截至今天，创建博客非常简单。WordPress，jekyll，hugo等开源软件可以帮助您创建和设置博客。在这篇文章中，我将带领你了解 Jekyll 以及如何在 Github 中免费托管它。 github 创建个人 Blog[TOC] 目的::  使用 github 创建个人主页, 并将自己的学习心得和个人项目代码放到上面 使用 Jekyll 的模板来美化 blog 使用 Markdown 形式的内容来写 blog资源  如何创建 github 主页 我的主页地址 Jekyll 主页地址 如何用 Jekyll 创建 github 主页 一个基于 Jekyll 的 blog 模板 - mediumish-theme-jekyllgithub 上 创建静态网站: 可以用 user page , 也可以用 project page 项目网站:  用于项目 github 每个 repository 都可以有一个网站 http://. github. io/个人网站:  用于个人或者公司网站 一个 github 账号只能有一个网站 http://. github. io/ 需要用 master branch创建流程  在 github 自己的 账户下建立一个 github repository, 名字要是dalongli. github. io 这里的 dalongli 要和 自己的 github 账号的名字一致 方法一 命令行     选择一个本地目录 我这里的目录是 /Users/lidalong/Desktop/   命令    123456git clone https://github. com/dalongli/dalongli. github. iocd dalongli. github. ioecho  Hello World  &gt; index. htmlgit add --allgit commit -m  Initial commit git push -u origin main -f          访问 https://dalongli. github. io. 就可以看到 hellow world 了    方法二 Github Desktop app     下载 github desktop app   clone dalongli. github. io   在提示中 用 VS Code 打开目录   在vsc 中修改内容, 并提交   刷新 https://dalongli. github. io. 可以看到新的内容了    用 vscode 打开创建的项目 把用 Jekyll 生成的 _site 目录中的内容拷贝到 dalongli. github. io 目录下, commit 并提交到 github 上Jekyll: Jekyll 的工作原理, 分为以下几个步骤  读取 configuration file 读取 所有的其他文件, 并 YAML 配置文件(front matter) 使用 Liquid 模板语言处理内容和模板 读取 assets (资产文件, 比如 css, js, image) 创建最终的输出到 _site 目录资源: Youtube 用 Docker + VSCode 开发 Jekyll 的 视频jsbin. com 一个写 html 代码就可以看到效果的 沙箱网站codepen. io, 也是一个 所见即所得的 html, css, js 沙箱网站 截图上传工具, 类似 Pic 这个 app?jekyll 官网Jekyll DocFront Matter 语法Markdown 语法Jekyll 目录结构 在 Mac 上直接安装 Jekyll: 123456gem sources -lgem sources --remove https://rubygems. org/gem sources -a https://gems. ruby-china. com/gem install jekyll bundler# 如果成功了jekyll -v # jekyll 4. 3. 2用 Docker 安装 Jekyll:  在 VSCode 中安装两个扩展, Docker 和 Dev Containers 重启 VSCode 在 github 上创建一个新的 repository     选中 rademe 和 ignore 和 gitignore (jekyll)   进入 repository -&gt; setting -&gt; page   点击 github action 按钮, 保存 yml 文件 (什么都不用改)   会发现原来的 repository 中多个一个 yaml 文件, 并且在 setting 下面可以看到网站的 url 了   url https://dalongli. github. io/my-jekyll-docker-website/   回到 repo 主页, 把 url 复制到 setting 中 的 web url 地址栏中, 方便以后点击进入查看                    在vscode 中 clone 刚刚创建的 repo     cmd + shift + p 输入 git clone 然后黏贴 repo 的 url https://github. com/dalongli/my-jekyll-docker-website 回车, 打开    到 https://jekyllrb. com/docs/ 查看 jekyll 需要的 ruby (2. 5. 0 或更高版本) 的最低版本 , 还有网站右上角的 jekyll 的版本 (4. 3. 2)     看一下 github pages 需要的 安装的包和依赖版本         jekyll 3. 9. 3     ruby 2. 5. 0 或以上          注意:         For Jekyll 3. 9. (also GitHub Pages)             Ruby 2. x       Alpine 3. x                For Jekyll 4. 2. (non-GitHub Pages)             Ruby 3. 0. x       Alpine 3. x                      在 Docker hub 上找到对应的 Docker 镜像     我们要找一个 ruby image 然后手动把 jekyll 加进去   搜索 ruby alpine    回到 vscode 中 , 创建一个 docker file     在 repo 的目录下新建一个文件 dockerfile         A dockerfile defines how to build your container from an image      # Create a Jekyll container from a Ruby Alpine image# At a minimum, use Ruby 2. 5 or laterFROM ruby:2. 7-alpine3. 15# Add Jekyll dependencies to AlpineRUN apk updateRUN apk add --no-cache build-base gcc cmake git# Update the Ruby bundler and install JekyllRUN gem update bundler &amp;&amp; gem install bundler jekyll               保存文件, cmd + shift + p 输入 commit all cmd + shift + p 输入 git push    进入到 Docker Desktop app , 并登录 VSCode 中 cmd + shit + p 输入 open folder in container     选择 刚刚 克隆下来的那个 repo 的目录   弹框出来, 选择 docker file. (这里是要用我们刚刚创建的那个 dockerfile 来创建容器)   这里启动开发容器要等一会, 才能完成容器的创建   完成后可以在 docker desktop app 中看到新建的 volume, image, docker 容器   创建 jekyll 项目: 12345# 进入到桌面文件夹内jekyll new myblogcd myblogbundle exec jekyll serve # # 访问http://127. 0. 0. 1:4000/执行 bundle 命令后 jekyll 会自动生成一个 _site 的目录 修改 默认的模板为新的模板:  从 _config. yml 文件中的 theme 可以看到默认模板的名字是 minima 命令 bundle show minima 可以看到模板的路径 是 /Users/lidalong/. rvm/gems/ruby-3. 1. 1/gems/minima-2. 5. 1 关闭 bundle 服务 control + c 删掉三个文件     2023-07-20-welcome-to-jekyll. markdown   about. markdown   index. markdown    删掉 _config. yml 中 有 minima 的行 删掉 Gemfile 中 有 minima 的行 把模板的所有文件拷贝到 项目目录下 从新启动服务 bundle exec jekyll serve  , 这时新的模板文件会被编译到 _site 目录下 访问 http://localhost:4000 可以看到新的页面了这里 jekyll 背后的逻辑是  读取 _config. yml 查看 _posts 中的内容 在读取目录下所有非 _ 开头的静态文件 在 _site 中创建对应的静态网站所需的html, css, js 在jekyll 运行时, 你对项目目录下的任何已经存在的文件或者目录进行修改, 都会同步更新到 _site 文件夹下语法 {{ content }} , layout , include, 变量, if else endif:  是模板语法 {{ content }} 是一个在 html 中被替换的变量 在 项目目录下创建一个 _layout 的目录, 目录下创建一个新的 default. html , 文件中把 index. html 的 header 和 footer 的代码块放进去, 中间用 {{ content }} 修改其他 html 文件, 去掉 header 和 footer     再在每个html 文件的头 用下面Front Matter 语法代替. (注意, 必须放在文件的开头位置)    1234---layout: defaulttitle: 很棒的网站, 有创意---          这里的 layout 对应 _layout 目录   这里的 default 对应 default. html   每个 html 页面加载的时候都会把自己的 html 内容放到 default. html 的 {{ content }} 中   然后把自己的 page. title 放到 default. html 的 标签中    用下面的 if, else 代替 default. html 中的  中间的内容  12345{% if page. title %} {{page. title}}{% else %} Greate Web Agency{% endif %}    include 语法是从 _include 文件夹下找到相应的 html 文件 作为子页面嵌入到当前页面中     这个 html 文件需要放在 _include 目录下   任何需要抽象出来可以被复用的html 代码块都可以单独放到 _include 目录下面    1{% include subpage-header. html %}          _config. yml 配置文件: 官方配置文件详解markdown 解析器, 能把 markdown 转成 HTML?YMML 语法  这里的配置 在 html 中用 {{ site. 变量名 }} 替换 需要重启web 服务, yaml 中的配置才能生效```yamltitle: 伟大的网站 # 这里是 site. title, 不是 page. titleemail: dalong@example. comdescription: &gt;- # this means to ignore newlines until “baseurl:” Write an awesome description for your new site here. You can edit this line in _config. yml. It will appear in your document head meta (for Google search results) and in your feed. xml site description. baseurl: “” # the subpath of your site, e. g. /blogurl: “http://127. 0. 0. 1:4000” # the base hostname &amp; protocol for your site, e. g. http://example. comtwitter_username: jekyllrbgithub_username: jekyll plugins:  jekyll-feed```YAML front matter:  Front matter 是 Jekyll 用的 yaml 的一个格式, 用来格式化 页面和页面内容的信息 必须在html 页面的头进行定义 也是 yaml 格式的, 和 _config. yml 中的变量定义格式一样 比如  1234---layout: defaulttitle: Portfolio---    变量分两种     Jekyll 预定义变量         site - 全局变量             site. pages       site. posts       site. data                page - 全局变量, 包含了当前页面的数据信息     content          自定义变量   Liquid 模板语言: Liquid 官方文档  是 shopify 最早使用的一种模板语言, 后来被shopify 开源了 Jekyll 用的模板语言是 Liquid {{}}     output markup, 放变量的 , 比如 page. title, site. title    {%%}     tag markup, 放带有逻辑的标签, 比如 if, for                  是过滤器, 用于对变量进行操作, 比如格式化日期 {{ page. date | date:  %Y-%m-%d %H:%M }}          制作 静态 Blog:  blog 功能已经被继承到 jekyll 中 可以通过在一个文件夹中放置 markdown 文件就可以创作新的 blog 所有的 blog markdown 文章都需要使用 YAML Front Matter 语法, jekyll 会把 markdown 转成 html 页面 不需要数据库, 不会有读取的performance 问题, 不会有安全问题 markdown 文件的格式必须是 year-month-day-title. md 需要在项目根目录下建立一个 _posts 目录, 然后在目录下建立一个文件 需要在. md 文件中的 front mater 中提前设置好文章的基础变量, 比如  123456789---layout: posttitle: China MLPSdate: 2023-07-22 07:30author: dalongimage: http://placehold. it/900x300category: Network Securitysubtitle: MLPS---    同时, 我们也要去创建 _layout 目录下的 post. html 编辑好 post 之后, 访问单个文章的路径是     http://127. 0. 0. 1:4000/metaverse/2023/07/21/学习元宇宙. html   可以通过在 _config. yml 中设置 permalink: none 来让url 变得简单   http://127. 0. 0. 1:4000/metaverse/学习元宇宙. html   为 blog 做一些 widget:  比如 blog category list,123{% for category_name in page. categories limit:1 %}  {{ category_name }} {% endfor %} # 获取当前Post 的所有所属分类 某个 category 中的 blog list12345{% for post in site. categories[category_name] limit:5 %}  &lt;li&gt;   &lt;a href= {{ post. url }} &gt;{{ post. title }}&lt;/a&gt;  &lt;/li&gt; {% endfor %} 列出所有的 post12345678910{% for post in site. posts %}  {{ post. url }}   {{ post. title }}  {{ post. author }}  {{ post. date | date:  %Y-%m-%d %H:%M  }}  {% if post. image %}   {{post. image}}  {% endif %}  {% post. excerpt %} # POST 简介{% endfor %}设置默认值:  很多时候需要在 front matter 中重复设置很多变量信息 可以在 _config. yml 中设置默认值, 这样就不需要在每个页面的 front matter 中重复设置那些值 jekyll 是先读取 _config. yml 中的默认值作为 front matter, 然后在读取每个页面的 front matter. 如果单独页面没有设置 frontend matter, 就会用默认的值.  scope 定义了变量使用的范围, 可以通过路径和类型指定 value 定义默认的变量名和变量值123456789101112# In _config. yml. . . defaults: -  scope:   path:  projects    type:  pages   values:   layout:  project    author:  Mr. Hyde    category:  project . . . 1234567defaults: -  scope:   path:    # an empty string here means all files in the project   type:  posts  # previously `post` in Jekyll 2. 2.   values:   layout:  default  我们可以用下面这个  123456789defaults:- scope:  path:    # an empty string here means all files in the project  type:  posts  # previously `post` in Jekyll 2. 2.  values:  layout:  post   author:  Dalong   category:  学习    制作 form 提交表单: 制作 表单的方法用 formspree. io 创作自定义表单的快速方法  可以通过以上方法创造在 jekyll 中用的表单 表单可以以邮件的形式发送到我们自己的邮箱中读取 data 数据:  jekyll 支持从 _data 目录读取 YAML, JSON, CSV 等文件的数据 创建一个文件 _data/navigation. yml, 文件内容可以在 site. data. navigation 变量中获取12345- name: Home link: /- name: About link: /about. html HTML 页面中读取的方式1234567&lt;nav&gt; {% for item in site. data. navigation %}  &lt;a href= {{ item. link }}  {% if page. url == item. link %}style= color: red; {% endif %}&gt;   {{ item. name }}  &lt;/a&gt; {% endfor %}&lt;/nav&gt; services. yml     icon 可以在 这里 找到```yaml      icon: “icon ion-coffee”title: “Branding”content: “Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut”     icon: “ion-compass”title: “Web Design”content: “Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut”   icon: “ion-earth”title: “Brand Identity”content: “Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut”```12345{% for service in site. data. services %}  {{ service. icon }}  {{ service. title }}  {{ service. content }}{% endfor%} 另一个例子     把 json 文件的 data 展示出来    projects. json  1234567891011121314151617181920[{ category :  Branding , img :  img/portfolio/work1. jpg , title :  Tesla Motors , content :  Labore et dolore magna aliqua. Ut enim ad },{ category :  Branding , img :  img/portfolio/work4. jpg , title :  Tesla Motors , content :  Labore et dolore magna aliqua. Ut enim ad },{ category :  Video , img :  img/portfolio/work1. jpg , title :  Tesla Motors , content :  Labore et dolore magna aliqua. Ut enim ad }   ]   12345678910111213{% assign filters = site. data. projects | group_by: 'category' %} {% for filter in filters %}  {{ filter. name }}{% endfor %}{% for project in site. data. projects %}  {{ project. category }}  {{ project. img }}  {{ project. title }}  {{ project. content }}{% endfor%} 这里的 assign 是 liquid 语法     设置一个变量, 这个变量是获取所有 Projects 的 category , 目的是用于为 projects 进行分类   为 博客增加 评论 (comments) 功能: 参考 这篇文章视频教学是用 Disqus, 已经被 Jekyll 集成了  为 静态 blog 增加 评论功能 的 一个比较常见的方案是用 Disqus     进入 https://disqus. com 完成注册   点击右上角 setting, 点击 Add Disqus to site   半天收到个邮件, 点击Complete Installation -&gt;   把生成的 html 代码黏贴到 blog post html 页面的下面   把本地代码上传到github: 这里展示的是搞一个新的 repository ,然后有这个 repository 作为 静态页面的站  在 github 上创建一个新的 repository my-jekyll-docker-website 进入 jekyll 项目目录 修改 _config. yml 中的 url 为 https://dalongli. github. io/my-jekyll-docker-website/ 用 git 初始化项目目录, 提交代码  1234567891011# gh repo clone dalongli/dalongli. github. iogit initgit statusgit add . git statusgit commit -m  my first commit git remote add origin https://github. io/dalongli/my-jekyll-docker-website. gitgit push -u origin maingit checkout -b gh-pages # 切换成 gh-pages, 因为只有这个 branch 才能作 静态网站git branchgit push origin gh-pages    访问网站 https://dalongli. github. io/my-jekyll-docker-website为 github 的静态网站做 定制化域名:  如果想用自己的域名换到 github 的域名, 需要自己购买一个新的域名 这里用 dalong. work 作为例子 步骤如下     进入已经初始化过 github 的项目目录   项目根目录下创建一个新的文件叫做 CNAME, 内容只有一行: dalong. work    123git add . git commit -m  Add CNAME file for dalong. work git push origin gh-pages          到域名供应商的网站上, 对域名进行设置 (GoDaddy site or others )         增加一个 A 记录, name=@, value = 192. 30. 252. 153?     增加一个 CNAME, name=www, value=dalongli. github. io     等待一会才能生效          Wait for change to propogate   安装和调试 新的 Jekyll 模板到 github:  这里用 Mediumish Jekyll theme 作为例子     目的是下载模板到本地   运行本地网站   修改内容为自己的个人blog   上传到 github   运行模板  首先在本地的桌面创建新的目录 themes, 井cd 进去 命令行git clone https://github. com/wowthemesnet/mediumish-theme-jekyll. git vscode 打开目录 vscode 命令行 bundle exec jekyll serve , 提示需要安装依赖包 bundle install 提示 “An error occurred while installing ffi (1. 10. 0), and Bundlercannot continue. ” 网上找答案, 提示运行 bundle update ffi 再运行 bundle install 成功安装所有gem 依赖包 提示 “cannot load such file – rexml/parsers/baseparser (LoadError)”     通过在 Gemfile 文件中增加两行解决.     12gem  jekyll ,  ~&gt; 3. 9 gem  kramdown-parser-gfm ,  ~&gt; 1. 1            提示 “cannot load such file – webrick (LoadError)” , 运行 bundle add webrick 解决 bundle exec jekyll serve 成功启动模板#### logo 制作favicon 制作 "
    }, {
    "id": 10,
    "url": "http://localhost:4000/ITIL-Certification-In-Action/",
    "title": "Preparing for ITIL Certification",
    "body": "2023/06/24 - 如果您正在考虑 2023 年的 ITIL 认证，您会在这里找到很多有用的信息和指导。ITIL认证是全球公认的IT服务管理（ITSM）标准。这些认证为个人和组织提供了与ITIL最佳实践相关的能力和知识的明确指示。 让我带你一起探索 ITIL 的理念和如何获得这个认证. ITIL 4[TOC] 资源: ITIL 官网ITIL WIKIITIL 和云ITIL 模板/check listUdemy 讲课老师的个人网站一个ITIL 讲师的网站, 包含了模板 考试准备/考试技巧:  考试入口 60 分钟的考试 非英语母语可以多 15 分钟 (需要book 考试之前额外申请) 40 道题, 都是多选题 ,考你对 ITIL4 的概念和理解     9 道题考的是 recall , 就是概念题, 准确match 官方材料的概念定义   31 道题是 understand, 就是给你一个场景, 你要理解并运用 ITIL 的知识, 从答案中找到正确的对一个定义/名词/概念的解释和描述    26 道题正确就可以通过考试 考前建议做 8 套 60 道的模拟题.      做模拟题的时候, 如果 75%正确率就可以预约考试了.     考试可以在线考     400 美元考试费 + optional 的 100 美元的保险 (第一次考不过, 可以考第二次)    考题类型     虽然都是单选题, 也有三种类型         Classic             迪斯尼发明了什么? 米奇老鼠       下面哪个不是车的品牌? Cisco                Missing Word             [?] 是水果的一种? Apple                List             三明治中要放什么? 1苹果, 2培根, 3葡萄果冻, 4核桃奶油 - 1 and 4                      考试技巧     如果答案都是很短的内容, 可以先扫一眼问题和答案, 再完整的看问题   六、重要提示 1. 线上考试，遇到任何问题，请第一时间与官方客服建立联系和交流。避免被默认为”缺考 Absent” 2. 7*24 客服联系方式•E-mail: customerservice@peoplecert. org•To join us via chat: https://www. peoplecert. org/help-and-support•Call from China: 400-882-2053 (中文客服周一-周日，早上 9 点-晚上 21 点) 需要背的英文: tangible  有型的intangible 无形的inventory目录adequately足够的personnel全体员工malfunctioned故障nurturing培养, 培育(感情/关系)assembled(一些人)组合(成一个团队)competencies资质perceived察觉到的(利益) ITIL 认证路径:  先拿 ITIL Foundation 然后分两条路     拿4 个证书后 自动获得 MP   拿 2 个证书后自动获得 SL    拿全 5 个, 获得 Master1 ITIL 是一个服务管理框架:  ITIL4 Sutdy Guide         ITIL认证是从事IT服务管理的人才必备认证。IT服务管理是ITIL框架的核心，它是一套协同流程（Process），并通过服务级别协议（SLA）来保证IT服务的质量。它融合了系统管理、网络管理、系统开发管理等管理活动和变更管理、资产管理、问题管理等许多流程的理论和实践。 汽车制造商本质是出行体验的服务商, 为客户提供良好的出行体验     以用户为核心的服务体验设计   2 Service Management 概念: 2. 1价值 - 需求:  要先了解企业部分, 最终客户的价值和需求, 了解他们对 IT , 云的需求是什么 全面的了解了不同利益方的需求, 才能知道我们如何最好的满足他们的要求2. 2组织和人:  组织和人包括     外包公司/供应商人员   上下游/生态合作伙伴   IT 团队人员   企业部门员工   最终客户    每个组织和人都有自己的价值和需求 要梳理清楚不同组织和人之间利益关系, 谁是 supplier, 谁是 comsumer (user, customer, sponsor)     user 服务的使用者   customer/client 服务需求定义者, 服务产出的负责者   sponsor 服务消费的资金提供者    要梳理清楚不同组织和人之间的冲突/gap和mitigation plan 要梳理不同组织和人之间的合作模式, 以便给企业带来最大的价值2. 3 服务和产品:  服务     服务是合作开启的价值, 是客户想要的产出         A service enables value co-creation by facilitating outcomes that customers want to achieve.           由服务提供者提供成本和风险管理, 不需要消费者管         “A service is a means of enabling value co-creation by facilitating outcomes that customers want to achieve, without the customer having to manage specific costs and risks. ”           产品     服务可以基于产品之上   产品是组织把可配置的资源打包成一个 offer   组织创造的配置的资源会给它的客户带来价值    比如     AWS云服务   iphone 14   2. 3. 1 Service Offering:  为客户提供的,可以满足客户需求的服务的描述 service action     服务期间, 合同中规定的行为, 比如免费技术培训, 免费的技术支持, 免费的沙箱试用等    比如     AWS 的 S3 的描述/价格模型/适用场景      2. 4 Service relationship management:  是服务提供者和服务消费者的关系     Relationship management is the practice of establishing and nurturing links between an organization and its stakeholders at strategic and tactical levels.     包括了 服务提供, 服务消费, 服务关系管理     服务提供 - 是一堆活动,         比如 AWS 提供的 S3 服务的咨询服务活动, 培训活动, 定期会议, 系统升级维护等          一个公司内部可以有 service provider 和 consumer   2. 5 outcome, cost and risk: 2. 5. 1 Outcome:  给一个 stakeholder 的一个或者多个output的结果     影响的结果   成本介入   风险介入   支持输出   成本节约   风险避免    比如, 使用了 AWS 云之后     不需要自己维护数据中心   不需要建立庞大的运维团队   需要额外增加 AWS云的管理员角色   无需自己开发管理 Portal   花费的成本   有核心数据上云泄露的风险   解约了大笔管理费用   避免了因为自己知识不足而可能产生的网络数据泄露风险   2. 5. 2 Cost:  可以是     项目成本   人天/人时/人头(FTE(Full Time Equivalent))   云的初次购买   RedHat - SYB(Single Year Booking)   AWS - ARR(Annual Recurring Revenue)   pay as you use (severless 服务)   2. 5. 3 Risk 风险:  可能发生的, 产生影响, 损失, 让达到目标变得困难的事情 风险就等于成本 选择云的风险与规避的风险     降低数据中心的单次投入, 多中心灾备, 硬件设备维护折旧, 运营成本   降低了数据安全保障成本   降低了硬件知识储备成本   增加了云知识储备和运营成本   增加了云之间进行切换/多云的成本   降低了接触和使用最新的云技术的成本    风险应对方法     Avoid         用其他解决方案          accept         低风险低成本的          transfer         通过找供应商来承担高风险工作, 或者买保险          mitigate         比如通过云的高可用灾备方案来降低单数据中心地震灾难发生, 通过 replica 的 DB 备份快速恢复数据           降低风险的方法     prehand         充分沟通, 了解清楚上云的限制和关键成功要素          Middle         定期获取风险列表, 定期评估风险, 并对高风险问题执行风险规避行动          afterhand         执行快速应急处置预案, 快速处理发生的风险.           2. 5. 4 Utility 和 Warranty:  Utility     产品或者服务提供的满足一个特定需求的功能   满足需求的功能         “Utility is the functionality offered by a product or service to meet a particular need. ”           Warranty     保证产品或服务可以满足这个需求   满足需求的执行效率   ‘how the service performs’   ‘fit for use’    例子:     Warranty: 一个web 网站可以满足 1 万人同时访问   Utility: 这个网站上一个分享产品到社交媒体的功能   3 服务管理的四个维度 Four Dimensons:  Orgnizations &amp; People     focus         roles and responsibility     formal organizational structures     culture     required staffing and competencies          比如:         组织的扁平化结构     侧重于公司如何构建其人力资源                Information &amp; Technology     focus         information     knowledge     technology required (相关技术)          比如:         通信系统 (communication system)     知识库 (knowledge base)     是否把web 网站从 on-premise 迁移到 cloud     install a Microsoft SharePoint server to serve as the organization’s knowledge base           Partners &amp; suppliers     focus         the organization’s relationships with other organizations that are involved in the design, development, deployment, delivery, support, and/or continual improvement of services. This also incorporates contracts and other agreements between the organization and its partners or suppliers.      roles and responsibility ?     contract and agreement ?     security and compliance ?           Value streams &amp; processes     focus                                       activities that transform inputs to outputs         价值流就是 将输入转化为输出的活动                                  The ‘value streams and processes’ dimension focuses on what activities the organization undertakes, and how they are organized, as well as how the organization ensures that it is enabling value creation for all stakeholders efficiently and effectively. A key focus of the ‘value streams and processes’ dimension are processes which are activities that transform inputs into outputs.                 比如:             公司要做一个自动化的系统, 有 9 个步骤来帮助学生高效完成考试券的购买.                       中间是 Products and Services     核心是 Value    例子: 公司发生了一个云的事故, 那么如何影响这四个维度     O &amp; P         技术支持, help desk, 升级到不同的层级, 不同的部门…          IT         注册事故到系统, 事故跟踪, 事故分析, root cause 分析, 知识库          P &amp; S         主要合作伙伴或者外包公司帮忙解决事故          V &amp; P         改变流程来避免事故, 把低价值的步骤去掉          4 服务价值系统 Service Value System (SVS):  描述组织如何可以把各个components 和 activities 有效结合, 开启共创的价值     The purpose of the service value system is to ensure that the organization continually co-creates value with all stakeholders through the use and management of products and services.          服务价值体系的目的是确保组织通过对产品和服务的使用和管理，不断地与所有利益相关者共同创造价值。           不同的 components 可以有不同的组合 说白了就是要团队高效协作, 功能完成一致的目标 先来个需求/商机 中间要做一堆事     指导原则   治理方案   服务价值链 (流程制定)   实践 (ITIL 4 提供 34 种实践方式)   持续改进    最后产出价值4. 3 Guiding Principles 指导原则:  一个可以在所有条件下引导组织的一个推荐 允许组织把自己通过一个方法整合到服务管理中 可以指导各种活动 Guiding principles are considered universally applicable and enduring.  They should never be changed within the organization, but instead provide the organization with 7 areas to always focus on during their continual improvement efforts.  当你遇到问题, 可以在 指导原则中找到方向 7 条指导原则                           focus on value       专注价值                        According to the guiding principle of ‘focus on value’, “Everything that the organization does needs to map, directly or indirectly, to value for the stakeholders.      案例             影响用户体验(CX)       了解用户: why the consumer uses the service, what the services help them to do, and how the services help them achieve their own goals       也和 cost and financial consequences 相关                                           start where you are       从目前的位置出发                        The ‘start where you are’ guiding principle says that the current state should be investigated and observed directly to make sure it is fully understood before you attempt to create a brand new service or process. There is usually something in the existing service or process that can be reused, saving you time, effort, and resources.      案例             有一个老服务, 要建一个新的自动化服务, 要参考老服务的材料和流程是否可以重用       邮件服务不好用了, 你一顿调查, 发现邮件服务无法满足 utility and warranty requirements                                           progress iteratively with feedback       迭代前进并不断获取反馈                        案例             做一个应用, 让部分人使用, 通过反馈在进行改进       做一个移动应用, 不要一次上 15 个功能, 而是先发布已经完成的4 个功能, 通过用户反馈后, 把没上的 3 个进行了改进, 之后就是每3 周上一个功能       开发方式从瀑布改成敏捷, 2 周一次发布, beta 测试人员测试新功能并给出反馈                                           collaborate and promote visibility       注重合作并提升透明度       不论在项目内外,都要提升沟通                        案例             为了设计新的服务, 组成一个虚拟小组, 邀请不同部门的人参与进来, 希望让大家未来都可以接受这个服务       识别和管理所有利益相关者群体，以便在整个工作人员中建立更有力的沟通。       大家把一个项目的 ‘information radiator’(信息宣传纸) 贴在办公室不同部门容易看到的地方, 内容包括了项目的项目工作项, 项目进度, 风险, 整体目标等.                                            think and work holistically 全面       整体思考和工作 (集思广益, 共同协作)       了解组织的所有部分如何以集成的方式协同工作                        The guiding principle ‘think and work holistically’ recognizes the complexity of a system and seeks to understand the relationships between the various components that make up a service. This is because services are delivered to internal and external service consumers through the coordination and integration of the four dimensions of service management and, therefore, requires you to think about a service from end-to-end by understanding how an organization can work together in an integrated way to achieve the desired objectives.      案例             甲乙分别负责一个服务的两个部分, 两个人密切沟通讨论如何完成工作       协调整个组织，以便最好地了解复杂服务的工作原理                                           keep it simple and practical       保持简单实用                        案例             有 15 个步骤的流程, review 发现有 3 个步骤没啥用, 直接删了       有 150 个 metrics 需要分析, 但是发现不是都有用, 所以你把那些没价值的删了       一个账号创建流程, 先用 3 种账号创建方法cover 99% 的场景, 1% 的特殊情况先手动解决. 几个小时就搞定了                                           optimize and automate       优化和自动化                        The guiding principle of ‘optimize and automate’ is used to maximize the value of the work carried out by the human and technical resources within an organization.              优化和自动化最好的适配场景就是通过人 和 技术 实现的工作内容                The guiding principle of ‘optimize and automate’ relies heavily on optimization. Optimization is the process of improving and increasing the efficiency of a process or service.           4. 5 Service Value Chain (SVC):  通过创造和管理产品和服务来满足需求和促进价值实现所需的关键活动     the key activities required to respond to demand and facilitate value realization through the creation and management of products and services    Service value chain activities are connected to and interact with one another, with each activity receiving and providing triggers for further actions to be taken.      服务价值链 的活动是联系在一起的, 互相加护的, 每个活动都会收到或提供 trigger 给其他 活动    可以理解为一个流程, 包括如下步骤     Engage -&gt; ( Design &amp; transition -&gt; Obtain(resources)/build -&gt; Deliver &amp; support ) -&gt; products &amp; Service   这个流程的上面和下面是 plan 和 improvement, 就是要不断的计划和改进    SVC 的流程结合到一起, 相互作用   SVC 可以用到 34 个 practice 的各种组合   Service Level management (比如 供应商的服务质量经理 ) 对 SVC 几个部分的贡献     engage         Collect and process feedback from customer and users     收集和处理来自客户和用户的反馈          design and transition         The ‘design and transition’ activity in the service level management practice provides feedback from interactions with customers into new or changed services.      将与客户互动的反馈意见纳入新的或改变的服务中。          Obtain/build         The ‘obtain/build’ activity in the service level management practice provides objectives for component and service performance for products and services     为组件提供目标, 为产品和服务提供服务性能          delivery and support         The ‘deliver and support’ activity in the service level management practice collects feedback during interactions and communicates service performance objectives to the operations and support teams.      在交互过程中收集反馈，并将服务绩效目标传达给运营和支持团队           Service request management employee (比如 help desk 接线员) 对于 SVC 的几个部分的对应的责任     improve         提供 trend, quality, feedback info 给 服务需求 ( Improve includes the analysis of data to identify opportunities to provide new service request options. )             比如: 通过分析飞机晚点的趋势, 质量和客户反馈来帮助客户提供合适的时间点的机票预定                     engage         communicate with consumers to understand their requirement             和消费者沟通了解他们的需求                     design and transition         启动 standard 服务 来满足服务需求             比如: 定机票                     obtain/build         获取 pre-approved 服务内容 来满足服务需求             比如: 改签机票; 换个更大的路由器                     Deliver &amp; support         当客户需要从服务商那里获得支持的时候, 确保客户一直是 productive 的.              比如: 客户飞机延期了, 及时与客户联系询问并提供帮助        Plan:                             服务价值链的 plan 不是一个甘特图, 而是 目标的理解, 当前的状态, 所有 4 个维度与产品和服务的的改进方向         create a shared understanding of the vision, current status, and improvement direction for all four dimensions and all products and services across the organization.      比如: 订机票服务的 vision, current status, improvement direction          output         设计和转换的 组合的描述             portfolio decisions for design and transition        Improve:                             The ‘improve’ value chain activity ensures continual improvement of products, services, and practices across all value chain activities and the four dimensions of service management.          improve 确保 产品, 服务, 实践 的 持续改进          The ‘improve’ activity in the service level management practice uses feedback from users about the service and requirements from customers to make recommendations to improve the service.          服务水平管理实践中的 “改进 “活动利用用户对服务的反馈和客户的要求来提出改进服务的建议。     白话: 客户提供反馈, 我们 improve SLA     比如: 通过用户反馈提高订机票的 SLA          会把 incident record 作为 improve 的 input ,    Engage:       Engage focuses on problems that have a significant impact on services will be visible to customers and users. In some cases, customers may wish to be involved in problem prioritization, and the status and plans for managing problems should be communicated. Workarounds are often presented to users via a service portal. The purpose of engage value chain activity is to provide continual engagement with all stakeholders.          engage 侧重于对服务有重大影响的问题，这些问题需要对客户和用户可见。     在某些情况下，客户可能希望参与问题的优先级排序，管理问题的状态和计划     engage 的目的是提供与所有利益相关者的持续接触     比如: 订票服务没有订到需要的票, 影响客户出行, 客服经理亲自联系客户进行安抚     案例:             有一个 workaround, 把它放到 web portal 上公示给用户的行为, 这就算是一种 engage.                      Design &amp; transition:  The ‘design and transition’ value chain activity ensures that products and services continually meet stakeholder expectations for quality, costs, and time to market.      设计和转化, 可以理解为 确保产品和服务可以持续的满足 stakeholder 对质量, 成本和时间的期望    Obtain/build:        确保服务的内容无论在何时何地需要, 都可以满足需要     ensures that service components are available when and where they are needed and meet agreed specifications                  满足服务请求可能需要获取预先批准的服务组件。     比如, 客户需要一些预先批准的内容, 免得和客户交流的时候还要等待审批(比如给客户订机票)              The fulfillment of service requests may require acquisition of pre-approved service components. ”    Obtain/build will manage the solution to the problem once it has been identified by problem management. They will be responsible for the release and deployment of the latest version of the application or service.      obtain/build 负责更新 patch, 发布和部署最新版本的应用或者服务,    比如:     需要换一个更大的路由器, 你的团队开始对进行配置准备工作.    要更新 最新版本的浏览器来避免应用的安全事故.     Delivery and support:        Deliver and support ensures users continue to be productive when they need assistance from the service provider.      交付和支持可确保用户在需要服务提供商的帮助时继续保持高效。    Product and Services:       4. 6 Continual Improvement:  Continual Improvement 是 34 个 Practice 中的一个, 但是因为他很重要, 所以被单独拿出来说 说白了就是不断改进产品和服务, 降本增效 There are many methods that can be used for improvement initiatives and too many should not be used. It is a good idea to select a few key methods that are appropriate to the types of improvements the organization typically handles and to cultivate those methods.      不需要都用, 可以先选几个方法(a few key methods)    7 个循环流程                           What is the vision?       未来景象是啥                        每个改进行动都要支持组织的目标     要确保             high-level direction of the initiative has been understood                                           Where are we now?       现在在哪                        ‘Where are we now’ is focused on determining the current state of the organization, including mapping out existing processes, conducting objective measurement through metrics, and available resources.              这个阶段会设定一个目标 objective baseline measurement                例子:             招聘了一个新的流程改进专员, 你给她分享了一些材料, 她开始分析这些材料                                           Where do we want to be?       想要去哪                        这个步骤专注于:             defining the goal for the organization based on the vision       what can be measured and quantified (such as Critical Success Factors and Key Performance Indicators).                  这里定的是 成功因素 和 KPI, 并不是 objective baseline measurement                      例子:                 数据中心每年的能耗费用是 1. 2 M, CEO 说要减少能耗. IT 总监创建了一个目标(goal), 明年数据中心的能耗费用要少于 1M. 这时还没有制定计划, 所以是 where do we want to be 阶段                                                          How do we get there?       如何到那                        fucus             focused on outlining the plan of action to be undertaken to accomplish the goals set forth in the ‘where do we want to be’ step of the model.                 例子:             公司设定了减少 10% 物理机的目标, 我们正在做这个计划, 迁移 50%的机器到虚机上, 这个阶段就是how do we get there                                                  Take action       做些事情                        The ‘take action’ step of the continual improvement model is focused on performing the actual work involved in order to reach the goals set forth in the ‘where do we want to be’ step. To do this, change management is used to implement a change in the environment and release management is used to make new and changed features available for use.      包含的一些 practice             change management 变化管理       release management 发布管理                例子:             公司要减少数据中心 50% 的物理机, 你目前正在做把物理机往虚拟机上迁移, 并去掉数据中心的物理机, 目前这个阶段就是 take action.                                            Did we get there?       到了吗                        The ‘did we get there’ step is focused on checking the new state of the improvement initiative and comparing it to the original baseline to determine if the desired goal has been reached.      例子             公司过去 12 个月设定了一个提高用户满意度的改进, 目标是提高 4%, 你在查看现在的满意度状态, 并对比过去 12 个月的满意度.                                            How do we keep the momentum going?       如何继续                        The ‘how do we keep the momentum going’ step of the continual improvement model is used once the improvement has delivered the expected value and the focus is now shifting to increased improvement or to maintain the gains made by the improvement initiative.      例子             你通过两步验证提高了密码安全度, 并超过了预期的用户接受度, 接下来你希望让所有用户都使用这个服务.                 这个步骤的下一个步骤就循环回到 what is the vision           5 34 个 priactices:  为了达到目的, 组织进行的一些实践方法 分 3 类     General Management (14个)   Service Management (17个)   Technical Management (3个)   通用管理 5. 1. 1 架构管理 | Architecture management                  5. 1. 2 持续改进     Continual improvement                         5. 1. 3 信息安全管理     Information security management                         5. 1. 4 知识管理     Knowledge management                         5. 1. 5 度量与报告     Measurement and reporting                         5. 1. 5. 1 KPI 和行为                         5. 1. 6 组织变革管理     Organizational change management                         5. 1. 7 组合管理     Portfolio management                         5. 1. 8 项目管理     Project management                         5. 1. 9 关系管理     Relationship management                         5. 1. 10 风險管理     Risk management                         5. 1. 11 服务财务管理     Service financial management                         5. 1. 12 战路管理     Strategy management                         5. 1. 13 供应商管理     Supplier management              采购, 供应商战略关系   评估和选择供应商   活动   服务整合    5. 1. 14 人力与人才管理 | Workforce and talent management  服务管理                5. 2. 1 可用性管理     Availability management                         5. 2. 2 业务分析     Business analysis                         5. 2. 3 容量和性能管理     Capacity and performance management                         5. 2. 4 变更实施     Change control              change 按照紧急程度分为                                       standard change         低风险, 已经被理解并记入文档的, 不需要额外的授权                                                             normal change         需要 scheduled, assessed, and authorized following a standard process                                  比如: 一个正常运转的系统, 需要发布一些新的改进功能, 这种变化就是 normal .                                               emergency change         需要马上报告, 立即处理                                    change authority         变更委员会, 负责审批变更                         5. 2. 5 事件管理     Incident management              通过对 incident 进行分类, 可以让对业务影响大的事件优先被处理   所有的 incident 都需要被 loged 和 managed                  5. 2. 6 1姿产答理     IT asset management                         5. 2. 7 监视和事态管理     Monitoring and event management                         5. 2. 8 问题管理     Problem management              问题管理的按个阶段  - problem identification,   - problem control, and   - error control.    无法立刻解决的问题, 可以用 workaround (临时方案)                      Workaround 的方案通常 通过 web portal 展示                                    5. 2. 9 发布管理     Release management                         5. 2. 10 服务目录管理     Service catalogue management                         5. 2. 11 服务配置管理     Service configuration management                         5. 2. 12 服务连续性管理     Service continuity management                         5. 2. 13 服务设计     Service design              设计思维   客户和用户体验                  5. 2. 14 服务台     Service desk              The Service desk is the entry point for all contact between the service provider and its users                         服务台是服务提供者和用户的联系接口       手机坏了, 找服务台                     A good service desk should have a practical understanding of the wider organization, the business processes, and the users.    A centralized service desk includes a team of employees working in a single location.    A centralized service desk require         Knowledge bask     workflow systems for routing and escalation     workforce management and resource planning systems     intelligent telephony systems     automatic call distribution     remote access tools                         5. 2. 15 服务级别管理     Service level management              Service level agreements are used to measure the performance of services from a customer’s point of view.    They may measure availability and capability, but only from the customer’s point of view.    SLA 对于价值链的 “plan” 帮助是: 提供当前服务performance 和trend 的信息, 最终可以计划更好的产品和服务的组合   每个 SLA 都要清晰的定义 service outcomes                  5. 2. 16 服务请求管理     Service request management              给服务台用的管理方式         需要标准化和自动化          服务请求管理给 SVC 的价值是:         analysis of data to identify opportunities to provide new service request options.            5. 2. 17 服务验证和測试 | Service validation and testing  技术管理                5. 3. 1 部署管理     Deployment management                         5. 3. 2 基础架构和平台管理     Intrastructure and plattorm management                         5. 3. 3 软件开发与管理     Software development and management          案例分析 (把 SVC 串起来):  亚马逊物流仓库的 WIFI覆盖方案不咋地, 不能让员工及时接收到货物搬运的任务信息, 降低了工作效率     这是第一步 - demand    实践向上报告给 物流队长, 队长进行分析     engage    队长报告给 help desk, help desk 报告给了 wifi management team     Delivery &amp;* Support   涉及到了多个 practice         IT Asset management     Service configuration management     change control     incident management           wifi management team 计划 1 小时候进行修复     plan    wifi management team 修复了问题     products &amp; services    仓库搬运工提高了工作效率     Value    队长和班运工收到服务 servey 邮件, 提供满意度调查和改进意见     Improve   Glossary: A: acceptance criteria  A list of minimum requirements that a service or service component must meet for it to be acceptable to key stakeholders. Agile  An umbrella term for a collection of frameworks and techniques that together enable teams and individuals to work in a way that is typified by collaboration, prioritization, iterative and incremental delivery, and timeboxing. There are several specific methods (or frameworks) that are classed as Agile, such as Scrum, Lean, and Kanban. architecture management practice  The practice of providing an understanding of all the different elements that make up an organization and how those elements relate to one another. asset register  A database or list of assets, capturing key attributes such as ownership and financial value. availability  The ability of an IT service or other configuration item to perform its agreed function when required. availability management practice  The practice of ensuring that services deliver agreed levels of availability to meet the needs of customers and users. B: baseline  A report or metric that serves as a starting point against which progress or change can be assessed. best practice  A way of working that has been proven to be successful by multiple organizations. big data  The use of very large volumes of structured and unstructured data from a variety of sources to gain new insights.  business analysis practiceThe practice of analysing a business or some element of a business, defining its needs and recommending solutions to address these needs and/or solve a business problem, and create value for stakeholders. business case  A justification for expenditure of organizational resources, providing information about costs, benefits, options, risks, and issues. business impact analysis (BIA)  A key activity in the practice of service continuity management that identifies vital business functions and their dependencies. business relationship manager (BRM)  A role responsible for maintaining good relationships with one or more customers. C: call  An interaction (e. g. a telephone call) with the service desk. A call could result in an incident or a service request being logged. call/contact centre  An organization or business unit that handles large numbers of incoming and outgoing calls and other interactions. capability  The ability of an organization, person, process, application, configuration item, or IT service to carry out an activity. capacity and performance management practice  The practice of ensuring that services achieve agreed and expected performance levels, satisfying current and future demand in a cost-effective way. capacity planning  The activity of creating a plan that manages resources to meet demand for services. change  The addition, modification, or removal of anything that could have a direct or indirect effect on services. change authority  A person or group responsible for authorizing a change. standard change, 一般提前预授权了, 直接就可以变了 | 改机票normal chagne, 需要正常的审批流程 | 换岗emergency chage, 快速审批流程 | DDOS 攻击, 需要紧急审批来在防火墙中 block 一个 IP 地址the change might be pre-authorized for all future changes of the same type. For an emergency change, this might be the IT director. Regardless of their named position, when they are authorizing a change based on the organization’s defined level of authority, they are the change authority. change authority is assigned to each type of change and change models change control practice 也叫 change enablement  The practice of ensuring that risks are properly assessed, authorizing changes to proceed and managing a change schedule in order to maximize the number of successful service and product changes. change model  A repeatable approach to the management of a particular type of change. change schedule  A calendar that shows planned and historical changes. A change schedule is used to help plan(计划) changes, assist(辅助) in communication, avoid(避免) conflicts, and assign resources. By publishing the change schedule, everyone in the organization can know when a change is occurring, what people or components will be affected by the change, and when downtime or outages may occur. Change schedules are not used to develop features for a service. charging  The activity that assigns a price for services. cloud computing  A model for enabling on-demand network access to a shared pool of configurable computing resources that can be rapidly provided with minimal management effort or provider interaction. compliance  The act of ensuring that a standard or set of guidelines is followed, or that proper, consistent accounting or other practices are being employed. confidentiality  A security objective that ensures information is not made available or disclosed to unauthorized entities. configuration  An arrangement of configuration items (CIs) or other resources that work together to deliver a product or service. Can also be used to describe the parameter settings for one or more CIs. 比如一个球队/订票服务的配置, 一堆配置项和资源的组合, 为了拿到总冠军戒指/提供给客户更好的订票服务 configuration item (CI)  Any component that needs to be managed in order to deliver an IT service. 为了提供 IT 服务而需要管理的任何组件。比如, 给库里的配置项(力量训练分析/睡眠监测), 为了拿到总冠军而使用的 IT 服务 (要有科技感, 要和 IT 服务相关)比如, 给订票服务的配置项, 耳机, 好的电脑, 知识库. . configuration management database (CMDB)  A database used to store configuration records throughout their lifecycle. The CMDB also maintains the relationships between configuration records. configuration management system (CMS)  A set of tools, data, and information that is used to support service configuration management. configuration record  A record containing the details of a configuration item (CI). Each configuration record documents the lifecycle of a single CI. Configuration records are stored in a configuration management database. continual improvement practice  The practice of aligning an organization’s practices and services with changing business needs through the ongoing identification and improvement of all elements involved in the effective management of products and services. 通过不断识别和改进有效管理产品和服务所涉及的所有要素，使组织的实践和服务与不断变化的业务需求保持一致的做法。 continuous deployment  An integrated set of practices and tools used to deploy software changes into the production environment. These software changes have already passed pre-defined automated tests. continuous integration/continuous delivery  An integrated set of practices and tools used to merge developers’ code, build and test the resulting software, and package it so that it is ready for deployment. control  The means of managing a risk, ensuring that a business objective is achieved, or that a process is followed. cost  The amount of money spent on a specific activity or resource. 成本可以同时从服务消费者身上去除，并强加给服务提供者。 例如，将一项服务外包给服务提供商可能会使消费者不再需要拥有自己的IT基础设施，但这可能需要他们安装更快的互联网连接，以到达服务提供商的服务器。说白了, 一个服务外包了, 并不代表你这个成本就没了, 而是转移了 cost centre  A business unit or project to which costs are assigned. critical success factor (CSF)  A necessary precondition for the achievement of intended results. culture  A set of values that is shared by a group of people, including expectations about how people should behave, ideas, beliefs, and practices. customer  A person who defines the requirements for a service and takes responsibility for the outcomes of service consumption. 一个人, 定义了服务的需求, 并担任了服务消费产出的责任user - A person who uses services. customer experience (CX)  The sum of functional and emotional interactions with a service and service provider as perceived by a service consumer. D: dashboard  A real-time graphical representation of data. deliver and support  The value chain activity that ensures services are delivered and supported according to agreed specifications and stakeholders’ expectations. demand  Input to the service value system based on opportunities and needs from internal and external stakeholders. deployment  The movement of any service component into any environment. deployment management practice  The practice of moving new or changed hardware, software, documentation, processes, or any other service component to live environments. 把硬件,软件,文档, 流程 等服务内容搬到 live 环境 design and transition  The value chain activity that ensures products and services continually meet stakeholder expectations for quality, costs, and time to market. design thinking  A practical and human-centred approach used by product and service designers to solve complex problems and find practical and creative solutions that meet the needs of an organization and its customers. development environment  An environment used to create or modify IT services or applications. DevOps  An organizational culture that aims to improve the flow of value to customers. DevOps focuses on culture, automation, Lean, measurement, and sharing (CALMS). digital transformation  The evolution of traditional business models to meet the needs of highly empowered customers, with technology playing an enabling role. disaster  A sudden unplanned event that causes great damage or serious loss to an organization. A disaster results in an organization failing to provide critical business functions for some predetermined minimum period of time. disaster recovery plans  A set of clearly defined plans related to how an organization will recover from a disaster as well as return to a pre-disaster condition, considering the four dimensions of service management. driver  Something that influences strategy, objectives, or requirements. E: effectiveness  A measure of whether the objectives of a practice, service or activity have been achieved. efficiency  A measure of whether the right amount of resources have been used by a practice, service, or activity. emergency change  A change that must be introduced as soon as possible. 例子:    DDOS 攻击, 需要紧急审批来在防火墙上 block 一个 IP 地址 engage  The value chain activity that provides a good understanding of stakeholder needs, transparency, continual engagement, and good relationships with all stakeholders. environment  A subset of the IT infrastructure that is used for a particular purpose, for example a live environment or test environment. Can also mean the external conditions that influence or affect something. error  A flaw or vulnerability that may cause incidents. error control  Problem management activities used to manage known errors. escalation  The act of sharing awareness or transferring ownership of an issue or work item. Escalation occurs when an incident is more complicated and needs a higher level of analysis and support to resolve. event  Any change of state that has significance for the management of a service or other configuration item. 对服务或其他配置项目的管理具有重要意义的任何状态更改。 external customer  A customer who works for an organization other than the service provider. failure  A loss of ability to operate to specification, or to deliver the required output or outcome. four dimensions of service management  The four perspectives that are critical to the effective and efficient facilitation of value for customers and other stakeholders in the form of products and services. goods  Tangible resources that are transferred or available for transfer from a service provider to a service consumer, together with ownership and associated rights and responsibilities. governance  The means by which an organization is directed and controlled. I: identity  A unique name that is used to identify and grant system access rights to a user, person, or role. improve  The value chain activity that ensures continual improvement of products, services, and practices across all value chain activities and the four dimensions of service management. incident  An unplanned interruption to a service or reduction in the quality of a service. 一个计划外的 服务中断, 或者 服务质量的降低 比如: 你想通过网络打印机打印文档, 发现失败了. 你的电脑突然连不上公司的wifi 了 incident management  The practice of minimizing the negative impact of incidents by restoring normal service operation as quickly as possible. 通过尽快恢复正常的服务操作来最大程度地减少事件的负面影响的做法。incident management 会经常包含    获取事故信息的日志/脚本, 来帮助后面的分析  保持 performance levels  监控 services 识别 any change  使用 specialized knowledge  用来高效的分析和诊断事故 的 技术但是不会包含具体的过程,比如诊断, 分析, 解决事故.  incident management system  can provide automated matching of incidents to other incidents, problems, or known errors. information and technology  One of the four dimensions of service management. It includes the information and knowledge used to deliver services, and the information and technologies used to manage all aspects of the service value system. information security management practice  The practice of protecting an organization by understanding and managing risks to the confidentiality, integrity, and availability of information. information security policy  The policy that governs an organization’s approach to information security management. infrastructure and platform management practice  The practice of overseeing the infrastructure and platforms used by an organization. This enables the monitoring of technology solutions available, including solutions from third parties. integrity  A security objective that ensures information is only modified by authorized personnel and activities. feedback loop  A technique whereby the outputs of one part of a system are used as inputs to the same part of the system. internal customer  A customer who works for the same organization as the service provider. Internet of Things  The interconnection of devices via the internet that were not traditionally thought of as IT assets, but now include embedded computing capability and network connectivity. IT asset  Any financially valuable component that can contribute to the delivery of an IT product or service. IT asset management practice  The practice of planning and managing the full lifecycle of all IT assets. IT 资产管理, 就是 IT 资产的生命周期管理 IT infrastructure  All of the hardware, software, networks, and facilities that are required to develop, test, deliver, monitor, manage, and support IT services. IT service  A service based on the use of information technology. ITIL  Best-practice guidance for IT service management. ITIL guiding principles  Recommendations that can guide an organization in all circumstances, regardless of changes in its goals, strategies, type of work, or management structure. ITIL service value chain  An operating model for service providers that covers all the key activities required to effectively manage products and services. K: Kanban  A method for visualizing work, identifying potential blockages and resource conflicts, and managing work in progress. key performance indicator (KPI)  An important metric used to evaluate the success in meeting an objective. knowledge management practice  The practice of maintaining and improving the effective, efficient, and convenient use of information and knowledge across an organization. known error  A problem that has been analysed but has not been resolved. 比如:    发现 4 楼的员工都上不了网, 你分析是应为路由器坏了, 但是查了库存发现没有备货, 你估计需要 24 小时候订货到了才能修复, 这时一个已知错误, 但是还没有修复.   笔记本硬盘满了, 员工在外地没办法进行在线备份, help desk 建议员工外面买个移动硬盘把大文件copy 上去. 这个问题是 已知错误, 不是 workaround. 因为 随便买个磁盘不符合大部分公司安全规定, 不能那么干. 那么就是一个已知的, 还没办法解决的问题  比如: 网络打印机墨没了, 但是暂时还换不了, 要等到2 周后到货了才能换 Lean  An approach that focuses on improving workflows by maximizing value through the elimination of waste. lifecycle  The full set of stages, transitions, and associated statuses in the life of a service, product, practice, or other entity. live  Refers to a service or other configuration item operating in the live environment. live environment  A controlled environment used in the delivery of IT services to service consumers. M: maintainability  The ease with which a service or other entity can be repaired or modified. major incident  An incident with significant business impact, requiring an immediate coordinated resolution. management system  Interrelated or interacting elements that establish policy and objectives and enable the achievement of those objectives. maturity  A measure of the reliability, efficiency and effectiveness of an organization, practice, or process. mean time between failures (MTBF)  A metric of how frequently a service or other configuration item fails. mean time to restore service (MTRS)  A metric of how quickly a service is restored after a failure. measurement and reporting  The practice of supporting good decision-making and continual improvement by decreasing levels of uncertainty. metric  A measurement or calculation that is monitored or reported for management and improvement. minimum viable product (MVP)  A product with just enough features to satisfy early customers, and to provide feedback for future product development. mission statement  A short but complete description of the overall purpose and intentions of an organization. It states what is to be achieved, but not how this should be done. model  A representation of a system, practice, process, service, or other entity that is used to understand and predict its behaviour and relationships. modelling  The activity of creating, maintaining, and utilizing models. monitoring  Repeated observation of a system, practice, process, service, or other entity to detect events and to ensure that the current status is known. monitoring and event management practice  The practice of systematically observing services and service components, and recording and reporting selected changes of state identified as events. N: Normal changes  are changes which need to be scheduled, assessed, and authorized following a standard process. These changes are not considered routine (like a standard change). They are also not considered urgent and don’t need to be implemented as soon as possible to recover from an incident (like an emergency change). 例子:    你在配置一个邮件服务, 需要获得权限来安装一个服务器到公司网络中进行测试.  O: obtain/build  The value chain activity that ensures service components are available when and where they are needed, and that they meet agreed specifications. operation  The routine running and management of an activity, product, service, or other configuration item. operational technology  The hardware and software solutions that detect or cause changes in physical processes through direct monitoring and/or control of physical devices such as valves, pumps, etc. organization  A person or a group of people that has its own functions with responsibilities, authorities, and relationships to achieve its objectives. organizational change management practice  The practice of ensuring that changes in an organization are smoothly and successfully implemented and that lasting benefits are achieved by managing the human aspects of the changes. organizational resilience  The ability of an organization to anticipate, prepare for, respond to, and adapt to unplanned external influences. organizational velocity  The speed, effectiveness, and efficiency with which an organization operates. Organizational velocity influences time to market, quality, safety, costs, and risks. organizations and people  One of the four dimensions of service management. It ensures that the way an organization is structured and managed, as well as its roles, responsibilities, and systems of authority and communication, is well defined and supports its overall strategy and operating model. outcome  A result for a stakeholder enabled by one or more outputs. 由一个或多个 outputs 促成的利益相关者的结果多个 output 组成 outcome output  A tangible or intangible deliverable of an activity. 一个活动的 有形的或者无形的 一个交付物比如: 一个新的财务软件, 生成的 一个 P&amp;L 报告 outsourcing  The process of having external suppliers provide products and services that were previously provided internally. P: partners and suppliers  One of the four dimensions of service management. It encompasses the relationships an organization has with other organizations that are involved in the design, development, deployment, delivery, support, and/or continual improvement of services. partnership  A relationship between two organizations that involves working closely together to achieve common goals and objectives. performance  A measure of what is achieved or delivered by a system, person, team, practice, or service. pilot  A test implementation of a service with a limited scope in a live environment. plan  The value chain activity that ensures a shared understanding of the vision, current status, and improvement direction for all four dimensions and all products and services across an organization. policy  Formally documented management expectations and intentions, used to direct decisions and activities. portfolio management practice  The practice of ensuring that an organization has the right mix of programmes, projects, products, and services to execute its strategy within its funding and resource constraints. post-implementation review (PIR)  A review after the implementation of a change, to evaluate success and identify opportunities for improvement. practice  A set of organizational resources designed for performing work or accomplishing an objective. problem  A cause, or potential cause, of one or more incidents. 一个事故的 原因, 或者潜在原因比如:    你作为 help desk , 连续接到多个用户的无法上网的报告, 你分析是因为域控制器挂了, 这就是潜在的可能原因, 是 problem  用户尝试用用户名和密码登录, 收到报错 “认证服务器没有响应”. 因为已经知道了可能得原因, 所以是 problem problem identification 的例子:  Detecting duplicate and recurring issues 研究重复发生的问题  Performing trend analysis of incident records 分析事故发生的趋势  Analysis of multiple incidents that may be linked together 分析多个事故之间可能有关联Logging an incident 不属于 problem identification 也不是 problem management 的步骤 problem management practice  The practice of reducing the likelihood and impact of incidents by identifying actual and potential causes of incidents, and managing workarounds and known errors. 通过识别事故的实际和潜在原因，以及管理变通方法和已知错误，减少事故的可能性和影响的做法。 procedure  A documented way to carry out an activity or a process. process  A set of interrelated or interacting activities that transform inputs into outputs. A process takes one or more defined inputs and turns them into defined outputs. Processes define the sequence of actions and their dependencies. product  A configuration of an organization’s resources designed to offer value for a consumer. production environment  See live environment. programme  A set of related projects and activities, and an organization structure created to direct and oversee them. project  A temporary structure that is created for the purpose of delivering one or more outputs (or products) according to an agreed business case. project management practice  The practice of ensuring that all an organization’s projects are successfully delivered. quick win  An improvement that is expected to provide a return on investment in a short period of time with relatively small cost and effort. R: record  A document stating results achieved and providing evidence of activities performed. recovery  The activity of returning a configuration item to normal operation after a failure. recovery point objective (RPO)  The point to which information used by an activity must be restored to enable the activity to operate on resumption. recovery time objective (RTO)  The maximum acceptable period of time following a service disruption that can elapse before the lack of business functionality severely impacts the organization. relationship management practice  The practice of establishing and nurturing links between an organization and its stakeholders at strategic and tactical levels. release  A version of a service or other configuration item, or a collection of configuration items, that is made available for use. release management practice  The practice of making new and changed services and features available for use. reliability  The ability of a product, service, or other configuration item to perform its intended function for a specified period of time or number of cycles. request catalogue  A view of the service catalogue, providing details on service requests for existing and new services, which is made available for the user. request for change (RFC)  A description of a proposed change used to initiate change control. resolution  The action of solving an incident or problem. resource  A person, or other entity, that is required for the execution of an activity or the achievement of an objective. Resources used by an organization may be owned by the organization or used according to an agreement with the resource owner. retire  The act of permanently withdrawing a product, service, or other configuration item from use. risk  A possible event that could cause harm or loss, or make it more difficult to achieve objectives. Can also be defined as uncertainty of outcome, and can be used in the context of measuring the probability of positive outcomes as well as negative outcomes. risk assessment  An activity to identify, analyse, and evaluate risks. risk management practice  The practice of ensuring that an organization understands and effectively handles risks. S: service  A means of enabling value co-creation by facilitating outcomes that customers want to achieve, without the customer having to manage specific costs and risks. service action  Any action required to deliver a service output to a user. Service actions may be performed by a service provider resource, by service users, or jointly. service architecture  A view of all the services provided by an organization. It includes interactions between the services, and service models that describe the structure and dynamics of each service. service catalogue  Structured information about all the services and service offerings of a service provider, relevant for a specific target audience. service catalogue management practice  The practice of providing a single source of consistent information on all services and service offerings, and ensuring that it is available to the relevant audience. service configuration management practice  The practice of ensuring that accurate and reliable information about the configuration of services, and the configuration items that support them, is available when and where needed. 确保 随时随地 准确可靠的提供 服务配置 和 配置项 service consumption  Activities performed by an organization to consume services. It includes the management of the consumer’s resources needed to use the service, service actions performed by users, and the receiving (acquiring) of goods (if required). 组织为了消费服务而进行的活动 (主语是组织) , 包括 1. 管理消费者使用服务所需的资源、2. 相关的行动，3. 接收的（获取）商品。    应该理解成 consumption service , 为了消费而提供的服务比如:  公司把培训视频放在第三方的视频托管平台, 用户可以通过在公司的网站请求直接从第三方平台看视频流, 公司可以管理第三方公司的存储空间, 并决定/决定用户会看什么视频.   公司把培训视频应用外包给第三方, 只有付费用户通过支付订阅费才能使用应用, 公司把订阅费的部分给第三方公司作为开发费用.  service continuity management practice  The practice of ensuring that service availability and performance are maintained at a sufficient level in case of a disaster. service design practice  The practice of designing products and services that are fit for purpose, fit for use, and that can be delivered by the organization and its ecosystem. service desk  The point of communication between the service provider and all its users. The service desk may utilize a                      local,      比如全球有 5 个 办公室, 每个办公室配一个服务台                                  follow-the-sun      多个 service desk, 每个 service desk 接力, 保证 7x24 小时的服务                                  centralized, or      大家都在一个办公室                virtualized model.  | 多个 service deck 虚拟办公或者 work from homeIt just depends on your organization, business needs, and user requirements. is the main channel for communication and collaboration with usersA centralized service desk requires supporting technologies like                    workflow systems for routing and escalation,      方便上报                                  workforce management and resource planning systems,      管人和资源                                  a centralized knowledge base,      知识库                                  intelligent telephony systems,      智能电话系统                                  automatic call distribution, and      自动电话分发系统                                  remote access tools.       远程访问                   But, if you have a centralized service desk, you will need good remote access tools to be able to support users that are not located at your same location. 不一定非要包含的是:       Automation is wonderful and can provide efficiencies, but it is not required. Also,  24x7 support may not be a business requirement for some organizations. A virtual service desk allows agents to work from multiple locations which are geographically dispersed. Often, agents may also be working from home under this model. 好的服务台需要了解 wider organization, the business processes, the users and their needs.  service desk practice  The practice of capturing demand for incident resolution and service requests. service financial management practice  The practice of supporting an organization’s strategies and plans for service management by ensuring that the organization’s financial resources and investments are being used effectively. service level  One or more metrics that define expected or achieved service quality. service level management data service level agreement (SLA)  A documented agreement between a service provider and a customer that identifies both services required and the expected level of service. They should be written in an easy-to-understand language, avoid complex legal terminology, and agreed by both parties. Clear defined service outcome shoud be included in every service level agreement案例: 供应商说他们的 SLA 全是绿的, 你的经理说经常听客户说服务不能用, 可能得原因是: 供应商的数据不是基于 business outcome . 解释, 这说明之前定义的 SLA 可能是技术层面的, 但不是以 business outcome (用户价值为导向)的 metrics, 所以供应商说 应用和数据库的 avaibility 都是绿的, 但是客户却因为其他原因无法访问服务. 用来分析 SLA 的数据源可以包括    customer feedback  operational metrics  business metrics  customer engagement  business measures  不需要包括竞争对手信息 service level management practice  The practice of setting clear business-based targets for service performance so that the delivery of a service can be properly assessed, monitored, and managed against these targets. 服务级别管理, 需要设定清晰的 target , 可以被 assessd, monitored, managed Service level management 使用 measure 和 metrics 是为了 provide representation of the actual customer’s experience service management  A set of specialized organizational capabilities for enabling value for customers in the form of services. 一套专门的组织的 能力,用服务的形式 为客户提供价值 服务管理 理解成 能力的服务包 service offering  A formal description of one or more services, designed to address the needs of a target consumer group. A service offering may include goods, access to resources, and service actions. service owner  A role that is accountable for the delivery of a specific service. service portfolio  A complete set of products and services that are managed throughout their lifecycles by an organization. service provider  A role performed by an organization in a service relationship to provide services to consumers. service provision  Activities performed by an organization to provide services. It includes management of the provider’s resources, configured to deliver the service; ensuring access to these resources for users; fulfilment of the agreed service actions; service level management; and continual improvement. It may also include the supply of goods. 组织为提供服务而执行的活动。它包括管理提供商的资源，配置为提供服务;确保用户访问这些资源;履行约定的服务行动;服务水平管理;并持续改进。它还可能包括货物的供应。比如: 公司为新入职的 3 个员工每人提供 10G 的文件存储空间 service relationship  A cooperation between a service provider and service consumer. Service relationships include service provision, service consumption, and service relationship management. service relationship management  Joint activities performed by a service provider and a service consumer to ensure continual value co-creation based on agreed and available service offerings. 例子:    公司提供培训认证课程, 学生要通过认证考试, 公司和学生共创价值, 帮学生通过考试的活动叫做 ~关系管理与服务关系管理的区别: 关系管理是 组织和 stakeholder 的关系 ; 服务关系管理是 服务提供者 和 服务消费者 的 关系 service request  A request from a user or a user’s authorized representative that initiates a service action which has been agreed as a normal part of service delivery. Service requests and their fulfillment should be standardized and automated to the greatest degree possible. By using automation, we can decrease the consumer’s wait time for a resolution, and ensure that they get back to productive work as soon as possible. service request management practice  The practice of supporting the agreed quality of a service by handling all pre-defined, user-initiated service requests in an effective and user-friendly manner. 以 有效, 用户友好的方式 提供 预定义的, 用户发起的 同意质量的 支持服务 service validation and testing practice  The practice of ensuring that new or changed products and services meet defined requirements. service value system (SVS)  A model representing how all the components and activities of an organization work together to facilitate value creation. software development and management practice  The practice of ensuring that applications meet stakeholder needs in terms of functionality, reliability, maintainability, compliance, and auditability. sourcing  The activity of planning and obtaining resources from a particular source type, which could be internal or external, centralized or distributed, and open or proprietary. specification  A documented description of the properties of a product, service, or other configuration item. sponsor  A person who authorizes budget for service consumption. Can also be used to describe an organization or individual that provides financial or other support for an initiative. stakeholder  A person or organization that has an interest or involvement in an organization, product, service, practice, or other entity. standard  A document, established by consensus and approved by a recognized body, that provides for common and repeated use, mandatory requirements, guidelines, or characteristics for its subject. standard change  A low-risk, pre-authorized change that is well understood and fully documented, and which can be implemented without needing additional authorization. 比如: 创建新的用户账号 status  A description of the specific states an entity can have at a given time. strategy management practice  The practice of formulating the goals of an organization and adopting the courses of action and allocation of resources necessary for achieving those goals. supplier  A stakeholder responsible for providing services that are used by an organization. supplier management practice  The practice of ensuring that an organization’s suppliers and their performance levels are managed appropriately to support the provision of seamless quality products and services. support team  A team with the responsibility to maintain normal operations, address users’ requests, and resolve incidents and problems related to specified products, services, or other configuration items. system  A combination of interacting elements organized and maintained to achieve one or more stated purposes. systems thinking  A holistic approach to analysis that focuses on the way that a system’s constituent parts work, interrelate, and interact over time, and within the context of other systems. T: technical debt  The total rework backlog accumulated by choosing workarounds instead of system solutions that would take longer. test environment  A controlled environment established to test products, services, and other configuration items. third party  A stakeholder external to an organization. throughput  A measure of the amount of work performed by a product, service, or other system over a given period of time. transaction  A unit of work consisting of an exchange between two or more participants or systems. U: use case  A technique using realistic practical scenarios to define functional requirements and to design tests. user  A person who uses services. utility  The functionality offered by a product or service to meet a particular need. Utility can be summarized as ‘what the service does’ and can be used to determine whether a service is ‘fit for purpose’. To have utility, a service must either support the performance of the consumer or remove constraints from the consumer. Many services do both. utility requirements  Functional requirements which have been defined by the customer and are unique to a specific product. V: validation  Confirmation that the system, product, service, or other entity meets the agreed specification. value  The perceived benefits, usefulness, and importance of something. 任何有利益的, 有用的, 重要的东西 value stream  A series of steps an organization undertakes to create and deliver products and services to consumers. value streams and processes  One of the four dimensions of service management. It defines the activities, workflows, controls, and procedures needed to achieve the agreed objectives. vision  A defined aspiration of what an organization would like to become in the future. W: warranty  Assurance that a product or service will meet agreed requirements. Warranty can be summarized as ‘how the service performs’ and can be used to determine whether a service is ‘fit for use’. Warranty often relates to service levels aligned with the needs of service consumers. This may be based on a formal agreement, or it may be a marketing message or brand image. Warranty typically addresses such areas as the availability of the service, its capacity, levels of security, and continuity. A service may be said to provide acceptable assurance, or ‘warranty’, if all defined and agreed conditions are met. warranty requirements  Typically non-functional requirements captured as inputs from key stakeholders and other practices. waterfall method  A development approach that is linear and sequential with distinct objectives for each phase of development. work instruction  A detailed description to be followed in order to perform an activity. workaround  A solution that reduces or eliminates the impact of an incident or problem for which a full resolution is not yet available. Some workarounds reduce the likelihood of incidents. 例子:    公司墨盒没墨了, 新的墨盒好几天才能到, 同事把现在的墨盒拿出来晃了晃, 还能用.  workforce and talent management practice  The practice of ensuring that an organization has the right people with the appropriate skills and knowledge and in the correct roles to support its business objectives. "
    }, {
    "id": 11,
    "url": "http://localhost:4000/Microsoft-Azure-AI-Solution/",
    "title": "Designing and Implementing a Microsoft Azure AI Solution",
    "body": "2023/05/30 - 随着微软收购了 OpenAI, ChatGPT 的能力也融入到了 微软的 Azure 中. 我将带领大家了解 Microsoft Azure AI 不同服务, 包括 Cognitive Services, Computer Vision, Custom Vision, Video indexer, Natural Language Processing (NLP), Speech Service, Translate Language, OpenAI 等等. 还等什么, 让我们开始吧. AZ-102 Designing and Implementing a Microsoft Azure AI Solution[TOC] 资源: 官网考试介绍官网提供的练习题预约考试 + 考试券申请 + 模拟考题练习 目前只能申请 50% 的折扣 Wendy: 这个事儿global正在协调，会给我们发免费券。具体流程announce后大家就可以考了。2023年 5 月 2 日之后的考试的技能要求考试便利相关说明英语作为第二语言考试需要填写的表格, 填写后可以获取额外的考试时间 要先拿到这个 额外时间的 approval, 再去 schedule 考试时间github 上的 azure sample code repositoriescognitive-services-python-sdk-samplesAzure OpenAI 服务Azure cognitive Service API Azure Cognitive Service OverviewAzure Cognitive Service 登录后的服务认知服务定价Mac 上安装 Azure 命令行Mac 命令行创建 Cognitive Service微软的 AI principles 原则可用 PIP 安装的 Python 的 Azure libraryAzure SDK Release for Python 考试计划相关: 考试当天： 建议考生提前 30 分钟登录帐户开始检录流程，以便留出时间解决任何故障问题。这将让考生做好充分准备，但并不保证可提前入场或开始考试。如果考生迟到15分钟以上，将不得参加考试，且不大可能获得退款。 进入考试页面： 在考试确认、改期或提醒邮件中选择签到（Check-in）以开始考试。登录修改预约考试入口查看即将进行的考试预约，按照屏幕上的提示完成检录流程，考生须完成检录后方可开始考试。 考试改期规定 如果您需要改期考试预约，请确保至少在您预约的考试时间 24 个小时前改期。如果您没有在考试预约时间 24 个小时前改期，我们将不会退还您的考试费用和/或您所在的公司可能需要付费。 考试取消规定 如果您需要取消考试预约，请确保至少在您预约的考试时间 24 个小时前取消。如果您没有在考试预约时间 24 个小时前取消，我们将不会退还您的考试费用和/或您所在的公司可能需要付费。 考试大纲: 自 2023 年 5 月 2 日起测试的技能, 基本没啥变化58 道题, 120 分钟, 可以额外申请 30 分钟? 总共 1000 分, 700 分通过 , 70% 正确率 受众概况: Microsoft Azure AI 工程师构建、管理和部署充分利用 Azure 认知服务和 Azure 服务的 AI 解决方案。 他们的职责包括参与 AI 解决方案开发的所有阶段，从需求定义和设计到开发、部署、集成、维护、性能优化和监视。这些专业人员与解决方案架构师合作将其愿景转化为事实，并与数据科学家、数据工程师、IoT 专家、基础结构管理员和其他软件开发人员合作，以构建完整的端到端 AI 解决方案。Azure AI 工程师具有开发使用 Python 或 C# 等语言的解决方案的经验，并且应该能够使用基于 REST 的 API 和软件开发工具包 (SDK) 来构建安全的图像处理、视频处理、自然语言处理 (NLP)、知识挖掘和对话式 AI 解决方案。 他们应熟悉实现 AI 解决方案的所有方法。 此外，他们了解构成 Azure AI 产品组合的组件以及可用的数据存储选项。 Azure AI 工程师还需要理解并能够应用负责任 AI 原则。  计划和管理 Azure AI 解决方案 (25-30%) 实现图像和视频处理解决方案 (15-20%) 实现自然语言处理解决方案 (25-30%) 实现知识挖掘解决方案 (5-10%) 实现对话式 AI 解决方案 (15-20%)计划和管理 Azure AI 解决方案 (25-30%): 选择适当的 Azure AI 服务  为视觉解决方案选择适当的服务 为语言分析解决方案选择适当的服务 为决策支持解决方案选择适当的服务 为语音解决方案选择适当的服务选择适当的应用 AI 服务 为 Azure AI 服务计划和配置安全性 管理帐户密钥 管理资源的身份验证 使用 Azure 虚拟网络保护服务 计划符合负责任 AI 原则的解决方案创建和管理 Azure AI 服务 创建 Azure AI 资源 配置诊断日志记录 管理 Azure AI 服务的成本 监视 Azure AI 资源部署 Azure AI 服务 确定服务的默认终结点 使用 Azure 门户创建资源 将 Azure AI 服务集成到持续集成/持续部署 (CI/CD) 管道中 计划容器部署 在连接的环境中实现预生成容器创建解决方案以检测异常并改进内容 创建使用异常检测器（认知服务的一部分）的解决方案 创建使用 Azure 内容审查器（认知服务的一部分）的解决方案 创建使用个性化体验创建服务（认知服务的一部分）的解决方案 创建使用 Azure 指标顾问（Azure 应用 AI 服务的一部分）的解决方案 创建使用 Azure 沉浸式阅读器（Azure 应用 AI 服务的一部分）的解决方案实现图像和视频处理解决方案 (15-20%): 分析图像  选择适当的视觉特征以满足图像处理要求 创建图像处理请求以包含适当的图像分析功能 解释图像处理响应从图像中提取文本 使用计算机视觉服务从图像或 PDF 中提取文本 使用计算机视觉服务转换手写文本 使用 Azure 表单识别器中的预生成模型提取信息 为 Azure 表单识别器生成和优化自定义模型使用自定义视觉服务（Azure 认知服务的一部分）实现图像分类和物体检测 在图像分类和物体检测模型之间选择 指定模型配置选项，包括类别、版本和压缩 标记图像 训练自定义图像模型，包括分类器和检测器 管理训练迭代 评估模型指标 发布已训练的模型迭代 导出模型以在特定目标上运行 以 Docker 容器形式实现自定义视觉模型 解释模型响应处理视频 使用 Azure 视频索引器处理视频 使用 Azure 视频索引器从视频或实时流中提取见解 使用 Azure 视频索引器实现内容审核 将自定义语言模型集成到 Azure 视频索引器中实现自然语言处理解决方案 (25-30%): 分析文本  检索和处理关键短语 检索和处理实体 检索和处理情绪 检测文本中使用的语言 检测个人身份信息 (PII)处理语音 实现和自定义文本转语音 实现和自定义语音转文本 使用 SSML 和神经网络定制声音改进文本转语音 使用短语列表和自定义语音识别改进语音转文本 实现意向识别 实现关键字识别转换语言 使用转换器服务转换文本和文档 实现自定义转换，包括训练、改进和发布自定义模型 使用语音服务将语音转换为语音 使用语音服务将语音转换为文本 同时转换为多种语言生成和管理语言理解模型 创建意向并添加语句 创建实体 训练、评估、部署和测试语言理解模型 优化语言理解 (LUIS) 模型 使用 Orchestrator 集成多种语言服务模型 导入和导出语言理解模型创建问题解答解决方案 创建问题解答项目 手动添加问答对 导入源 训练和测试知识库 发布知识库 创建多回合对话 添加备用措辞 向知识库添加聊天内容 导入知识库 创建多语言问题解答解决方案 创建多域问题解答解决方案 对问答对使用元数据实现知识挖掘解决方案 (5-10%): 实现认知搜索解决方案  预配认知搜索资源 创建数据源 定义索引 创建并运行索引器 查询索引，包括语法、排序、筛选和通配符 管理知识存储投影，包括文件、对象和表投影将 AI 扩充技能应用于索引器管道 将认知服务帐户附加到技能组 选择并包含文档的内置技能 实现自定义技能并将其包含在技能组中 实现增量扩充实现对话式 AI 解决方案 (15-20%): 设计和实现对话流  设计机器人的对话逻辑 为机器人选择合适的活动处理程序、对话或主题、触发器和状态处理生成对话机器人 通过模板创建机器人 从头开始创建机器人 实现活动处理程序、对话或主题和触发器 实现特定于通道的逻辑 实现自适应卡片 在机器人中实现多语言支持 实现多步骤对话 管理机器人的状态 将认知服务集成到机器人中，包括问题解答、语言理解和语音服务测试、发布和维护对话机器人 使用 Bot Framework Emulator 或 Power Virtual Agents Web 应用测试机器人 在特定于通道的环境中测试机器人 排查对话机器人问题 部署机器人逻辑Plan and Manage: Azure Cognitive Service 的概念  预先训练好的 ML models 可以方便的把 AI functions 集成到我们自己的代码中 不需要 ML 的经验Cognitive API  包含了4大类     Vision Service         Image analysis             分析图片和视频中的物体                Spatial analysis             空间识别, 实时分析空间中的人和移动       比如商店 checkout 区域的排队人数分析                Optical character recognition (OCR)             识别图片中的文字                Facial recognition             探测面部, 识别人, 识别情感                基于我们自己的图片训练 ML model, 来使用我们自己的业务需要          Speech Service         speech to text             语音转文字                text to speech             文字转语音                speech translation             语音翻译                speaker recognition (preview)             语音识别是谁                     Language Service         Language Understanding(LUIS)             内容可以用于Bing, 给一个关键词, bing 通过 LUIS 来进行搜索                QnA Maker             和 chatbot 关联                Text Analytics     Translator (90种语言)             不但可以翻译, 还可以分析被翻译的内容的情感, 链接, PII 等信息                     Decision Service         Anomaly Detector (异常检测)     Content Moderator (内容审查器)             分析数据趋势, 画图, 预测波峰波谷                Personalizer (个性化)             是一个决策服务, 通过强化学习大规模做出更明智的决策       比如商品推荐, 电影推荐, 文章推荐, 广告推荐, 智能消息推送                     创建认知服务资源: 概念  Subscription     订阅, 所有在同一个订阅下的资源分享同一个bill (收费订单)   目前有的两个 subscription         Free Tier     Pay as you go           Resource Group     资源组, 同一个资源组下面的资源分享同样的生命周期, 权限和 policy   目前创建的 Resource Group         dl-cogsvs-group           Region     国家    Instance Name     资源的名字, 只能是字母和中划线   目前创建的 Cognitive Service 是         dl-cogsvs           Pricing Tier     定价   目前的几种定价         s0     s1           Key     用来访问认知服务 API 的, 不能分享给别人, 否则别人用了, 我需要付费.    &lt;your own key&gt;   默认有2个key   key 可以 regenerate 重建    Endpoint     登录点, 访问认知服务 API 的 URL 地址   https://dl-cogsvs. cognitiveservices. azure. com/         dl-cogsvs 是我们创建的认知服务资源名字     cognitiveservices. azure. com 是官方确认的域名     具体的认知服务会拼接到这个域名的后面          创建流程  进入认知服务 console 底部看到 Cognitive Service Multi-service Account 点进去, 创建一个认知服务账户 完成创建为认知服务创建 Alert: 概念  alert rule     就是一个 alert 的规则   一个 alert rule 可以产生多个 alert   这里的alert rule 的名字是         dl-alert-rule-checking-total-calls           Signal     信号, 就是需要监控的目标, 可以是         Total calls     Total errors     latency     Client errors     Server errors     …           Action Group     一系列的action 可以一起应用到一个 alert rule 中   当alert rule 被触发, 可以同时触发 action   action 包括         通过邮件形式发送信息给指定邮件地址     发送邮件给指定的当前 cognitive service 的 相关人员     调用 function     调用某个网址     …          通过 Metrics 来监控 认知服务: Metrics 可以通过监控一些指标来 实时通过 chart 方式查看认知服务情况  Metrics 可监控的指标包括     http 请求相关         Avaibility     BlockedCalls     ClentError     Latency     ServerError     Data in/out     Total Calls     Total Errors     Total Token Calls     …          Usage 相关         Auto seconds translated     Face images trained     LUIS Speech Requested     Processed Images     QA text records     Text charactors translated     Voice model training minutes     还有很多…           创建好一个新的 chart 后就可以通过图表实时监控了用 postman 调用 speech service API: 网上的教学  先创建一个 speech service 进入postman, 创建一个 post request     填入 speech service 提供的 url         https://eastus. api. cognitive. microsoft. com/sts/v1. 0/issuetoken          在 header 中填入 key-value         key: &lt;your own key&gt;     value: &lt;your own key&gt;           点击 send, 之后就返回了一个 token     &lt;your own token&gt;    未完成, 后面需要继续设置 诊断 Diagnostic settings:  诊断设置用于配置将资源的平台日志和指标流式导出到所选目标。最多可以创建五个不同的诊断设置，以将不同的日志和指标发送到不同的目标。 诊断可以获取的信息     Audit Logs   Request and Response Logs   Trace Logs   AllMetrics    下游     Send to Log Analytics workspace 分析   Archive to a storage account 存储   Stream to an event hub   Send to partner solution 第三方   Cognitive Service Security 安全: Networking  创建一个新的Speech 服务后, 进入这个服务的 Networking, 会发现提示”Custom Domain Name is required for VNet” 创建一个 custom domian dl-work-domain 创建后, Speech 的 endpoint URL 就会增加dl-work-domain 这个 domain 这样就可以选择 “Selected Network and Private Endpoints”     增加 Firewall 功能   只允许 on-premise 的网络访问这个 Speech 服务.          要指定 CIDR          或者只允许自己本机 IP 访问 Speech 功能    或者通过 “Private endpoint connections” 建立一个私有 endpoint 通道, 让其他 AZ 服务访问 Speech 服务Identity  A system assigned managed identity is restricted to one per resource and is tied to the lifecycle of this resource. You can grant permissions to the managed identity by using Azure role-based access control (Azure RBAC). The managed identity is authenticated with Azure AD, so you don’t have to store any credentials in code.      系统分配的托管标识限制为每个资源一个，并绑定到此资源的生命周期。可以使用 Azure 基于角色的访问控制 （Azure RBAC） 向托管标识授予权限。托管标识使用 Azure AD 进行身份验证，因此无需在代码中存储任何凭据。   使用了 ID, 就不需要使用 key 了.     分为 System Assigned 和 User Assigned     System Assigned ID         一旦开启, 系统会分配一个 ID 给这个资源, 并注册到 Azure AD中     一旦注册到 AZ AD 中, 这个资源就可以访问 Az 的其他资源, 比如 Storage 了.           微软的 AI Policies /Policy: Azure 官网的 Policy Azure console 中 Policy 服务中定义的 Cognitive Service 的policy  Policy 是一段 Json 规则, 可以应用到某个创建的 订阅 的 资源组 的 某个资源上     比如, 选中 Cognitive Services accounts should restrict network access 然后assign 给 Free Teir -&gt; dl-cogsvs-group -&gt; dl-cogsvs / dl-speech / dl-alert-rule-checking-total-calls    这里的 Policy 包含了     限制网络访问   关闭公共网络访问   关闭本地认证方法   使用自己定制的存储   用 customer managed key 加密数据   使用私有 DNS zone   使用私有链接   使用 private endpoints   使用 managed identity    TODO: 仔细看一下这个链接中的内容, 这是考试中的一部分认知服务 容器 containers: 官网介绍  Azure认知服务提供了几个Docker容器，允许您使用本地Azure中可用的相同API。使用这些容器可以灵活地使认知服务更接近您的数据，以符合性、安全性或其他操作原因。容器支持目前可用于Azure认知服务的子集 认知容器中包含了     决策容器 Decision containers   语言容器 Language containers   语音容器 Speech containers   视觉容器 Vision containers    好处     可以把认知服务部署在容器中, 放在客户的 On-premise 环境中   提高安全性, 满足合规要求   降低网络延迟    不需要依赖Cloud 可以通过 docker pull 命令获取认知服务的容器 计费     对于容器的使用, 会把使用记录(meteringz)发到 Azure. com 进行计费计算   Implement Computer Vision Soluiton 计算机视觉方案: 官方文档 12pip install azure-cognitiveservices-vision-customvisionpip install pillow # PIL 的图片处理类库获取图片的 tag: 提供一张照片, 然后让 AI 给图片中的内容进行标注tag 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657from azure. cognitiveservices. vision. computervision import ComputerVisionClientfrom azure. cognitiveservices. vision. computervision. models import OperationStatusCodesfrom azure. cognitiveservices. vision. computervision. models import VisualFeatureTypesfrom msrest. authentication import CognitiveServicesCredentialsfrom array import arrayimport osfrom PIL import Imageimport sysimport timesubscription_key =  &lt;your own key&gt; endpoint =  https://dlvision1. cognitiveservices. azure. com/ computervision_client = ComputerVisionClient(endpoint, CognitiveServicesCredentials(subscription_key))remote_image_url =  https://raw. githubusercontent. com/gottagetgit/AI102Files/main/Computer_Vision  \           /Analyze_images_using_Computer_Vision_API/Images/Landmark. jpg  '''Tag an Image - remoteThis example returns a tag (key word) for each thing in the image. '''print( ===== Tag an image - remote ===== )# Call API with remote imagetags_result_remote = computervision_client. tag_image(remote_image_url)# Print results with confidence scoreprint( Tags in the remote image: \n )if len(tags_result_remote. tags) == 0:  print( No tags detected.  )else:  for tag in tags_result_remote. tags:    print( '{}' with confidence {:. 2f}% . format(tag. name, tag. confidence * 100))print()# '''# Tag an Image - local# This example returns a tag (key word) for each thing in the image. # '''# print( ===== Tag an Image - local ===== )# # Open local image file# local_image_path =  Images/Landmark. jpg # local_image = open(local_image_path,  rb )# # Call API local image# tags_result_local = computervision_client. tag_image_in_stream(local_image)# # Print results with confidence score# print( Tags in the local image: \n )# if len(tags_result_local. tags) == 0:#   print( No tags detected.  )# else:#   for tag in tags_result_local. tags:#     print( '{}' with confidence {:. 2f}% . format(tag. name, tag. confidence * 100))# print()# '''# END - Tag an Image - local# '''代码运行结果 获取图片的 描述 Description: 提供一张照片, 然后让 AI 给图片中的内容进行文字描述 和 打 tag 一样, 只是调用的方法由 tag_image 改为 describe_image description_results = computervision_client. tag_image(remote_image_url) 代码运行结果 Identify landmarks and Celebrities 识别地标和名人: 识别地标和名人都属于 domain-specific models  celebraties     使用了图片的人物分类   使用了outdoor 和 building 的图片分类   和图片描述的代码类似, 区别是使用了 analyze_image_by_domain detect_domain_results_landmarks = computervision_client. analyze_image_by_domain( landmarks , remote_image_url)  这里可以把 “landmarks” 改成 “selebraties” 然后去检测图片中的名人代码运行结果 Identify Brands in image, 品牌检测:  和地标名人检测类似, 区别是使用了 analyze_image     这里的feature type 是 brands   其他featrue type 还包括         Faces     Brands     Adult     Objects     Color     ImageType     Description     Categories     Tags           可以获得品牌名称 和 品牌在图片中的像素位置123456# Select the visual feature(s) you wantremote_image_features = [ brands ]# Call API with URL and featuresdetect_brands_results_remote = computervision_client. analyze_image(remote_image_url, remote_image_features)代码运行结果  检测到两个品牌信息     logo 和位置   品牌名称和位置   Moderate Adult Content 适度(中度)的成人内容检测: Content flag definitions 内容标注定义 分三类  Adult image, 成人图片, 内容有展示裸体和性行为的 ( nudity , saxual acts) Racy, 不雅的行为, 有性暗示的, 比 Audit image 稍微好点 (saxually suggestive) Gory, 血腥暴力, (blood, gore )和品牌检测用的是一个API 调用, 也是使用 analyze_image 方法  这里的 feature type 是 adult调用检测 API 后返回的是对应的三个 boolean 值 (true/false) 和对应分数的 JSON 格式 isAdultContent and adultScore isRacyContent and racyScore isGoryContent and goryScore12345# Select the visual feature(s) you wantremote_image_features = [ adult ]# Call API with URL and featuresdetect_adult_results_remote = computervision_client. analyze_image(remote_image_url, remote_image_features)创建缩略图: 提供一个本地或者网络的图片, 然后提交给 API, 然后保存回到本地调用了 generate_thumbnail_in_stream 并提供了图片的长宽参数  这里用 AI 服务的价值是, 人工智能可以自动的按照图片中物体的位置和空白的比例来更好的做缩略图     最后一个参数标成 true 就可以使用智能缩略图功能其他代码和前面的例子一样   1234567891011121314# Generate a thumbnail from a local imagelocal_image_path_thumb =  Images/Faces. jpg local_image_thumb = open(local_image_path_thumb,  rb )print( Generating thumbnail from a local image. . .  )# Call the API with a local image, set the width/height if desired (pixels)# Returns a Generator object, a thumbnail image binary (list). thumb_local = computervision_client. generate_thumbnail_in_stream(100, 100, local_image_thumb, True)# Write the image binary to filewith open( thumb_local. png ,  wb ) as f:  for chunk in thumb_local:    f. write(chunk)从图片中获取文字 - OCR: 注意: 从 2021 年 7 月 29 日开始, 手写文字识别和 OCR 已经不在考试范围内了 有几种技术  图片获取打印文字用 OCR API 图片或 PDF 中获取文字用 Read API 从 图片的表格或者 receipt 收据中获取文字需要 pre-build 一个 收据model, 用 Form Recognizer方法和之前的类似, 用  recognize_printed_text_in_stream 进行流式读取图片中的打印文字 recognize_printed_text 一次性读取图片中的打印文字 read 读取手写文字读取图片中的打印文字的示例代码 12345678ocr_result_local = computervision_client. recognize_printed_text_in_stream(local_image_printed_text)for region in ocr_result_local. regions:  for line in region. lines:    print( Bounding box: {} . format(line. bounding_box))    s =       for word in line. words:      s += word. text +        print(s)读取图片中的表格信息的示例代码  这里没有用 ComputerVisionClient, 而是用了另一个Python 的类库 FormRecognizerClient 来实现的     begin_recognize_content_from_url 方法用来读取表格   begin_recognize_receipts_from_url 方法用来读取收据   1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import osfrom azure. core. exceptions import ResourceNotFoundErrorfrom azure. ai. formrecognizer import FormRecognizerClientfrom azure. ai. formrecognizer import FormTrainingClientfrom azure. core. credentials import AzureKeyCredentialkey =  &lt;enter your key here&gt; endpoint =  &lt;enter your endpoint URL here&gt; form_recognizer_client = FormRecognizerClient(endpoint, AzureKeyCredential(key))form_training_client = FormTrainingClient(endpoint, AzureKeyCredential(key))formUrl =  https://raw. githubusercontent. com/Azure/azure-sdk-for-python/master/sdk/formrecognizer/azure-ai  \      -formrecognizer/tests/sample_forms/forms/Form_1. jpg  print( --------Recognizing form-------- )poller = form_recognizer_client. begin_recognize_content_from_url(formUrl)page = poller. result()table = page[0]. tables[0] # page 1, table 1print( Table found on page {}: . format(table. page_number))for cell in table. cells:  print( Cell text: {} . format(cell. text))  print( Location: {} . format(cell. bounding_box))  print( Confidence score: {}\n . format(cell. confidence))receiptUrl =  https://raw. githubusercontent. com/Azure/azure-sdk-for-python/master/sdk/formrecognizer/azure-ai  \        -formrecognizer/tests/sample_forms/receipt/contoso-receipt. png  print( --------Recognizing receipt-------- )poller = form_recognizer_client. begin_recognize_receipts_from_url(receiptUrl)result = poller. result()for receipt in result:  for name, field in receipt. fields. items():    if name ==  Items :      print()      print( Receipt Items: )      for idx, items in enumerate(field. value):        print()        print( . . . Item #{} . format(idx + 1))        for item_name, item in items. value. items():          print( . . . . . . {}: {} has confidence {} . format(item_name, item. value, item. confidence))    else:      print()      print( {}: {} has confidence {} . format(name, field. value, field. confidence))从图片中获取 面部信息 Facial Infomation: 官方文档 注意  为了支持我们负责任的 AI 原则，基于资格和使用标准对人脸服务访问进行限制。 人脸服务仅适用于 Microsoft 托管客户和合作伙伴。 使用人脸识别引入表单来申请访问。 有关详细信息，请参阅人脸受限访问页面。 人脸识别只有微软大客户和合作伙伴才可以用 要用还要填表格申请   2023 年 6 月 30 日之后，在这些功能被停用。   要使用 Face API Face detection可以识别     Age, 年龄   Blur, 污渍   Emotion, 情感   Exposure, 暴露程度   Facial hair, 面部毛发   Glasses, 眼镜   Hair, 头发   Head pose, 头部角度(3D)   Makeup, 化妆   Noise, 噪点   Occlusion, 遮挡   Smile, 微笑   口罩   在图片中辨别和匹配一张脸 Face API: 逻辑  先给一张单个脸的照片 再给一张多个脸的照片 再在多个脸的照片中匹配单个脸使用了 FaceClient 类库  包含的方法有     . face. detect_with_url 检测图片中的脸, 并把每个脸的数据返回   . face. find_similar 在多个脸的数据中匹配某一个脸   1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980import asyncioimport ioimport globimport osimport sysimport timeimport uuidimport requestsfrom urllib. parse import urlparsefrom io import BytesIOfrom PIL import Image, ImageDrawfrom azure. cognitiveservices. vision. face import FaceClientfrom msrest. authentication import CognitiveServicesCredentialsfrom azure. cognitiveservices. vision. face. models import TrainingStatusType, Personsubscription_key =  &lt;enter your key here&gt; endpoint =  &lt;enter your endpoint URL here&gt; # Create an authenticated FaceClient. face_client = FaceClient(endpoint, CognitiveServicesCredentials(subscription_key))# Detect a face in an image that contains a single facesingle_face_image_url = 'https://www. biography. com/. image/t_share/MTQ1MzAyNzYzOTgxNTE0NTEz/john-f-kennedy---mini' \            '-biography. jpg 'single_image_name = os. path. basename(single_face_image_url)# We use detection model 3 to get better performance. detected_faces = face_client. face. detect_with_url(url=single_face_image_url, detection_model='detection_03')if not detected_faces:  raise Exception('No face detected from image {}'. format(single_image_name))# Display the detected face ID in the first single-face image. # Face IDs are used for comparison to faces (their IDs) detected in other images. print( ==== Detect faces in an image === )print()print('Detected face ID from', single_image_name, ':')for face in detected_faces: print(face. face_id)# Save this ID for use in Find Similarfirst_image_face_ID = detected_faces[0]. face_idprint()'''Find a similar faceThis example uses detected faces in a group photo to find a similar face using a single-faced image as query. '''print( ===== Detect the faces in an image that contains multiple faces ===== )print()# Detect the faces in an image that contains multiple faces# Each detected face gets assigned a new IDmulti_face_image_url =  http://www. historyplace. com/kennedy/president-family-portrait-closeup. jpg multi_image_name = os. path. basename(multi_face_image_url)# We use detection model 3 to get better performance. detected_faces2 = face_client. face. detect_with_url(url=multi_face_image_url, detection_model='detection_03')# Search through faces detected in group image for the single face from first image. # First, create a list of the face IDs found in the second image. second_image_face_IDs = list(map(lambda x: x. face_id, detected_faces2))# Next, find similar face IDs like the one detected in the first image. similar_faces = face_client. face. find_similar(face_id=first_image_face_ID, face_ids=second_image_face_IDs)if not similar_faces:  print('No similar faces found in', multi_image_name, '. ')# Print the details of the similar faces detectedelse:  print('Similar faces found in', multi_image_name + ':')  for face in similar_faces:    first_image_face_ID = face. face_id    # The similar face IDs of the single face image and the group image do not need to match,    # they are only used for identification purposes in each image.     # The similar faces are matched using the Cognitive Services algorithm in find_similar().     face_info = next(x for x in detected_faces2 if x. face_id == first_image_face_ID)    if face_info:      print(' Face ID: ', first_image_face_ID)      print(' Face rectangle:')      print('  Left: ', str(face_info. face_rectangle. left))      print('  Top: ', str(face_info. face_rectangle. top))      print('  Width: ', str(face_info. face_rectangle. width))      print('  Height: ', str(face_info. face_rectangle. height))代码运行结果 在训练的模型中识别出一张脸: 逻辑  为三个人创建3 个 person group 的 Face AI 对象, 分别是 man, woman, children 为每个对象(每个人)导入 3 张照片进行训练 给一张多人照片, 然后分别识别是否有之前训练的三个人使用了 TrainingStatusType, Person 和 FaceClient 类库  face_client. person_group. create 先创建一个组 face_client. person_group_person. create(PERSON_GROUP_ID,  Woman ) 在分别创建三个 person face_client. person_group_person. add_face_from_stream 给每个person 加入他们的日常照片 face_client. person_group. train 开始训练 face_client. person_group. get_training_status 获取训练状态(成功或者失败) face_client. face. detect_with_stream(image, detection_model='detection_03') 输入多人测试图片, 并检测图片中的人脸, 返回多个脸 face_client. face. identify(face_ids, PERSON_GROUP_ID) 把返回的脸和之前的组中的person 进行匹配源代码地址 代码运行结果 获取面部的参数信息:  Face detection可以识别     Age, 年龄   Blur, 污渍   Emotion, 情感   Exposure, 暴露程度   Facial hair, 面部毛发   Glasses, 眼镜   Hair, 头发   Head pose, 头部角度(3D)   Makeup, 化妆   Noise, 噪点   Occlusion, 遮挡   Smile, 微笑   口罩    代码通过 http request 的方式请求 Azure Face API     获取 age,gender,headPose,smile,facialHair,glasses,emotion 信息   1234567891011121314151617181920212223242526272829303132333435import requestsimport matplotlib. pyplot as pltfrom PIL import Imagefrom matplotlib import patchesfrom io import BytesIOimport osimport jsondef config():  print( Call Config )  return subscription_key, face_api_urlimage_path = os. path. join(r'/Computer_Vision/Extract_facial_information_from_images/CapFrame. jpg')image_data = open(image_path,  rb )subscription_key =  &lt;enter your key&gt; face_api_url =  &lt;place the API URL here&gt; headers = {'Content-Type': 'application/octet-stream',      'Ocp-Apim-Subscription-Key': subscription_key}params = {  'returnFaceId': 'true',  'returnFaceLandmarks': 'true',  'returnFaceAttributes': 'age,gender,headPose,smile,facialHair,glasses,emotion'}response = requests. post(face_api_url, params=params, headers=headers, data=image_data)response. raise_for_status()faces = response. json()print(faces)with open( faces. json ,  w ) as faces_file:  json. dump(faces, faces_file, indent=4, sort_keys=True)代码运行结果保存在文件中 用 Custom Vision 自定义视觉 进行图片分类: 使用 Azure 认知服务中的自定义视觉为特定域自定义和嵌入最先进的计算机视觉图像分析。构建顺畅的客户体验、优化制造流程、加速数字营销活动等。  也是 Cumputer Vision, 只是可以更灵活的进行定制(customize) custom vision 主要是会把标签打到图片上, 而不是从图片获取标签 定制场景     用自己的图片训练和设置自己的模型   模型可以部署到 edge 或 cloud   可以部署到本地, 有更大的安全和隐私保障   可以适用于不同的行业   分两个 model  trainning model     训练模型    prediction model     正式使用的模型 (预测模型)   概念  iteration     一个训练好的模型实例就叫 一个 iteration   用 web portal 训练 视觉分类模型 (Clasification Model): 实验场景:  用两种植物的多张照片对模型进行训练 然后上传一种植物的一张图片, 让模型预测是那种植物   需要从 custom vision console 中的 overview 中的 Get Started 的连接进入 https://www. customvision. ai 进行配置     分为几个步骤      创建 project         project 类型分为             classification                 为图像分类 - 打tags                     可以分为一个图片多个tag, 或者一个图片一个 tag                                         object detection                 检测图像中的对象                               project 可以指定 domain, 比如 generial, food, landmarks, retail 等等          上传已经标注过 tag 的图片         分别上传 Hemlock 和 Japanese Cherry 的多张图片并打上标签          训练         点击训练按钮     训练完成的页面展示                                  测试         点击测试按钮, 上传一张测试照片, 然后让模型预测是哪种植物     测试结果                                  publish         点击publish, 让预测模型对外提供服务     对外服务的预测模型地址展示                             通过命令行 curl 来调用预测模型的 API 来测试                                  用 Python 训练 视觉分类模型 (Clasification Model): 流程和 web portal 一样, 只不过是通过代买实现 代码使用了多个 类库  CustomVisionTrainingClient 训练库 CustomVisionPredictionClient 预测库 ImageFileCreateBatch 批量建图 ImageFileCreateEntry 图片实体12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576from azure. cognitiveservices. vision. customvision. training import CustomVisionTrainingClientfrom azure. cognitiveservices. vision. customvision. prediction import CustomVisionPredictionClientfrom azure. cognitiveservices. vision. customvision. training. models import ImageFileCreateBatch, ImageFileCreateEntry, Regionfrom msrest. authentication import ApiKeyCredentialsimport time# Replace with valid valuesENDPOINT =  &lt;Your endpoint&gt; training_key =  &lt;Your training key&gt; prediction_key =  &lt;Your prediction key&gt; prediction_resource_id =  &lt;Your prediction resource ID&gt; credentials = ApiKeyCredentials(in_headers={ Training-key : training_key})trainer = CustomVisionTrainingClient(ENDPOINT, credentials)prediction_credentials = ApiKeyCredentials(in_headers={ Prediction-key : prediction_key})predictor = CustomVisionPredictionClient(ENDPOINT, prediction_credentials)publish_iteration_name =  classifyModel credentials = ApiKeyCredentials(in_headers={ Training-key : training_key})trainer = CustomVisionTrainingClient(ENDPOINT, credentials)# Create a new projectprint( Creating project. . .  )project = trainer. create_project( My New Project )# Make two tags in the new projecthemlock_tag = trainer. create_tag(project. id,  Hemlock )cherry_tag = trainer. create_tag(project. id,  Japanese Cherry )base_image_location =  &lt;Your image location&gt; print( Adding images. . .  )image_list = []for image_num in range(1, 11):  file_name =  hemlock_{}. jpg . format(image_num)  with open(base_image_location +  images/Hemlock/  + file_name,  rb ) as image_contents:    image_list. append(ImageFileCreateEntry(name=file_name, contents=image_contents. read(), tag_ids=[hemlock_tag. id]))for image_num in range(1, 11):  file_name =  japanese_cherry_{}. jpg . format(image_num)  with open(base_image_location +  images/Japanese Cherry/  + file_name,  rb ) as image_contents:    image_list. append(ImageFileCreateEntry(name=file_name, contents=image_contents. read(), tag_ids=[cherry_tag. id]))upload_result = trainer. create_images_from_files(project. id, ImageFileCreateBatch(images=image_list))if not upload_result. is_batch_successful:  print( Image batch upload failed.  )  for image in upload_result. images:    print( Image status:  , image. status)  exit(-1)print( Training. . .  )iteration = trainer. train_project(project. id)while iteration. status !=  Completed :  iteration = trainer. get_iteration(project. id, iteration. id)  print( Training status:   + iteration. status)  time. sleep(1)# The iteration is now trained. Publish it to the project endpointtrainer. publish_iteration(project. id, iteration. id, publish_iteration_name, prediction_resource_id)print( Done! )# Now there is a trained endpoint that can be used to make a predictionprediction_credentials = ApiKeyCredentials(in_headers={ Prediction-key : prediction_key})predictor = CustomVisionPredictionClient(ENDPOINT, prediction_credentials)with open(base_image_location +  images/Test/test_image. jpg ,  rb ) as image_contents:  results = predictor. classify_image(    project. id, publish_iteration_name, image_contents. read())  # Display the results.   for prediction in results. predictions:    print( \t  + prediction. tag_name +        : {0:. 2f}% . format(prediction. probability * 100))代码运行结果 用 Object Detection with Custom Version 自定义视觉 进行图片中的对象检测: 对图片中的内容(特定方框的 4 个像素坐标)打标签  可以通过 web Portal 或者 SDK API 对 custom object detection model 进行训练 管理模型 iteration 把模型发布, 并部署到容器中用 web portal 来训练和使用 Oblect Detection Model: 实验场景:  分别用剪子和叉子的多张照片对模型进行训练     每个照片都会标注照片中的物体的四个点位的坐标   并提供一个 tag 名字 fork/scissors    然后上传一张叉子的图片, 让模型给出是什么物体, 以及 4 个点位的坐标实验步骤  进入 https://www. customvision. ai/projects 创建一个新的project     类型选 “Object Detection”   domain 选 “generial”    分别上传剪子和叉子的照片, 并给特定区域打上标签 点击训练按钮     训练结果       完成后, 点击测试, 上传一个测试的叉子照片, 然后看看结果.      测试结果       发布这个 API, 可以让外部通过 API 访问用 python 来训练和使用 object detection model: python 代码 的流程和 web portal 一样, 只是用 代码来实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131from azure. cognitiveservices. vision. customvision. training import CustomVisionTrainingClientfrom azure. cognitiveservices. vision. customvision. prediction import CustomVisionPredictionClientfrom azure. cognitiveservices. vision. customvision. training. models import ImageFileCreateBatch, ImageFileCreateEntry, \  Regionfrom msrest. authentication import ApiKeyCredentialsimport time# Replace with valid valuesENDPOINT =  &lt;your API endpoint&gt; training_key =  &lt;your training key&gt; prediction_key =  &lt;your prediction key&gt; prediction_resource_id =  &lt;your prediction resource id&gt; credentials = ApiKeyCredentials(in_headers={ Training-key : training_key})trainer = CustomVisionTrainingClient(ENDPOINT, credentials)prediction_credentials = ApiKeyCredentials(in_headers={ Prediction-key : prediction_key})predictor = CustomVisionPredictionClient(ENDPOINT, prediction_credentials)publish_iteration_name =  detectModel # Find the object detection domainobj_detection_domain = next(  domain for domain in trainer. get_domains() if domain. type ==  ObjectDetection  and domain. name ==  General )# Create a new projectprint( Creating project. . .  )project = trainer. create_project( My Detection Project , domain_id=obj_detection_domain. id)# Make two tags in the new projectfork_tag = trainer. create_tag(project. id,  fork )scissors_tag = trainer. create_tag(project. id,  scissors )fork_image_regions = {   fork_1 : [0. 145833328, 0. 3509314, 0. 5894608, 0. 238562092],   fork_2 : [0. 294117659, 0. 216944471, 0. 534313738, 0. 5980392],   fork_3 : [0. 09191177, 0. 0682516545, 0. 757352948, 0. 6143791],   fork_4 : [0. 254901975, 0. 185898721, 0. 5232843, 0. 594771266],   fork_5 : [0. 2365196, 0. 128709182, 0. 5845588, 0. 71405226],   fork_6 : [0. 115196079, 0. 133611143, 0. 676470637, 0. 6993464],   fork_7 : [0. 164215669, 0. 31008172, 0. 767156839, 0. 410130739],   fork_8 : [0. 118872553, 0. 318251669, 0. 817401946, 0. 225490168],   fork_9 : [0. 18259804, 0. 2136765, 0. 6335784, 0. 643790841],   fork_10 : [0. 05269608, 0. 282303959, 0. 8088235, 0. 452614367],   fork_11 : [0. 05759804, 0. 0894935, 0. 9007353, 0. 3251634],   fork_12 : [0. 3345588, 0. 07315363, 0. 375, 0. 9150327],   fork_13 : [0. 269607842, 0. 194068655, 0. 4093137, 0. 6732026],   fork_14 : [0. 143382356, 0. 218578458, 0. 7977941, 0. 295751631],   fork_15 : [0. 19240196, 0. 0633497, 0. 5710784, 0. 8398692],   fork_16 : [0. 140931368, 0. 480016381, 0. 6838235, 0. 240196079],   fork_17 : [0. 305147052, 0. 2512582, 0. 4791667, 0. 5408496],   fork_18 : [0. 234068632, 0. 445702642, 0. 6127451, 0. 344771236],   fork_19 : [0. 219362751, 0. 141781077, 0. 5919118, 0. 6683006],   fork_20 : [0. 180147052, 0. 239820287, 0. 6887255, 0. 235294119]}scissors_image_regions = {   scissors_1 : [0. 4007353, 0. 194068655, 0. 259803921, 0. 6617647],   scissors_2 : [0. 426470578, 0. 185898721, 0. 172794119, 0. 5539216],   scissors_3 : [0. 289215684, 0. 259428144, 0. 403186262, 0. 421568632],   scissors_4 : [0. 343137264, 0. 105833367, 0. 332107842, 0. 8055556],   scissors_5 : [0. 3125, 0. 09766343, 0. 435049027, 0. 71405226],   scissors_6 : [0. 379901975, 0. 24308826, 0. 32107842, 0. 5718954],   scissors_7 : [0. 341911763, 0. 20714055, 0. 3137255, 0. 6356209],   scissors_8 : [0. 231617644, 0. 08459154, 0. 504901946, 0. 8480392],   scissors_9 : [0. 170343131, 0. 332957536, 0. 767156839, 0. 403594762],   scissors_10 : [0. 204656869, 0. 120539248, 0. 5245098, 0. 743464053],   scissors_11 : [0. 05514706, 0. 159754932, 0. 799019635, 0. 730392158],   scissors_12 : [0. 265931368, 0. 169558853, 0. 5061275, 0. 606209159],   scissors_13 : [0. 241421565, 0. 184264734, 0. 448529422, 0. 6830065],   scissors_14 : [0. 05759804, 0. 05027781, 0. 75, 0. 882352948],   scissors_15 : [0. 191176474, 0. 169558853, 0. 6936275, 0. 6748366],   scissors_16 : [0. 1004902, 0. 279036, 0. 6911765, 0. 477124184],   scissors_17 : [0. 2720588, 0. 131977156, 0. 4987745, 0. 6911765],   scissors_18 : [0. 180147052, 0. 112369314, 0. 6262255, 0. 6666667],   scissors_19 : [0. 333333343, 0. 0274019931, 0. 443627447, 0. 852941155],   scissors_20 : [0. 158088237, 0. 04047389, 0. 6691176, 0. 843137264]}# Update this with the path to where you downloaded the images. base_image_location =  &lt;path to directory # Go through the data table above and create the imagesprint( Adding images. . .  )tagged_images_with_regions = []for file_name in fork_image_regions. keys():  x, y, w, h = fork_image_regions[file_name]  regions = [Region(tag_id=fork_tag. id, left=x, top=y, width=w, height=h)]  with open(base_image_location +  images/fork/  + file_name +  . jpg , mode= rb ) as image_contents:    tagged_images_with_regions. append(      ImageFileCreateEntry(name=file_name, contents=image_contents. read(), regions=regions))for file_name in scissors_image_regions. keys():  x, y, w, h = scissors_image_regions[file_name]  regions = [Region(tag_id=scissors_tag. id, left=x, top=y, width=w, height=h)]  with open(base_image_location +  images/scissors/  + file_name +  . jpg , mode= rb ) as image_contents:    tagged_images_with_regions. append(      ImageFileCreateEntry(name=file_name, contents=image_contents. read(), regions=regions))upload_result = trainer. create_images_from_files(project. id, ImageFileCreateBatch(images=tagged_images_with_regions))if not upload_result. is_batch_successful:  print( Image batch upload failed.  )  for image in upload_result. images:    print( Image status:  , image. status)  exit(-1)print( Training. . .  )iteration = trainer. train_project(project. id)while iteration. status !=  Completed :  iteration = trainer. get_iteration(project. id, iteration. id)  print( Training status:   + iteration. status)  time. sleep(1)# The iteration is now trained. Publish it to the project endpointtrainer. publish_iteration(project. id, iteration. id, publish_iteration_name, prediction_resource_id)print( Done! )# Now there is a trained endpoint that can be used to make a prediction# Open the sample image and get back the prediction results. with open(base_image_location +  images/Test/test_od_image. jpg , mode= rb ) as test_data:  results = predictor. detect_image(project. id, publish_iteration_name, test_data)# Display the results. for prediction in results. predictions:  print(     \t  + prediction. tag_name +  : {0:. 2f}% bbox. left = {1:. 2f}, bbox. top = {2:. 2f}, bbox. width = {3:. 2f}, bbox. height = {4:. 2f} . format(      prediction. probability * 100, prediction. bounding_box. left, prediction. bounding_box. top,      prediction. bounding_box. width, prediction. bounding_box. height))用 video indexer 来分析视频: 官方文档 上传一个视频, 然后通过 AI 来获取视频的一些 insight  Deep search     可以把视频中说的话, 出现的脸进行索引, 然后通过搜索找到视频中的一个画面   可以获取的内容包括         Face detection 面部     celebrity identification 名人     品牌     物体     insight     文字           辅助功能     提取视频中人物和他们说的话, 然后把话在视频中叠加到说话人上, 方便听觉障碍的人可以快速理解   可以做实时翻译    参与度     一段教育视频前 30 分钟讲解球体，后 30 分钟讲解棱锥。如果从 30 分钟时的标记开始定位视频，那么阅读棱锥的学生会受益更多。同样，视频推荐系统可以利用视频见解来更好地决定应该向用户推荐哪个视频。    变现     通过理解视频内容, 然后可以在合适的位置插入对应的广告来获取变现能力   video indexer in action: video indexer 有自己的 web portal  通过 AAD 账户登录 上传视频 (https://aka. ms/responsible-ai-video)     提供视频名称, 语言种类, 是否是privacy 还是 public    提交后, 开始对视频进行 indexing 视频index 成功后, 点击该视频     找到了 26 个人, 和对应的名字(有些事unknow) , 对应在视频中出现的时间点, 视频中的占比   找到了8 个topic         教育     科技     商业     …          1 个audoi   27 个 keyword   55 个 labels   2 个 entity (品牌)         Eric Horvitz     Microsoft          30 个场景 (拍视频的段场景)   搜索 Microsoft 可以找到所有和视频相关的元素  视频中出现的文字 品牌 讨论的话题 附加功能  可以把带有insight 的视频内容通过 HTML 源代码的 embed 形式提供     Share 的地址 https://www. videoindexer. ai/accounts/1771eab4-fb17-463d-97f5-aa30eceaee38/videos/9448d2362b/   Enbed 的地址 https://www. videoindexer. ai/embed/insights/1771eab4-fb17-463d-97f5-aa30eceaee38/9448d2362b/?accessToken=&lt;your own token&gt;    可以把视频中的insight 以 JSON 的格式下载 可以查看整个视频中的按时间顺序的 insight 列表, 并且可以按照不同的分类对内容进行筛选实现自然语言处理 Natural Language Processing (NLP): 实现任何计算机之间的自然语言处理, 包括  Text Analytics Speech Service Translation 创建自己的语言模型 LUIS , 并进行优化和使用文字处理 Text Analytics: 内容包括了  处理 关键字/短语 抽取 (key phrases) 处理重要实体 (人, 地址, URL 等等) 处理 态度 (sentiment) 探测文本中使用的是什么语言Key Phrases extraction 关键字/短语 抽取: 官方文档API 文档支持的语言类型  识别文字中的重要短语 使用关键短语提取来快速识别文本中的主要概念。例如，在“食物很美味，工作人员很棒”的文本中，关键短语提取将返回主要主题：“食物”和“很棒的员工”。流程  Portal 中 创建一个Azure Cognitive Service 的 Language Service     默认的功能包含了         Sentiment analysis     Key phrase extraction     Pre-built question answering     Conversational language understanding     Named entity recognition     Text Summarization     Text analytics for Health          可选的功能包括:         Custom question answering     Custom text classification &amp; Custom named entity recognition           使用REST API或C#、Java、JavaScript和Python的客户端库创建请求。您还可以使用批处理请求发送异步调用，将多个功能的API请求合并到单个调用中。 将包含您的数据的请求作为原始非结构化文本发送。您的密钥和端点将用于身份验证。 在本地流式传输或存储响应。代码使用了 TextAnalyticsClient 这个类库  代码逻辑是把 4 种语言做成一个字典, 发给文字分析服务     response = client. key_phrases(documents=documents)    然后在把结果展示出来1234567891011121314151617181920212223242526272829303132333435363738394041424344from azure. cognitiveservices. language. textanalytics import TextAnalyticsClientfrom msrest. authentication import CognitiveServicesCredentials# variables to store subscription key and root URL for the Cognitive Service resourcesubscription_key =  YourKey endpoint =  YourEndpoint def authenticateclient():  credentials = CognitiveServicesCredentials(subscription_key)  text_analytics_client = TextAnalyticsClient(endpoint=endpoint, credentials=credentials)  return text_analytics_clientdef key_phrases():  client = authenticateclient()  try:    documents = [      { id :  1 ,  language :  ja ,  text :  猫は幸せ },      { id :  2 ,  language :  de ,        text :  Fahrt nach Stuttgart und dann zum Hotel zu Fu.  },      { id :  3 ,  language :  en ,        text :  My cat might need to see a veterinarian.  },      { id :  4 ,  language :  es ,  text :  A mi me encanta el fútbol! }    ]    for document in documents:      print(         Asking key-phrases on '{}' (id: {}) . format(document['text'], document['id']))    response = client. key_phrases(documents=documents)    for document in response. documents:      print( Document Id:  , document. id)      print( \tKey Phrases: )      for phrase in document. key_phrases:        print( \t\t , phrase)  except Exception as err:    print( Encountered exception. {} . format(err))key_phrases()代码运行结果  英文结果     从 “My cat might need to see a veterinarian. ” 提取了   cat 猫   veterinarian 兽医   Entity information extract 抽取 文字中的实体信息: Entity 的概念: 就比如在 wikipedia 中的一段文字, 有些文字有链接, 这些文字往往有更明确的解释, 可以理解为 entity 这里的代码是用了 Rest API 直接调用  将2种语言的句子组成一个字典 将字典发送给 endpint , 并获取结果     endpoint 是 https://dl-language-service-1. cognitiveservices. azure. com/text/analytics/v2. 1/entities   response = requests. post(entities_url, headers=headers, json=documents)    将结果打印123456789101112131415161718192021222324252627282930313233from pip. _vendor import requests# pprint is used to format the JSON responsefrom pprint import pprint# variables to store subscription key and root URL for the Cognitive Service resourcesubscription_key =  a3c9bca3107644108def6f2dd02822d2 endpoint =  https://dl-language-service-1. cognitiveservices. azure. com/ # append the Text Analytics endpoint information to the URLentities_url = endpoint +  /text/analytics/v2. 1/entities # variable to store a JSON formatted document that contains two entries in a JSON array. documents = { documents : [  { id :  1 ,  language :  en ,    text :  Microsoft was founded by Bill Gates and Paul Allen on April 4, 1975, to develop and sell          BASIC interpreters for the Altair 8800.  },  { id :  2 ,  language :  es ,    text :  La sede principal de Microsoft se encuentra en la ciudad de Redmond, a 21 kilómetros de          Seattle.  }]}# Setup the header information for the REST request passing in the subscription keyheaders = { Ocp-Apim-Subscription-Key : subscription_key}# Build the REST request by passing in the complete URL, header information for authentication, and the JSON documentresponse = requests. post(entities_url, headers=headers, json=documents)# Create a variable to store the results that are returned from the REST requestentities = response. json()# Output the result using pprint. pprint(entities)下面是输出的json结果  分析出了以下 entity 的名字, 类型, 和 对应的 wikipedia 的链接     比尔盖茨   微软   Paul Allen   123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117{'documents': [{'entities': [{'bingId': 'a093e9b9-90f5-a3d5-c4b8-5855e1b01f85',               'matches': [{'entityTypeScore': 1. 0,                      'length': 9,                      'offset': 0,                      'text': 'Microsoft',                      'wikipediaScore': 0. 5080587142195991}],               'name': 'Microsoft',               'type': 'Organization',               'wikipediaId': 'Microsoft',               'wikipediaLanguage': 'en',               'wikipediaUrl': 'https://en. wikipedia. org/wiki/Microsoft'},               {'bingId': '0d47c987-0042-5576-15e8-97af601614fa',               'matches': [{'entityTypeScore': 0. 67,                      'length': 10,                      'offset': 25,                      'text': 'Bill Gates',                      'wikipediaScore': 0. 5601095924849657}],               'name': 'Bill Gates',               'type': 'Person',               'wikipediaId': 'Bill Gates',               'wikipediaLanguage': 'en',               'wikipediaUrl': 'https://en. wikipedia. org/wiki/Bill_Gates'},               {'bingId': 'df2c4376-9923-6a54-893f-2ee5a5badbc7',               'matches': [{'entityTypeScore': 0. 81,                      'length': 10,                      'offset': 40,                      'text': 'Paul Allen',                      'wikipediaScore': 0. 5647133956039234}],               'name': 'Paul Allen',               'type': 'Person',               'wikipediaId': 'Paul Allen',               'wikipediaLanguage': 'en',               'wikipediaUrl': 'https://en. wikipedia. org/wiki/Paul_Allen'},               {'bingId': '52535f87-235e-b513-54fe-c03e4233ac6e',               'matches': [{'length': 7,                      'offset': 54,                      'text': 'April 4',                      'wikipediaScore': 0. 34538928435757743}],               'name': 'April 4',               'type': 'Other',               'wikipediaId': 'April 4',               'wikipediaLanguage': 'en',               'wikipediaUrl': 'https://en. wikipedia. org/wiki/April_4'},               {'matches': [{'entityTypeScore': 0. 8,                      'length': 13,                      'offset': 54,                      'text': 'April 4, 1975'}],               'name': 'April 4, 1975',               'subType': 'Date',               'type': 'DateTime'},               {'bingId': '5b16443d-501c-58f3-352e-611bbe75aa6e',               'matches': [{'length': 5,                      'offset': 89,                      'text': 'BASIC',                      'wikipediaScore': 0. 313848923908594}],               'name': 'BASIC',               'type': 'Other',               'wikipediaId': 'BASIC',               'wikipediaLanguage': 'en',               'wikipediaUrl': 'https://en. wikipedia. org/wiki/BASIC'},               {'bingId': '7216c654-3779-68a2-c7b7-12ff3dad5606',               'matches': [{'length': 11,                      'offset': 116,                      'text': 'Altair 8800',                      'wikipediaScore': 0. 8690828819485741}],               'name': 'Altair 8800',               'type': 'Other',               'wikipediaId': 'Altair 8800',               'wikipediaLanguage': 'en',               'wikipediaUrl': 'https://en. wikipedia. org/wiki/Altair_8800'},               {'matches': [{'entityTypeScore': 0. 8,                      'length': 4,                      'offset': 123,                      'text': '8800'}],               'name': '8800',               'subType': 'Number',               'type': 'Quantity'}],        'id': '1'},        {'entities': [{'bingId': 'a093e9b9-90f5-a3d5-c4b8-5855e1b01f85',               'matches': [{'length': 9,                      'offset': 21,                      'text': 'Microsoft',                      'wikipediaScore': 0. 3868553907750074}],               'name': 'Microsoft',               'type': 'Organization',               'wikipediaId': 'Microsoft',               'wikipediaLanguage': 'es',               'wikipediaUrl': 'https://es. wikipedia. org/wiki/Microsoft'},               {'bingId': '8769d4c0-b645-70ac-03ec-6eebabf6d26e',               'matches': [{'length': 7,                      'offset': 60,                      'text': 'Redmond',                      'wikipediaScore': 0. 4737390069279388}],               'name': 'Redmond (Washington)',               'type': 'Location',               'wikipediaId': 'Redmond (Washington)',               'wikipediaLanguage': 'es',               'wikipediaUrl': 'https://es. wikipedia. org/wiki/Redmond_(Washington)'},               {'matches': [{'entityTypeScore': 0. 8,                      'length': 13,                      'offset': 71,                      'text': '21 kilómetros'}],               'name': '21 kilómetros',               'subType': 'Dimension',               'type': 'Quantity'},               {'bingId': '5fbba6b8-85e1-4d41-9444-d9055436e473',               'matches': [{'length': 7,                      'offset': 88,                      'text': 'Seattle',                      'wikipediaScore': 0. 34624433748176875}],               'name': 'Seattle',               'type': 'Location',               'wikipediaId': 'Seattle',               'wikipediaLanguage': 'es',               'wikipediaUrl': 'https://es. wikipedia. org/wiki/Seattle'}],        'id': '2'}], 'errors': []}下面是一段 . Net 代码的类似的输出结果 抽取文字中的 sentiment (观点/情感) 信息: sentiment 主要包含三种结果  positive 积极的 nagitave 消极的 neutral 中等的 结果的数字打分是 0-1, 越大越好代码发送2 段不同语言的句子给到 sentiment API 和  endpoint https://dl-language-service-1. cognitiveservices. azure. com/text/analytics/v3. 0/sentiment 123456789101112131415161718192021import requests# pprint is used to format the JSON responsefrom pprint import pprintsubscription_key =  a3c9bca3107644108def6f2dd02822d2 endpoint =  https://dl-language-service-1. cognitiveservices. azure. com/ sentiment_url = endpoint +  /text/analytics/v3. 0/sentiment documents = { documents : [  { id :  1 ,  language :  en ,    text :  I really enjoy the new XBox One S. It has a clean look, it has 4K/HDR resolution and it is affordable.  },  { id :  2 ,  language :  es ,    text :  Este ha sido un dia terrible, llegué tarde al trabajo debido a un accidente automobilistico.  }]}headers = { Ocp-Apim-Subscription-Key : subscription_key}response = requests. post(sentiment_url, headers=headers, json=documents)sentiments = response. json()pprint(sentiments)返回的json 内容如下 1234567891011121314151617181920212223242526272829303132333435363738{'documents': [{'confidenceScores': {'negative': 0. 0,                   'neutral': 0. 01,                   'positive': 0. 99},        'id': '1',        'sentences': [{'confidenceScores': {'negative': 0. 0,                          'neutral': 0. 01,                          'positive': 0. 99},                'length': 35,                'offset': 0,                'sentiment': 'positive',                'text': 'I really enjoy the new XBox One S. '},               {'confidenceScores': {'negative': 0. 0,                          'neutral': 0. 59,                          'positive': 0. 4},                'length': 67,                'offset': 35,                'sentiment': 'neutral',                'text': 'It has a clean look, it has 4K/HDR '                    'resolution and it is affordable. '}],        'sentiment': 'positive',        'warnings': []},        {'confidenceScores': {'negative': 1. 0,                   'neutral': 0. 0,                   'positive': 0. 0},        'id': '2',        'sentences': [{'confidenceScores': {'negative': 1. 0,                          'neutral': 0. 0,                          'positive': 0. 0},                'length': 92,                'offset': 0,                'sentiment': 'negative',                'text': 'Este ha sido un dia terrible, llegué '                    'tarde al trabajo debido a un accidente '                    'automobilistico. '}],        'sentiment': 'negative',        'warnings': []}], 'errors': [], 'modelVersion': '2022-11-01'}detect the language used in text 检测文本用了什么语言: 检测语言类型需要发送文字信息到 lanugage API  endpoint https://dl-language-service-1. cognitiveservices. azure. com/text/analytics/v3. 0/languages 把多段不同语言的文字放到字典中发送给 endpoint 打印Json返回结果     结果是语言的 iso 名称, 名称, 和信任值(0-1)   12345678910111213141516171819import requests# pprint is used to format the JSON responsefrom pprint import pprintsubscription_key =  a3c9bca3107644108def6f2dd02822d2 endpoint =  https://dl-language-service-1. cognitiveservices. azure. com/ language_api_url = endpoint +  /text/analytics/v3. 0/languages documents = { documents : [  { id :  1 ,  text :  This is a document written in English.  },  { id :  2 ,  text :  Este es un document escrito en Español.  },  { id :  3 ,  text :  这是一个用中文写的文件 }]}headers = { Ocp-Apim-Subscription-Key : subscription_key}response = requests. post(language_api_url, headers=headers, json=documents)languages = response. json()pprint(languages)返回的json 结果 1234567891011121314151617{'documents': [{'detectedLanguage': {'confidenceScore': 0. 99,                   'iso6391Name': 'en',                   'name': 'English'},        'id': '1',        'warnings': []},        {'detectedLanguage': {'confidenceScore': 1. 0,                   'iso6391Name': 'es',                   'name': 'Spanish'},        'id': '2',        'warnings': []},        {'detectedLanguage': {'confidenceScore': 1. 0,                   'iso6391Name': 'zh_chs',                   'name': 'Chinese_Simplified'},        'id': '3',        'warnings': []}], 'errors': [], 'modelVersion': '2022-10-01'}Speech Service (语音 - 文字互转): 官方介绍说明文档支持的语言 需要先安装 认知语音库 12# pip install azure-cognitiveservices-speechpip install -i https://pypi. tuna. tsinghua. edu. cn/simple azure-cognitiveservices-speech包含的 features  speech to text     说的话转成文字   场景:         会议记录     实时翻译     语音助手     help desk/呼叫中心           text to speech     文字转语音   Azure 使用的是 Neural vices, 是深度神经网络模仿人类的说话语音语调而产生的语音. 更接近人类自然发音.    场景:         小说, 新闻等应用把内容读出来     自动外呼广告电话           实时翻译/同声传译     结合speech to text &amp; text to speech   把一种语言翻译成另一种语言    批量转录     把一批语音转成其他格式或者语言    recognize speaker     识别说话的人   场景         识别会议/视频中说话的人是谁           语音操作的应用     场景         开灯     关门     …           声音语言识别 Language identification     听一下说话人说的是哪国话    发音评估 Pronunciation assessment     评估发音是否准确    意图识别 Intent recognition     识别说话人的意图   附加功能  可以定制化自己的模型, 语音库中文语音可以识别  普通话 四川话 山东话 江浙沪方言 Wu Chinese 广东话 香港话 台湾话text to speeck 可以说更多的功能, 支持 方言     上海话   河南话   …    男女 角色     Boy   Girl   Narrator (中性)   YoungAdultMale   …    风格     生气   愉快   …   使用Speech CLI、Speech SDK、Speech Studio或REST API，语音可以轻松启用您的应用程序、工具和设备  Speech Studio     Speech Studio是一套基于UI的工具，用于在您的应用程序中构建和集成Azure认知服务语音服务的功能。您使用无代码方法在Speech Studio中创建项目，然后使用Speech SDK、Speech CLI或REST API在应用程序中引用这些资产。   Text to speech: python 代码  使用了 import azure. cognitiveservices. speech as speechsdk 类库 选择一个语言类型, 支持的语言 输入一段要说的文字, 通过 SDK 发送给Azure text to Speech 服务, 调用 speak_text_async 方法 speech_synthesizer = speechsdk. SpeechSynthesizer(speech_config=speech_config, audio_config=audio_output)     这行代码初始化一个 speech syntyhesizer (语音合成器)   第二个参数 audio_config=audio_output 不给的话就直接朗读, 如果给的话,就会存到文件中    result = speech_synthesizer. speak_text_async(text). get()     执行这行代码就能听到声音了    将另一个语音存到文件中123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657import azure. cognitiveservices. speech as speechsdk# Creates an instance of a speech config with specified subscription key and service region. # Replace with your own subscription key and service region (e. g. ,  westus ). speech_key, service_region =  6dc37625b4e04652821d23239f58f33a ,  eastus speech_config = speechsdk. SpeechConfig(subscription=speech_key, region=service_region)# uncomment this line to change the voice used for synthesisspeech_config. speech_synthesis_voice_name =  zh-CN-YunhaoNeural # Creates a speech synthesizer using the default speaker as audio output. speech_synthesizer = speechsdk. SpeechSynthesizer(speech_config=speech_config)# Receives a text from console input. print( Type some text that you want to speak. . .  )text = input()# Synthesizes the received text to speech. # The synthesized speech is expected to be heard on the speaker with this line executed. result = speech_synthesizer. speak_text_async(text). get()# Checks result. if result. reason == speechsdk. ResultReason. SynthesizingAudioCompleted:  print( Speech synthesized to speaker for text [{}] . format(text))elif result. reason == speechsdk. ResultReason. Canceled:  cancellation_details = result. cancellation_details  print( Speech synthesis canceled: {} . format(cancellation_details. reason))  if cancellation_details. reason == speechsdk. CancellationReason. Error:    if cancellation_details. error_details:      print( Error details: {} . format(cancellation_details. error_details))  print( Did you update the subscription info? )# Creates an audio configuration that points to an audio file. # Replace with your own audio filename. audio_filename =  . /text-to-speech-py. wav audio_output = speechsdk. audio. AudioOutputConfig(filename=audio_filename)# Creates a synthesizer with the given settingsspeech_synthesizer = speechsdk. SpeechSynthesizer(speech_config=speech_config, audio_config=audio_output)# Receives a text from console input. print( Type some text that you want to speak. . .  )text = input()# Synthesizes the text to speech. result = speech_synthesizer. speak_text_async(text). get()# Checks result. if result. reason == speechsdk. ResultReason. SynthesizingAudioCompleted:  print( Speech synthesized to [{}] for text [{}] . format(audio_filename, text))elif result. reason == speechsdk. ResultReason. Canceled:  cancellation_details = result. cancellation_details  print( Speech synthesis canceled: {} . format(cancellation_details. reason))  if cancellation_details. reason == speechsdk. CancellationReason. Error:    if cancellation_details. error_details:      print( Error details: {} . format(cancellation_details. error_details))    print( Did you update the subscription info? )代码执行的输出: Speech to Text: 官方文档 将说话的语音内容转换成对应的文字 既可以从文件中读取语音, 也可以实时获取语音  text 2 speech 使用的是 SpeechSynthesizer. speak_text_async(text). get() 方法 speech 2 text 使用的是     speech_recognizer = speechsdk. SpeechRecognizer(speech_config=speech_config, audio_config=audio_input)         如果不指定 audio_config 参数, 那么就是实时录入信息          speech_config. speech_recognition_language= zh-CN  配置成中文语音输入   SpeechRecognizer. recognize_once() 方法 执行语音转文字   1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import azure. cognitiveservices. speech as speechsdk'''In this exercise you will see an example of a speech to text translations from a local file. '''speech_key, service_region =  6dc37625b4e04652821d23239f58f33a ,  eastus speech_config = speechsdk. SpeechConfig(subscription=speech_key, region=service_region)audio_input = speechsdk. AudioConfig(filename= . /Speech_Media_narration. wav )speech_recognizer = speechsdk. SpeechRecognizer(speech_config=speech_config, audio_config=audio_input)print( Recognizing first result. . .  )result = speech_recognizer. recognize_once()if result. reason == speechsdk. ResultReason. RecognizedSpeech:  print( Recognized: {} . format(result. text))elif result. reason == speechsdk. ResultReason. NoMatch:  print( No speech could be recognized: {} . format(result. no_match_details))elif result. reason == speechsdk. ResultReason. Canceled:  cancellation_details = result. cancellation_details  print( Speech Recognition canceled: {} . format(cancellation_details. reason))  if cancellation_details. reason == speechsdk. CancellationReason. Error:    print( Error details: {} . format(cancellation_details. error_details))'''In this exercise you will see an example of listening for speech from a microphone and translating it to text output. '''speech_key, service_region =  6dc37625b4e04652821d23239f58f33a ,  eastus speech_config = speechsdk. SpeechConfig(subscription=speech_key, region=service_region)speech_config. speech_recognition_language= zh-CN  # 指定中文输入speech_recognizer = speechsdk. SpeechRecognizer(speech_config=speech_config)print( Begin speaking. . .  )result = speech_recognizer. recognize_once()if result. reason == speechsdk. ResultReason. RecognizedSpeech:  print( Recognized: {} . format(result. text))elif result. reason == speechsdk. ResultReason. NoMatch:  print( No speech could be recognized: {} . format(result. no_match_details))elif result. reason == speechsdk. ResultReason. Canceled:  cancellation_details = result. cancellation_details  print( Speech Recognition canceled: {} . format(cancellation_details. reason))  if cancellation_details. reason == speechsdk. CancellationReason. Error:    print( Error details: {} . format(cancellation_details. error_details))代码输出 Translate Language 翻译服务: 官方文档 Translator Service是一种基于云的神经机器翻译服务 Neural Machine Translation (NMT)主要功能包括  文本翻译 文件翻译     支持的文件格式包括         txt     pdf     csv     html     markdown     xls     ppt     msg (Microsoft Outlook)     doc     odt/odp (openDocument)     rtf           自定义翻译     构建自定义模型，以翻译特定于领域和行业的语言、术语和风格。   可以指定一些术语不翻译, 比如 Azure   AI 102 翻译 考点:  文本到文本的翻译 speech 到 speech 的翻译, 要用speech 服务 speach 到 text 的分宜, 要用 speech 服务text to text 翻译: 代码示例 - 文本到文本  一段英语翻译成 法语和中文 endpoint 是 endpoint =  https://api. cognitive. microsofttranslator. com/translate 1234567891011121314151617181920212223242526272829303132333435import requests, uuid, json# Add your key and endpointkey =  c5aa54650eff4d09a5009198e46a0bcc endpoint =  https://api. cognitive. microsofttranslator. com # location, also known as region. # required if you're using a multi-service or regional (not global) resource. It can be found in the Azure portal on the Keys and Endpoint page. location =  eastus path = '/translate'constructed_url = endpoint + pathparams = {  'api-version': '3. 0',  'from': 'en',  'to': ['fr', 'zh-Hans']}headers = {  'Ocp-Apim-Subscription-Key': key,  # location required if you're using a multi-service or regional (not global) resource.   'Ocp-Apim-Subscription-Region': location,  'Content-type': 'application/json',  'X-ClientTraceId': str(uuid. uuid4())}# You can pass more than one object in body. body = [{  'text': 'I would really like to drive your car around the block a few times!'}]request = requests. post(constructed_url, params=params, headers=headers, json=body)response = request. json()print(json. dumps(response, sort_keys=True, ensure_ascii=False, indent=4, separators=(',', ': ')))代码执行的结果是输出一段 JSON 1234567891011121314[  {     translations : [      {         text :  J’aimerais vraiment conduire votre voiture autour du pâté de maisons plusieurs fois! ,         to :  fr       },      {         text :  我真的很想开你的车绕街区转几次！ ,         to :  zh-Hans       }    ]  }]speech to speech 翻译: 注意, 这里用的是 Azure Cognitive Service 的 speech 的 translation服务, 而不是 translator 服务 代码目标:  从英文语音输入翻译成法语和中文 把原始英文语音输入的文字打印出来 把翻译成的法文和中文输出到屏幕 把翻译成的法文和中文朗读出来代码逻辑:  通过 result = recognizer. recognize_once() 方法获取输入的英文语音 打印输入语音对应的英文文本 print( RECOGNIZED '{}': {} . format(fromLanguage, result. text)) 分别把英文文本翻译成法文和中文文本 print( TRANSLATED into {}: {} . format(key, result. translations[key])) 更新 speech_synthesizer, 将原来的英语改成法语和中文并指定语音 用指定的语音把翻译后的文本读出来 speech_synthesizer. speak_text_async(result. translations[key]). get()123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384import azure. cognitiveservices. speech as speechsdkspeech_key, service_region =  6dc37625b4e04652821d23239f58f33a ,  eastus def translate_speech_to_speech():  # Creates an instance of a speech translation config with specified subscription key and service region.   # Replace with your own subscription key and region identifier from here: https://aka. ms/speech/sdkregion  translation_config = speechsdk. translation. SpeechTranslationConfig(subscription=speech_key, region=service_region)  speech_config = speechsdk. SpeechConfig(subscription=speech_key, region=service_region)  # Creates a speech synthesizer using the configured voice for audio output.   speech_synthesizer = speechsdk. SpeechSynthesizer(speech_config=speech_config)  # Sets source and target languages. In this example, the service will translate a US English spoken input,  # to French and Indonesian language spoken output Replace with the languages of your choice, from list found  # here: https://aka. ms/speech/sttt-languages  fromLanguage = 'en-US'  translation_config. speech_recognition_language = fromLanguage  # Add more than one language to the collection.   # using the add_target_language() method  translation_config. add_target_language( fr )  translation_config. add_target_language( zh-Hans )  # Creates a translation recognizer using and audio file as input.   recognizer = speechsdk. translation. TranslationRecognizer(translation_config=translation_config)  # Starts translation, and returns after a single utterance is recognized. The end of a  # single utterance is determined by listening for silence at the end or until a maximum of 15  # seconds of audio is processed. It returns the recognized text as well as the translation.   # Note: Since recognize_once() returns only a single utterance, it is suitable only for single  # shot recognition like command or query.   # For long-running multi-utterance recognition, use start_continuous_recognition() instead.   print( Say something. . .  )  result = recognizer. recognize_once()  # Check the result  if result. reason == speechsdk. ResultReason. TranslatedSpeech:    # Output the text for the recognized speech input    print( RECOGNIZED '{}': {} . format(fromLanguage, result. text))    # Loop through the returned translation results    for key in result. translations:      # Using the Key and Value components of the returned dictionary for the translated results      # The first portion gets the key (language code) while the second gets the Value      # which is the translated text for the language specified      # Output the language and then the translated text      print( TRANSLATED into {}: {} . format(key, result. translations[key]))      # If the language code is 'fr' for French, then use the French voice for Julie      # If you change the languages in the 'AddTargetLanguage' above, ensure you modify this if statement as well      if key ==  fr :        speech_config. speech_synthesis_voice_name =  fr-FR-AlainNeural         # Update the speech synthesizer to use the proper voice        speech_synthesizer = speechsdk. SpeechSynthesizer(speech_config=speech_config)        # Use the proper voice, from the speech synthesizer configuration, to narrate the translated result        # in the native speaker voice.         speech_synthesizer. speak_text_async(result. translations[key]). get()      else: # Otherwise, use the voice for the Indonesian translation        speech_config. speech_synthesis_voice_name =  zh-CN-XiaochenNeural         # Update the speech synthesizer to use the proper voice        speech_synthesizer = speechsdk. SpeechSynthesizer(speech_config=speech_config)        # Use the proper voice, from the speech synthesizer configuration, to narrate the translated result        # in the native speaker voice.         speech_synthesizer. speak_text_async(result. translations[key]). get()  elif result. reason == speechsdk. ResultReason. RecognizedSpeech:    print( RECOGNIZED: {} (text could not be translated) . format(result. text))  elif result. reason == speechsdk. ResultReason. NoMatch:    print( NOMATCH: Speech could not be recognized: {} . format(result. no_match_details))  elif result. reason == speechsdk. ResultReason. Canceled:    print( CANCELED: Reason={} . format(result. cancellation_details. reason))    if result. cancellation_details. reason == speechsdk. CancellationReason. Error:      print( CANCELED: ErrorDetails={} . format(result. cancellation_details. error_details))translate_speech_to_speech()代码输出结果: 并有语音朗读 Speech to text 翻译: 是 speech to speech 的简化版 1234567891011121314151617181920212223242526272829303132333435363738394041424344import azure. cognitiveservices. speech as speechsdkspeech_key, service_region =  YourKey ,  YourRegion def translate_speech_to_text():  # Creates an instance of a speech translation config with specified subscription key and service region.   # Replace with your own subscription key and region identifier from here: https://aka. ms/speech/sdkregion  translation_config = speechsdk. translation. SpeechTranslationConfig(subscription=speech_key, region=service_region)  # Sets source and target languages.   # Replace with the languages of your choice, from list found here: https://aka. ms/speech/sttt-languages  fromLanguage = 'en-US'  toLanguage = 'de'  translation_config. speech_recognition_language = fromLanguage  translation_config. add_target_language(toLanguage)  # Creates a translation recognizer using and audio file as input.   recognizer = speechsdk. translation. TranslationRecognizer(translation_config=translation_config)  # Starts translation, and returns after a single utterance is recognized. The end of a  # single utterance is determined by listening for silence at the end or until a maximum of 15  # seconds of audio is processed. It returns the recognized text as well as the translation.   # Note: Since recognize_once() returns only a single utterance, it is suitable only for single  # shot recognition like command or query.   # For long-running multi-utterance recognition, use start_continuous_recognition() instead.   print( Say something. . .  )  result = recognizer. recognize_once()  # Check the result  if result. reason == speechsdk. ResultReason. TranslatedSpeech:    print( RECOGNIZED '{}': {} . format(fromLanguage, result. text))    print( TRANSLATED into {}: {} . format(toLanguage, result. translations[toLanguage]))  elif result. reason == speechsdk. ResultReason. RecognizedSpeech:    print( RECOGNIZED: {} (text could not be translated) . format(result. text))  elif result. reason == speechsdk. ResultReason. NoMatch:    print( NOMATCH: Speech could not be recognized: {} . format(result. no_match_details))  elif result. reason == speechsdk. ResultReason. Canceled:    print( CANCELED: Reason={} . format(result. cancellation_details. reason))    if result. cancellation_details. reason == speechsdk. CancellationReason. Error:      print( CANCELED: ErrorDetails={} . format(result. cancellation_details. error_details))translate_speech_to_text()LUIS Language Understanding Service 语言理解服务: 官方文档 注意:  LUIS 会在 2025 年 10 月就下架了, 以后会使用 conversational language understanding 来代替 1 April 2023 开始就不能再创建新的 LUIS 应用了语言理解（LUIS）是一种基于云的对话式人工智能服务，将自定义机器学习智能应用于用户的对话、自然语言文本，以预测整体含义，并提取相关的详细信息。 LUIS 可以通过以下方式访问  自定义 portal API SDK几个场景  构建企业级对话机器人：     此参考架构描述了如何使用Azure Bot框架构建企业级对话机器人（聊天机器人）。    商业聊天机器人：     Azure机器人服务和语言理解服务使开发人员能够为银行、旅行和娱乐等各种场景创建对话界面。    使用语音助理控制物联网设备：     与所有可访问互联网的设备创建无缝对话界面——从连接的电视或冰箱到连接的发电厂的设备。   需要安装 LUIS 的 python 扩展库pip install azure-cognitiveservices-language-luis 需要在 azure portal 创建一个 LUIS 项目的过程  现在 Azure Portal 中创建一个 LUIS 服务 然后到 https://www. luis. ai/applications 创建 LUIS 项目通过 LUIS portal 使用 LUIS AI: https://www. luis. ai/applications 概念:  Intend 是意图 (动作)     比如:         add flag     cancel     delete     forward     read aloud     reply     …          examples 是 意图下面的一些文字性描述的例子         比如: “请把这封邮件标记成重要”           Entities 是实体 (名词/术语),     让 AI 知道这个领域的一些实体的专有名词   相当于数据库的字段   比如:         附件     分类     …           Prebuild Domains 预先做好的模板, 直接用就可以自动创建 intend 和 Entities使用流程  创建一个应用 从 Prebuild domain 那里创建 一个 email 的 entities 和 intentions 点击 “train” 对模型进行训练 点击 “Test” 对模型进行测试     给模型输入 “Send email to bob for lunch tomorrow”    点击 “Publish” 把模型发布出去, 就可以有endpoint 可以提供给外部进行 Rest API 访问. Portal 测试结果 LUIS 代码示例: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242# To run this sample, install the following modules. # pip install azure-cognitiveservices-language-luisimport jsonimport time# &lt;Dependencies&gt;from azure. cognitiveservices. language. luis. authoring import LUISAuthoringClientfrom azure. cognitiveservices. language. luis. runtime import LUISRuntimeClientfrom msrest. authentication import CognitiveServicesCredentials# &lt;/Dependencies&gt;def quickstart():  # &lt;VariablesYouChange&gt;  authoringKey = 'YourAuthoringKey'  authoringEndpoint = 'YourAuthoringEndpoint'  predictionKey = 'YourAuthoringKey'  predictionEndpoint = 'YourPredictionEndpoint'  # &lt;/VariablesYouChange&gt;  # &lt;VariablesYouDontNeedToChangeChange&gt;  appName =  Contoso Pizza Company   versionId =  0. 1   intentName =  OrderPizzaIntent   # &lt;/VariablesYouDontNeedToChangeChange&gt;  # &lt;AuthoringCreateClient&gt;  client = LUISAuthoringClient(authoringEndpoint, CognitiveServicesCredentials(authoringKey))  # &lt;/AuthoringCreateClient&gt;  # Create app  app_id = create_app(client, appName, versionId)  # &lt;AddIntent&gt;  client. model. add_intent(app_id, versionId, intentName)  # &lt;/AddIntent&gt;  # Add Entities  add_entities(client, app_id, versionId)  # Add labeled examples  add_labeled_examples(client, app_id, versionId, intentName)  # &lt;TrainAppVersion&gt;  client. train. train_version(app_id, versionId)  waiting = True  while waiting:    info = client. train. get_status(app_id, versionId)    # get_status returns a list of training statuses, one for each model. Loop through them and make sure all are    # done.     waiting = any(map(lambda x: 'Queued' == x. details. status or 'InProgress' == x. details. status, info))    if waiting:      print( Waiting 10 seconds for training to complete. . .  )      time. sleep(10)    else:      print( trained )      waiting = False  # &lt;/TrainAppVersion&gt;  # &lt;PublishVersion&gt;  client. apps. publish(app_id, versionId, is_staging=False)  # &lt;/PublishVersion&gt;  # &lt;PredictionCreateClient&gt;  runtimeCredentials = CognitiveServicesCredentials(predictionKey)  clientRuntime = LUISRuntimeClient(endpoint=predictionEndpoint, credentials=runtimeCredentials)  # &lt;/PredictionCreateClient&gt;  # &lt;QueryPredictionEndpoint&gt;  # Production == slot name  predictionRequest = { query :  I want two small pepperoni pizzas with more salsa }  predictionResponse = clientRuntime. prediction. get_slot_prediction(app_id,  Production , predictionRequest)  print( Top intent: {} . format(predictionResponse. prediction. top_intent))  print( Sentiment: {} . format(predictionResponse. prediction. sentiment))  print( Intents:  )  for intent in predictionResponse. prediction. intents:    print( \t{} . format(json. dumps(intent)))  print( Entities: {} . format(predictionResponse. prediction. entities))# &lt;/QueryPredictionEndpoint&gt;def create_app(client, appName, versionId):  # &lt;AuthoringCreateApplication&gt;  # define app basics  appDefinition = {     name : appName,     initial_version_id : versionId,     culture :  en-us   }  # create app  app_id = client. apps. add(appDefinition)  # get app id - necessary for all other changes  print( Created LUIS app with ID {} . format(app_id))  # &lt;/AuthoringCreateApplication&gt;  return app_id# &lt;/createApp&gt;def add_entities(client, app_id, versionId):  # &lt;AuthoringAddEntities&gt;  # Add Prebuilt entity  client. model. add_prebuilt(app_id, versionId, prebuilt_extractor_names=[ number ])  # define machine-learned entity  mlEntityDefinition = [    {       name :  Pizza ,       children : [        { name :  Quantity },        { name :  Type },        { name :  Size }      ]    },    {       name :  Toppings ,       children : [        { name :  Type },        { name :  Quantity }      ]    }]  # add entity to app  modelId = client. model. add_entity(app_id, versionId, name= Pizza order , children=mlEntityDefinition)  # define phraselist - add phrases as significant vocabulary to app  phraseList = {     enabledForAllModels : False,     isExchangeable : True,     name :  QuantityPhraselist ,     phrases :  few,more,extra   }  # add phrase list to app  client. features. add_phrase_list(app_id, versionId, phraseList)  # Get entity and subentities  modelObject = client. model. get_entity(app_id, versionId, modelId)  toppingQuantityId = get_grandchild_id(modelObject,  Toppings ,  Quantity )  pizzaQuantityId = get_grandchild_id(modelObject,  Pizza ,  Quantity )  # add model as feature to subentity model  prebuiltFeatureRequiredDefinition = { model_name :  number ,  is_required : True}  client. features. add_entity_feature(app_id, versionId, pizzaQuantityId, prebuiltFeatureRequiredDefinition)  # add model as feature to subentity model  prebuiltFeatureNotRequiredDefinition = { model_name :  number }  client. features. add_entity_feature(app_id, versionId, toppingQuantityId, prebuiltFeatureNotRequiredDefinition)  # add phrase list as feature to subentity model  phraseListFeatureDefinition = { feature_name :  QuantityPhraselist ,  model_name : None}  client. features. add_entity_feature(app_id, versionId, toppingQuantityId, phraseListFeatureDefinition)# &lt;/AuthoringAddEntities&gt;def add_labeled_examples(client, app_id, versionId, intentName):  # &lt;AuthoringAddLabeledExamples&gt;  # Define labeled example  labeledExampleUtteranceWithMLEntity = {     text :  I want two small seafood pizzas with extra cheese.  ,     intentName : intentName,     entityLabels : [      {         startCharIndex : 7,         endCharIndex : 48,         entityName :  Pizza order ,         children : [          {             startCharIndex : 7,             endCharIndex : 30,             entityName :  Pizza ,             children : [              {                 startCharIndex : 7,                 endCharIndex : 9,                 entityName :  Quantity               },              {                 startCharIndex : 11,                 endCharIndex : 15,                 entityName :  Size               },              {                 startCharIndex : 17,                 endCharIndex : 23,                 entityName :  Type               }]          },          {             startCharIndex : 37,             endCharIndex : 48,             entityName :  Toppings ,             children : [              {                 startCharIndex : 37,                 endCharIndex : 41,                 entityName :  Quantity               },              {                 startCharIndex : 43,                 endCharIndex : 48,                 entityName :  Type               }]          }        ]      }    ]  }  print( Labeled Example Utterance: , labeledExampleUtteranceWithMLEntity)  # Add an example for the entity.   # Enable nested children to allow using multiple models with the same name.   # The quantity subentity and the phraselist could have the same exact name if this is set to True  client. examples. add(app_id, versionId, labeledExampleUtteranceWithMLEntity, { enableNestedChildren : True})# &lt;/AuthoringAddLabeledExamples&gt;# &lt;AuthoringSortModelObject&gt;def get_grandchild_id(model, childName, grandChildName):  theseChildren = next(filter((lambda child: child. name == childName), model. children))  theseGrandchildren = next(filter((lambda child: child. name == grandChildName), theseChildren. children))  grandChildId = theseGrandchildren. id  return grandChildId# &lt;/AuthoringSortModelObject&gt;quickstart()Cognitive Search: 官方文档 通过用户提供数据, 文档, 图片等信息创建一个认知搜索引擎  创建数据源 定义索引 query 索引 配置所以提供 autocomplete (自动完成)和 autosuggest (自动推荐) 根据相关性提供更多结果 实现同义词在 Azure 中创建了 Cognitive Search 的截图 导入数据的步骤  import data     导入一个 hotel 酒店的 sample   导入完成后, 就可以看到有         data source     indexers             index 创建器                indexes             50 个 document       进入 index 页面就可以搜索了       搜索 pool 的结果如下                       页面提供了 app 如果需要整合搜索所需要的 URL 地址.           创建新的 data source 可以从以下数据源来创建  Azure Blob Storage Azure Data Lake Storage Gen2 Azure SQL Database Azure Table Storage Cosmos DBQnA Maker: QnA maker 是一个 NLP 服务, 是用来从一个知识库(knowledge) 中获取合适的答案.  问答机器人 需要先创建一个 knowledge base knowledge base 来源是客户自己通过的内容, 比如 PDF 等文档注意:  QnA Maker 服务将于 2025 年 3 月 31 日停用。 新的功能将转到 Azure Cognitive Service for Language 现在已经无法创建新的 QnA 服务https://azurefaq. ca 这个网站就是用 QnA 做的 生命周期  创建 knowledge base 设置 训练 测试 发布 创建Web App Bot 测试和 Bot 聊天        可以下载 web app bot 的源代码 (. net). 源代码可以部署到任何云上. 源代码会调用 Azure 的 chat bot API    发布 QnA Bot     QnA Bot 可以发布到很多 Channel, 包括         Slack     Microsoft Teams     Skype     Facebook     Email     …          QnA Bot 可以生成一段 enbeded 的 ifrmae html 代码, 直接嵌入到网站代码中   Bot Service - Framework SDK: 官方文档 官方的 github 的例子 注意:  The Bot Framework Python SDK is being retired with final long-term support ending in November 2023 Bot Framework Python SDK 2023 年 11 月开始就不在更新了Microsoft Bot Framework和Azure Bot Service是库、工具和服务的集合，可让您构建、测试、部署和管理智能机器人。机器人框架包括一个模块化和可扩展的SDK，用于构建机器人和连接到人工智能服务。使用此框架，开发人员可以创建使用语音、理解自然语言、回答问题等的机器人。  对比 QnA Bot, Bot framework (Bot service) 更加高级 这里做的 Bot , 主要还是聊天机器人流程(定披萨为例子):  计划     设计沟通逻辑   创建 . chat 配置文件   增加response 语言   adaptive cards    构建 测试 发布 连接 评估Echo Bot 聊天机器人: echo bot 就是你发什么给他, 他就回什么给你 下面用 官方提供的 Echo Bot Github 的例子来构建一个可以聊天的机器人 注意:  Bot Framework Python SDK 2023 年 11 月开始就不在更新了Echo Bot 安装步骤  github 代码下载到本地 安装并运行 python 代码 安装一个 Bot Framework 的 emulator (模拟器) 的 Mac 版本到本地     模拟器的作用是模拟Bot 所需要的 云端的 Azure 服务, 比如计算, 存储等.     在 bot 模拟器中打开python 提示的地址就可以和 Echo bot 聊天了     echo bot 就是你发什么给他, 他就回什么给你   Welcome Bot:  Bot 会有一段欢迎词给你, 后面还是 Echo 你说的话github 代码和 Echo Bot 安装流程一样 Bot Dialogs: 和机器人对话  给 bot 预先设定好问题和答案选项的内容 当你进入聊天之后, 机器人会先问你问题, 你都答好了之后, 机器人会把你的回答记下来. 作为后续对话的基础. github 代码 Adaptive Cards 适应卡: github 代码 自适应卡片 - 根据聊天框/web 页面大小自适应的一些卡片  包括     航班信息   天气信息   pizza 点评   …    卡片信息是 JSON 格式的文件, 可以编辑聊天截图 Tracking Events with Application Insights: github 地址 功能: 把 IOT 的遥感数据( telemetry )加到 bot 中, 然后让 Bot 给出一些见解 把 Bot 和其他 cognitive 服务整合: github 地址 比如  QnA LUIS     使用LUIS 来判断了用户说的话的intension (意图)    Speech …用 BotFramework-Composer 来创建机器人: guthub Bot Framework Composer是一个开源的视觉创作画布，供开发人员和多学科团队使用语言理解和QnA Maker设计和构建对话体验，以及机器人回复的复杂组合（语言生成）。在这个工具中，您将拥有构建复杂的对话体验所需的一切。  用于对话流的可视化编辑画布 语言理解上下文编辑（NLU） 培训、测试和管理语言理解（NLU）和QnA组件的工具 语言生成和模板系统 现成的机器人运行时可执行文件使用界面https://github. com/microsoft/BotFramework-Composer/blob/main/docs/Assets/Screenshot-ComposerV2-overview. png 有web 和 desktop 两种模式 步骤  下载bot framework composer for mac 创建一个 Empty bot (C#) 设置 dialog, trigger 用 bot emulator 进行测试 发布 bot 到 Azure     会创建必要的 Azure 资源来为 bot 提供服务    可以转到 Azure Bot Service 查看bot 和管理. 练习题总结: Bot Framework:  对接其他认知服务/第三方服务     接 speech         Direct line speech             其他几种 Direct Line                 Direct Line Channel 是用来开启 bot 和 一个客户端通过 https 进行通信         Direct Line Speech 是用来开启 Bot 的语音交互能力 ( voice in/out)         Direct Line App Service 可以让 bot 和 客户端的通信只在 Azure 内网进行                               三个步骤             在 Azure Portal 中开启一个 Cognitive Service Speech resource       到现有的 Azure Bot 中, 在 channel 中选 Direct Line Speech       在 Azure Bot 的配置页 开启 Streaming Endpoint                     接 Luis         C# 用 appsetting. json     date entity 可以给机器人设置时间提醒功能 (应用于医疗机器人)     C# 需要安装 Microsoft. Bot. Builder. AI. Luis NuGet 包          接 其他bot         Orchestrator     防止欢迎信息发给其他 Bot 的方法             检查 turnContext 查看来的人是否是个机器人                多个 bot 的关系             能为其他 bot 执行任务的 bot 叫做 skill       调用其他 bot 的 bot 叫做 skill consumer       skill manifest 是一个 json 文件, 由 skill 这边暴露给 consumer 有哪些命令可以被调用.                      接 middleware         多语言翻译          接 Facebook         attachement 负责设置 logo     notyification-type 负责通知类型           创建一个 Echo bot 的步骤     从github 下载代码   pip install -r requirements. txt   python app. py   Run Azure Bot Framework Emulator    Bot Composer     Composer 需要用 Node. js 来创建一个新的 bot   Composer 会为你的 bot 创建一个默认的 dialog (会话)   但 Composer 不需要一开始就有 child dialog. 可以后面创建   验证 输入数字限制 (比如请假天数)         Validation Rule          抽取 报销金额         Regular Expression           卡片     HeroCard         一个图和少量文字, 适合欢迎信息          Thumbnail card         缩略图卡, 只能包含一张缩略提, 一个按钮和少量文字          Receipt card         收据卡, 包含收据          Adaptive Card         适配卡, 可以定制包含图片, 文字, 按钮, 语音, 视频等各种类型的内容同           按钮     ImBack         展示文字, 比如: 包含隐私条款的信息          OpenURL         点击跳转到指定 URL           TTL     默认 20 分钟, 可设置更久   可以把会话信息保存在 cosmos DB    状态管理     private conversation 私有对话         比如 投票          conversation 普通对话         比如 审批流程          user         比如 用户工作title           通过测试文本对 bot 测试     Bot Framework emulator   Chatdown    对话模型     activity handler 活动处理         比如, 用户进来了就发欢迎信息          component dialog 内容对话         steps 的集合          waterfall dialog 瀑布对话         顺序的对话集合           关于和机器人的对话     Chat files 是 markdown 格式的文件 - 正确   展示的 avatar 是 水平的   bot 可以配置成和多个用户进行交互 - 正确    可以发布 bot 的 Azure 资源     Azure Function   Azure App Service    Debug     Bot Emulator         是一个桌面应用, 用于给 chat bot 进行debug 的工具          ngrok         是一个开源的跨平台应用监控工具          QnA 和 Knowledge Base 知识库:  关于发布一个 QnA 的新的 knowledge base     可以再 portal 上通过 点击 publish 按钮发布   还可以通过 Rest API 发布新的 QnA knowledge base   发布后的 knowledge base 的内容保存在 Azure Search 的索引中 (a prod index of Azure Search)   每次发布新的 Knowledge base 之后, 不需要重新连接 bot 和知识库, 他们是自动重新连接的    Role / 最小权限 Read     测试知识库   导出知识库    Editor     更新知识库    Contributer     除了给用户分配权限之外, 其他都可以    Owner     什么都可以    Active learning     让 QnA 可以在和客户交流的时候提供一些额外的建议问题让用户问    开启 QnA 的 active learning 的 3个步骤     导航到 https://www. qnamaker. ai/   导航页面右上角的服务设置   开启 active learning    QnA knowledge base 支持的导出格式     Excel   TSV         TSV 是 tab separrated value           Knowledge base 启动方法     LUDown         会调用 . lu 文件中的 question/answer 列表在           当你创建一个 QnA Maker 的时候, 还会自动创建 2 个服务     认知搜索         会搜索 knowledge base          Azure App Service         运行 bot web 页面           认知搜索的请求数在稳定增长, 已经达到最高上限了(阈值throttled) , 解决方案是     使用 higher tier   增加 replicas   LUIS:  即可以和用户进行自然语言交流, 又可以从会话中获取相关的详细信息 ( revelant 和 detailed ), 最快创建一个LUIS 聊天机器人模型的步骤     Add a new app, 增加一个新的应用   add sample utterances, 增加用户发送的信息的例子   train the app, 训练app   publish the app, 发布app    utterance     翻译成其他语言         把模型导出为 LU 格式     运行 Ludown 翻译命令          用户输入的语句可以在 utterance page 找到    容器     要用 GZIP 格式导出 LUIS 模型到容器   可以使用 trained 或者 published LUIS 模型导出到容器   把 LUIS 应用通过 Azure Container Instance (ACI)运行到容器中的三个步骤         把 LUIS 应用 export 成 . gz (GZIP) model 文件             容器需要读取 . gz 文件                upload model file 到 Azue File Shard             Azure file shard 可以被容器 mount 上                用 Azure CLI 和 YAML 部署容器             YAML 中包含了 . gz 文件所在的 shard 的 mount 点                注意: 因为用了 ACI, 所以不需要 docker run, docker pull 这样的命令          用 ducker run 命令启动容器的话, 需要保证 增加 Eula=accept 这个参数    训练好的模型不会自动发布到endpoint, 需要手动 publish 你可以有选择的发布模型到生产还是 stagging slot 生产和staging 环境是彼此独立的, 他们可以有相同的版本, 也可以有不同的版本.  无法识别 multi-words 的解决方案              Clone a new version     add Patten. Any 实体 (用大括号把 multi-words 括起来 )     Train the model     publish to the staging slot           Entity     ML entity         基于 labeled 例子, 从上下文中抽取实体          Regex entity         基于你提供的特定的模式来抽取实体 (正则表达式)          List entity         基于一组固定的同义词          date entity         可以给LUIS 对应的 bot 增加时间提醒功能 (比如对病人吃药提醒)           Intent     无法准确通过 utterance 判断用户意图         enable active learning     correct prediction          intend for location 的 例子 :         Find contacts in London Who do I know in Seattle? Search for contacts in Ukraine           给 LUIS 设置 schema 的四个步骤              identify your domain (定义你的域)     identify your intends (定义你的意图)     create sample utterance for your intends (给每个意图创建用户提的问题的例子)     identify your entities (定义你的实体)           geographyV2 预测模型     ROLE         可以对行盘出发和到达进行区分           重新训练 LUIS 获取状态的方法              request trainning with http POST     request trainning status with http GET           LUIS 做的多语言模型有口音问题, 解决方法     通过 API endpoint 把 NormalizeDiacritics 设置成 True    要限制, 只有 虚拟网络 vnet1 内的资源可以访问 LUIS ta1 的服务, 怎么做?     the virtual network settings for ta1   说明: 为 LUIS 服务 ta1 设置 virtual network    LUIS 的 bot, 获取 账单地址, 用     ML    想要开启 LUIS 的 active learning, 需要:     Add log=true to the prediction endpoint query.     你已经做了一个 LUIS , 用于查找联系人, 现在需要增加一些 intend, 比如 Find contacts in London. , Who do I know in Seattle? , Search for contacts in Ukraine. .  方案是:     You create a new entity for the domain.    说明:         原来是找人, 现在是在一个地方找人, 这个地方就是 domain entity          认知搜索:  概念     Search Unit: 认知搜索的一个计费标准单位   Replica: 分担请求负载的一个 实例   Partition: 读/写操作的一个物理存储和 I/O 的单元   Shard: 是组成 Index 的更小的单位,    创建 认知搜索 indexer 的 4 个步骤     Create a data source   Define an index   Create a indexer   Schedule a indexer    使用认知搜索的步骤     Provision service 提供服务   Create index 创建索引   Load Data 读取数据   Send query and handle response 发送请求, 处理返回    pull 数据 的 Azure 数据源     Azure Table Storage   Azure Data Lake Gen2   Azure Cosmos DB   Azure Blob Storage ?    Index     为 index (knowledge base) 增加酒店名字的步骤         Drop existing index     Create a revised index     Load the index with document          为索引追加(房屋)数据 并重新索引 的 代码         上传新的房屋数据用 IndexDocumentsAction. Upload() 方法     上传后要重新创建索引用 searchClient. indexDocument(entry) 方法          Index 类型         filter             过滤的情景                sortable - 设置成true 就可排序     retrievable - 设置成true 就可搜索 (可返回该字段信息)     facetable - 设置成true, 就返回分类的点击数          为了确保新的测试结果可以满足定义的需求, 你需要用哪个 REST API ?         Run Indexer          扫描图片中的文字进行索引         把这些数据放到多个 virtual folders (虚拟目录).      为每个目录创建一个 indexer.      增加 search units (搜索单元).      在每个 indexer 定义中, 都执行同样的 runtime execution 模式           Indixer     需要创建一个 indexer才能把数据源中的数据值与索引字段关联起来    安全  1. 每个folder 需要有自己的 data source  2. 可以使用 Azure AD 安全组来过滤搜索结果, 让不同领域的人可搜索自己领域的内容.   3. 不需要 access control 为认知搜索使用了 CMK 加密后的影响     索引变大   搜索返回时间变长   Azure Key 是需要提供的    Porperty     把增强的数据存到 knowledge store 中 需要的 property  1. source  2. tableName   把skillset (skillset1) 的增强的数据包含到索引中, 需要的 proeperties         skillSetName     outputFieldMap           query 慢     增加 partition (增加读写速度)    增加 HA (SLA)     Click Scale in Azure portal   把 replica 设置成 3 个   点击保存按钮    应对高负载     Provision and attach 认知搜索到一个 skillset 中    Pricing tier     一旦创建认知服务, 不能修改 pricing tier   basic 比 standard 差   pricing tier 类型在 sku 中设置   文档的 Text extraction 是免费的   Image extraction (图片抽取)是付费的, 1000 张图片 1 美元.     Projection 类型     Table Projection         存文本          Objection Projection         存 JSON          File Projection         存图片           Build-in skills     文本分析         搜索扫描过的图片中的文字     对多语言的文档进行翻译 (再搜索)     从 PDF 格式的内容中抽取文本     检测语言     分析 sentiment          Computer Vision         OCR          不提供的服务         Bing 搜索     Form Recognizer           搜索结果排名     Scoring profile         让新加的产品想排名靠前 (date)           Endpoint URL     https://csearch1. search. windows. net/index/index1/docs/autocomplete?api-verison=20200630   认知服务名字放最前面   index1 放在 index 后面   doc 是 autocomplete    给一个搜索服务创建 key 的 API URL     https://management. azure. com/subscriptions/{订阅号}/resourceGroups/{GroupName}/providers/Microsoft. Search/searchService/Search1/createQueryKey/Clientapp1?api-version=2020-08-01         management. azure. com 是管理服务, 创建key 用这个服务     Microsoft. Search 是搜索服务, 包含了认知搜索     createQueryKey 用来创建 key           同义词 synonym Map     synonym map 用的 explicit mapping rule , 不是 equivalency rule   大小写不敏感, 所有同义词在匹配前都会变成小写   只能通过 认识搜索的 RestAPI 来定义 synonym map, 不能通过 Azure portal 来定义.     认知搜索的table project 是存文本的, 可以有效的和     Power BI 结合, 方便索引    要让认知搜索可以对 某个字段进行索引, 并可以搜索到, 可以返回, 可以过滤, 那么:     retrievable - 设置成true 就可搜索 指定字段   facetable - 设置成true, 可以提供过滤 (filter)   Searchable - 设置成true 就可以返回指定的 字段   Speech:  API     SDK 连接 speech 服务需要 Location 和 Key         不需要 endpoint URL     Location 也是 Region, 比如 eastus     Speech Service 不需要 endpoint, 就可以使用 , 因为 endpoint 只是一个 token           Speech to Speech     初始化的顺序                      SpeechTranslationConfig       SpeechRecognizer       SpeechSynthesizer                     语音文件的位置         AudoOutputConfig           speech to text     如果没有特殊指定, 就是用电脑麦克风作为默认输入   获取用户 intention         LUIS     Patten Matching          长时间的录入语音, 并识别某个文字         speechsdk. SpeechRecognizer     productlist. addPhase( productA )     speech_recgonizer. start_continues_recognition()          短暂录入         once recognition          提高转化能正确率         Additional marketing text document     Calls transcribed by human (人工转录的通话)          获取容器资源的命令         docker pull mcr. microsoft. com/azure-cognitive-service/speechservices/speech-to-text          代码中指定Speech to text 的翻译的 目标语言都是:         2 个字母的简称, 比如 fr, de, es     都会用 TranslationRecognizer 方法           text to speech     步骤         Text input     Text Analyzer     Neural acoustic model     Neural Vocoder     Audio output          提高语调         SSML          指定语音名字         SpeechSynthesisVioceName , 比如林志玲          加快翻译速度         AudioDataStream          提高语音质量         Tunning File (在线工具)          获取容器资源的 URL         mcr. microsoft. com/azure-cognitive-service/speechservices/text-to-speech           语音翻译成的语言 的 SDK     SpeechTranslationConfig    想要给 speech 模型做训练, 要:     上传一个 . zip 包, 包含 . wav 文件和对应的 文本 transcript   Translator:  API     https://api. cognitive. microsofttranslator. com/translate?api-version=3. 0&amp;from=zh-hans&amp;to=en&amp;toscript=Latn   说明:   1.  源语言 - From  2.  目标语言 - to  3.  记录成的文本类型 - toScript    文本翻译, 要求文本内容需要保留在美国, 那么 API 调用就是:     https://api-nam. cognitive. microsofttranslator. com/translate   说明:         nam 表示美国     apc 表示亚洲     eur 表示欧洲     api 表示 global           参数     profanityAction         delete, 可以去掉翻译中的攻击性文字          includeAlignment         对齐翻译前和翻译后的文字格式          dynamic dictionary         翻译转悠名词          notranslate tag         某些单词不用翻译          Text Analytics:  减少 API 请求次数的方法     用 /analyze 这个 endpoint   异步请求 API    语言检测     confidenceScore 是从 0-1, 不0-100   如果无法分辨文本中的语言, 有默认的语言, 是 us   不储存检测结果    Sentiment     API         eastus. api. congitive. microsoft. com/text/analytics/v2. 0/sentiment          部署了一个 sentiment 分析的容器, 那么:         http://localhost:5000/status 可以验证 API key 是否好用     The container logging provider will write log data.      http://localhost:5000/swagger 可以查看访问 API 的详情           Key phrases     抽取 key phrases 的步骤         组织 POST 请求     Call https://ta1. cognitiveservices. azure. com/text/analytics/V3. 1-preview. 5/analyze endpoint.      在 app 中处理 JSON 反馈          返回结果不会包含 confidence level    Sentiment     可以获得人们的态度的认知服务         Key phrase extraction     Sentiment analysis          运行容器的命令         docker run -rm -it -p 8080:5000 --memory 2g --cpus 1 mcr. microsoft. com/azure-cognitiveservices/textanalytics/sentiment Eula=accept Billing=https://sentiment1. cognitiveservices. azure. com ApiKey=xxxyyyzzz123     Docker 命令参数说明:             8080 是 VM 端口, 5000 是容器端口       Billing 是 Endpoint       Eula 是 end user 同意了 license       -p 端口       -rm 选项，在容器退出时就能够自动清理容器内部的文件系统。       -it 以交互模式运行 Docker                     分析结果         如果 结果是 mixed with a confidence score 0. 9 那么 sentiments 组合是             至少有一个是 positive , 其他是 negative                     Video analyzer:  几个概念     Shots - 镜头         同一个摄像机在同一个时间连续的拍摄帧     视频索引器将视频拆分为的临时区段          Keyframes - 关键帧         Keyframe 一个shot 中最有代表性的帧 ，根据对比度和稳定性等属性进行选择。          Scenes - 场景         几个连续的镜头(shots) 组成的一个语义关联的单个事件           几种模型     Brands Model         识别产品, 服务和公司          Person Model         识别名人          Language Model         识别行业术语和特定单词           找人步骤     在现有的员工 Person 模型中增加一个Person, 并包含名字   上传一张面部照片给这个 员工   reindex employee model    根据视频中出现者搜索视频     创建一个 person model   把这个 model 和视频进行关联    找品牌     让一个品牌可以/不可以被找到         enable: false/true           搜索结果     会返回匹配到的内容在视频里的时间点    测试一个检测人的样例视频三个步骤     sign in 视频分析网站 (www. videoindexer. ai)   点击上传按钮 (上传视频)   检查邮箱并点击链接 (因为这里要花的视频分析时间可能会长, 等分析完了会发邮件给你)    通过 API 分析视频的正确流程是:     上传视频到 blob storage   用 Video Indexer APi 对视频进行索引   从 Video Indexer API 抽取 transcript   通过 translator API 对脚本进行翻译   Custom Vision:  概念     Precision 精度         识别了 100 个鸟, 正确识别了 56 个          Recall 回忆         有 100 个鸟, 识别了 56 个是鸟           domain 的区别     food         食品          product on shelves         货架上的商品, 适合超市          Retail         适合衣服类的日用品          (compact)         边缘侧的, 放容器的方案           目标检测     步骤:         创建一个 custom vision resource     创建一个 Object Detection Project     上传图片并给图片打标签     训练模型     获取 预测模型的 URL 和 key 给应用使用.           提高检测准确率方法         增加一些带有不同背景的对象的图片, 可以改进 model 的 performance - 正确     使用提交到生产的新图片对模型进行训练 - 正确          训练效果不佳 的解决办法         上传新的多个图片,     使用 smart labeler,     重新训练模型,     发布最后的 iteration           分类     步骤:         创建项目     上传照片     给图片打tag     训练项目           容器 / 边缘侧 /edge     domain 中带有 compact (紧密) 的就是支持容器的   一个 custom vision 的 训练好的检测模型用了 General domain, 无法连接外网的情况下, 想要导出模型, 怎么办?         把 domain 改成 General(compact)     重新训练模型     导出模型           API     GetIterationPerformance         获得模型的准确率          GetImagePerformance         获取的是被标记了某个tag 的所有图片          GetProject         返回的是 project 的名字          ExportIteration         只是返回了一个模型, 不包含 metrics           和 TensorFlow 整合     需要两个文件         lable. txt     model. pb           要把一个 Custom Vision 模型放到手机端, 需要做成容器化(compact) , 就需要:     change the model domain   retrain the model   Test (optional)   export the model    如果想把一个 Custom Vision 模型 要从开发环境 导入到 生产环境, 那么     GetProjects on acvDEV   ExportProjects on acvDEV   ImportProjects on avcPROD    要识别生产线上的产品内容的失败, 用 Custom Vision 的三个步骤:     创建项目   上传照片和tag   训练 classifier 模型    给花的分类模型新增加一种新的花, 不需要重新建一个模型, 只需要 在现有模型上     上传图片, 并给图片标注新的tag 名字   训练   发布    要用 1000 张没有任何相关信息的照片 做一个 custom vision 模型, 最短时间可以完成的方法:     Upload all images   Get suggested tags   Review the suggestion and confirm tags   Computer Vision:  API     识别品牌和名人         https://cv1. cognitiveservices. azure. com/vision/v3. 2/alanyze?visualFeature=Brands&amp;detail=Celebrities&amp;Language=en&amp;model-version=latest           Read 和 OCR     可识别的图片类型, 大小及分辨率         JPEG 和 PNG 类型图片, 不超过 30M     GIF 类型 的 2M 文件, 分辨率在 200 x 300     BMP 类型图片 分别率大于 100x100 像素          API         eastus. api. cognitive. microsoft. com/version/v2. 0/ocr     其他Computer Vision API 请求地址             图片分析: https://{endpoint}/vision/v3. 2/analyze[?visualFeatures][&amp;details][&amp;language][&amp;model-version]       图片描述: https://{endpoint}/vision/v3. 2/describe[?maxCandidates][&amp;language][&amp;model-version]       对象检测: https://{endpoint}/vision/v3. 2/detect[?model-version]       获取兴趣点: https://{endpoint}/vision/v3. 2/areaOfInterest[?model-version]       获取分析结果: https://{endpoint}/vision/v3. 2/read/analyzeResults/{operationId}       创建缩略图: https://{endpoint}/vision/v3. 2/generateThumbnail[?width][&amp;height][&amp;smartCropping][&amp;model-version]       获取模型列表: https://{endpoint}/vision/v3. 2/models       密集复杂多页文本读取(高级 OCR): https://{endpoint}/vision/v3. 2/read/analyze[?language][&amp;pages][&amp;readingOrder][&amp;model-version]       识别特定域中的图像内容: https://{endpoint}/vision/v3. 2/models/{model}/analyze[?language][&amp;model-version]       根据图像中的对象、生物、风景或操作返回标记: https://{endpoint}/vision/v3. 2/tag[?language][&amp;model-version]                     C# 代码         GetReadResultAsync 是要通过异步的获取图片中读到的结果信息     while 循环的逻辑是 如果状态还是在 运行中或者是还没开始, 那么就继续循环的打印输出读到的信息, 所以判断逻辑是 running 和 NotStarted           场景     识别商场排队结账人数         Spatial Analysis          上传照片, 防止 offensive         Content Moderator             属于 Decision API                      Form Recognizer     分析结果不能导出为 XML 格式. 可以支持的格式是 JSON 和 CSV   Form recognizer 可以 从 web URL 中获取 发票   Form recognizer 可以使用 F0 免费处理 2 页的发票   S0 可以处理 2000 页的 PDF 或 TIFF 的文档   Form Recognizer 可接受的文件格式和大小分别是         JPG, PDF, PNG, TIFF     文件大小小于 50M          定制化表格识别模型用 Blob stroage 上的文件进行训练的步骤         用Shared Access Signature 来连接到 Azure 存储     创建一个项目, 输入 endpoint 和 key     run layout     apply tags to the forms     train the model          创建一个定制化模型的步骤         assemble a training dataset with PDF document             收集需要被训练的 PDF 文档                upload training dataset to Azure Blob storage             上传训练数据集到 blob                train the model using the sample labeling tool             用 labeling tool 训练模型                     获取容器的 URL         mcr. microsoft. com/azure-cognitive-services/custom-form/labeltool说明:             mcr: microsoft container repository       custom form: Custom Vision Form Recognizer       labeltool 是 表格识别器的 镜像名字                     Form Recognizer 要读取收据, 要用         begin_recognize_receipts_from_url 方法用来读取收据     其他: begin_recognize_content_from_url 方法用来读取表格           VisualFeature 参数 包含了     Tags :         to generate a list of words for the recognizable objects in the image     为图片中识别出来的对象创建一个文字列表          Categories:         使用分类学来描述图片建筑, 食品, 室外 和动物          Adult         detect if the content violent scenes     检测是否有暴力内容场景          Brand         检测图片中知名的品牌和 logo          Description:         generate a complete sentence that describes the image     完整的句子描述图片          Object         detect objects within the image and their location     检测图片中的对象和他们的位置          其他:         名人和地标不是 visural feature, 而是 detail 参数的值           Detail 参数包含了     landmark 地标   celebrity 名人    通过 Rest API 获取图片信息的步骤     创建一个 computer vision 资源, 并获取key   在 quick start 下面进入 API console   使用 Detect Object API创建和运行 CURL 命令    API     创建缩略图         https:cv1. cognitiveservices. azure. com/vision/v3. 1/generateThubmail     https:uksouth. api. cognitive. microsoft. com/vision/v3. 1/generateThubmail             说明:                 如果指定了服务的名字(cv1) , 后面要接 cognitiveservices. azure. com         如果制定了 region, 后面要接 api. cognitive. microsoft. com                                    获取图片的描述和 landmark         curl https://westus. api. cognitive. microsoft. com/vision/v2. 0/analyze?visualFeature=Description&amp;detail=Landmark -H  Ocp-apim-subscription-key:abcdefg12345hijklmnop67890  -H  content-type:applicaiton/json  -d  {'url':'https://images. com/travel_picture. jpeg'}            图片分析中的 captions 就是 tag 的意思     C# 的语法 results. Description. Captions   Python 的语法: tags_result_remote. tags   脸部识别 Face API:  概念     Person         一个人的模型, 包含这个人的多张照片          PersonGroup         所有人的照片, 最多支持 1000 人, 需要额外训练时间          LargePersonGroup         所有人的照片, 最多支持 100 万人, 需要额外训练时间          PersonDirectory         所有人的照片, 可以支持 7500 万人, 不需要训练新的脸          DynamicPersonGroup         按条件过滤后的脸部信息          Location         在图片中识别到的面部的方形坐标, 包含了 top, left, height, width (方形脸部在距离图片顶部, 左侧的距离, 方形的高度, 宽度)           场景     Face Verification (Verify API)         大门刷脸          Face identification (Identify API)         识别一张图片中的多个脸 , 比如谁参加了第一堂课          Detect API         是检测, 检测上课的同学的面部表情参数信息来了解学生的参与度(engagement)          Find similar face         找到和目标脸相似的脸     . face. detect_with_url 检测图片中的脸, 并把每个脸的数据返回     . face. find_similar 在多个脸的数据中匹配某一个脸          瞳孔跟踪         通过 DetectWithStreamAsync 方法获取瞳孔信息     把 returnFaceLandmarks 参数设置成 true           . net 识别人脸的代码步骤     Initialize variables and helper functions   Authorize API call   Create Marketing PersonGroup   Create person   Add faces    面部匹配,     如果小于 1000 张图片, 用 faceListId,   如果 1000 - 100,000, 用 largeFaceListId    通过一个人的样例照片找到这个人的所有照片     API 用 findSimilars   mode 用 matchPerson    Face API 的 detection 模型用了 detection_01 发现多模糊的照片或侧脸无法识别, 那么方法是:     用其他模型         detection_02     detection_03           用 Face API 查看摄像头前面的人是否是真人, 最好的方式:     FaceAttributes. HeadPose   认知服务 Policy / 日志 / Principle:  关于认知服务的 privacy policy (隐私政策) , 下面的说法:     用文本分析服务分析的文本会被自动删除   面部识别服务识别的数据可以删除   发送给 LUIS 的 Utterance (用户的提问) 默认不会保存, 如果日志打开了, utterance 可以保存 30 天.     你在给认知服务配置诊断日志, 几个场景匹配如下:     发送日志到一个 repository, 可以通过 log query 访问         Log Analytics workspace          把数据以流的方式发送到外部的日志分析方案.          Event Hub          归档日志, 用来审计, 静态分析, 备份. 以 JSON 文件形式保存         Azure Blob storage           关于给学生在社区大学进行工作推荐要使用 认知服务, 会牵扯到几个 AI principle. 下面的匹配关系是:     工作推荐不会受到学生社会背景的影响 - Fairness 公平   学生的学术数据, 兴趣爱好等信息不会被学校外部访问 - Privacy and Security 隐私与安全   AI的行为会通过 guideline 解释给学生, 让他们找到更好的工作 - Transparency 透明    您正在开发一个新的销售系统，该系统将处理来自面向公众的网站的视频和文本。 您计划监视销售系统，以确保无论用户的位置或背景如何，它都能提供公平的结果。 哪两个负责任的 AI 原则提供指导以满足监视要求？每个正确答案都表示解决方案的一部分。 注意：每个正确的选择都值一分。     fairness 公平   inclusiveness 包容    如果要让对话机器人更加 包容 (inclusiveness) , 那么就要增加:     Direct line speech   说明:         包容就是让残疾人也能用, 比如不能打字的人, 小孩等, 靠说话也可以和机器人对话, 这就是包容.            要用情感分析来决定客服人员的奖金, 怎么做可以满足 responsible AI 的 principles?     再决定财务奖金之前, 增加一个人工review.    安全:  为了让多个医疗扫描中心使用 Custom Vision 服务并防止数据泄露, 你需要配置 Azure Policy. 三个步骤     导航到 Azure Policy , 点击 Assign Policy   选择你的 Custom Vision 对应的 subscription 和 resource group   选择认知服务账号, 关闭 public network access policy.     认知服务和其他 Azure 服务整合, 需要确保只能内网访问, 只能接受来自 vnet 的请求, 那么:     在 Firewall and virtual networks 下面把 Allow 设置成 Disable   创建一个 private endpoint    Key 1 泄露了     update your app to use key2   reset your key1   update your app to use key1    重置key2 的 API     POST https://management. azure. com/subscriptions/18c51a87-3a69-47a8-aedc-a54745f708a1/resourceGroups/RG1/providers/Microsoft. CognitiveServices/accounts/contoso1/regenerateKey?api-version=2017-04-18 Body{“keyName”: “Key2”}    要收集大量的 sensor 的 IOT 数据, 并需要 检测异常, 定义root cause, 发送 incident alert, 那么方案是     Azure Metrics Advisor    您正在开发一个监控系统，该系统将分析发动机传感器数据，例如转速、角度、温度和压力。系统必须生成警报以响应非典型值。     Multivariate Anomaly Detection   DevOps:  ARM     你想要用 ARM (Azure Resource Manager) template 来重复批量的部署多个 认知服务, ARM 模板文件是什么格式的?         JSON           通过 API call 创建资源, 要用 PUT     PUT https://management. azure. com/subscriptions/{subscriptionId}/resourcegroups/{resourceGroupName}/providers/{resourceProviderNamespace}/{parentResourcePath}/{resourceType}/{resourceName}?api-version=2021-04-01    要做一个异常检测的容器版模型, 并且要保证容器的历史命令行数据不会泄露, 并且要使用 Azure 的 RBAC, 所以建容器的步骤是:     Pull the Anomaly Detector container image.    Create a custom Dockerfile.    Push the image to an Azure container registry.    Distribute a docker run script.    Cost:  budget alert     如果 设置了 600美金 是 Actural 类型的阈值, 那么如果某个月真的花超了 600 才会收到报警   如果 设置了 900 是 Forecase 类型的阈值, 还没到 900, 就会收到报警邮件了, 会提前发送报警.     5 万张图片要做 OCR 和 文本分析, 需要用什么服务最省钱:     S0 的认知服务资源(容纳多种服务的)   "
    }, {
    "id": 12,
    "url": "http://localhost:4000/ChatGPT-and-OpenAI-Python-API-Bootcamp/",
    "title": "Use ChatGPT in Python",
    "body": "2023/05/09 - ChatGPT API 是一个应用程序编程接口（API），由 OpenAI 提供，使开发人员能够将强大的 ChatGPT 模型无缝集成到他们的应用程序中。 这种大型语言模型(LLM)拥有能够理解自然语言并生成类似人类的回应。要在 Python 中使用 ChatGPT API，必须遵循一系列一般步骤。 我将带您了解如何在应用程序中使用 ChatGPT API Python，从在 OpenAl 网站注册到 Python 编程语言。 Use ChatGPT in Python[TOC] 资源:  copy. ai - 类似 ChatGPT 的一个 AI tome. ai - 写 ppt 比较好用 I want to create several slides to present how DevOps can help a luxury company and bring values.  GPT-4 API Wating List 极光 KVM 美国服务器购买     主机名: JG2303163OOB   IP: 154. 40. 40. 31   端口: 3000   用户名: root   主机密码:    面板密码: xxxxxx   账号: xxx_xxx@hotmail. com   密码: xxxxxxxxxxxx   您的订单号是： 9110312237   API 访问: http://154. 40. 40. 31:3000/v1/chat/completions   API Key:    Orgnization Name: Personal   Orginization ID: org-W592UZabrT97qTd9rqSIZFO3    如何设置 Postman  telegram bot 的名字: t. me/longerlibot     用户名是手机号码   Token:    ChatGPT 提问技巧:  问问题的时候如果能给出问问题的人, 以及听众, 答案会更有针对性 大段文章, 可以让 ChatGPT 帮你总结 可以给一个 有文字的 URL, 然后让 GPT 帮你翻译或者总结想要了解某个不熟悉的领域: ChatGPT, what are 15 questions that I could ask ChatGPT to help me build a online reselling business?How can I use ChatGPT to help me build my landscaping business 面试: I have an interview tomorrow for a software sales position. Can you give me ten questions I am likely to be asked. I have an interview tomorrow for Imperial College Undergraduate entrance interview for chemistry. Can you give me ten questions I am likely to be asked, and give each question a good response?Give me a good example of a good response of the following queztions 把问题考过来Chatgpt, 你可以模拟一个DevOps 工程师的面试吗? 面试官会问相关的问题, 而面试者会进行良好的回答. ChatGPT, can you emulate an roleplay of a Imperial College interview for a student who love chemistry?Can you pretent to be a interviewer interviering me foa an account executive position for a marketing agency? 找工作:  把工作经历(不包括总结部分)拷贝到 GPT , 让 GPT 帮你写 summary based on the resume I provided you, what kinds of job be good for me to look for list 10 companys that hire xxx job保持身体健康: 我 身高 183cm, 体重 80 公斤, 年龄 46 岁, 希望能够在 1 个月内减掉 5 斤, 请给我提供一个月的减重 calendar 开发: how can chatgpt be used to help programmers代码生成write me a python funciton that will allow a user to play rock paper scissors against the computer. Have the function prompt the user for an input, randomly generate a response and let them know if they won or lost. debugcan you identify the error in this code? OpenAI 开发教程: 基本概念:  word - 一个单词 token     我们输入文字给 GPT , GPT把文字转成 token, 再转成 victor   因为空格, 分隔符, 长文字分割等情况, 750 个 word 大概等于 1000 个 token (一个文字大概等于 1. 33 个 token)    vactor     在神经网络中token 会被 encode 成 victors 被使用   GPT3 大概有12288 个向量    Context Windows     上下文越大, 记忆力越好   GPT 3 有 2048 个 上下文 token   GPT3. 5 有 4000 个上下文token   GPT4 有 8192 个上下文token   GPT4-32k 版本有 32768个上下文    GPT3 的运行模式     Word text is converted to token set.    Tokens are converted to vectors.    Vectors are passed into GPT-3 and the model returns a probability distribution across the vocabulary of vectors.    We can then specify via “temperature” which vector should appear next (which can then be reversed back into text).     DALL-E 定价API 中常用的一些参数  Model     不同的模型, 功能不一样, 价格不一样   比如 GPT3, 3. 5, 4 的模型, 比如 聊天模型和 completion 模型, 比如 图片生成模型等    Prompt     问 API 的问题    Temperature     温度   越高, 回答的答案越有创意   越低, 回答的越保守, 越倾向于正确答案    Top P     P 是 posibility 概率   和温度类似, 越高, 回答越发散   越低越趋于保守的答案    N     默认是 1   和同一个问题问 N 次是一个概念, 可能会带来更多的 token 消耗, 花更多的钱    Frequency Penalty     -2 到 +2   越高结果越多样化, 越低, 多个答案可能会越有重复的感觉    Presence Penalty     -2 到 +2   越高越愿意讨论新的话题, 越愿意根据之前的话题进行回顾和讨论    Python 的 openai 库用法:     安装```bashpip install –upgrade openaivi ~/. zshrc    # 把这行加入到文件最下面 export OPENAI_API_KEY=’'source ~/. zshrc # 让变量永久生效       123456789101112131415161718192021222324使用```pythonimport openaiimport os# os. environ[ OPENAI_API_KEY ] = '&lt;your own key&gt;'openai. api_key = os. getenv( OPENAI_API_KEY )openai. api_base='http://154. 40. 40. 31:3000/v1' # 改成代理服务器# 下面代码按具体需要来调整# list modelsmodels = openai. Model. list()# print the first model's idprint(models. data[0]. id)# create a completioncompletion = openai. Completion. create(model= ada , prompt= Hello world )# print the completionprint(completion. choices[0]. text)NLP to SQL (项目一):  需求:     通过 penda 把一个 CSV 文件读到内存中转化昵称 SQL database, 可以通过SQL读取数据   通过OpenAPI 把自然语言翻译成 SQL 查询语句, 然后再把 SQL 发给数据库查询数据   目标是把每个月的销售总额翻译成 Select POSTALCODE, SUM(SALES) FROM Sales GROUP BY POSTALCODE   一共三个文件  db_utils. py```pythonfrom sqlalchemy import create_enginefrom sqlalchemy import textdef dataframe_to_database(df, table_name):  “ ”Convert a pandas dataframe to a database.     Args:      df (dataframe): pd. DataFrame which is to be converted to a database      table_name (string): Name of the table within the database    Returns:      engine: SQLAlchemy engine object  “””  engine = create_engine(f’sqlite:///:memory:’, echo=False)  df. to_sql(name=table_name, con=engine, index=False)  return engine def handle_response(response):  “ ”Handles the response from OpenAI. 12345678910Args:  response (openAi response): Response json from OpenAIReturns:  string: Proposed SQL query   query = response[ choices ][0][ text ]if query. startswith(   ):  query =  Select + queryreturn querydef execute_query(engine, query):  “ ”Execute a query on a database. 12345678910Args:  engine (SQLAlchemy engine object): database engine  query (string): SQL queryReturns:  list: List of tuples containing the result of the query   with engine. connect() as conn:  result = conn. execute(text(query))  return result. fetchall() ``` - 第二个文件 openai_utils. py ```python import openaidef create_table_definition_prompt(df, table_name):  “ ”This function creates a prompt for the OpenAI API to generate SQL queries. 12345678910Args:  df (dataframe): pd. DataFrame object to automtically extract the table columns  table_name (string): Name of the table within the database  Returns: string containing the prompt for OpenAI   prompt = '''### sqlite table, with its properties: # # {}({}) # '''. format(table_name,  , . join(str(x) for x in df. columns))return promptdef user_query_input():  “ ”Ask the user what they want to know about the data. 12345Returns:  string: User input   user_input = input( Tell OpenAi what you want to know about the data:  )return user_inputdef combine_prompts(fixed_sql_prompt, user_query):  “ ”Combine the fixed SQL prompt with the user query. 123456789Args:  fixed_sql_prompt (string): Fixed SQL prompt  user_query (string): User queryReturns:  string: Combined prompt   final_user_input = f ### A query to answer: {user_query}\nSELECT return fixed_sql_prompt + final_user_inputdef send_to_openai(prompt):  “ ”Send the prompt to OpenAI 1234567891011121314151617Args:  prompt (string): Prompt to send to OpenAIReturns:  string: Response from OpenAI   response = openai. Completion. create(  engine= code-davinci-002 ,  prompt=prompt,  temperature=0,  max_tokens=150,  top_p=1. 0,  frequency_penalty=0. 0,  presence_penalty=0. 0,  stop=[ # ,  ; ])return response1234567891011121314151617181920212223242526272829303132333435363738394041- 第三个文件 main. py```python# Dataset based on #https://www. kaggle. com/datasets/kyanyoga/sample-sales-dataimport osimport loggingimport pandas as pdimport openaiimport db_utilsimport openai_utilslogging. basicConfig(format='%(asctime)s - %(message)s', level=logging. INFO)openai. api_key = os. environ[ OPENAI_API_KEY ]if __name__ ==  __main__ :  logging. info( Loading data. . .  )  df = pd. read_csv( data/sales_data_sample. csv )  logging. info(f Data Format: {df. shape} )  logging. info( Converting to database. . .  )  database = db_utils. dataframe_to_database(df,  Sales )    fixed_sql_prompt = openai_utils. create_table_definition_prompt(df,  Sales )  logging. info(f Fixed SQL Prompt: {fixed_sql_prompt} )  logging. info( Waiting for user input. . .  )  user_input = openai_utils. user_query_input()  final_prompt = openai_utils. combine_prompts(fixed_sql_prompt, user_input)  logging. info(f Final Prompt: {final_prompt} )  logging. info( Sending to OpenAI. . .  )  response = openai_utils. send_to_openai(final_prompt)  proposed_query = response[ choices ][0][ text ]  proposed_query_postprocessed = db_utils. handle_response(response)  logging. info(f Response obtained. Proposed sql query: {proposed_query_postprocessed} )  result = db_utils. execute_query(database, proposed_query_postprocessed)  logging. info(f Result: {result} )  print(result)Auto Exam Creator (项目二):  需求:     做一个模板, 我们要为学校的老师要考试的主题, 单选还是多选题, 问题个数, 答案个数, 以及指出来哪一个是正确答案   还要开发一个考试交互界面, 让学生可以进行答题. 要循环出来每一道题, 然后让学生输入选择的答案   应用通过对比正确答案和学生输入的答案给学生一个分数   把最后的得分记录下来   一共 4 个文件  main. py1234567891011121314from teacher import Teacherfrom exam_simulator import Exam### Create an instance of the Teacher class which asks the user about the topic, number of possible answers per question, and number of questions. teacher = Teacher()student_view, answers = teacher. create_full_test()### Create an instance of the Exam class which runs the exam simulation and grades the exam. exam = Exam(student_view, answers, store_test=True, topic=teacher. test_creator. topic)student_answers = exam. take()print(student_answers)grade = exam. grade(student_answers)print(grade) teacher. py1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162import loggingfrom test_creator import TestGeneratorlogging. basicConfig(format='%(asctime)s - %(message)s', level=logging. INFO)dummy_test = '\n\nQ1. What is the syntax for declaring a variable in Python?\nA. #variable\nB. var variable\nC. !variable\nD. var = variable\nCorrect Answer: D. var = variable\n\nQ2. What type of language is Python?\nA. Interpreted\nB. Compiled\nC. Assembly\nD. Machine\nCorrect Answer: A. Interpreted\n\nQ3. What type of loop is used when a set of instructions need to be repeated until a condition is met?\nA. For loop\nB. While loop\nC. Do-while loop\nD. If-else loop\nCorrect Answer: B. While loop\n\nQ4. What is the result of the following expression?\n2 + 5 * 3\nA. 23\nB. 17\nC. 11\nD. 25\nCorrect Answer: B. 17'class Teacher:  def __init__(self):    print( Welcome! Run create_full_test() to create a test.  )  def create_full_test(self):    topic = input( What topic would you like to create a test on?  )    num_possible_answers = int(input( How many possible answers would you like to have?  ))    num_questions = int(input( How many possible answers per question would you like to have?  ))    self. test_creator = TestGenerator(topic, num_possible_answers, num_questions)    test = self. test_creator. run()    #test = dummy_test    #logging. info(test)    student_view = self. create_student_view(test, num_questions)    answers = self. extract_answers(test, num_questions)    return student_view, answers  def create_student_view(self, test, num_questions):    student_view = {1 :   }    question_number = 1    for line in test. split( \n ):      if not line. startswith( Correct Answer: ):        student_view[question_number] += line+ \n       else:                if question_number &lt; num_questions:          question_number+=1          student_view[question_number] =       return student_view  def extract_answers(self, test, num_questions):    answers = {1 :   }    question_number = 1    for line in test. split( \n ):      if line. startswith( Correct Answer: ):        answers[question_number] += line+ \n         if question_number &lt; num_questions:          question_number+=1          answers[question_number] =       return answers    if __name__ ==  __main__ :  teacher = Teacher()  student_view, answers = teacher. create_full_test()  print(student_view)  print(answers) exam_simulator. py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354import datetimefrom teacher import Teacherclass Exam:  def __init__(self, student_view, answers, store_test=False, topic=  ):    self. student_view = student_view    self. answers = answers    if store_test:      self. store_test(topic)    def take(self):    answers = {}    for question, question_view in self. student_view. items():      print(question_view)      answer = input( Enter your answer:  )      answers[question] = answer    return answers  def grade(self, answers):    correct_answers = 0    for question, answer in answers. items():      if answer. upper() == self. answers[question]. upper()[16]:        correct_answers+=1    grade = 100 * correct_answers / len(answers)    if grade &lt; 60:      passed =  Not passed!     else:      passed =  Passed!     return f {correct_answers} out of {len(answers)} correct! You achieved: {grade} % : {passed}   def store_test(self, topic):    with open(f'Test_{topic}_{datetime. datetime. now(). strftime( %Y-%m-%d_%H-%M-%S )}. txt',  w ) as file:      for question, question_view in self. student_view. items():        file. write(question_view)        file. write( \n )        file. write(self. answers[question])        file. write( \n )if __name__ ==  __main__ :  teacher = Teacher()  student_view, answers = teacher. create_full_test()  exam = Exam(student_view, answers, store_test=True, topic=teacher. test_creator. topic)  student_answers = exam. take()  print(student_answers)  grade = exam. grade(student_answers)  print(grade) test_creator. py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657import osimport openaiopenai. api_key = os. environ[ OPENAI_API_KEY ]class TestGenerator:  def __init__(self, topic, num_possible_answers, num_questions):    self. topic = topic    self. num_possible_answers = num_possible_answers    self. num_questions = num_questions    if self. num_questions &gt; 6:      raise ValueError( Attention! Generation of many questions might be expensive! )    if self. num_possible_answers &gt; 5:      raise ValueError( More than 5 possible answers is not supported! )  def run(self):    prompt = self. create_prompt()    if TestGenerator. _verify_prompt(prompt):      response = self. generate_quiz(prompt)      return response[ choices ][0][ text ]        raise ValueError( Prompt not accepted.  )  def generate_quiz(self, prompt):    response = openai. Completion. create(engine= text-davinci-003 ,                      prompt=prompt,                      max_tokens=256,                      temperature=0. 7)    return response      @staticmethod  def _verify_prompt(prompt):    print(prompt)    response = input( Are you happy with the prompt? (y/n) )    if response. upper() ==  Y :      return True    return False  def create_prompt(self):    prompt = f Create a multiple choice quiz on the topic of {self. topic} consisting of {self. num_questions} questions.   \         + f Each question should have {self. num_possible_answers} options.  \         + f Also include the correct answer for each question using the starting string 'Correct Answer: '.      return prompt    if __name__ ==  __main__ :  gen = TestGenerator( Python , 4, 2)  response = gen. run()  print(response)  Auto Recipt Creator (项目三):  需求: 回家看到冰箱里有一堆菜和原料, 想要根据现有食材做个食谱, 并生成一个做好的菜的图片     提供一个原料列表, 先请求 GPT 获取带有标题的一个食谱, 然后在把标题发给 Dall-E 获取食谱照片   三个文件  main. py```pythonfrom recipe import RecipeGeneratorfrom dalle import Dish2ImageCreate an instance of the RecipeGenerator class: gen = RecipeGenerator() Ask the user for input and create the recipe: recipe = gen. generate_recipe() Print the recipe: print(recipe) Create an instance of the Dish2Image class: dalle = Dish2Image(recipe) Visualize the dish: dalle. generate_image() Store the recipe in a text file: gen. store_recipe(recipe, f”{dalle. title}. txt”) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667682. dalle. py```pythonimport reimport requestsimport shutilimport openaiclass Dish2Image:  def __init__(self, recipe):    self. recipe = recipe    self. title = Dish2Image. _extract_title(recipe)  def generate_image(self):    prompt = self. dalle2_prompt()    if Dish2Image. _verify_prompt(prompt):      response = Dish2Image. generate(prompt)      image_url = response['data'][0]['url']      return Dish2Image. save_image(image_url, f {self. title}. jpg )    raise ValueError( Prompt not accepted.  )  def dalle2_prompt(self):    prompt = f '{self. title}', professional food photography, 15mm, studio lighting     return prompt  @staticmethod  def _verify_prompt(prompt):    print(prompt)    response = input( Are you happy with the prompt? (y/n) )    if response. upper() ==  Y :      return True    return False  @staticmethod  def _extract_title(recipe):    return re. findall( ^. *Recipe Title: . *$ , recipe, re. MULTILINE)[0]. strip(). split( Recipe Title:  )[1]  @staticmethod  def generate(image_prompt):    response = openai. Image. create(prompt=image_prompt,                    n=1,                    size= 1024x1024                     )    return response  @staticmethod  def save_image(image_url, file_name):    image_res = requests. get(image_url, stream = True)        if image_res. status_code == 200:      with open(file_name,'wb') as f:        shutil. copyfileobj(image_res. raw, f)    else:      print( Error downloading image! )    return image_res. status_codeif __name__ ==  __main__ :       Test Dish2Image class based on the Recipe Title: Savory Tomato Apple Spaghetti       dalle = Dish2Image( Recipe Title: Savory Tomato Apple Spaghetti )  dalle. generate_image() recipe. py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869import osimport openaiopenai. api_key = os. getenv( OPENAI_API_KEY )class RecipeGenerator:    def __init__(self):              self. list_of_ingredients = self. ask_for_ingredients()  @staticmethod  def ask_for_ingredients():    list_of_ingredients = []    while True:      ingredient = input( Enter an ingredient (or type 'done' to finish):  )      if ingredient. lower() ==  done :        break      list_of_ingredients. append(ingredient)        print(f Your ingredients are: {', '. join(list_of_ingredients)} )        return list_of_ingredients  def generate_recipe(self):    prompt = RecipeGenerator. create_recipe_prompt(self. list_of_ingredients)    if RecipeGenerator. _verify_prompt(prompt):      response = RecipeGenerator. generate(prompt)      return response[ choices ][0][ text ]    raise ValueError( Prompt not accepted.  )  @staticmethod  def create_recipe_prompt(list_of_ingredients):    prompt = f Create a detailed recipe based on only the following ingredients: {', '. join(list_of_ingredients)}. \n  \        + f Additionally, assign a title starting with 'Recipe Title: ' to this dish, which can be used to create a photorealistic image of it.      return prompt  @staticmethod  def _verify_prompt(prompt):    print(prompt)    response = input( Are you happy with the prompt? (y/n) )    if response. upper() ==  Y :      return True    return False  @staticmethod  def generate(prompt):    response = openai. Completion. create(engine= text-davinci-003 ,                          prompt=prompt,                          max_tokens=256,                          temperature=0. 7)    return response  def store_recipe(self, recipe, filename):    with open(filename,  w ) as f:      f. write(recipe)if __name__ ==  __main__ :       Test RecipeGenerator class without creating an image of the dish.        gen = RecipeGenerator()  recipe = gen. generate_recipe()  print(recipe)Sentiment Analysis (项目五):  需求: 从一个社交网站(reddit. com)上抓取某个公司/人物/分类相关的文章, 然后通过 GPT 对文章的评论进行 sentiment 分析, 给出结果     中国可以用头条, 抖音代替 ,查看新闻和视频的用户评论, 分析评论情况    结果包括正面, 一般, 负面三个文件  main. py123456789101112131415161718192021222324from pathlib import Pathimport blog_utilsimport openai_utils### Define your paths here#PATH_TO_BLOG_REPO = Path('C/path/to/. git')PATH_TO_BLOG = PATH_TO_BLOG_REPO. parentPATH_TO_CONTENT = PATH_TO_BLOG/ content PATH_TO_CONTENT. mkdir(exist_ok=True, parents=True)#### Define a title and get the blog content from OpenAItitle =  Why AI will never replace the radiologist print(openai_utils. create_prompt(title))blog_content = openai_utils. get_blog_from_openai(title)# Get the cover image and create the blog_, cover_image_save_path = openai_utils. get_cover_image(title,  cover_image. png )path_to_new_content = blog_utils. create_new_blog(PATH_TO_CONTENT, title, blog_content, cover_image_save_path)blog_utils. write_to_index(PATH_TO_BLOG, path_to_new_content)# Update the blogblog_utils. update_blog(PATH_TO_BLOG_REPO) blog_utils. py12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667from pathlib import Pathimport shutilimport osfrom bs4 import BeautifulSoup as Soupfrom git import Repodef create_new_blog(path_to_content, title, content, cover_image=Path( . . /PT Centered Purple. png )):    cover_image = Path(cover_image)    files =len(list(path_to_content. glob( *. html )))  new_title = f {files+1}. html   path_to_new_content = path_to_content/new_title    shutil. copy(cover_image, path_to_content)  if not os. path. exists(path_to_new_content):    with open(path_to_new_content,  w ) as f:      f. write( &lt;!DOCTYPE html&gt;\n )      f. write( &lt;html&gt;\n )      f. write( &lt;head&gt;\n )      f. write(f &lt;title&gt; {title} &lt;/title&gt;\n )      f. write( &lt;/head&gt;\n )            f. write( &lt;body&gt;\n )      f. write(f &lt;img src='{cover_image. name}' alt='Cover Image'&gt; &lt;br /&gt;\n )      f. write(f &lt;h1&gt; {title} &lt;/h1&gt; )      f. write(content. replace( \n ,  &lt;br /&gt;\n ))      f. write( &lt;/body&gt;\n )      f. write( &lt;/html&gt;\n )      print( Blog created )      return path_to_new_content  else:    raise FileExistsError( File already exist! Abort )def check_for_duplicate_links(path_to_new_content, links):  urls = [str(link. get( href )) for link in links]  content_path = str(Path(*path_to_new_content. parts[-2:]))  return content_path in urlsdef write_to_index(path_to_blog, path_to_new_content):  with open(path_to_blog/ index. html ) as index:    soup = Soup(index. read())  links = soup. find_all( a )  last_link = links[-1]    if check_for_duplicate_links(path_to_new_content, links):    raise ValueError( Link does already exist! )      link_to_new_blog = soup. new_tag( a , href=Path(*path_to_new_content. parts[-2:]))  link_to_new_blog. string = path_to_new_content. name. split( .  )[0]  last_link. insert_after(link_to_new_blog)    with open(path_to_blog/ index. html ,  w ) as f:    f. write(str(soup. prettify(formatter='html')))def update_blog(path_to_blog_repo, commit_message= Updated blog ):  repo = Repo(path_to_blog_repo)  repo. git. add(all=True)  repo. index. commit(commit_message)  origin = repo. remote(name='origin')  origin. push() openai_utils. py12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758import osimport requestsimport shutilimport openaiopenai. api_key = os. getenv( OPENAI_API_KEY )def create_prompt(title):  prompt =    Jose's Website Biography I am a Python instructor teaching people machine learning! Blog Jan 31, 2023 Title: Why AI will never replace the radiologist tags: tech, machine-learning, radiology Summary: I talk about the cons of machine learning in radiology. I explain why I think that AI will never replace the radiologist. Full text:   . format(title)  return promptdef get_blog_from_openai(blog_title):  response = openai. Completion. create(engine= text-davinci-003 ,                      prompt=create_prompt(blog_title),                      max_tokens=512, # we increased the tokens to get a longer blog post                      temperature=0. 7)  return response[ choices ][0][ text ]def dalle2_prompt(title):  prompt = f Pixel art showing '{title}'.    return promptdef save_image(image_url, file_name):  image_res = requests. get(image_url, stream = True)    if image_res. status_code == 200:    with open(file_name,'wb') as f:      shutil. copyfileobj(image_res. raw, f)  else:    print( Error downloading image! )  return image_res. status_code, file_namedef get_cover_image(title, save_path):  response = openai. Image. create(    prompt=dalle2_prompt(title),    n=1,    size= 1024x1024     )  image_url = response['data'][0]['url']  status_code, file_name = save_image(image_url, save_path)  return status_code, file_name为代码添加注释 (项目六):  需求:     把python 代码 以 . py 的文件形式输入给 GPT, 让它们我们把注释描述出来, 然后以同一个文件在返回给我们   我们在 GPT 外面包装了文件上传下载功能, 和文件展示功能   "
    }, {
    "id": 13,
    "url": "http://localhost:4000/Prepare-for-TOGAF-Certification/",
    "title": "Prepare TOGAF, The Open Group Architecture Framework Certification",
    "body": "2023/04/05 - TOGAF® 基本上是一套清晰的组织发展规则和实践，旨在帮助组织。它指导企业如何在单一战略中创建、解释、分析和利用不同的要素。该框架还使用了明确定义的术语。这样可以简化部门之间的沟通，并大大减少浪费错误的可能性。就让我带领你揭开他的面纱。 TOGAF[TOC] 资源: TOGIF 9. 2标准Togaf librarysample deliverablesTOGAF 官网在线的 Togaf 9. 2 标准文档 题库: 125 套练习题, 需要 16 美元, 已购买CSDN 上的一些题, 要付费在线的 part1 307 道题在线的 part2 80 道题 考试入口: https://www. opengroup. org/certifications/take-examhttps://home. pearsonvue. com/theopengroup在线考试介绍 重要的英文单词: acronym - 首字母缩写 学习技巧:  跟着视频学一章, 然后回头去手册对应章节讲的内容. 架构概念:  一个企业架构的架构框架 框架: 方法工具和定义 ADM Architecture Development Method (ADM) 架构的ISO/IEC/IEEE 42010: 2011定义     ISO/IEC/IEEE 42010: 2011“The fundamental concepts or properties of a system in its environment embodied in its elements, relationships, and in the principles of its design and evolution. ”    架构的 TOGAF 定义     “The structure of components, their inter-relationships, and the principles and guidelines governing their design and evolution over time. ”    架构的 大白话 解释     一个系统的多个部分和他们的关系, 以及他们的治理原则, 设计和进化   TOGAF 是一个架构框架   我们通过架构框架来指导我们进行架构工作   工作中要用 togaf , 还需要购买 togaf 的 license   TOGAG 认证 与 考试:  有两个级别的考试 Level 1 and Level 2 Or Part 1 and Part 2     level 1 过了可以拿到 TOGAF Foundation 认证   level 1 和 level 2 都过了, 才能拿到完整的 TOGAF 认证    TOGAF Part 1 ● A test of core knowledge of the standard   标准的核心概念 ● A multiple-choice test, only 1 correct answer    多选题, 一个正确答案 ● 40 questions over 60 minutes    40 道题 60分钟的考试 ● Need 55% to pass, or 22 correct out of 40   答对 22 道题算过 ● Each correct answer is worth 1 point   每个题算一分     中英文都有    TOGAF Part 2 ● A test of how you apply the TOGAF Standard   考灵活运用 ● A scenario based test, scored using gradient scoring   基于场景的测试 ● Open book exam   开卷考试, 可以访问考试提供的标准信息 ● 8 questions over 90 minutes   90 分钟, 8 个问题 ● Need 60% to pass ● Must have passed Part 1 to take Part 2   考过了 part1 才能考 part2 可以两个考试合并到一个, 一起考, 费用如下     part1 or part2 - 考试费用 US$360 per test   part1 + part2 一起考 - 考试费用 US$550 for the combined test    考试语言 中英文 考试版本, 还是以 9. 2 为考试基础, togaf 10 还没有考试togaf 9. 2 的标准:  标准分成 6 个部分     Introduction         介绍核心概念和定义     学习的时候要对照中英文手册, 确保概念理解          ADM         架构开发方法     是 togaf 的核心     架构师做架构的流程和迭代方式          Guidelines and techniques         指导方针和技术     应用架构的标准一些材料, 把这些材料为我们自己的场景进行定制化          Architect content framework         结构化的原始模型, 为最终的架构交付物作为原型和基础          Enterprise Continuum         企业连续性     是一个目录编辑模型     当我们把架构设计放到 架构的 repository中, 进行分类管理和版本管理?          Architecture Capability Framework         架构能力框架     为了定义一个企业架构, 我们需要告诉大家这个架构的能力           togaf library     是一个图书馆, 或者叫做材料库   当我们做架构项目, 可以在 library 中找到一些文档材料    生成版本的版本号     0. 1 表示 draft 的第一个版本   1. 0 表示已经被 同意和签字了的版本   Part 1 togaf 介绍 - 核心和基础概念:    官网的togaf 9. 2 标准     7 个核心概念 ● The definition of enterprise ● The architecture domains, or BDAT ● The architecture development method, or ADM ● Deliverables, artifacts and building blocks ● The enterprise continuum ● The architecture repository ● And, defining architecture capability  企业的定义:  “Highest level of description for an organization”     可能是一个 组织, 或者是多个组织的组合         比如麦当劳和他们的生态伙伴,也可以理解为一个大的企业           架构师的工作范围     可能是企业的某个部分的架构设计, 也可能是负责企业和生态的所有架构   四个架构域 BDAT:  企业架构可以分四层     Business, Data, Application, Technology   Business Domain 是一层, 主要是业务架构   DAT 是剩下的一大层, 是技术架构   技术架构再分层 Data, Applicaiton, Technology    Business Domain     由企业执行层来决定的业务单元   定义现在和未来的 business capability    Data Domain     数据域处理逻辑/物理数据资产   处理现在和未来企业数据的主要类型和资源的交互与结构    Application Domain     处理每个应用的开发和部署的方式   每个应用和核心业务流程的关系   处理现在和未来一组应用提供的业务功能的交互与结构的能力    Technology Domain     技术域包括了计算, 网络, 中间件, 第三方应用等   是业务的支撑系统   处理现在和未来技术服务与技术内容的交互与结构   ADM 架构开发方法:  是 togaf 的核心 adm 是定义企业架构的流程 可以被测试, 可以重复使用 分成几个阶段     Preliminary Phase         发布基本的 principles, setup 架构能力, 对任务进行指导, 让决策更容易          ADM cycle         是一个循环, 包括了 8 个阶段, 从 A 到 H          A:         Vision          B, C, D:         定义架构     定义 BDAT 四个域     需要管理层 Sign off          E, F:         定义工作计划     定义实现架构的流程     一系列的 transition architectures     帮助我们从目前的阶段到未来要达到的阶段          G:         实施     和开发团队一起实现这个架构     发布架构合同 ?          H:         变化管理     架构实现之后的变化管理     变化需要再走一遍这个 ADM cycle     变化包括新的业务需求,           Requriement Management Phase     是上面的多个阶段中心的一个阶段   对需求进行管理的流程    一共 10 个 phases     有 Preliminary Phase 和 A-H 的 cycle, 中间还有一个 requirement management phase   每个阶段都有自己对应的 Objective, Inpute, Steps, Outputs (目标, 输入, 步骤, 输出)         输入来自 repository 中的现有资产, 或者自己创建新的资产, 需要和stakeholders 进行合作          phases 可以被暂停, 可以返回和修改一个过去的phase   Iteration 是 cycle 中的某些流程可以进行循环的迭代   ADM 可以按照我们的需要就行定制和调整   Part 2 ADM: Preliminary Phase 预备阶段:  描述打造架构能力所需的准备工作和启 动工作，包括客制化 TOGAF 框架和定 义架构原则。 定义组织期望的架构能力的级别, 包含了架构的 principles Where, what, why, who, and how we do architecture 包括 收集和发布 架构能力   Business imperative (业务的势在必行) 会在 预备阶段, scoping 企业架构工作的时候, 驱动 需求和性能 metrics   Objectives                           Determine the Architecture Capability desired by the organization:       组织期待的决定的架构能力                        Review the organizational context for conducting Enterprise Architecture     Identify and scope the elements of the enterprise organizations affected by the Architecture Capability     Identify the established frameworks, methods, and processes that intersect with the Architecture Capability     Establish Capability Maturity target                                Establish the Architecture Capability:       发布架构能力                        Define and establish the Organizational Model for Enterprise Architecture     Define and establish the detailed process and resources for Architecture Governance     Select and implement tools that support the Architecture Capability     Define the Architecture Principles           Preliminary phase 的输入                           The TOGAF Library       togaf 官方的图书馆                                           Other architecture frameworks       其他架构框架, 比如 PMP, ITIL                                           Board strategies and board business plans, business strategy, IT strategy,business principles, business goals, and business drivers       公司的战略, 原则, 商业目标, 驱动力等                                           Major frameworks operating in the business (e. g. SCRUM)       业务的运营框架, 比如敏捷 scrume                                           Governance and legal frameworks, including Architecture Governance strategy       公司的治理和法律框架, 比如架构治理                                           Architecture capability       架构能力                                           Partnership and contract agreements       合作伙伴关系和合同                                           Existing documents relating to architecture capability       关于架构能力的现有文档                                           Organizational Model for Enterprise Architecture       企业架构的组织模型                                           Existing Architecture Framework       现有的架构框架                      Preliminary Phase 的步骤 (不是 part 1 的考试范围, 是 part2 的考试范围)                           Scope the enterprise organizations impacted       划定架构对企业组织的影响                                           Confirm governance and support frameworks       确认治理和支持的框架                                           Define and establish architecture team       定义和建立架构团队                                           Identify and establish architecture principles       定义和建立架构原则                                           Tailor TOGAF and other frameworks       量身定制 Togaf 和其他框架                                           Implement architecture tools       使用架构工具                      Outputs from Preliminary Phase                           Organizational Model for Enterprise Architecture       组织模型                                           Tailored Architecture Framework       量身定制的 架构框架                                           Initial Architecture Repository       架构 repository                                           Business principles, business goals, and business drivers       业务目标, 原则, 驱动因素                                           Request for Architecture Work (optional)       架构需求                                           Architecture Governance Framework       架构治理框架                      Artifacts Produced     Principles catalog   A - Vision 架构愿景:  “A succinct description of the Target Architecture that describes its business value and the changes to the enterprise that will result from its successful deployment. It serves as an aspirational vision and a boundary for detailed architecture development. ” 描述架构搭建的初始阶段;包括定义架构开发举措的范围信息、识别利益攸关者、确立架构愿景、和获得可启动开发架构的许可。 Vision 中的输入大部分是 preliminary phase 的输出 考试经常会被考到: 下面哪个阶段有 xxx activity 发生. 答案是 phase A, phase B, … phase A 的目的:     开发一个 high-level 的, 可以交付的, aspirational(有抱负的), 业务价值的 愿景   获取架构 SOW 的 approval    Inputs to Phase A                           Architecture reference materials       架构参考材料                                           Request for Architecture Work       架构需求                                           Business principles, business goals, and business drivers       业务目标, 原则, 驱动因素                                           Organizational Model for Enterprise Architecture       企业架构的组织模型                                           Tailored Architecture Framework       量身定制的架构框架                                           Populated Architecture Repository       放架构文档的 repository                      Steps to Phase A (不是 part 1 的考试范围, 是 part2 的考试范围)                           Establish architecture project       架构项目立项                                           Identify stakeholders, concerns and business requirements       识别 Stakeholders, 业务需求 和 担忧的地方                                           Confirm business goals, drivers and constraints       确定业务目标, 驱动力和限制                                           Evaluate capabilities (updated for TOGAF 9. 2)       验证能力                                           Assess readiness for transformation       评估转型的准备程度 有的公司比较容易接受新的变化, 有的比较难, 如果太难, 就多分几步                                           Define scope       定义范围                                           Confirm architecture principles, including business principles       确认架构原则, 包括业务原则                                           Develop architecture vision       开发架构 vision , 比如                                           Define the target architecture value and KPIs       定义目标架构价值和 KPIs                                           Identify transformation risks and mitigation activities       定义转型风险和 降低风险的活动                                           Develop Statement of Architecture Work, secure approval       开发 架构 SOW, 获得批准                      Outputs from Phase A                           Approved Statement of Architecture Work       批准的 架构 SOW                                           Refined statements of business principles, goals and drivers       优化过的 业务原则, 目标和驱动力                                           Architecture principles       架构原则                                           Capability assessment       能力评估                                           Tailored architecture framework       定制的架构框架                                           Architecture vision       架构 vision                                           Draft architecture definition document (version 0. 1 of all baseline and target BDAT documents)       起草架构定义文档                                           Communications plan       沟通计划                                           Additional content in the Architecture Repository       额外的内容放在 架构 repository 中                      Artifacts Produced ● Matrices: Stakeholder map matrix | 干系人地图 ● Diagrams: Business model diagram, Business capability map, Value stream map, Value chain diagram, Solution concept diagram | 业务模型图, 业务能力图, 价值流图, 价值链图, 方案概念图B - 业务架构:  A - Vison 定义了 high-level 的 愿景, 那么B 及时定义如何达到这个愿景  Business Architecture 是用来定义 未来要达到的业务架构     要先了解目前的业务架构   再设计未来的目标业务架构   定义 一个 填补 gap 的 架构 roadmap, 和相应的 work items   需要基于A 中定义的 vision      Preliminary 和 A 的输出作为 B 的输入   Inputs to Phase B     External reference materials   Request for Architecture Work (optional) (Preliminary Phase)   Business principles, goals, and drivers (Phase A)   Capability assessment (Phase A)   Communications plan (Phase A)   Organization Model for Enterprise Architecture (Prelim Phase)   Tailored Architecture Framework (Prelim Phase)   Approved Statement of Architecture Work (Phase A)   Architecture principles (Phase A)   Enterprise continuum   Architecture Repository   Architecture vision (Phase A)   Draft architecture definition document (version 0. 1 of all baseline and target BDAT documents) (Phase A)    Steps for Phase B (不是 part 1 的考试范围, 是 part2 的考试范围)                           Select reference models, viewpoints, and tools       参考的模型, 观点和工具                                           Develop baseline business architecture description       对当前的/现有的 业务架构进行描述                                           Develop target business architecture description       对未来的业务架构进行描述                                           Perform gap analysis       gap 分析                                           Define candidate roadmap components       定义 roadmap 和其中的候选步骤                                           Resolve impacts across the architecture landscape       实现 gap 过程中的一些影响                                           Conduct formal stakeholder review       让 stakeholder review 并认可                                           Finalize business architecture       敲定 业务架构                                           Create architecture definition document       创建架构定义文档                      Outputs from Phase B                           Refined Phase A deliverables       对 A 中的交付物进行修正                                           Draft architecture definition document       起草的架构定义文档                   a. Baseline business architecture, v1. 0 (detailed)       当前业务架构                   b. Target business architecture, v1. 0 (detailed)       未来目标的业务架构                                           Draft architecture requirements specification – gap analysis       起草的架构需求文档                                           Business architecture components of an architecture roadmap       实现业务架构的 roadmap 和 内容                      Artifacts Produced                           Catalogs: Value Stream catalog, Business Capabilities catalog, Value Stream Stages catalog, Organization/actor catalog, Driver/goal/objective catalog, role catalog, business service/function catalog, location catalog, process/event/control/product catalog, contract/measure catalog       一堆目录                                           Matrices: Value Stream/Capability matrix, Strategy/Capability matrix, Capability/Organization matrix, Business interaction matrix, actor/role matrix       一堆二维表格/矩阵                                           Diagrams: Business Model diagram, Business Capability Map, Value Stream Map, Organization Map, Business footprint diagram, business service/information diagram, function decomposition diagram, product lifecycle diagram, goal/objective/service diagram, business use-case diagram, organization decomposition diagram, process flow diagram,       一堆图形                     C - 信息系统架构:  Information System Arcuitecture 包含了 Data 和 Application, 是 BDAT 中的 data 和 application Data     目的:         开发想要的数据架构     识别要填补进 gap 的 和数据架构相关的 roadmap items          数据架构的输入包含了 preliminary 阶段, Vison 阶段, 业务架构阶段的输出   Inputs to Phase C, Data Architecture         External reference materials     Request for Architecture Work (optional) (Preliminary Phase)     Capability assessment (Phase A)     Communications plan (Phase A)     Organization Model for Enterprise Architecture (Prelim Phase)     Tailored Architecture Framework (Prelim Phase)     Data principles (TOGAF Specification Chapter 20)     Approved Statement of Architecture Work (Phase A)     Architecture vision (Phase A)     Architecture Repository     Draft architecture documents and draft requirements (Phase B)     Business related architecture roadmap (Phase B)          Steps for Phase C, Data Architecture (不是 part 1 的考试范围, 是 part2 的考试范围)         Select reference models, viewpoints, and tools                                   Develop baseline data architecture description         开发数据架构的当前的基准描述                                                             Develop target data architecture description         开发未来目标的数据架构描述                                                             Perform gap analysis         分析 gap                                                             Define candidate roadmap components         定义 gap 中要填补的 items                                                             Resolve impacts across the architecture landscape         找到相关的 impact (比如影响了应用, 流程, 部门, 人等)和对应方案                                                             Conduct formal stakeholder review         让 stakeholder review 数据架构                                                             Finalize data architecture         把数据架构定下来                                                             Create architecture definition document         创建数据架构定义文档                                    Outputs from Phase C, Data Architecture         Refined Phase A deliverables     Draft architecture definition document a. Baseline data architecture, v1. 0 b. Target data architecture, v1. 0     Draft architecture requirements specification – gap analysis     Data architecture components of an architecture roadmap          Artifacts Produced                                       Catalogs: Data entity/data component catalog         数据实体和数据内容的目录                                                             Matrices: Data entity/business function matrix, Application/Data matrix         数据实体和业务功能的二维表格                                                             Diagrams: Conceptual data diagram, logical data diagram, data dissemination diagram, data security diagram, data migration diagram, data lifecycle diagram         数据概念图, 逻辑数据图, 数据散播图, 数据安全图, 数据迁移图, 数据生命周期图                                     应用     目的:         开发想要的数据架构     识别要填补进 gap 的 和应用架构相关的 roadmap items          Inputs to Phase C, Application Architecture         External reference materials     Request for Architecture Work (optional) (Preliminary Phase)     Capability assessment (Phase A)     Communications plan (Phase A)     Organization Model for Enterprise Architecture (Prelim Phase)     Tailored Architecture Framework (Prelim Phase)     Application principles (TOGAF Specification Chapter 20)     Approved Statement of Architecture Work (Phase A)     Architecture vision (Phase A)     Architecture Repository     Draft architecture documents and draft requirements (Phase B)     Business and data related architecture roadmap (Phase B)          Steps for Phase C, Application Architecture (不是 part 1 的考试范围, 是 part2 的考试范围)         Select reference models, viewpoints, and tools     Develop baseline application architecture description     Develop target application architecture description     Perform gap analysis     Define candidate roadmap components     Resolve impacts across the architecture landscape     Conduct formal stakeholder review     Finalize application architecture     Create architecture definition document          Outputs from Phase C, Application Architecture         Refined Phase A deliverables     Draft architecture definition document a. Baseline application architecture, v1. 0 b. Target application architecture, v1. 0     Draft architecture requirements specification – gap analysis     Application architecture components of an architecture roadmap          Artifacts Produced                                       Catalogs: Application portfolio catalog, interface catalog         应用组合目录, 接口目录                                                             Matrices: Application/organization matrix, role/application matrix, application function matrix, application interaction matrix         应用/组织 , 角色/应用, 应用/功能, 应用/交互的二维表                                                             Diagrams: Application Communication diagram, Application and User Location diagram, Application Use-Case diagram, Enterprise Manageability diagram, Process/Application Realization diagram, Software Engineering diagram, Application Migration diagram, Software Distribution diagram         应用通信图, 应用和用户位置图, 应用用例图, 企业管理图, 流程和应用关系图, 软件工程图, 应用迁移图, 软件分发图                                    D - Technology Architecture 技术架构:  目的:     开发想要的技术架构   识别要填补进 gap 的 和技术架构相关的 roadmap items    知道了应用和数据需要什么架构, 就需要推演出所需要的技术用什么     我的理解包括了移动端, 微信端, 前端,后端, 数据库, 云, 微服务, 容器化, 虚拟化, AI, IOT, 网络, 安全, 硬件, 操作系统, devops 等等    Inputs to Phase D     External reference materials                         Product information on candidate products       备选产品的产品信息                     Request for Architecture Work (optional) (Prelim Phase)   Capability assessment (Phase A)   Communications plan (Phase A)   Organization Model for Enterprise Architecture (Prelim Phase)   Tailored Architecture Framework (Prelim Phase)   Technology principles (TOGAF Specification Chapter 20)   Approved Statement of Architecture Work (Phase A)   Architecture vision (Phase A)   Architecture Repository   Draft architecture documents and draft requirements (Phase B and C)   Business, data and application components of an architecture roadmap (Phase B and C)    Steps for Phase D (不是 part 1 的考试范围, 是 part2 的考试范围)     Select reference models, viewpoints, and tools   Develop baseline technology architecture description   Develop target technology architecture description   Perform gap analysis   Define candidate roadmap components   Resolve impacts across the architecture landscape   Conduct formal stakeholder review   Finalize technology architecture   Create architecture definition document    Outputs from Phase D     Refined Phase A deliverables   Draft architecture definition document a. Baseline technology architecture, v1. 0 b. Target technology architecture, v1. 0   Draft architecture requirements specification – gap analysis   Technology architecture components of an architecture roadmap    Artifacts Produced                           Catalogs: Technology Standards catalog, Technology Portfolio catalog Matrices: Application/Technology matrix       技术标准, 技术组合分类二维表, 应用/技术二维表                                           Diagrams: Environments and Locations diagram, Platform Decomposition diagram, Processing diagram, Networked Computing/Hardware diagram, Communications Engineering diagram       环境和位置图, 平台分解图, 流程图, 网络计算/硬件图, 通信工程图                     E - Opportunity and Solution 机会与方案:  E 和 F 是 8 个 phase 的中间2个, 是承上启下的作用 要把前面 ABCD 中要输出的文档都最终定下来 要把最终的架构设计完成掉 要把后面的实施计划定义出来 后面就要开始真正的架构项目迁移和实施了 用新兴技术来找新的机会 目的:     generate the initial architecture roadmap         B, C, D 是用来找 candidate roadmap items 的.      E 是要把 BCD 这些 candidate 弄到一款, 最终做出来这个 架构 roadmap          identify if transition architecture are required         如果从现有架构到未来 roadmap 有很大的gap, 那么可能还需要有 transition architecture (中间阶段的架构)          define solution building blocks ( SBBs )         SBB - 描述在不同的架构图之间可以被重用的技术方案           Inputs to Phase E     External reference materials   Product information   Request for Architecture Work (optional) (Prelim Phase)   Capability assessment (Phase A)   Communications plan (Phase A)   Planning methodologies   Organization Model for Enterprise Architecture (Prelim Phase)   Tailored Architecture Framework (Prelim Phase)   Approved Statement of Architecture Work (Phase A)   Architecture vision (Phase A)   Architecture Repository   Draft architecture documents and draft requirements (BDAT)   Change requests for existing business programs and projects   Candidate architecture roadmap components    Steps for Phase E (不是 part 1 的考试范围, 是 part2 的考试范围)     Determine key corporate change attributes   Determine business constraints   Review and consolidate gap analysis from Phases B to D   Review consolidated requirements across business functions   Consolidate and reconcile interoperability requirements   Refine and validate dependencies   Confirm readiness and risk for business transformation   Formulate implementation and migration strategy   Identify and group major work packages   Identify transition architectures   Create the architecture roadmap &amp; implementation and migration plan    Outputs from Phase E     Refined Phase A deliverables   Draft architecture definition document – incl. baseline and target, v1. 0   Draft architecture requirements specification – incl. gap analysis   Capability assessments                         Architecture roadmap, version 0. 1       架构线路图                                           Implementation and migration plan, version 0. 1       实施和迁移计划                      Artifacts Produced                           Diagrams: Product context diagram, benefits diagram       产品上下文图, 利益图                     F - Migration Planning:  这个阶段, 要把架构设计的工作彻底完成掉     会有一堆的 finalized 文档    要把之前的架构包转交给实施团队     会有实施计划    要总结之前完成的架构工作, 以此提高之后的架构工作 目的:     finalize the architecture roadmap and migration plan   ensure migration plan is aligned with enterprise approach to change   ensure business value and cost of work packages is understood         work package 就是具体的甘特图中的每个工作项, 把业务价值放到 work package 中就是迁移计划要做的事情           Inputs to Phase F     External reference materials   Request for Architecture Work (optional) (Prelim Phase)   Capability assessment (Phase A)   Communications plan (Phase A)   Organization Model for Enterprise Architecture (Prelim Phase)   Governance Models and Frameworks   Tailored Architecture Framework (Prelim Phase)   Approved Statement of Architecture Work (Phase A)   Architecture vision (Phase A)   Architecture Repository   Draft architecture documents and draft requirements (BDAT)   Change requests for existing business programs and projects   Architecture roadmap, version 0. 1 (Phase E)   Capability Assessment   Implementation and migration plan , version 0. 1 (Phase E)    Steps for Phase F (不是 part 1 的考试范围, 是 part2 的考试范围)     Confirm management framework interactions   Assign a business value to each work package   Estimate resource requirements, project timings, etc   Prioritize the migration projects   Confirm architecture roadmap and update architecture definition document   Generate implementation and migration plan   Complete the architecture development cycle, lessons learned    Outputs from Phase F     Implementation and Migration Plan, version 1. 0   Finalized architecture definition document   Finalized architecture requirements   Finalized architecture roadmap   Reusable Architecture Building Blocks (ABBs)   Requests for Architecture Work for next ADM cycle (if any)   Implementation of governance model   Change requests for architecture capability from lessons learned   输出一个架构合同 Architecture Contract ?         It’s a contract between the implementation team and the architecture team that the architecture work that has been defined will be implemented as defined, and that the architecture governance process will be followed for any changes.            Artifacts Produced     None         之前的文档都写好了, 这个 phase 不产出任何图表          G - Implementation Governance 实施和治理:  目的:                           ensure conformance       确保实施和架构设计保持一致                        架构师并不是实施者, 实施部分架构师的职责是帮助实施者有效的按照架构设计进行实施, 并保持一致          perform architecture governance         实施过程中如果出现一些变化, 比如开发人员觉得有更合适,更节约成本的方案可以代替原来的架构方案, 那么必须要和架构师一起沟通, 并且通过 architecture governance 来进行处理 change request           Inputs to Phase G     External reference materials   Request for Architecture Work (optional) (Prelim Phase)   Capability assessment (Phase A)   Organization Model for Enterprise Architecture (Prelim Phase)   Tailored Architecture Framework (Prelim Phase)   Approved Statement of Architecture Work (Phase A)   Architecture vision (Phase A)   Architecture Repository   Architecture definition documents and requirements (BDAT)   Architecture roadmap, version 1. 0 (Phase F)   Implementation governance model   Architecture contract                         Request for architecture work for next ADM cycle (Phase F)       有些工作这次不能实施, 可以下次来做?                     Implementation and migration plan , version 1. 0 (Phase F)    Steps for Phase G (不是 part 1 的考试范围, 是 part2 的考试范围)                           Confirm Scope and Priorities for Deployment       确认部署的范围和优先级                                           Identify Deployment Resources and Skills       识别开发资源和技能                                           Guide Development of Solutions Deployment       指导方案部署的开发过程                                           Perform Enterprise Architecture Compliance Reviews       执行企业架构合规的 reivew                                           Implement Business and IT Operations       实现业务和 IT运营                                           Perform Post-Implementation Review and Close the Implementation       执行实施后的 review 并关闭实施                      Outputs from Phase G     Architecture contract | 架构合同 “…joint agreements between development partners and sponsors on the deliverables, quality, and fitness-for-purpose of an architecture” Read Chapter 43 of the TOGAF 9. 2 specification                         Compliance assessments       合规评估                                           Change requests       需求变更                                           Architecture-compliant solutions deployed       符合规定的架构的方案部署完成                      Artifacts Produced     None   H - Architecture Change Management:  架构变化管理, 就像一个一直定在那里的阶段, 我们完成了所有的架构设计, 完成了所有的实施工作之后, 就是在这里等待变化的发生 当积累一些变化之后, 我们就要考虑是否要开启新的一轮 ADM cycle 了     大的变化, 需要改变架构逻辑的变化也许需要新的 cycle   小的变化, 也许不需要启动 cycle ,而是在现有架构中就可以容纳    目的:     maintain architecture lifecycle   execute architecture governance   maintain architecture capability    Inputs to Phase H     External reference materials   Request for Architecture Work (optional) (Prelim Phase)   Organization Model for Enterprise Architecture (Prelim Phase)   Tailored Architecture Framework (Prelim Phase)   Statement of Architecture Work (Phase A)   Architecture vision (Phase A)   Architecture Repository   Architecture definition documents and requirements (BDAT)   Architecture roadmap (Phase F)   Change requests – technology changes, business changes, lessons learned   Implementation governance model   Architecture contract (Phase G)   Compliance assessments (Phase G)   Implementation and migration plan , version 1. 0 (Phase F)    Steps for Phase H (不是 part 1 的考试范围, 是 part2 的考试范围)     Establish Value Realization Process   Deploy Monitoring Tools   Manage Risks   Provide Analysis for Architecture Change Management   Develop Change Requirements to Meet Performance Targets   Manage Governance Process   Activate the Process to Implement Change    Outputs from Phase H     Architecture updates (maintenance)   Change to architecture framework and principles (maintenance)   New Request for Architecture Work (for major changes)   Statement of architecture work (updated if necessary)   Architecture contract (updated if necessary)   Compliance assessments (updated if necessary)    Artifacts Produced     None   Requirement Management 需求管理:  RM 位于 ADM 图的中心, RM 在ADM 图中和 A -&gt; H 的每个阶段都有互动 技术上来说, 他是一个 phase, 但是是一个静态的阶段 RM 管理需求的流程     收集, 确认, 排序, 过滤, …   来个新的 业务需求, 就要回到 B 阶段, 然后再推出 C, D    RM 是管理 repository 的流程 在 ADM 的各个阶段都要持续的处理需求     Being able to handle change at any time is crucial for the ADM to succeed.    在 ADM cycle 中处理需求要有节奏, 比如按敏捷的理念, 需求要有一个 backlog 来处理, 开发前期制定需求. 或者尽量在 ADM 的 ABCD 阶段收集需求, EF 阶段按需求制定方案和计划, G 实施阶段按需求进行实施         所以 EFG 阶段的新需求尽量累积到下一个 ADM cycle 再去做            Requirements Management is the process that manages the repository 目标:     ensure the process is sustained through all phases   manage change while the ADM cycle is in progress   provide the requirements to each ADM phase   Part 3 ADM Guideline and techniques:  要执行 ADM 的 cycle , 需要一些 指南和工具如下: ADM Guidelines and Techniques● Using the TOGAF ADM in the context of a specific Architectural Style ● Architecture Principles● Stakeholder Management● Architecture Patterns● Business Scenarios● Gap Analysis● Migration Planning Techniques● Interoperability Requirements● Business Transformation Readiness Assessment● Risk Management● Capability-Based PlanningADM and architectural styles (9. 2 新加的):  架构开发管理和架构风格 应用架构有不同的风格 Togaf 不需要我们改变我们的风格 但是可能我们需要改变那我们的文档的内容 采用不同的 views, models, tools 来创建文档, 图, 表格等 togaf 在使用一个 Service Oriented Architecture 的 styleArchitecture principles:  An enduring set of general rules and guidelines about architecture 是架构的一般规则和指南 不会经常变化 Defined during the Preliminary Phase 在需要做决策的时候可以用到 例子:     Data Architecture Principle         Data Trustee             “Each data element has a trustee accountable for data quality. ”                      Five Elements of a Good Principle● Understandability | 容易理解● Robustness | 不要有漏洞● Completeness | 涵盖各种情况● Consistency | 一致性, 不要有前后矛盾● Stability | 稳固Stakeholder Management 干系人管理:  stakeholder 的定义     “An individual, team, organization, or class thereof, having an interest in a system. ”   对一个系统感兴趣的个人, 团队, 组织或者他们其中的一种形式    As an architect, being able to win support for your plans is a skill.  Knowing who the most powerful stakeholders are… Makes things easier for approval and budgets.  Can establish a communication plan to keep them informed Identify problems and conflicts early in order to avoid them.      避免 stakeholder 之间的问题和冲突.     通过下图, 了解哪些 stakeholder 和对他们的定位 Architecture Patterns 架构模式:  和设计模式有点像 “an idea that has been useful in one practical context and will probably be useful in others” - M. Fowler togaf 的 building block 说的是 : what you use 架构模式是: when, why, how you use 一旦我们创建了一些 building blocks, 我们就要去为他们定义 patterns 例子:     设计模式中有代理模式, 监听模式等, 是不是也可以用于架构模式?   Business Scenarios 业务场景:  业务场景的定义              A business process, application, or set of applications                   The business and technology environment                   The people and software systems (“actors”) involved                   The desired outcome           Anyone should be able to understand a business problem and solution by reading the business scenario.  一个精心设计的商业案例。     A neatly laid out business case.    Gap Analysis:  Key step in the BDAT Phases and Migration Planning (Phases B-E) 分析 Baseline architecture 和 Target architecture 之间的 gap Gaps are anything added, changed or intentionally omitted     Gaps 是任何新增, 修改, 故意删除掉的, 意外删掉的内容    目的是要让大家清晰的知道为什么增加或者删除了一些内容     It should be clear and easy to explain why things were added or removed   Migration Planning:  togaf 提供了一些迁移计划需要用到的工具     Migration Planning, Phase F         Implementation Factor Assessment &amp; Deduction Matrix实施因素评估和扣减矩阵     Consolidated Gaps, Solutions, &amp; Dependencies Matrix综合的差距、解决方案和依赖性矩阵     Architecture Definition Increments Table架构定义增量表     Transition Architecture State Evolution Table过渡结构状态演变表     Business Value Assessment Technique商业价值评估技术          Interoperability 协同工作的能力:  “the ability to share information and services” Business Interoperability: how business teams work together Information Interoperability: how data is shared Technical Interoperability: how technical services connect to one another Each of the architecture definition phases has elements of interoperability.  Architects think about how what they do interacts with the “outside world” during the processBusiness Transformation Readiness Assessment (BTRA):  “evaluating and quantifying an organization’s readiness to undergo change”     评估和定量分析一个企业是否准备好了迎接变化    No point creating architecture if it’ll be ignored.      如果大家不想接受变化, 就没必要做架构      Understanding how to get your organization to change is a key to success.   推荐的 BTRA 的活动     Determine the readiness factors that will impact the organization   Present the readiness factors using maturity models   Assess the readiness factors, including determination of readiness factor ratings   Assess the risks for each readiness factor and identify improvement actions to mitigate the risk   Work these actions into Phase E and F Implementation and Migration Plan    Example Factors     Vision   Desire, Willingness, Resolve   Need         How strong is the business need for enterprise architecture?     Does every part of the business recognize this as being required?     How committed is the senior leadership to this?          Business Case   Funding   Leadership, Sponsor, Champion   Governance   Accountability   IT Capacity to Execute   Enterprise Capacity to Execute   Risk Management:  What are the risks of migrating to the target architecture? Identify them, and track them TOGAF has the concept of Initial Level of Risk and Residual Level of Risk     Initial Level of Risk is the risk that exists if you do nothing to mitigate them   Residual Level of Risk is the risk that remains after you have mitigated them    例子: Example: Client acceptance risk     How do you measure it?   How do you mitigate it?   And what would the cost to the business be if it comes true?   Capability-Based Planning 基于能力的规划:  是一个规划, 一个跨部门的, 需要多部门通力协作的规划 “a business planning technique that focuses on business outcomes”     一种专注于商业成果的商业规划技术    Capabilities are business-driven and ideally business-led.      能力是由业务驱动的，最好是由业务主导的。    Designed to combat the fact that organizations are split along functions     旨在消除组织按职能分裂的事实    Many business functions work together to deliver one capability 例子: Example: 创建一个完美的产品     会涉及到多个部门, 产品设计, 软件开发, 工厂制造, 零售/销售   公司计划的是生产一个 “完美产品”, 而不是 “最佳销售团队”    Part 4 Architecture Content Framework:  在架构设计过程中, 会产生一些输出/交付物, 我们不需要重新发明轮子, 而是可以利用 Architect content framework 帮助我们生产出架构来 Architect content framework     提供结构化的原始模型, 为最终的架构交付物作为原型和基础    Artifacts 工件     工件是在架构工作期间生成的, 用于描述架构的某一个方面的, 工作成果 (work product)   ADM 的 8 个阶段中, 每个阶段都定义了一系列的 artifacts 工件         记住每个阶段和对应的工件对考试很重要          工件可以被分成三类         Lists     Matrices (母体/子宫)     Diagrams           Deliverables     “contractually specified and in turn formally reviewed, agreed, and signed off by the stakeholders”         “合同规定，并由利益相关者正式审查、同意和签署”          说白了就是合同规定的架构交付内容   不是一个图, 而是完整的文档   需要被 stakeholders 进行 review, agree and signed off    Building Blocks     架构中可以被创造和重用的部分   分为两种         Architecture building blocks (ABBs)     Solution building blocks (SBBs)          ABBs         描述在不同的架构图之间可以被重用的业务流程          SBBs         描述在不同的架构图之间可以被重用的技术方案          案例:         查询消费者的 profile     因为组织内多个部门都有这个需求, 所以把 customer profile lookup 做成一个 ABB, 可以在不同业务部门的需求和 架构 dragram 中进行拷贝     背后的逻辑是, 需要一个 CRM 的用户搜索模块, 是技术实现的方式, 做成一个 SBB          Part 5 Enterprise Continuum and Tools:  企业连续体 ” a way of classifying architectural work products according to their specificity - from extremely generic to extremely specific” 一种对架构 repository 中的项目进行分类的方法 分为极常规架构, 极特殊架构, 或者中间的架构, 大体分为如下从左到右的架构     Foundation Architectures         最基础的架构, 所有公司都通用的基础架构          Common System Architectures         普通系统架构, 一般公司都可以通用          Industry Architectures         针对于某个行业的架构, 多个同行业的公司可以通用          Organization-specific architectures         针对于某个企业定制的专门的架构          黄色为 Abbs   绿色为 Sbbs   Architecture Repository:  放置架构文档的 storage 存储/repository 可以是一个 file sercer, 或者 git, Jira, Confluence, wiki 等, 具体形式考试不考 有统一的命名规则和目录 大家都可以访问, 进行文档写作 The Architecture Repository contains…     ● Architecture Metamodel         为企业定制的架构模型          ● Architecture Capability         架构治理规则          ● Architecture Landscape         已经部署了的架构资产, 系统和流程          ● Standards Information Base (SIB)         做架构的时候需要用的一些信息数据, 比如 隐私信息标准 (GDPR, HIPPA          ● Reference Library         引用库的材料          ● Governance Log         记录了架构治理的活动日志, 比如记录了谁做了架构的创建, 调整, 谁进行了审批和 review , 相关的会议记录等          ● Architecture Requirements Repository (*new in 9. 2)         需求文档的 repository, 相当于 backlog          ● Solutions Landscape (*new in 9. 2)         一些被审批过, 签过字的解决方案的文档内容, 可以用来支持架构设计          Part 6 Architecture Capability Framework:  企业的架构能力根据企业中是否有架构师人员, 是否有相应的培训, 是否有治理方法, 是否已经发布了架构框架等综合因素构成 通过 ADM 的 Preliminary Phase 和 cycle (A-&gt; H)可以提升架构能力     A-F 和组织内部的 leaders 定义架构, 制定计划   G 签订架构合同, 大家都约定遵守这个架构, 实施团队按照架构进行实施, 架构师在实施阶段监控实施团队的工作, 确保实施的内容按照架构设计执行.     需要有资金来支持架构能力的提升, 花钱买人, 买知识, 搭建治理模型等等     就像管理一个项目一样对架构能力进行提升, 要有 ,SOW, budget, 时间节点, 里程碑, 和定期的报告    治理 和 repository 和 架构能力有直接的关系 治理在上面 任何流程在中间 repository 在下面其他 重要度 TOGAF 概念: Architecture Governance:  A system of controls over architecture A system to ensure compliance with architecture Effective management of those systems Accountability to the business for that Governance is layered within most organizations     Corporate governance   Technology governance   IT governance   Architecture governance    Principles of Good Governance● Discipline● Transparency ● Independence ● Accountability ● Responsibility ● FairnessArchitecture Board:  一个跨组织的 委员会, 帮我们进行架构治理工作, 需要能够代表所有的 key stakeholders 需要是一些 来组不同部门的 senior members , 每月开一次会, 做一些架构决策, key principles 等 Representative of all the key stakeholders in the architectureArchitecture Contracts:  “joint agreements between development partners and sponsors on the deliverables, quality, and fitness-for-purpose of an architecture”Architecture Compliance:  包含了     Irrelevant   Consistent   Compliant   Conformant   Fullyconformant ● Non-conformant    Compliance Reviews"
    }, {
    "id": 14,
    "url": "http://localhost:4000/%E5%AD%A6%E4%B9%A0%E5%85%83%E5%AE%87%E5%AE%99/",
    "title": "Metaverse research",
    "body": "2023/03/21 - “元宇宙（Metaverse）是一个虚拟时空间的集合，由一系列的增强现实（AR）、虚拟现实（VR）和互联网（Internet）所组成。表示“超越宇宙”的概念：一个平行于现实世界运行的虚拟空间。随着 Apple Vision Pro 的发布, 接下来的元宇宙生态和应用场景将会更加的丰富. 让我们一起来了解元宇宙和它的生态吧. 元宇宙[TOC] 元宇宙相关公司:  meta Every realm 元宇宙地产开发团队     推出了一个地产元宇宙项目 The Row , 是和纽约地产Alexander 团队合作的.    主流 VR 设备:  Oculus Quest 2     Meta 推出的    Pico Neo 3 HTC VR Hololens     微软的    Vision Pro     苹果的   元宇宙相关技术: 可实现数字孪生的 3D 技术:  Unity / Unreal     游戏引擎   逼真度高, 效果佳   资源消耗高   也可以通过 像素流 技术, 编译出 web assembly 版本的项目在 web 端使用         技术复杂           webGL     web 3D 引擎   逼真度低   资源消耗低   方便实现, 方便共享    Blender     免费的 3D 模型创作工具    MAYA     3D MAX   出了 VR 版本    A-Frame     用于构建 VR 的 Web 框架   是基于 three. js 的组件框架    OpenVR     元域智辰的模拟体验应用 好像用的就是这个   图表和游戏的 StreamVR 是密切相关的    VRTK     创建 VR 程序的工具包   文本生成 3D 模型技术 (text-to-3D):  英伟达的 Magic 3D 谷歌的 DreamFusion 英伟达的 Get 3D OPUS UNITY AI数字孪生的创建流程:  数字孪生建模流程     产品原型   场景建模   界面设计   数据融合   元宇宙开发流程:  空间原画设计 建模 材质 动画 界面设计 前端页面设计 3D引擎开发 服务器开发"
    }, {
    "id": 15,
    "url": "http://localhost:4000/%E7%BD%91%E7%BB%9C%E5%AE%89%E5%85%A8%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB/",
    "title": "Interpretation of New China MLPS 2.0",
    "body": "2023/03/15 - MLPS 2. 0: 等级保护是对信息和信息载体按照重要性等级分级别进行保护的一种工作，指对国家重要信息、法人和其他组织及公民的专有信息以及公开信息和存储、传输、处理这些信息的信息系统分等级实行安全保护，对信息系统中使用的信息安全产品实行按等级管理，对信息系统中发生的信息安全事件分等级响应、处置。 网络安全知识体系[TOC] 资源: 中华人民共和国网络安全法（全文）中华人民共和国个人信息保护法中华人民共和国数据安全法（全文）网络安全等级保护 MLSP Multiple-level Protection SchemeMLPS 英文解读信息安全等级保护网络安全法 Yutube 视频网络安全等级保护 Youtube 视频系列课程 ISO 27001 ISO 21434 Automotive Cybersecurity Standards 课程介绍 -企业级网络安全与等保 2. 0: 网络安全等级保护 Youtube 视频系列课程 课程大纲:  网络安全概述 网络安全大事件 网络安全厂商 网络安全产品 网络安全法律法规 等级保护政策规范解读 等保解决方案 等保实战案例网络安全岗位  安全专家/研究员 安全运维/安全工程师 安全厂商工程师 系统集成安全工程师网络安全厂商  网络兼容安全厂商     思科   华为   华三   锐捷   迈普   迪普    传统网络安全厂商     奇安信   深信服   启明星辰   天融信   绿盟    细分领域安全厂商     堡垒机: 齐治   终端安全: 北信源   云安全: 亚信安全   移动安全: 梆梆安全   其他    互联网安全厂商     360   腾讯   阿里   网络安全部门  公安部     网络安全保卫局 (网安)   技侦/刑侦/经侦   下属单位         公安部一所/三所     中电 30 所     信息安全评测/认证中心             国家信息安全漏洞库 (CNNVD)                      网信办     CNCERT 国家互联网应急中心   CNNIC 中国互联网络信息中心   国家计算机网络与信息安全管理中心    工信部     网络安全管理局   通管局/无委会    国安     南京 841 研究所    军队     56 所 无锡江南所   30 所 卫士通    机要和保密局     国家保密局, 保密科技评测中心   什么是等保: 全称:  网络安全等级保护 MLPS 指对国家重要信息,法人和其他组织及公民的转悠信息以及公开信息和存储, 传输, 处理这些信息的信息系统分等级实施安全保护, 对信息系统中使用的信息安全产品实行按等级管理, 对信息系统重发生的安全事件分等级响应和处置. 为什么:  国家信息形势严峻 维护国家安全需求 建立国家信息安全的基本制度 开展信息安全基本方法目的:  明确信息安全重点 优化信息安全资源配置执法部门 - 公安部门  警察法规定: 监督, 管理计算信息系统的安全保护工作 公安部门的网络安全保卫局会进行监督, 管理, 不符合要求的需要整改, 罚款等措施.  网络安全保卫局 11 局, 省网络总队, 市支队相关部门和职责  公安牵头, 领导, 对全国这项工作开展监督指导 保密局: 负责有关保密工作的监督, 检查, 指导, 并设计国家秘密信息系统的分析保护工作 (只针对国家秘密的信息系统) 密码局: 负责有关密码工作的监督, 检查, 指导 工业和信息化部: 负责各个部门之间的协调等保发展历史:  04 年出台了等级保护规范标准 05 年开展了等级保护调查工作 06 年开展了等级保护试点工作 07 年部署开展定级工作 08 年出正式标准 19 年 2. 0 出来了 , 8 则, 看文件包等保级别标准: 参考: 信息系统按照重要性和受破坏的危害性进行分级 (1-5 级, 数字越大级别越高) 第一级: 自主安全保护级  信息系统受到破坏后, 对公民, 法人和其他组织权益造成损害, 但不损害 社会秩序和公共利益 和 国家安全第二级: 审计安全保护级 信息系统受到破坏后, 对公民, 法人和其他组织权益造成严重损害, 或者对社会秩序和公共利益造成损害, 但不损害国家安全. 第三极: 强制安全保护级 信息系统受到破坏后, 对社会秩序和公共利益造成严重损害, 或者损害国家安全. 第四级: 结构化保护级 信息系统受到破坏后, 对社会秩序和公共利益造成特别严重损害, 或者严重损害国家安全. 第五级: 访问验证保护级 信息系统受到破坏后, 特别严重损害国家安全. 目的:  对信息系统安全防护体系能力的分析和确认 帮助运营单位发现存在的安全隐患, 帮助运营单位认识不足, 及时改进, 有效提升其信息安全防护水平说明:  第一级和第五级基本上接触到的时候比较少     第一级可做可不做   第五级是国家机密, 一般间谍才会碰到    第四级也很少碰到, 危害等级也很高了 日常见到的基本都是第三极 (98%)     一般企事业单位    二级以前一般常见于区县级政府单位, 现在也基本上是做三级了. 实际落地定级参考 等级保护项目过程: 一般来说过程分为 5 个步骤  系统定级     企业带着材料去公安局进行定级, 备案   公安给出定级要求, 企业要回去按照要求进行等级相关安全整改工作的规划和实施    规划设计     企业要找专业咨询公司进行规划设计, 咨询公司给出建议和规划设计   企业需要采购设备, 比如网络设备, 防火墙设备等 (硬件厂商可能会把规划服务免费卖给企业)    建设实施     一般由设备厂商/集成商进行实施    等级测评     每个地区/省份都有专业的测评机构, 比如四川有 5 家   企业要额外花钱请测评人员来测评   评测通过后要再到公安备案    运维管理     公安进行定期监控, 检查, 持续改进    系统废止定级流程: 2. 0 的流程  定级 备案 系统建设,整改 开展等级评测 信息安全监管部门定期开展监督检查等保基本要求:   基本要求     技术基本要求         物理安全     网络安全     主机系统安全     应用安全     数据安全          管理基本要求         安全管理机构     安全管理制度     人员安全管理     系统建设管理     系统运维管理           比如:  三级等保需要的指标是 73 类, 288 项网络安全风险  2. 0 标准内容变化 等保安全产品解析: 等保二级和三级需要的安全产品  等保涉及到的硬件 安全产品要求  等保涉及到的服务/咨询产品   往往都是买设备送服务 最主要的服务     差距分析   等保合规审计   等保管理要求和建设整改 (三级为例子): 安全管理制度  总体方针, 策略, 规范 管理制度 操作规程 整改要点     形成信息安全管理制度体系, 统一发布, 定期修订等.    安全管理机构  从管理层到执行层到业务层的管理结构, 整改要点     信息安全领导小组于职能部门, 专职安全员, 定期全面安全检查, 定期协调会议, 外部沟通与合作等   人员安全管理  内部和外部人员的安全管理 整改要点     安全保密协议, 关键岗位人员管理, 针对不同岗位的培训计划, 外部人员访问管理   系统建设管理  分别从定级, 设计建设实施, 验收交付, 评测等方面考虑, 关注各项安全管理活动系统运维管理  涉及日常管理, 变更管理, 制度化管理, 安全事件处置, 应急预案管理和安管中心等保2. 0 标准解读: 1. 0 和 2. 0 的对比  2. 0 在技术部分的基础上做了蹄奥正, 做了一些合并和删减关键信息基础设施认定标准:  关键信息系统和基础设施不能低于 三级     网站类         比如 lecake. com          平台类         比如阿里云          生产业务类         比如交警 12123. com          "
    }, {
    "id": 16,
    "url": "http://localhost:4000/Google-Cloud-Platform-Professional-Architect/",
    "title": "Prepare for Google CLoud Platform Professional Architect Certification",
    "body": "2023/02/23 - 我一直以来都是谷歌的忠实粉丝, 从 08 年开始就开始使用谷歌的各种产品和服务. 因为项目需要, 我花了 2 个多月的时间筹备谷歌专业认证架构师的认证并一次性通过了考试. 这里我把一些学习心得总结了下来跟大家一起分享. 如果你也在准备这门考试, 看这篇文章就足够了. Google CLoud Platform Professional Architect Certification[TOC] 考试准备:  考试名称: GCP - PCA 200 美金 120分钟(2 小时) 50 - 60 道题, 65 道 (答对 70% 算及格)     单选题, 4 选 1   多选题, 5 选 2   考试里面有大概10-12题案例分析题，这些案例都摘自于Google官网上面的案例分析，所以请你一定要提早熟悉这些案例，了解一下这些案例中想要通过云服务达到的业务目标和技术目标是什么    考完马上知道结果 考试技巧     如果答案的文字少, 先扫一眼答案   读完整的题目   注意看 “you want to”   识别和记录问题的关键部分         比如:             题目希望 Most cost-effective way       那么就应该优先考虑 regional 而不是 multi-regional       题目如果说希望 HA, 那么就应该是 multi-regional                     排除法   不确定的, 先打个flag, 提交前再review 一下    第一次没通过, 14 天后再考, 第二次没通过 60 天后再考, 还没通过 365 天后再考 考试网址 重新schedule 需要提前 3 天 (72 小时) 提前15 分钟进入网页 考试考试按钮会在预约时间的前 10 分钟出现 考官可能会因为当时的忙碌情况, 迟到 15-20 分钟 考前准备, 包括, 准备好身份证, 护照 , 桌面要整齐, 不能有双屏幕等.  关掉所有无关软件 用chrome 浏览器进入 考试网址GCP 服务: Compute: Compute Engine - GCE:  GCP 的 VM的服务 按秒计费, 虚机release 掉了就不计费了 VM 就是一个虚机, 也称作实例     VM 只能属于某一个 zone   可以跨 zone 迁移 VM         用命令 gcloud compute instances move my-instance --zone us- central1-a --destination-zone us-central1-b 进行迁移     如下情况, vm 无法跨zone 移动             Instances that are part of a MIG       Instances attached with local SSDs       Instances in TERMINATED status                跨 Region 迁移 vm 只能通过 手动方式                              为 vm 的 persistent disk 创建一个快照 snapshot , 把 snapshot 拷贝到另一个 region (的一个zone)                     gcloud compute disks snapshot my-disk-a --snapshot-names my-pd-snapshot --zone ZONE                                                          用快照 创建 persistent disk                     gcloud compute disks create my-disk-b --source-snapshot my-pd-snapshot --zone ZONE                                                          在新的region (的一个zone)创建一个 instance , 然后 attach 上 persistent disk                                     创建GCE的前置条件     Project   Billing Account   Compute Engines Admin APIs should be enabled    GCE Machine Family机器家族 有三种, 每种里面有有不同的机器类型     General Purpose (E2, N2, N2D, N1)   Memory Optimized (M2, M1)   Compute Optimized (C2)   GPU         图形计算或者数学计算, AI/ML 会用到     很贵     不是所有机器类型都支持     需要再 VM 上安装 GPU Libraries 软件           GCE Machine Type机器类型     e2-standard-2 例子         e2 表示family 名字     standard 表示性能(workload )     2 表示vCPU个数,     vCPU个数越大的机器类型, 内存,网络, 磁盘的大小也会更大     实例停止后, 可以修改及其类型           Image     用来放各种类型的操作系统镜像         可以是public的(google 或 开源组织, 或第三方社区)     也可以是 custom 的(用户自建的)             创建custom image 的主要目的是把一个已经安装和配置了很多内容的实例做成定制的镜像, 以后再创建这个镜像的实例的时候, 无需重新安装配置, 大大减少实例第一次安装启动时间.        custom image 可以通过以下内容创建                 vm实例         a disk         a snapshot         另一个image         一个文件                      比 startup script 好用                     通过 Image 生成 Instance 的操作系统, 存在 disk上    disk     image 会安装操作系统到磁盘   可以被单独用来做成 custom image, 让 instance template 用    把 VM 拷贝到另一个 project 的方法     给 VM 的 boot disk创建一个 snapshot   通过 snapshot 创建一个 image   把 image 分享到其他 project   通过 image 创建新的 VM    IP 地址     一个instance 可以有一个external IP(可选), 和一个internal IP (必须有)   external IP 是临时的, 实例停了, external IP 就丢了   static IP 是单独申请的(VPC 管理界面里面) , 创建后可以attach 到一个虚机上, 就替换掉了原来的external IP         static IP 单独收费, 不attach 更贵,     虚机删了, static IP 还在, 要单独删掉     static IP 可以从一个虚机换到两一个虚机上           Startup script     将脚本写到 startup script 中, 可以让虚机第一次启动的时候就把我们需要执行的ssh命令做好, 比如安装apache2并启动   脚本一般是安装操作系统补丁或者安装软件   也可以编辑一个存在的实例的startup script   startup script 的一个例子: 部署新的应用版本到 compute engine    12345678910# Setup logging agentcurl -sSO https://dl. google. com/cloudagents/install-logging-agent. shsudo bash install-logging-agent. sh# Install GITapt-get updateapt-get install -yq git# Clone Repogit clone https://github. com/in28minutes/your-app. git /opt/app# Build and Run app//. . .            troubleshootting vm 启动失败的步骤              看看 Quota 是否有报错.              比如实例个数限制                              检查boot disk 是否满了             超过了 95% 可能就启动不起来了                              检查串行端口输出 Serial port output             每个 vm 有4 个 串行端口       OS, BIOS 和其他系统级别的服务都往 串行端口写数据       可以通过 Cloud console, gcloud 命令, Compute Engine API 访问 串行端口       可以把串行端口的信息输出到 cloud logging 中, 方便查询                 Serial console logs                                             检查 file system 是否除了问题             把boot disk 挂载到其他虚机上试试 是否可以正常启动.                       shutdown script     在 vm stop, restart, terminate 之前执行的命令脚本.          比如脚本内容是 清除或者导出日志          任何 VM 都可以用   和 startup script 很像         Linux 系统 中是 root 用户来执行脚本     Windows 系统是 System account 执行脚本     作为 metadata 来存储             创建 VM 的时候在 startup script 下面的metadata 那里设置 shutdown script                 -metadata-from-file shutdown-script=script. sh                      也可以把脚本放在 cloud storage 中, 然后在 metadata 中设置key valuae时指向存贮的bucket                 -metadata shutdown-script-url=gs://bucket-in-cloud-storage/file                                    例外, 不会执行 shutdown script 的情况         如果 vm 是 硬重置(比如断电), hard reset(instances. reset)     如果 vm 是 preemptible 实例(临时用的那种便宜的), 且过了 grace period , 就不能执行了.            Instance template     把常用配置虚机的参数放到 instance template 中, 省去每次创建虚机都要在console 选一遍的麻烦   包括机器家族, 机器类型, 镜像选择, startup 脚本等   一旦创建不能更新   Instance template 配合 rolling update 可以快速给 MIG 部署新版本, 也可以快速给 MIG 降版本    GCE 的使用场景     客户希望用独占的服务器(or BYOL)硬件来运行实例 (公司合规, 证书, 管理等需要)         方案: Sole tenant nodes             一个物理机, 专门给你用的, 上面有 VMs       创建 VM 实例, 基于 node name 来给每个node 做 node affinity labels. 确保每个客户有自己专属的 host.                      客户有1000个 vm ,需要能够自动给OS 打补丁, OS 版本管理, OS 安装软件管理         方案: VM manager           ssh 进虚机的方法     option1: GCP console 中的 ssh 界面直接登录 vm         compute engine 自动创建了 sshk key pair          option2: Gcloud 命令         用命令 gcloud compute ssh     compute engine VM 自动创建了 sshk key pair 和用户名          option3: Customized ssh key (用户自建的ssh key 登录vm)         madadata managed 和 OS login 都可以用     上传自己的key 到 VM 的 metadata 中          可以设置 project 禁止某些instance 使用 ssh key         gcloud compute instances add-metadata [INSTANCE_NAME] --metadata block-project-ssh-keys=TRUE           自建 ssh key 连接GCE 的虚机的方法     两种方法            Meta managed ssh key (Mac 的 ssh 可以用)```bashssh-keygen -t rsa -f ~/. ssh/gc_rsa -C dalongco //会生成叫做 gc-rsa 的一对公钥和私钥cd ~/. ssh cat gc_rsa. pub  // 这里把输出的公钥内容复制, 黏贴到GCE 的 VM虚机的 metadata 中, 方法是编辑一下VM, 找到SSH 那里, 新建一条, 把这个黏贴进去chmod 400 gc_rsa  // 修改私钥的可执行权限ssh -i gc_rsa dalongco@34. 125. 210. 83  //尝试登录          123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187 - 2. OS Login 的方法   - 这个方法不需要自己生成 SSK key   - 可以在需要多个用户访问项目或者实例的时候使用   - 方法:     - 在 instance 中 的 metadata 中开启 OS login, 让 linux 操作系统中的用户直接访问 instance.      - 可以用命令 `gcloud compute project-info/instances add-metadata --metadata enable-oslogin=TRUE`    - 也可以把 Linux OS 中的已有的 AD 和 LDAP account 导入到google cloud 中    - 需要有 roles/compute. osLogin or roles/compute. osAdminLogin 权限 - Windows 的实例, 用户可以用 用户名密码登录  - 用户名和密码在 console 中 或者 gcloud 命令生成    - `gcloud compute reset-windows-password`##### Instance Group - 把多个VM 当做一个实体进行统一管理- 一个 MIG 只能在一个regional , 但MIG 中的 VMs 可以跨zones - 两种IG - MIG - managed instance group  - 用一个instance template 创建的 IG 叫 MIG  - 可以进行 auto scaling, auto healing 和 管理版本升级(release managemeng)  - Auto scaling   - 可以维持固定个数的实例, 如果有一个挂了, 会自动创建另一个一样的实例   - 可以用 Load balance 分布流量   - 跨zone 的   - 可设置最多和最少实例个数   - 可以根据设定指标自动增减实例个数    - CPU 使用率    - LB 的流量负载    - Stack Driver (一个 google 的监控软件)   - cool down period    - 为了防止过于频繁的增减实例, 通过cool down 来设置每次实例变化后要冷静的时间   - scale in control     - 防止实例掉的太猛    - 比如, 5 分钟内不能掉 10% 或 3 个实例      - Health check   - 可以检测实例的健康   - auto healing    - 设置个时间段, 让实例启动一会, 应用跑起来了再进行第一次健康检查, 否则以为实例挂了   - 确保 firewall rule 正常配置了, LB 可以 对 MIG/VM 进行 health check  - 版本升级 - Release manage    - rolling update - 滚动升级    - 通过给 MIG 原来的 instance template 增加一个新的instance template 来升级实例    - 给同一个MIG下面的实例升级, 不会有downtime.     - 升级是可以设置成按批更换, 按百分比或者是个数    - 可以设置几个几个更新(max surge), 最少保留几个活的实例(max unavailable)    - Rolling Update for MIG 有两种模式     - Proactive Update Mode 主动模式      - 如果希望自动部署更新, 可以选择主动模式      - 适合关键更新     - Opportunistic Update Mode 机会模式      - 如果怕自动更新破坏力太大, 可以选用机会模式      - 应用的更新需要人工的启动, 并选择哪些实例要被更新, 或新的实例被创建      - GCE 不会主动的选择用机会模式来更新实例.       - 适合非关键更新   - rolling restart/replace - 滚动重启/替换    - 滚动重启可以设置是否都重启,还是几个几个重启    - 滚动替换是用同一个 instance template 生成新的实例来替换现有实例     - 比如改了 startup script 内容     - 可以设置几个几个换(max surge), 最少保留几个活的实例(max unavailable)   - Canary Deployment - 金丝雀部署    - 对指定的一部分用户进行升级, 其他的不动    - Deploys a release to a subset of users - UMIG - unmanaged instance group  - IG 下面是不同类型的VM, 或者是无法进行 Horizontal scaling 的应用  - 不能进行 auto scalling, auto healing 和 管理版本升级  - 场景: Lift and Shift of on-prem workload that need a load balancer to serve trafic   - Lift and Shift - 通常指用于将本地资产在不重构的情况下, 迁移到云环境上云方式. 这种方式不会对原有(单体)应用进行微服务化改造, 一般是直接迁移到 云的 VM 环境.    - OP 应用接收 TCP请求, 不支持 Horizontal scaling. 需要访问文件系统(file system), 可接受 downtime. 迁移到 GCP 的方案?    - network LB + UMIG with active and standby in different zones + regional PD      1.  因为是 TCP, 所以 network LB 有更好的performance      2.  因为不能 HS , 所以 UMIG with active/standby 更合适      3.  因为 file system, 所以 regional PD 更合适##### bastion host 堡垒机- bastion host 是一个特殊功能的 VM - 提供用户从 GCP 外部 SSH 到堡垒机, 然后在堡垒机上再对 堡垒机所在的 VPC 的服务进行访问.  - 在不暴露 VPC 资源的情况下,允许用户从外部访问 GCP 资源- 使用流程  1. 在 VPC 中建立一个堡垒机 VM  1. 给 堡垒机配一个 public IP  2. 用 SSH 连接到 堡垒机, 再在堡垒机上 SSH 连接到其他 VM  3. 工作完成了, 把堡垒机关掉#### Google App Engine - GAE- GAE 是谷歌提供的 Serverless PaaS - Google 负责管理 hardware 和 network infra- App Engine 是完全托管式无服务器平台，可用于大规模开发和托管 Web 应用。您可以选择多种主流的语言、库和框架来开发应用，然后让 App Engine 为您预配服务器并根据需求扩缩应用实例数量。- App Engine 和 GKE 的区别 - App Engine 是基于容器的 - GKE 是基于 K8s 的- 支持 Go, Java, . Net, Node. js, PHP, Ruby, python- 适合web 应用部署- 按用量付费 - 如果没有用量, 每天的费用可以降到 0- 自动的 LB 和 Auto scalling- 使用前需要先开启 Kubernetes Engine API - 需要手动用 gcloud shell 在命令行中部署服务, 创建版本, 分配流量- 提供端到端的应用管理 - 代码上传 - 应用部署 - 应用版本管理  - 基于版本的流量分配管理   - 三种配置    - 按 IP    - 按 Cookie    - 随机 random - 应用实例管理 - 提供应用连接到其他谷歌云服务- 和 Compute Engine 相比, APP Enginede  - 不需要自己管理底层 - 但是会缺少一些灵活度- 两种 App engine 环境 - 两种类型的区别 - ![](http://dalong-1251052260. cos. ap-shanghai. myqcloud. com/2023-02-08-062026. jpg) - Standard 标准型  - 应用实例跑在预先配置好的开发语言的沙箱/容器环境中   - 应用需要遵守沙箱预设的约束条件   - 只支持部分开发语言 Java Python、PHP Go、Node. js 和 Ruby   - V2 比 V1 支持更多的开发语言 ,但也只是特定的语言  - 无法 ssh 访问底层进程和磁盘  - 标准型包含了   - Persistent Storage(可以 query , sorting, transaction)   - Autoscaling   - LB   - Sub/pub   - Scheduler   - 其他 API  - 使用的三个步骤   - 本地开发   - 部署到 App Engine   - App Engine 为应用提供沙箱的安全服务 - Flexible 灵活型  - 应用实例跑在Docker 容器中   - 容器可以是自己提供的容器, 更加灵活  - 容器跑在虚机上   - 可以定制化 容器的 Image 和 Docker file   - 可以定制化VM的OS   - 应用可以写到本地磁盘上(local disk)  - 支持任何开发语言  - 可以 ssh 访问底层的进程和磁盘  - 可以安装 third-party binaries. - App Engine 的分层 - 从上到下  - application  - Service  - Version  - Instance - 备注  - 一个项目对应一个应用  - 一个应用可以有多个服务  - 一个服务可以理解成一个微服务  - 一个服务可以有多个版本   - 每个版本对应了不同的代码和配置   - 多个版本可以共享流量分配  - 一个版本可以有多个实例, 也可以没有实例. 没有实例的版本也可以运行   - 实例是由系统自动分配给版本的- gcloud shell 在命令行中部署服务, 创建版本, 分配流量 - 步骤:  1. 将python 的两个服务的代码目录传到 gcloud editor中 我自己的home目录中  2. 在 gcloud terminal 中进入其中一个目录```bashcd default-service gcloud app deploy   # 部署这个目录为一个服务gcloud app services list gcloud app versions list # 已经产生了默认的第一个版本gcloud app instances list # 已经产生了自动创建的第一个实例gcloud app deploy --version=v2 # 修改了代码重新部署. 可以指定版本号, 如果不指定, 默认会有一个新的版本. 默认部署了新的版本, 当前服务就会用最新版本对外服务(接收流量)gcloud app versions list gcloud app browse  # 查看当前的服务的浏览器地址gcloud app browse --version 20210215t072907 # 查看指定版本服务的浏览器地址gcloud app deploy --version=v3 --no-promote # 部署新的版本, 但是不提供对外服务gcloud app browse --version v3 watch curl https://melodic-furnace-304906. uc. r. appspot. com/ # 监控网页内容变化, 每秒重新刷新页面gcloud app services set-traffic --splits=v3=. 5,v2=. 5 --split-by=random  # 设置不同版本的分流, 分流规则是 random.  cd . . /my-first-service/ # 部署另一个服务gcloud app deploygcloud app browse --service=my-first-service gcloud app services list gcloud app regions list GAE 命令总结     Migrate Traffic 这个参数是用来回滚到之前的版本的   Google Kubernetes Engine - GKE:  谷歌云管理的 K8S 服务 是 CaaS     容器的操作系统使用的是基于分享的内核层         Containers use a shared base operating system stored in a shared kernel layer.            底层是 Compute Engine (VM 实例) GKE 中的 K8s 是自动更新的, 我们不需要管 比起 App Engine, 更适合复杂的微服务架构, 更灵活 有自己的 OS - Container-Optimized OS GKE 的硬件层级     硬件的管理用 gcloud 命令   从上到下分为几个层级         Cluster             是物理概念, 一个集群. 需要先创建集群, 才能部署容器应用       创建了 Cluster , 默认就会创建一个 Node Pool       cluster 包含了很多 Compute engine 实例                 A group of machines where Kubernetes can schedule workloads.                       分为 Master Nodes 和 work Nodes                 Master nodes 管理cluster         work nodes 运行workload in pods                      Cluster type                 Single Zone         Multi zone         reginal         private         alpha                      默认, GEK cluster 上的nodes 是 有 public IP 地址的. 但是好的practice 是创建一个 private cluster , 让所有的 nodes 只有内网 IP. 外面可以接一个 HTTPS LB 或者 Network LB. 外面再配一个 Cloud NAT 访问外网                Node Pool             集群中全都具有相同配置的一组节点       Node Pool 中默认会创建 3 个 Nodes       可以为 Cluster 增加更多的 Node Pool       应用场景                 当我们需要部署一个第三方的, 安全性未知的应用, 最好部署单独的 node pool         对于 GPU 有特殊需求的应用, 单独部署一个 node pool                               Node             集群中的一个工作机器, 可以是一个 VM 或一个实例.        分为 Master Nodes 和 work Nodes                 Master nodes 管理cluster                     API Server 提供 nodes 之间的通信和对外通信           Scheduler 提供 pods 的 替换           etcd 是一个分布式存储记录集群状态                            work nodes 运行workload in pods                     kubelet 是运行在 work notes上跟 master notes 通信的                                                        GKE 的软件层级     软件的管理用 kubectl 命令   Deoloyment         deployment 管理 Pods     deoployment 代表了一个微服务和微服务的所有release     默认一个deployment 有一个 instance 对应             每个instance 都有一个 pod 来运行部署的应用       刷新页面, 就可以看到两个instance 提供的服务                可以通过在 Console 中编辑 Yaml 来修改 workload     Replica set (版本) ReplicaSets             确保 deplyoment 有指定的pods 个数可以运行指定的微服务的某个版本                 当一个pod 挂了, 马上会创建一个新的                                    Service         expose deployment     可以理解成 Microservices, 是包含了若干个 Pod     可以通过在 Console 中编辑 Yaml 来修改 service     有三种类型的 Service             Cluster IP                 只能在集群内部访问的 IP                      LoadBalancer                 可以对外服务, 每个微服务可以有一个 LB                      Nodeport,                 将一个静态端口提供给每个Node’s IP .          如果不想给每个微服务都配一个 LB, 就用这个         NodePort服务是将外部流量直接发送给您的服务的最原始方式。 顾名思义，NodePort在所有节点（VM）上打开一个特定端口，并且发送到该端口的任何流量都将转发到该服务。                               Service DNS             让GKE 中的微服务通过内网进行互相访问                 为每个微服务在 GKE cluster 中创建 Deployment,         使用 Service 暴露 deployment         通过 Service DNS, Cluster 中的每个微服务可以和其他微服务进行通信                                    Ingress         Ingress是k8s的标准对象，Ingress对象本身只是定义了一系列的流量路由规则，类似nginx. conf配置     Ingress controller是负载均衡器的实体，由各个厂商独立实现，通过读取ingress的规则进行流量路由。     提供 LB 和 SSL 终端     在使用 NodePort的时候, 可以使用 Ingress          Pod         Pod是 K8S 的最小对象, 包含一个或多个容器，例如Docker 容器             Pod 里面是容器                pod 部署在worker node 里面     每个 Pod 有一个临时 IP 地址     Pod 内的多个容器共享 IP地址, 网络, 磁盘, 端口, volumes.           Docker         运行应用的独立容器环境          Docker Image         创建Docker 内容的镜像     Docker File             保存了创建一个镜像的命令                     Container Registry         存放 Docker Image Repository 镜像仓库          FROM node:8. 16. 1-alpine   # 基础镜像WORKDIR /app        # 镜像创建的 Docker 时 的工作目录COPY . /app         # 将本地目录的内容拷贝到工作目录RUN npm install       # 运行一个命令EXPOSE 5000         # 暴露一个端口CMD node index. js      # 启动 Docker 时执行一个命令 StatefulSet ,是一个 GKE的 一组稳定的, 保持状态的, 有稳定的域名 (hostname) 的 Pods集合. 不管是否会被 redeployment 或重启, 都会保持状态.      一些场景:         GKE 中一组 Redis pods 组成的 statefulSet     GKE 中一组 Cassandra sharding Pods 组成的 statefulSet     GKE 中一组 MongoDB read replicas 组成的 statefulSet              GKE 的 autoscaling     GKE Cluster 既支持 Node 的 伸缩(GKE Cluster Autoscaler), 也支持 Pod 的伸缩 (Horizontal Pod Autoscaler)   在 deployment 中开启         开启 GKE cluster 的 autoscaling 选项     开启 horizontal Pod autoscaling 选项             一旦开启 Horizontal Pod autoscaling, pod 个数会随着流量的大小而变化                      sidecar container     Sidecar containers let you run two tightly coupled containers together. In this article, you’ll learn when and how to use sidecar containers, as well as best practices for using them.    案例         php 应用提供上传照片功能, C 应用负责压缩照片, 两个应用用 sidecar container 进行捆绑提高效率.      日志记录应用和日志分析应用捆绑           Cloud Operations for GKE     You can view a cluster’s key metrics, such as CPU utilization, memory utilization, and the number of open incidents.    You can view clusters by their infrastructure, workloads, or services.    You can inspect namespaces, nodes, workloads, services, pods, and containers.    For pods and containers, you can view metrics as a function of time and view log entries.    注意: Cloud Operation 只能服务于在 GCP 上的 GKE cluster 的监控, GCP之外的服务无法监控; 如果需要多云混合云的监控, Google Cloud Managed Service for Prometheus 更合适    Istio     Istio 是个基于 K8s 的 开源 Service Mesh 扩展.          提供统一的微服务 连接, 管理和安全服务.      提供服务间的流量管理, 比如断掉某个服务的流量(模拟这个服务挂了 - Faul Injection)     不需要修改微服务代码          Istio 提供         HTTP、gRPC、WebSocket、MongoDB 和 TCP 流量的自动负载平衡。     通过丰富的路由规则、重试、故障转移和故障注入对流量行为进行精细控制。 支持访问控制、速率限制和配额的可配置策略层和 API。     集群内所有流量（包括集群入口和出口）的自动指标、日志和跟踪。 通过基于身份的强身份验证和授权，保护群集中的服务到服务通信。     GKE 上的 Istio 是一个工具，可以在您的 GKE 集群中自动安装和升级 Istio。当您升级 GKE 时，该附加组件会自动升级到最新的 GKE 支持的 Istio 版本。这使您可以轻松地管理 Istio 的安装和升级，作为 GKE 集群生命周期的一部分。           使用 GKE 的场景     优化 GKE 并且降低成本         方案:             尝试使用临时的 VM, 选合适的region, 用 committed discount       使用 E2, 不要用 N1       必要的话使用 多个 node pool                     需要一个完整高效的 auto scaling 的 GKE         方案:             配置 HPA, horizontal pod autoscaler                 kubectl autoscale deployment                      配置 cluster autoscaler for node pools                 gcloud container cluster update --enable-autoscaling                                    需要再 GKE 中运行不可信的第三方代码         方案:             通过 GKE Sandbox 创建新的 node pool , 把代码部署上来                     希望 GKE 中的 cluster 只进行内网通信         方案:             服务类型选择 ClusterIP                     发现 pod 一直是 Panding 状态         可能是 Pod 不能被正常 schedule 到Node 上          发现 pod 一直是 Waiting 状态         可能是 镜像 pull 不下来           CI/CD 安全     Binary authorization (可以去本文的Binary authorization专门章节看)    Standard GKE 的创建过程     创建一个新的项目   先开启 GKE 的 API   创建一个 Cluster (这里选Standard 版本)   Login to Cloud Shell   1234567891011121314151617181920212223242526272829303132333435363738394041gcloud config set project my-kubernetes-project-372302  # switch 到新的项目gcloud container clusters get-credentials my-cluster --zone us-central1-c --project my-kubernetes-project-372302 # 获取连接到 Kubernetes Cluster 的认证, 获取后就可以用k8s命令了kubectl create deployment hello-world-rest-api --image=in28min/hello-world-rest-api:0. 0. 1. RELEASE # 用 K8s 的命令创建一个github 的镜像生成的 deployment kubectl get deployment # 查看deployment情况kubectl expose deployment hello-world-rest-api --type=LoadBalancer --port=8080 # 将 workload 暴露成 Service, 给外部用户. kubectl scale deployment hello-world-rest-api --replicas=2 # 为 deployment 增加一个 pod (默认只有一个, 这里增加了1 个, 是replica set)kubectl get deployment watch curl http://34. 29. 41. 99:8080/hello-world # 查看LB 下不同实例返回的结果kubectl get pods # 查看pods gcloud container clusters resize my-cluster --node-pool default-pool --num-nodes 5 --zone us-central1-c # 增加 Node 的个数到 5 个kubectl autoscale deployment hello-world-rest-api --max=4 --cpu-percent=70 # 设置auto scaling , 最大pod 个数增加为 4 个, 检测 CPU 时用率kubectl get hpa # Horizential Pods Autoscalingkubectl create configmap hello-world-config --from-literal=RDS_DB_NAME=todos # 增加配置参数kubectl get configmapkubectl describe configmap hello-world-config # kubectl create secret generic hello-world-secrets-1 --from-literal=RDS_PASSWORD=dummytodos # 创建密码kubectl get secretkubectl describe secret hello-world-secrets-1kubectl apply -f deployment. yaml  # 应用 yaml 的配置(当修改了 Yaml 文件后应这个命令) gcloud container node-pools create POOL_NAME --cluster CLUSTER_NAME # 创建一个新的 Node poolgcloud container node-pools list --zone=us-central1-c --cluster=my-cluster kubectl get pods -o wide  # 查看pods 的完整信息列表, 包含了外部 IP 地址 kubectl set image deployment hello-world-rest-api hello-world-rest-api=in28min/hello-world-rest-api:0. 0. 2. RELEASE ## 为服务创建一个新的release(版本 0. 0. 2) kubectl get services kubectl get replicasets kubectl get podskubectl delete pod hello-world-rest-api-58dc9d7fcc-8pv7r   kubectl scale deployment hello-world-rest-api --replicas=1 kubectl get replicasets gcloud projects list kubectl delete service hello-world-rest-api # 删除服务kubectl delete deployment hello-world-rest-api # 删除deployment / workstreamgcloud container clusters delete my-cluster --zone us-central1-c # 删除 custer 集群gcloud projects delete my-kubernetes-project-372302 # 删除项目 命令强调     nodes pool 大小调整用 resize 参数   nodes 的 autoscaling 的 最大最小nodes 限制使用 update 参数.     yaml 文件的例子 - deployment     修改后用kubectl apply -f deployment. yaml 来应用更新    1234567891011121314151617181920apiVersion: apps/v1kind: Deploymentmetadata:labels:  app: hello-world-rest-apiname: hello-world-rest-apinamespace: defaultspec:replicas: 3selector:  matchLabels:    app: hello-world-rest-apitemplate:  metadata:    labels:      app: hello-world-rest-api  spec:    containers:      - image: 'in28min/hello-world-rest-api:0. 0. 3. RELEASE'       name: hello-world-rest-api          Cloud Functions:  需要先开启 Cloud Funciton API, Google Pub/Sub API, Cloud Logging API , Cloud Build API, Cloud Storage API, Cloud firestore API, Eventarc API.  事件驱动型 Serverless 平台, 不需要关心底层架构, autoscaling .  用户选择 Cloud Function 的原因:     应用程序包含事件驱动代码, 用户不想为其配置其他计算资源   Cloud Function 可以无缝的 scaling    Cloud Functions 对代码的部署方式施加了更多限制（显然易见，您需要将其打包为一个函数） Cloud Function v2 在后台其实是运行在 Cloud Run 上面的     当创建了一个新的 Cloud Function, 也可以在 Cloud Run 看到对应的服务    适合被动被调用, 短时间计算的逻辑, 不适合长期运行的应用 计费     按调用时间, 请求次数, 内存 cpu 消耗情况计费    超时     默认function 执行 1 分钟就会自动停掉. 最大可以设置超时时间为 60 分钟   function v1 最大9 分钟, v2 是 60 分钟    触发场景例子     文件上传到 Cloud Storage   Error log 写到了 Cloud Logging   一条消息传到了 Cloud Pub/Sub (消息队列)   一个 Http/Https 的网页或 Rest API 被调用   Firebase   Cloud Firestore   Stack driver logging    怎么让两个 Cloud Function 的一个调用另一个?     给 function B “request authentication”   创建一个 service account, 并关联到 function a 上   为 service accoutn 赋予 访问 function b 的权限   在 function a 中创建一个 ID token , 当a 调用 b 的时候, 把这个 token pass 过去.     支持语言     . net   java   php   python   go   ruby   node. js    V1 和 V2 的区别     V2是基于Cloud Run and Eventarc   V2 增强了         Longer Request timeout: Up to 60 分钟             V1 9分钟                Larger instance sizes: Up to 16GiB RAM with 4 vCPU             V1 8G 2vCPU                Concurrency: Upto 1000 concurrent requests per function instance             V1 一个并发                 V1 默认是一个funciton 处理一个请求, 如果第二个请求来了, autoscaler 会创建第二个function 来处理第二个请求, 但这时个cold start(冷创建, 有个创建过程)         解决方法是设置最少的默认function 实例个数, 或者用 V2                      V2 虽然支持1000并发, 但我们要自己负责代码的并发可以被安全的执行.                 Multiple Function Revisions and Traffic splitting supported             V1 不支持                Support for 90+ event types - enabled by Eventarc             V1 支持 7 个                      HTTP 请求 Node. js 的function 样例  1234const escapeHtml = require('escape-html');exports. helloHttp = (req, res) =&gt; {res. send(`Hello ${escapeHtml(req. query. name || req. body. name || 'World')}!`);};    命令行操作 function     gcloud functions deploy [NAME]   参数         –gen2 第二代     –runtime 指定语言     –timeout 超时时间     –max-instances, –min-instances 最大, 最小实例个数     –source 指定源代码     –trigger-bucket 触发方式     –serve-all-traffic-latest-revision 指定服务的版本     –docker-registry, –docker-repository 存放镜像位置     –service-account 设置服务账号           function 的最佳实践     为了避免cold start, 设置 min 最小实例个数 (成本会相应升高)   为了加快实例启动时间, 尽量减少function 的依赖关系   设置max 最大实例数, 可以避免异常的超大流量带来的额外费用    用Cloud Endpints (或 APIgee 或 API gateway) 给 function 设置版本, 方便用户可以访问不同的版本的function     用 V2 的 function, 可以在不同版本间分配流量, 可以rollback 到之前的版本   用 Secret Manager 管理 API 的 Key   为每个 function 设置不同的 service account, 让function 之间调用可以更安全         Grant roles/cloudfunctions. invoker role to invoke a cloud function          通过 npm, pip 等包管理工具来管理 function 的依赖   Google Cloud Run:  和 function 很像, 但是支持所有的开发语言, 有更长的超时时间, 可以同时接受更多的请求数.  就是无服务版本的 GKE, 用 GKE 的用户建议迁到这个 Cloud Run 也是 serverless 基于 Knative 构建， 是 Google 最新的 Serverless 产品。 方便在不同的云上迁移, 因为是基于容器的 使您可以将代码打包在无状态容器中，然后通过 HTTP 请求(Web, pub/sub)调用它 开发者用 Cloud Run 的三步流程     write code   build and package app in container   deploy to cloud run         Cloud Run 只能deploy 放在 Artifact Registry 中的 Image           三种无服务(App Engine, Cluod Function, Cloud Run)的选择     如果您已经将代码打包在 Docker 容器中或正在 Google Cloud 中运行 Kubernetes 集群，请针对您的 Serverless 工作负载考虑使用 Cloud Run 或 Knative。   对于运行响应实时事件的代码，或在不使用容器的情况下处理请求，请使用 Cloud Functions。         Cloud Function 在后台其实是运行在 Cloud Run 上面的          如果您需要在一个地方放置多个函数并且只想部署整个应用程序，请使用 App Engine。    集成了诸多 GCP 的服务     Cloud Code   Cloud Build   Cloud Monitoring   Cloud Logging    CI/CI 安全     Binary authorization (可以去专门章节看)    Pricing     两种收费模式         仅在请求处理期间分配 CPU             没有请求就不分配 CPU , 就不用付费       前 n 次请求免费, 超量收费                始终分配 CPU             没请求也分配 CPU       前 n 次请求免费, 超量收费                      Anthos     是一个集成环境, 可以让 K8s 在任何地方运行         私有机房/on premise     公有云(AWS, Azure, 等)     Cloud Run 可以运行在 anthos 上     相当于 OpenShift           场景     如果有容器应用, 但是觉得管理麻烦, 就用 Cloud Run   如果觉得到处(on-premise, 公有云)管理 K8S 太麻烦, 就用 Cloud Run for Anthos    命令行对 Cloud run 创建, 查看, 切换流量  123gcloud run deploy SERVICE-NAME --image IMAGE-URL --revision-suffix v1 gcloud run revisions listgcloud run services update-traffic myservice --to-revisions=v2=10,v1=90   GCE, GKE, GAE, Function, Cloud RUN 区别:  GCE 是 VM GKE 是 K8s 协调平台, 管容器 GAE 是 无服务的, 预设场景的, 支持多语言的开发环境, 比如弄个web 网站 Function 无服务的, 支持多语言的, 提供事件响应的一段函数, 有运行时间限制 Cloud Run 是无服务的 GKE, Function 的底层就是这个. 容器所在的服务选择流程图: Storage:  块存储和文件存储 Block Storage 块存储     我们电脑的硬盘就是块存储   块存储 和 VM 是多对一(含一对一)的关系         read only 的 block storage 可以 attach 到多个 VM 上          两种使用方法         DAS             direct attached storage, 直连                SAN, Storage Area Network             连到一个磁盘池, 有更高的网络连接       用在数据库场景                     Google 提供的两种块存储         Persistent Disks     Local SSDs (SSD)           File Storage 文件存储     Filestore 是google    SAN 和 NAS 的区别     Storage Area Network 是块存储, 存储结构化数据, 比如数据库. 所以和 PD 相关连   NAS 是文件存储, 往往存储视频和图片. 既可以和 PD 关联, 也可以和 Cloud Storage 关联.     Persistent Disks (PD):        是网络块存储 持久性更好     可以使用regional PD, 在 2 个zone 都有磁盘, 但会产生 2x 的价格   也可以使用 zonal PD 价格更便宜, 一般是给数据库用比较合适    可以随意增加 Size , 而不用停机     Step I: Resize the Disk   Step II: Take a snapshot (Just for backup in case things go wrong!)   Step III: Resize the file system and partitions    可以为 VM attache 新的 PD 不用停机     三个步骤:         A: Attach Disk to running or stopped VM     B: Format the disk     C: Mount the disk           单独的生命周期, 不和 VM 生命周期绑定     可以detach and attach 给另一个 VM    可以 regional, Regional PD 复制磁盘到同一个region 的另一个zone.  提升性能的方法     增加 PD size   增加 PD 的个数    Snapshot     快照是从 PD 定期生成的一个磁盘映像(很快, 是增量的). 镜像可以生成 PD (慢, 因为要多个镜像才能生成完整的 PD)         如果需要频繁的把镜像还原成 disk, 可以考虑把镜像创建成image, image 生成disk 很快.           可以schedule snapshot 生成的计划   可以设置snapshot 创建多久后被自动删除         镜像被删除, 其实只是删除了其他镜像不需要的数据, 不用担心.           snapshot 可以跨region, 跨项目   新的snapshot 是增量的, 只对最新变化的内容做快照         第一个snapshot 是全量     后面的snapshot 是针对第一个snapshot 的变化而生成的增量 snapshot     如果多个快照(比如 4 个), 中间两个被删掉了             删除的快照会被标记为已删除       如果被删除的快照没有被其他快照依赖它, 直接删除       如果被删除的快照 有其他快照依赖它                 所有快照上的被依赖的数据会被拷贝到后一个快照上 (第四个)         所有快照上的不被依赖的数据会被删除.          后一个快照将不再依赖这个被标记删除的快照, 而是依赖被删除的快照的前一个快照(第一个).                                              如果要删除快照上的数据, 就要删除所有快照   建议         在流量低谷时做快照           PD 也分三种类型     Standard PD         HDD 机械硬盘     性能一般, 最便宜          Balanced PD         SSD 固态硬盘     性能好, 价格适中          SSD PD         SSD 固态硬盘     性能最好, 价格最贵           可以通过 resize2fs 命令来给VM 的 PD 扩容 (只能增,不能减)     场景, 给正在运行的 VM 上的 数据库的 PD 扩容磁盘         先扩磁盘, 再扩文件系统和分区          Local SSDs (SSD)12345678910111213 - 本地块存储 - 物理 attache 到 VM 上 - 提供临时的数据存储服务 - 和 VM 有共同的生命周期  - 不能 detach and attach 给另一个 VM  - 和 VM 一样是 zonal - 提供更高的 IOPS 和 低延迟  - SSD size 越大, vCPU 越多, 性能越好  - 10 - 100 倍于 PD - 只有部分 machine type 支持 Local SSD - 支持 SCSI 和 NVMe 接口  - Choose NVMe-enabled and multi-queue SCSI images for best performance - 不支持 SnapshotFilestore:  File Storage 文件存储     Box 就算文件存储   一般用于媒体文件    是 Google 提供的高性能文件存储服务 需要先开启 Filestore API 创建时是创建了一个Filestore instance , 这个 instance 是一个google 管理的网络文件系统(NFS), 可以被 GCE, GKE 进行连接和使用 创建后会生成一个NFS mount 点 10. 88. 42. 58:/dalongShare 供计算服务进行mount 支持 NFS v3 protocol up to 320TB , 16GB/s, 480k IOPS 支持 HDD 和 SSD 场景 文件共享, 流媒体, 内容管理, 应用对 POSIX 文件系统高速的读写     Cloud Filestore is POSIX complaint.     NAS (Network attached storage) POSIX     POSIX: Portable Operating System Interface of Unix. ，可移植操作系统接口   目的是提高应用程序在不同操作系统之间的可移植性   技巧: 提到 POSIX 就是 Filestore   Machine Images:  给一个 VM 做完整的 image 是类似 Images, 但是支持多个磁盘attache 到 VM 时, 给VM instance 做镜像     Custom Image 一般只是给boot 磁盘做一个 Image, 包含了 OS 和已经安装的软件   Macine Image 是给 VM 做一个完整的Image    一个 Machine Image 包含了     configuration   metadata   permissiions   data from one or more disks    场景     磁盘备份, 实例clone 和 复制      Macine Image 和 Snapshot, Custom Image, Instance Template 的对比   Global, Regional and Zonal Resources 比较     Global         Images     Snapshots     Instance templates (Unless you use zonal resources in your templates)          Regional         Regional managed instance groups     Regional persistent disks          Zonal         Zonal managed instance groups     Instances     Persistent disks             You can attach a disk only to instances in the same zone as the disk                     Google Cloud Storage:  谷歌提供的对象存储是 Cloud Storage, 相当于 AWS 的 S3 题目如果说到了 data lake (数据湖), 往往是 Cloud Storage 对象存储在bucket 中, 需要先创建bucket, 再上传对象     bucket 名字需要全球唯一, 3-36 个字符 (小写字母,数字,下划线,中划线,点)   bucket 名字不能包含 goog   bucket 要属于某个project   bucket 中的对象个数不受限制   对象上传不支持 partial upload (分块上传)   单个对象最大 5T   每个对象有一个unique key    无服务, autoscaling, 无限扩容 提供 Rest API , 命令行(gsutil) 进行访问 对象储存(object storage)的分类     Standard Storage         随存随取, hot data          Nearline Storage         30 天取一次, once per month          Coldline Storage         60/90 天读一次          Archive Storage         1 年读一次           可以跨region     99. 95 for multi-region (不含 archive)   99. 99 for single-region (不含 archive)    版本 version     防止数据被误删除或者误修改   bucket level 随时开关   和 duration 功能有冲突, 只能二选一    Lifecycle 管理     可以把对象按照计划改变分类(standard -&gt; newrline -&gt; coldline -&gt; artchive)         和设置多个条件组合          可以设置多久后删掉         要写一个 JSON 格式的 lifecycle management rule (配置多久删掉什么object) , 并用 gsutil 命令 把他 push 到bucket 中生效           Bucket lock     如果企业对数据的合规性有要求, 可以通过 lock 来锁定设置的规则.    data retention policy 被锁定, 确保数据可以保留我们希望的时间, 规则不会被删除. 保存时间不但能被缩短, 但可以延长.    retention policy 负责数据保留多久, retention lock 负责在保留期间内数据不会被改动(包括删除).    lifecycle policy 是多久可以把一个 object 挪到另一个 bucket level 或删掉. 只有过了 retention policy 的约定时间的 object 才会被 lifecycle policy 删除.     Encryption 加密     默认用 server-side 加密         server side 加密分为             Google-managed                 默认的, 不需要配置, google 创建key, 管理 key, 对对象进行加密                      Customer-managed                 google 创建key, Cloud KMS 管理key, 客户通过key 对数据进行加密解密                      Customer-supplied                 客户自己的key, 在服务端对数据进行加密解密                               client side encryption             客户在上传前就完成了数据加密, 客户收到收据后自己解密, 和google 一点关系没有.                       对象的 metadata     每个对象存储好后都会有 metadata   是 key pair 数据   metadata 分为         Fixed-key metadata             key 固定的, 不能改, 但value 可以随便改       比如: Cache-Control, Content-Disposition, Content-Type                Custom metadata             key 和 value 都是自己创建的, 随便定义随便增删改                Non-editable metadata             google 预定义好的 metadata, 只能看, 不能删改                      数据迁移     如果想把数据迁移到 object storage, 有几种方法         gsuitl or API or console             适用于一次性的, 数据量较少(小于 1T )的迁移       适用于从 on premise 到 object storage 的迁移, 或 bucket 之间的迁移       成本低                Storage Transfer Service             使用与大规模数据迁移 (1-20T)       是适用于从 on premise, aws, azure, google cloud迁移到 object storage       可提供可重复的增量迁移.        有容错机制, 从哪里失败了, 还可以继续.        稳定                Transfer Appliance             相当于 AWS 的 snow 系列硬件       适合 20T 以上的迁移, 适合带宽低, 时间久的场景       先预定硬件, 到货了把数据拷贝到硬件上, 然后寄回给google. google 工程师帮你把数据拷贝到 bucket 中       快, 量大                      gsutil 命令行1234567891011121314151617181920gsutil mb gs://BKT_NAME # (Create Cloud Storage bucket)gsutil ls -a gs://BKT_NAME # (List current and non-current object versions) gsutil cp gs://SRC_BKT/SRC_OBJ gs://DESTN_BKT/NAME_COPY # (Copy objects) # -o 'GSUtil:encryption_key=ENCRYPTION_KEY' - Encrypt Object gsutil mv (Rename/Move objects)gsutil mv gs://BKT_NAME/OLD_OBJ_NAME gs://BKT_NAME/NEW_OBJ_NAMEgsutil mv gs://OLD_BUCKET_NAME/OLD_OBJECT_NAME gs://NEW_BKT_NAME/NEW_OBJ_NAMEgsutil rewrite -s STORAGE_CLASS gs://BKT_NAME/OBJ_PATH # (Ex: Change Storage Class for objects) # gsutil cp : Upload and Download Objectsgsutil cp LOCAL_LOCATION gs://DESTINATION_BKT_NAME/ # (Upload)gsutil cp gs://BKT_NAME/OBJ_PATH LOCAL_LOCATION # (Download)gsutil versioning set on/off gs://BKT_NAME # (Enable/Disable Versioning)gsutil uniformbucketlevelaccess set on/off gs://BKT_NAMEgsutil acl ch # (Set Access Permissions for Specific Objects)gsutil acl ch -u AllUsers:R gs://BKT_NAME/OBJ_PATH # (Make specific object public)gsutil acl ch -u john. doe@example. com:WRITE gs://BKT_NAME/OBJ_PATH # Permissions - READ (R), WRITE (W), OWNER (O) # Scope - User, allAuthenticatedUsers, allUsers(-u), Group (-g), Project (-p) etcgsutil acl set JSON_FILE gs://BKT_NAMEgsutil iam ch MBR_TYPE:MBR_NAME:IAM_ROLE gs://BKT_NAME # (Setup IAM role)gsutil iam ch user:me@myemail. com:objectCreator gs://BKT_NAMEgsutil iam ch allUsers:objectViewer gs://BKT_NAME # (make the entire bucket readable)gsutil signurl -d 10m YOUR_KEY gs://BUCKET_NAME/OBJECT_PATH # (Signed URL for temporary access) 额外的几个命令     通过 gsutil -m命令 拷贝文件 (-m 是多线程上传)   通过 gsutil hash -c FILENAME 来计算所有 on-premise 文件的 CRC32C 的 哈希值   通过 gsutil ls -L gs://[YOUR_BUCKET_NAME] 来获取所有上传到 backet 中的文件的 哈希值    最佳实践     数据存到离用户近的region   数据读取率不要变化太快, 要逐步提升读写量   遇到报错, 逐渐放缓读取频率   bucket 和 object 名字不要用敏感信息, 不要用序数, 不要用时间戳         具有顺序名称的对象是连续存储的，因此它们可能会访问同一后端服务器。发生这种情况时，吞吐量会受到限制。     可以用哈希值,          Cloud Storage FUSE 可以让bucket 像目录一样在linux 系统中被管理   尽量合并和压缩小文件成为大文件, 可以节省网络带宽, 节省bucket 的存储空间.    如果是用命令上传文件, 可以用 -m 多线程上传, 加快速度.     场景     数据 30 天后就不会被访问了, 但要保存 4 年满足合规.          方案: 用standard -&gt; archive -&gt; 4 年后删掉          需要把 2T 数据从 AWS 转到 google cloud         方案: Cloud Storage Tansfer Service          客户需要管理他们的 Key         方案: Customer-managed key          由于合规原因, 对象创建后两年内不能修改         方案: 配置 并 lock retention policy           Signed URL     看 IAM, Service Account (服务账号), ACL 这章    静态网站     看 IAM, Service Account (服务账号), ACL 这章   Database:  数据库的决策树         场景:     初创公司, schema 不确定,还在进化中         FireStore          No SQL 数据库, 存储量不大         FireStore          Transctional Global Database, 确定的schema, 需要处理大量数据         Spanner          Cache Data for web app         MemoryStore          Huge data from IOT         BigTable          Huge data with time series         BugTable           几个概念     Availability - 就是多少个 9         99. 99% 是一个月 4 分半宕机, 99. 999% 是有 26 秒宕机     增加 standby 数据库, multi-zones, multi-regions          Durability - 持久性, 就是保证多久不丢数据文件         99. 999999999% 表示 1000 万年保存 100万个文件, 丢了一个文件.      增加 standby, snapshots, transaction logs, replicas, multi-zones, multi-regions          RTO - 最快宕机回复时间 (最大可接受的宕机时间)   RPO - 回复后, 恢复到事故前的几个小时 (最大可接受的丢失数据时长)   几种数据库恢复机制和 RTO, RPO 关系         Hot Standby             一个数据库挂了, 另一个可以马上自动切换过来       RPO 1m, RTO 5m                Warm Standby             一个数据库挂了, 另一个需要有最minimum 的infra, 可以快速补充上来       RPO 1m, RTO 15m                Snapshot and transaction logs             一个数据库挂了, 要用 snapshot 和 log 恢复       RPO 1m, RTO few hours                     Consistency - 保持一致性, 一般说的是master 和 replica 之间数据同步的实时性         分为几种             Strong Consistency                 实时将数据同步到所有的 replica         如果 replica 太多, 同步的速度会降低                      Eventual consistency                 异步将数据传递给 replica         所有replica 都完成同步可能要花好几秒钟, 而且有可能从每个replica 获取的结果不太一样                      Read-after-Write consistency                 写入后马上就可以读到写入的数据,                                     数据库类型     RDB         提前定义了 Schema     OLTP 和 OLAP 使用关系型数据库             OLTP DB                 Online Transaction Processing         use row storage         适合 很多用户运行 small transactions         场景: ERP, CRM, E-Commerce, Banking app         Google 提供的无服务OLTP RDB 数据库                                     几种数据存储的比较  Cloud SQL:   支持 Mysql, PostgreSQL, SQL Server 如果上限不超过64 processor core, 30个 TB存储, 400G Ram 可以考虑 Replica 可以跨Region 99. 95% 的 HA 保障 带一个 network firewall 功能     自动加密         存在数据库中的数据被自动加密          HA         通过创建 HA Configuration     有 primary 和 Secondary(standby) 两个实例, 部署在两个不同的zone             primary 的变化会同步到 secondary       primary 挂了, 流量会auto failover 到 secondaryt instance                 就算primary 恢复了, failover 不会自动切换回 primary                               最好用 Cloud SQL Proxy 接收来自app 的请求     Cloud SQL 只能横向扩展读, 不能横向扩展写             最好把写的内容分成不同的小 Cloud SQL instance, 单独部署       不能有两个 Master 一起写                     Read replica         支持自动 replica     解决read workload     不解决 HA 的问题     创建 Read Replica 的前置条件             开启 automatic backup                 启动了 auto backup 会自动开启 Binary Logging                      至少有一个 backup 已经完成                如果replica 有 延迟 (lag), 可以对数据库进行 sharding (分成小块, 横向扩展)             Sharding. Sharding makes horizontal scaling possible by partitioning the database into smaller, more manageable parts (shards), then deploying the parts across a cluster of machines.                      自动增加容量 (automatic storage increase feature)   支持 DMS 数据库迁移   导入和导出         可以导出单个数据库或者表     导出会比较耗时, 导出太大的数据库会影响数据库性能             可以用 Serverless export 工具导出, 效果会好一点       把导入导出的内容分成小块来批处理, 效果会好很多                     自动备份         实例被删了, 备份也会被删掉     备份只能把实例整体进行备份, 不能只备份某个数据库或者表     实例成本包含了 7 个 backup           最佳实践 Cloud SQL 的 灾难恢复 Dister Recovery 方案     运行 Cloud SQL 的 HA Mode (一个 Primary instance, 一个 standby instance, 前面一个 对外的 IP)   定期bakup 数据, 可以在发生灾难之后, 快速创建一个新的数据库实例   创建 cross region read replica. 在发生地区灾难之后, 从另一个地区快速恢复.     使用场景     迁移本地的mysql, postgresql, sql server 上云   为简单关系数据库降低运维成本   Cloud Spanner:  可无限 scale     要开启 autoscalling    上限 几个 PB 99. 999% 的 HA 保证 (5 个 9) multi-regional 支持更好     可以有多个master    很贵 Google Cloud Spanner 的底层是一种分布式的关系型数据库引擎，它是基于 Google 自己的设计和开发的。 选择 Cloud Spanner 的理由     需要global scale   需要 5 个 9 的HA   需要无限扩展的容量   可横向扩展写   任务急, 不差钱    场景     内置高可用   关系性数据库, 需要第二个 index   强大的 global consistency   每秒大量的 Input/output   Cloud FireStore:  NO-Sql DB (Document DB) Flexible schema, 每个表是一个collection, 每条数据是一个document, 每个document 可能有不同的字段     Collection 和 Document 可以互相嵌套 (Document 里面可以有 collection, collection 中可以再有 document)    适合小公司刚启动业务, 有些schema 不清楚的场景 比 RDB 来说, 更加scalability 和 high-performance 自动的 multi-region data replication 付费     每个月 1G 的免费额度   read, write, delete 都要付费   存的数据的大小   带宽使用的大小   每天的免费用量是         5 万个document read     2万个 document write     2万个 delete     1G 存     超过这些用量才会收费           Google 提供的 No-SQL DB     老版本叫做 DataStore   适合 0-few TB   无服务   支持离线模式   支持一个表中有多个 index         创建 key 的时候不要用序数, 或者时间戳, 可以用 allocateIds() 这个函数创建 ID 作为 Key     只用经常用到的query 中需要查询的字段才用来创建index     如果经常有一些 ad hoc 的请求, 建议使用 Bigtable          不支持 join or agreegate (sum or count) 操作   提供客户端的类库         web, ios, android and etc.           两种模式         DataStore mode             在 Datastore 模式 中的数据对象称为实体(entities)       Each entity in a Datastore mode database has a key       key 和 identifier 关联, identifier 可以是 key 的名字或者一个数字                Native mode          导出         通过 gcloud 命令     不能通过console 导出          适用例子:         User profile     让用户将自己的照片存到 Cloud Storage, 然后将照片的meta data 放到 Firestore 中     product catalogs             Cloud BigTable:   不是无服务, 需要自己创建 instance Bigtable 的底层数据库引擎是 Google 自行开发的分布式存储系统，名为 Colossus。 适合大于 10 TB 的情景 , PB 级别支持没问题 很多 Google 核心应用都是用 BigTable     Google搜索   Analytics（分析）   Google 地图   Gmail    可以和其他 GCP/开源大数据 的 batch/stream 服务集成, 包括     Dataflow   Spark streaming   Stom   Hadoop Map/Reduce    通过 多个cluster 和 nodes 来提供 HA 和 横向扩展     cluster 可以 跨 region 和 zone   自动的 sharding data 到多个cluster 的 nodes 中的多个表中   创建两个 cluster 可以实现replication         可以提高 HA, durability, 自动 failover, 把数据放到离客户更近的地方     要配置一个 application profile 来设置replication     每个 change 都会自动的被 replica 到不同的 cluster 的nodes 中           支持 SSD 和 HDD     一般用 SSD 比较好   不怕延迟的数据, 对读的速度不敏感的场景, 可以用 HDD    适配 HBase API     和 HBase 用同样的 API   允许 on-premise HBase 和 BigTable 整合    通过 HBase 命令, 或者 java 命令来导出数据     gcloud 和 console 都不能导出数据    一个表只能有一个 index     每行(row)数据通过一个 row key 进行index   创建 row key 的时候不要用序数, 或者时间戳,    不支持多行的 transactions , 只支持单行 使用前     要做压力测试   用 Key Visualizer tool 分析数据库使用情况    适用例子:     IOT Stream 数据   Time Series 数据   Graph Data (图形数据)   real time 分析数据   股票数据/财务数据   MemoryStore:  In Memory DB 内存数据库, 更快的读取速度 减少读取关系数据库或者后端服务的实践 适合更新不频繁的数据, 用户可以接受数据并不是实时更新的 最大 300G HA, failover, patching and monitoring Google 提供的 in memory DB 无服务 底层是 Redis 和 MemCache  Memcached  场景: 数据库读数据后被缓存, 用户登录session, web应用的静态配置 比较合适     Cache 在 web 和 DB 之间   减少数据库压力    一旦缓存数据丢失, 如果有访问, 就会从后端重新重新读取后再缓存 两种服务级别     Shared Memcached (免费)   Dedicated memcache (\(\))         为应用提供固定的cache 缓存空间大小     每 G 每小时 $0. 06      Redis                场景: 游戏排名等 支持数据持久化(persistence)和高可用(HA) 可以被以下服务读取缓存     Compute Engine   App Engine   GKE   Cloud Function    创建并连接 Cloud SQL (mysql)     要先启动 Cloud SQL Admin API```bash   gcloud config set project my-nlp-project-68654gcloud sql connect dalong-cloud-sql-mysql-db –user=root –quiet # 下面是连接上了mysql 的命令, todos 是在console 中创建的一个 databseuse todoscreate table user (id integer, username varchar(30) );describe user;insert into user values (1, ‘Ranga’);select * from user; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117### Data Analytics#### BigQuery- 可以保存数据, PB 的 scale 能力的分布式数据仓库- 可以处理 near real time 数据的分析, 内嵌 ML 功能- OLAP 关系型DB - 可以用 SQL 查询- Online Analytics Processing- use columnar storage- 适合 对大数据(PB 级)进行分析- 场景: 数据仓库, BI, 分析报告应用, 分析几年之前的数据 - Google 提供的无服务 OLAP RDB 数据库- dataset - Bigquery 中的 dataset 是用来存储表和视图的容器，它们主要用于组织和管理大量的表和视图- 数据安全 - BigQuery 默认会加密数据 at rest. 但如果客户需要自己加密数据, 也可以用customer-managed encryption keys(CMEK), key 保存在 KMS 中- role  - BigQuery jobUser/user role on billing project (job User role 是用来在 BigQuery 中执行 query 的) - BigQuery dataViewer role on projects that contain the data (dataView role 是用来查看(不能update) 底层数据的 )#### BigQuery - PB 的 scale 能力的分布式数据仓库- 是 OLAP- BigQuery 的层级 Data Set &gt; Table &gt; Partitions- 适合复杂的 Query 数据查询.  - 如果是简单的数据查询 (查两个字段这种), 建议用 BigTable (NoSql)- BigQuery 的存储有两种 - 标准 - Long-term storage  - 相当于 Cloud Storage 的 Nearline, 就是访问量很少(90 天无人问津)  - 成本低- 是关系型数据库, 用 SQL 查询 - 查询方式  - Cloud Console  - bq 命令行工具  - BigQuery Rest API  - HYBase API (基于 Java, . Net, Python)- 费用 - 两部分  - BigQuery 根据 scan (扫描) 了多少数据来付费的, 可能会很贵  - BigQuery 还通过存储在 BigQuery 上的数据量付费 - 查前先算要花多少钱  - 1. 先用 Console 或者 `bq --dry-run` 命令 查一下要查询的数据的数据量  - 2. 用 Pricing Calculatyor 来算一下这些数据量要花多少钱 - 省钱的方法  - Partitioning 分割   - 把一个表分成几个表 (比如按日期/时间戳分割)   - 每个partition的 schema 不变   - partition 可以设置过期(删除)   - 注意, 不要分的太小 (小于1G)  - Clustering 分类(排序)   - 把一些字段提前排好序, 就不用额外的 scan 所有的行了.   - 定期删除(过期)数据   - 可以分别在 Data Set , Table, Partitions 级别设置过期    - Data Set 设置 default table expiration (default_table_expiration_days)     - Table 设置 expiration time (expiration_timestamp) , 比如 100 天过期    - partitioned tables 设置 partition expiration (partition_expiration_days) - 上游 - 导入数据 - 导入方式  - Batch 批量导入 - 免费   - 支持 Cloud Storage 和 本地文件   - 可以用 BigQuery Transfer Service   - 需要先用 Cloud Dataflow 和 Cloud Dataproc 处理数据  - Streaming 实时流导入 - 贵   - 支持 Cloud pub/sub, Streaming Insert   - 需要先用 Cloud Dataflow 和 Cloud Dataproc 处理数据   - 要去重(de-deulicate), 需要增加一个 `insertId` 到每一条stream insert中   - Streaming 有 Quota (吞吐量)限制 insert 的数量    - 每秒每 project 100M(带 insertId) - 每秒每project 1G(不带 insertId)     - 换算成每秒的insert 条数 (带 insertId) , 100k - 500k (欧洲地区) 条.     - 如果要每秒上百万条的需求, 就别用BigQuery 了, 用 BigTalbe (NoSql DB)  - Federation 直接用Query 读 GCP 其他服务的数据   - 支持 CloudStorage, Cloud SQL, BigTable, Google Drive  - DataTransferService 从第三方转换数据到 BigQuery   - 支持 Google Ads, Cloud Storage, Amazon S3, Amazon RedShift - 支持的格式: CSV, JSON, Avro, Parquet, ORC, Datastore backup- 下游 - 导出 - 支持的格式: CSV, JSON (Gzip 压缩), Avro- 自动的数据失效(过期)- 实时读取外部数据 - Cloud Storage, Cloud Sql , Bigtable, Google Drive- BigQuery 最佳实践 - 查之前先 评估 Query (评估数据量, 评估价格) - 尽量用 Partitioning 和 Clustering - 尽量避免 Streaming insert, 用批量导入代替  - 如果使用, 尽量使用 insertId 来去重 - 尽量设置 数据的过期时间, 没用的数据尽早删了 - 适合复杂的 Query 数据查询.   - 如果是简单的数据查询 (查两个字段这种), 建议用 BigTable (NoSql) - 可以通过 audit log 来优化 BigQuery 的查询  - 把 audit log stream 到 BigQuery- Partitioning and Clustering BigQuery Tables - 语法```sqlCREATE TABLE `my_data_set. questions_partitioned_and_clustered`. . . . . . PARTITIONED BY  DATE(created_date)  CLUSTER BY category. . . OPTIONS ( expiration_timestamp=TIMESTAMP  2025-01-01 00:00:00 UTC , partition_expiration_days=7)CREATE SCHEMA mydatasetOPTIONS( default_table_expiration_days=3. 75 )ALTER TABLE mydataset. mytableSET OPTIONS ( expiration_timestamp=TIMESTAMP  2025-01-01 00:00:00 UTC , partition_expiration_days=7)Dataproc:  GCP 提供的 的 Spark 和 Hadoop, Flink, Presto 和 30+ 服务 是数据湖 Data Lake 的现代化方案, 当需要进行批量数据处理, 比如处理ML 和 AI 的数据.      支持复杂的批处理    Cloud Dataproc 是个数据处理平台 只能导出配置, 不能导出数据 基于 standard VMs 或者 Preemptible(临时) VMs     standard worker instance 有点浪费了, preemptible 最省钱且完美适配 hadoop 迁移到 dataproc    支持 Spark, Pyspark, SparkR, Hive, SparkSQL, Pig, Hadoop 提供多个 Cluster 模式     Single Node   Standard   High Availability (3个 masters)    场景     需要迁移hadoop 和 spark clusters 到云上    代替方案     BigQuery   Pub/Sub 消息队列 异步数据通信:  Google 提供的异步数据通信服务是 Pub/Sub pubsub 是无服务 Pub/Sub 包含了AWS SQS 和 SNS 两种服务的所有功能 publisher 和 subscriber 是 多对多关系 工作原理     Publisher         发送https 消息到 pubsub. googleapis. com 的 topic     每个消息会到每个 subscription 那里          Subscriber         提供两种订阅方式             pull                 订阅者主动来取(pull)数据         Subscriber makes HTTPS requests to pubsub. googleapis. com         对应 AWS SNS                      push                 订阅者提供一个web hook endpoint 地址, 消息到了pub/sub之后, 会把数据推到web hook endpoint         对应 AWS SQS                               Subscriber 发送 acknowledgement 给 pub/sub             message(s) 从 subscriptions 的 message queue 中被删掉                      创建过程              创建Topic                   创建subscription(s)             每个订阅默认都能从publisher 哪里收到所有的, 同样的信息       订阅者可以通过filter 接收自己感兴趣的消息, 其他消息被过滤掉       发布者和订阅者可以设置按照 order 进行发送和接收信息       订阅者可以设置每个消息只接受一次 (去重)       订阅者可以发送 acknowledge 给 pub/sub , 告诉它自己已经收到了消息, 可以在 message queue 中 把这个消息删掉了                      pub/sub 的publis batching 批处理选项     pub/sub 默认是开启 message batching 功能的, subscriber 会一次性接收所有的未读数据.    应用一次读取过多的数据, 花费大量时间处理, 可能就会没时间处理下一批数据, 产生timeout.    最佳实践是关闭 message batching 功能, 然后通过 autoscaling 增加 subscriber 的数量来并行处理消息    pub/sub 与 autoscaling     用 pub/sub的 subscription/num_undelivered_messages 这个 metric 来作为 autoscaling 的伸缩条件, 如果超过一定的数量, 就可以增加处理消息的 VM 或者 GKE cluster 的 pode 数量         不能用 subscription/push_request_latencies 这个 metric, 因为这个只是来定义多久pub/sub发一个message 到 distination, 不能用来作为 autoscaling 的依据.            pub/sub FIFO     先进先出   进入到 pub/sub 的数据是按顺序进来的, 出去也是按顺序出去的    去重 Deduplication / exactly-once     pub/sub 是会 delivery message 最少一次的, 所以无法避免 duplicate   Dataflow 可以提供去重功能 - Dataflow exactly-once processing   或者通过定制开发应用去重    pub/sub 的好处     解耦 Decoupling         一个大应用做成多个小应用, 应用之间用pub/sub 通信     publisher 不需要知道谁是订阅者          高可用 Availability         订阅者挂了, publisher 也不用担心, 可以继续发信息          可伸缩 Scalability         订阅者可以快速扩展以消费大量的消息          持久性 Durability         信息会存在 pubsub 中, 就算订阅者挂了也没关系           适合场景     事件型应用         比如有文件上传到了 bucket, 就发送信息到pubsub, 再转到 Bigquery 进行分析          streaming 流分析类型         比如 IOT 的sensor 数据一直发送到pubsub, 再转到 Bigquery 进行分析          将应用从 synchronous 改成 asynchronous workflow         代替方案             RabitMQ, Apache Kafka                      创建 pubsub 的命令1234567891011121314gcloud config set project glowing-furnace-304608  # 转到项目gcloud pubsub topics create topic-from-gcloud   # 创建 Topicgcloud pubsub subscriptions create subscription-gcloud-1 --topic=topic-from-gcloud # 创建订阅1gcloud pubsub subscriptions create subscription-gcloud-2 --topic=topic-from-gcloud # 创建订阅2gcloud pubsub subscriptions pull subscription-gcloud-2 # 从订阅 2 订阅消息(pull 方式)gcloud pubsub subscriptions pull subscription-gcloud-1 # 从订阅 1 订阅消息(pull 方式)gcloud pubsub topics publish topic-from-gcloud --message= My First Message  # 向 topic 发布一个新的消息 gcloud pubsub topics publish topic-from-gcloud --message= My Second Message  # 向 topic 发布一个新的消息 gcloud pubsub topics publish topic-from-gcloud --message= My Third Message  # 向 topic 发布一个新的消息 gcloud pubsub subscriptions pull subscription-gcloud-1 --auto-ack # 从订阅 1 订阅消息(pull 方式) , 获取消息后自动给 acknolodge 反馈gcloud pubsub subscriptions pull subscription-gcloud-2 --auto-ack # 从订阅 2 订阅消息(pull 方式) , 获取消息后自动给 acknolodge 反馈gcloud pubsub topics list # 查看 topic listgcloud pubsub topics delete topic-from-gcloud # 删除 topicgcloud pubsub topics list-subscriptions my-first-topic # Dataflow:  是谷歌提供的 数据流服务 可以创建 batch/streaming 的 data pipeline (ETL - Extract Transfer and Load)     Batch: Cloud Storage -&gt; Dataflow -&gt; BigQuery   Streaming: Pub/sub -&gt; Dataflow -&gt; BigQuery    是无服务 基于 Apache Beam 使用了 pre-build 的模板 可能会用到 pub/sub 模板的几个例子     Pub/Sub &gt; Dataflow &gt; BigQuery (Streaming)   Pub/Sub &gt; Dataflow &gt; Cloud Storage (Streaming - files)   Cloud Storage &gt; Dataflow &gt; Bigtable/CloudSpanner/Datastore/BigQuery (Batch - Load data into databases)   Bulk compress files in Cloud Storage (Batch)   Convert file formats between Avro, Parquet &amp; csv (Batch)   IOT Core -&gt; Pub/Sub &gt; Dataflow &gt; BigQuery -&gt; (Vertex AI, AutoML, BigQuery ML)    场景     实时诈骗检测 (fraud detection)   sensor 数据处理   日志数据处理   批量数据处理 (数据读取, 数据格式转换)   数据的生命周期:  数据生命周期分四个阶段     Ingest 注入   Store 存储   Process &amp; Analysis 处理和分析   Explore &amp; Visualize 发现和展现    每个阶段都会用到不同的服务 Ingest 注入     流         Pub/sub          批量         Storage Transfer Service     BigQuery Transfer Service     Transfer Appliance     gsutil          数据库迁移         Database Migration Service     Dataflow           Store 存储     对象存储         Cloud Storage          关系数据库         Cloud SQL             mysql, postgresql, sql server       regional                Cloud Spanner             HA       Global scale                     No SQL         Firestore     Bigtable             petabyte scale       single row transactions                     数据仓库         BigQuery          定制数据库         自己把自己的数据库安装到 VM 上     在 marketplace 上可以找到开源镜像     mongoDB , Cassandra 等           Process &amp; Analysis 处理和分析     数据清洗和转换         DataPrep     Cloud Data Loss Prevention (DLP) API             数据检查服务, 对Cloud Storage, BigQuery, Cloud Logging, and Datastore 上的敏感数据(PII 等)进行扫描, 检查和分类, 把查询到的结果保存到 Data Catalog 中. 需要时可以查找 data catalog 并删除所有 PII , PCI DSS 等敏感信息.        GCP 的数据 , on-prem 的数据都可以检查.                 Dataflow             ETL pipeline                Dataproc             迁移 Spark 和 Hadoop 到google                      Explore &amp; Visualize 发现和展现     数据仓库         BigQuery          机器学习         ML - Pre build models     ML - Custom models          科学计算和画图         Cloud Datalab             Jupyter Notebook 的代替                     实时图表         Cloud Data Studio             把 Cloud SQL, BigQuery 内的数据实时展现出来                Cloud Data Catalog             是数据分类目录, 记录通过 Data Loss Prevension 扫描过的数据的扫描结果, 比如 PII 信息等.                       大数据, 流, IOT 数据处理用到的数据服务     IoT Core         获取 sensor 数据, 发到 pub/sub          pub/sub         实时数据导入     流数据处理的起点          Cloud Storage         批量数据处理, 先把数据放这里     成本低     存放时间长          Dataflow         ETL     可以是pub/sub的下游          BigQuery         数据仓库     不管是批量还是实时的数据处理, 最后数据都放这里     适合需要进行复杂数据分析的数据          Data Studio         数据展示          Dataproc         大数据处理, hadoop, spark功能          Dataprep         数据清洗和准备          Datalab         数据计算和绘图     可以用 Tensorflow, numpy 等类库          Cloud Composer         Managed workflow orchestration service     Create pipelines across clouds and on-premises data centers          BigTable         存储大规模 IoT 数据更合适     适合时间序列的数据          DataStore         存储 IoT 数据更合适          AI &amp; ML:  google 的机器学习服务分为几种  预制 ML API:   不需要使用者有 ML 知识, 直接就能用 google 训练数据, google 的模型 包括了     Vision API   Video API   Chat API   Natural Language API   Speech-to-Text API   Text-to- Speech API   Translation API   …    AutoML:        低代码的 AI 方案 谷歌训练的数据, 我们自己的模型 我们在谷歌已经训练好的模型基础上定制我们自己的 ML 模型 创建用户自定义的 AI 模型 不需要懂太多 ML 知识  Vertex AI:   我们自己的数据, 我们自己的模型 managed ML platform custom ML models 可以运行Tensorflow 以前可能叫 api platformBigQuery ML:  把 ML 和 SQL 进行整合Networking: Network Tier: Google Cloud Platform offers two different network tiers which differ both on price and performance.  Google Premium Tier Network     配置良好、低延迟、高度可靠的全球网络   支持 Global HTTPS LB         支持 across regions and overflow or fail over to other regions.           IPv4 or IPv6 Virtual IP   更贵    Google Standard Tier Network     支持 regional 的 LB   不支持跨区域的 failover   便宜    例题:     Which of these networking tiers is recommended if you want to use Global HTTPS Load Balancing?         Premium network tier             premium 是高级的意思, 支持 Global HTTPS LB ,       标准的(Standard network tier) 不支持 global HTTPS LB                     VPC:  Google 提供一个内部网络来保护我们的资源, 数据, 以及和内部外部的沟通 VPC 是 global resource, 是跨 region 和 zone 的     region         一个地区, 比如 us-east1          zone         一个数据中心, 比如 us-east1-a          Route Table         VPC 有一个 default routing table             build-in 的       没有管理和视图界面       用于 把 traffic 从一个 vm forward 到另一个 vm       不需要 external IP 地址                      Private Google Access     相当于 AWS 的 VPC Endpoint.    提供一个私有连接, 让这个subnet中的服务(VM, Bigquery and etc) 通过内网 IP 地址访问 google 的服务, 比如 youtubve, Gmap, (省钱)         如果用了 VPN tunnel, 也可以让 on-prem 用 private IP 互相访问              每个项目有一个default VPC     default VPC (auto mode vpc)中为所有region 都默认创建了一个 subnet.     VPC 有两种模式     Auto Mode VPC         为所有region 都默认创建了一个 subnet.      default vpc 就是 auto mode          Custom Mode VPC         要手动创建 subnet , CIDR 以及 VPC Fire Rule     生产环境建议使用 custom mode           subnet     vpc 可以包含多个 subnet   subnet 是 只能属于某一个region 的, regional resource.    subnet 是 跨zone 的, 比如这个subnet 在us-central1, 它就在任何一个 us-central1 的zone 可用.    subnet 下面的实例可以是某一个zone 的   每个subnet 有一个 CIDR range (IP 地址段)         32 表示 1 个 IP     26 表示 64 个 IP (2的(32-26)的 6 次方)     …          Flow logs 开启的话, 可以通过 flow log 查看subnet 的网络访问日志.    subnet 分为 public subnet 和 private subnet         public subnet 中的资源可以被外网访问(也可以访问外网), 可以访问private subnet     private subnet 中的资源不能被外网访问(但可以访问外网)           Firewall Rule     Firewall 是 Google 管理的 build-in 服务   为进出 VPC 设置流量限制         每条 rule 对应一个 VPC     每条 rule 可以设置成 ingress rule 或者 egress rule     每条 rule 可以设置成 allow 或者 deny     每条 rule 的影响范围可以是             all instance       指定 tag 的资源       指定 service account 的资源                每条 rule 有一个 Priority, 0 - 65535, 数字越大优先级越低     可以设置 protocol(TCP/UDP/Others) 和 port          默认的 rule 是         允许所有 egress     不允许所有的 ingress     可以设置不同 priority 的 rule 进行覆盖, 数字越小优先级越高          默认的 VPC 有 4 条 rule         Allow incoming traffic from VM instances in same network (default-allow-internal)             允许同一个网络的 VM访问                Allow Incoming TCP traffic on port 22 (SSH) default-allow-ssh             允许 ssh 访问                Allow Incoming TCP traffic on port 3389 (RDP) default-allow-rdp             允许远程桌面访问                Allow Incoming ICMP from any source on the network default-allow-icmp             允许 Ping 通不通                     Firewall Rule 最佳实践         尽量使用 tag (给资源打标签) 来被 firewall rule 进行允许外部流量访问     尽量只允许 LB 访问 instance, 不要外网直接访问     删掉 0. 0. 0. 0/0 这种地址段, 太宽了     地址段只允许特定的小范围地址段     允许 LB 的健康检查访问实例/MIG     如果不想让所有实例都默认可以 egress 访问外网             先建立一个low priority (比如 65000)的 egress rule 把所有访问都 deny       再建立一个high priority (比如 1000)的 egress rule 指定哪些资源可以通过什么端口来访问 外网                     Firewall Insights (防火墙洞察)         需要先开启 Firewall Rule Logging     可以通过 logging 查看这些 firewall rule 的使用效率. 方便理解, 提升安全, 优化配置           Shared VPC     同一个组织(organization) 有多个 project 希望可以通过内网 IP 互相通信   需要先创建组织   可以在一个Org 中的一个 Host Project 中创建一个 shared VPC, 把需要共享的应用跑在里面, 然后让org 下面的其他project 中的服务通过内网 IP访问.     VPC Service Control     Google managed 网络功能, 降低数据外泄(exfiltration)风险, 保护VPC 内数据隐私, 提供独立的访问控制   提供如下功能:         控制 VPC 网络可以访问哪些谷歌服务     multi-tenant, 保证不会泄露 (exfiltration)     敏感信息只能被经过认证的网络访问     限制资源访问 IP 地址, identity, 客户设备          Serverless VPC Access:  Serverless VPC Access 是一个私有访问选项, 允许你通过一个 internal IP 让一个 无服务(serverless) 连接到 VPC 网络.      VPN tunnel 连接到的on-premise 也算 VPC网络内   要无服务发起连接请求   如果是 VM 先发起的请求, 就要走外网的无服务入口了    无服务包括     Cloud Run   App Engine standard environment   Cloud Function   VPC Peering (VPC Network Peering):  VPC Peering 可以让不同组织之间的 VPC 互相连接 并使用 内网 IP 相互通信     前提是两个 VPC 的网络 IP 不能重叠    数据交换不需要额外付费 VPC1 和 VPC2 peer 了, PC2 和 VPC3 Peer 了, 并不能解决 VPC1 和 VPC3 的 Peering VPC 只能在 GCP 内部使用, 为两个 VPC 提供链接, 不能在 GCP 外部建立连接 VPC peering 有时候也叫 network peering. Carrier Peering:  运营商对等 Peer through a carrier 是一种通过第三方运营商来实现的互联，这意味着一个网络可以通过运营商来连接到另一个网络。Cloud Router: VPC/on-prem network route exchange (BGP)  Google 管理的 build-in 服务 Router 同一个 VPC 下面的不同subnet 之间的通信 不同的 VPC 之间需要通过 VPN Gateway 或者 Cloud Interconnect 才能让不同subnet 之间通信 Firewall 和 Router 的区别     Google Cloud Firewall rules are used to create and manage network access control lists (ACLs) that control the traffic that is allowed to and from a Google Cloud virtual private cloud (VPC) network. Cloud Router is a specialized virtual router that enables customers to route traffic between their on-premises networks and their Google Cloud VPC networks. Cloud Router also provides dynamic routing for customers who need to connect multiple VPC networks together or to other networks.          Google Cloud Firewall 规则用于创建和管理网络访问控制列表 （ACL），这些列表控制允许进出 Google Cloud 虚拟私有云 （VPC） 网络的流量。云路由器是一种专用的虚拟路由器，使客户能够在其本地网络和 Google Cloud VPC 网络之间路由流量。云路由器还为需要将多个VPC网络连接在一起或连接到其他网络的客户提供动态路由。          Direct Peering:  直连 (on-prem 到 GCP VPC) Direct Peering可以让你在企业网络和 Google 边缘网络之间建立直接对等互连连接，并交换高吞吐的云端流量 通过 network peering 直连 客户网络和 GCP 网络 low level 的连接, 不推荐 推荐使用 Cloud VPN 或者 Cloud InterconnectCloud NAT (Cloud NAT Gateway):  无服务 相当于 AWS 中的 NAT Gateway / Ineternet Gateway 和 堡垒机相反, 提供 VPC 内部的互联网请求 Cloud NAT 是一个网络组件, 并不存在于某个 VM 或者 应用上 让 VPC 中的资源在没有 public IP 的情况下也可以访问互联网 可以支持的 GCP 资源     VM   GKE   Cloud Run   App Engine   Cloud Function    不需要配置 firewall rules 需要所在的 VPC有一个默认的 router , 这个 router 的下一跳(hop) 是 internet gateway 案例     要部署应用到 GKE上, 需要访问外网, 但GKE所在的 GCE VM 没有外网 IP, 如何解决?         把 GKE Cluster 作为一个 private subnet     给这个 subnet 配置 CLoud NAT Gateway          VPN:  连接 on premise 和 GCP 网络 每个 VPN 最高速度为 3Gbps, 多建几个可以增加带宽 使用了 IPSec VPN Tunnel 实现 通过 internet 公网连接 用 Internet Key Exchange 协议加密 VPN Gateway     要建立 VPN tunnel, 就要在所在的 region 建立一个 VPN Gateway   Regional 的, 不是 global 的. 每个region 可以有一个 VPN gateway    两种 VPN 类型     HA VPN         SLA 99. 99%     对外暴露 2个 外部 IP 地址     2条线路, 是 HA (在一个 region)     只支持动态路由 (BGP)             Cloud Router                 支持动态路由, 当网络拓扑变化时, 可以自动更新路由                               gateway             GCP 端是 Cloud HA VPN gateway       on premise 端是 on-premise vpn gateway                 or peer gateway                                    Clasic VPN         SLA 99. 9%     对外暴露 1个 外部 IP 地址     1条线路, 非 HA     支持动态和静态路由 (BGP)     gateway             GCP 端是 Cloud Compute Engine VPN gateway       on premise 端是 on-premise vpn gateway                      适合场景     希望on premise 和 GCP 的通信进行加密   对吞吐量要求不高, 没有太多 cost    VPN 和 Cloud Interconnect 适用的场景区别  Cloud Interconnect:   专线网络, 连接 GCP 和 on premise 高速连接, 高可用, 低延迟的私有连接 通过私有网络连接     使用私有 IP 地址 (无外网 IP 接口)   省去了 egress 外网流量的成本    有两种类型     Dedicated Interconnect         on-prem data center 需要和 GCP data center 比较近     99. 99% SLA     独享专线, GCP 和 on-premise 直连     80 Gbps 的网速 (or 200 Gbps circuits)     缺点, 要花时间去建立, 一般几周到几个月          Partner Interconnect         on-prem data center 和 GCP data center 比较远, 才需要一个partner 在中间     GCP 和 on-premise 中间还有一个partner 网络             partner 可以是中国电信, 联通, 移动                和 Dedicated 相比, 稍微低一点的带宽     50 Mbps 到 10Gbps     partner 这部分出了问题, Google 不负责.            最佳实践     GCP 中和 on premise 连接的网络, 使用一个不同 IP 地址段         地址段不能相互重叠          网络连接要有备份方案         把 Cloud Interconnect 作为主要连接     把 VPN 作为备份方案, 就算主要连接挂了, 也可以保持连接          Cloud DNS:  谷歌云提供的 DNS 服务 (不含域名注册) Zone 是对某个域名的管理集合     分为 public zone (处理外网公有域名) 和 private zone (处理内网私有域名)    Record 是 步骤:     在域名注册商那里购买域名   将域名路由到 Cloud DNS                      在 Cloud DNS 中创建一个 Zone, 并指定要处理的域名                             在zone 中创建记录, 处理域名的不同解析方式                 比如:                     Route api. dalong. work to the IP address of api server           Route static. dalong. work to the IP address of http server           Route email (dalong@dalong. work) to the mail server(mail. dalong. work)                                                        Cloud DNS 路由 policies     Cloud DNS 有两种路由规则         weighted round robin (权重)             如果 DNS 下面有两个 IP, 那么对两个 IP设置不同的权重, 20% 到一个 IP, 80% 到另一个 IP                geo-location             通过用户 IP 地址判断客户所在地, 然后再把客户流量转给对应的后端的LB 或者对应region 的服务)                      Cloud CDN:  使用谷歌的 global edge network 让用户可以在全球范围内在就近, 快速获取到被缓存的静态资源 和 External Https/Http LB 整合 后端可以是     Cloud Storage buckets   Instance groups   App Engine   Cloud Run   Cloud Functions   客户自己的数据源    原理     和memcache 差不多   URL 如果map 到了Cloud CDN 中的cache 就直接返回给客户; 如果没map 到, 就去 origian 拿到静态数据, 然后返回给客户, 然后把缓存存到 CDN 中   可以通过 TTL 设置缓存过期时间    cache key     默认是完整的 url 作为 cache key   定制 cache key, 去掉 protocol 信息 可以提高cache 命中率         http 和 https 开头的, 后面 URL 完全相同的 cache key 合并, 可以提高命中率          缩短 cache 过期时间只会降低命中率    最佳实践     根据过期时间需要缓存静态内容         Example: Cache-Control: public, max-age=259200 (72 hours)     Example: Cache-Control: public, max-age=300 (5 minutes)          提高 cache hit ratio (缓存命中率)         如果 URL 是 https://yourwebsite. com/my-image/1. jpg 那么下面两种 URL都不会命中缓存     http://yourwebsite. com/my-image/1. jpg (http vs https)     http://yourwebsite. com/my-image/1. jpg?mobile=1 (query string does not match)          可以通过命令来设置 cache key         gcloud compute backend-services update BACKEND_SERVICE --enable-cdn --no-cache-key-include-protocol --no-cache-key-include-host --no-cache-key-include-query-string          在 URL 中通过版本好来更新内容         https://yourwebsite. com/my-image/1. jpg?v=1     https://yourwebsite. com/my-image/1. jpg?v=2          Media CDN:  CDN for Streaming and VideosLoad Balancing (Load Balancer):  可以把流量在一个或多个region 传给应用实例, 提供高可用 HA 配合 health check 和 auto scaling     LB 只会把流量给到 MIG 中的健康的VM   如果一个 region 的 backend 都挂了, LB会把流量转到其他region    可以是 internal (VM 之间) 或者 external (互联网用户流量)     external 到 LB - 建议 HTTPS, TLS   LB 到 VM - 建议 HTTPS, HTTP 也 OK   internal 建议 HTTP, TCP    支持的 protocol     HTTP/HTTPS         第七层     web, API, email(SMTP), FTP 等     大多数应用都在这一层     HTTP/HTTPS 是基于 第四层之上的          TCP(TLS)/UDP/ICMP(Ping)         第四层     游戏, 视频等用 UDP     性能更好           组成 LB 的 3 个部分     Frontend - 指定的 IP 地址, 端口和 protocol.          IP 地址客户可以直接访问          Host, path rules, http header - 就是路由规则         Host 就是用二级域名来判断把流量转到后台什么地方     Path rules 就是利用 路径来判断把流量送哪里去     Http header - 通过http header 决定怎么传递流量             authorization header       post/get/…                     Backend - 提供服务的 endpoint         一般一个 Backend 指一个 MIG, 也可能是一个instance 或者 bucket     因为 MIG 只能在一个 Region, 所以如果需要跨region 服务, 需要在多个region 部署几个跑同样应用的 MIG (同一个 project)     LB 会把用户流量转到离用户最近(延迟最低)的那个region 的 MIG          Protocol 的常用端口         HTTP/HTTPS - 80,8080, 443           负载均衡类型     Global HTTP(S)   Global SSL Proxy   Global TCP   Reginal   Reginal Internal         多层应用之间,内网互访           几个容易混淆的概念     Bacakend Service         跨region     一堆backend or bucket     一个backend service 可以有多个 MIG 分布在不同的region          Backend : 比如一个 MIG    几个负载均衡的场景     同样的服务在不同region         方案: 1个 LB + 1个 backend service + 多个 backend (分布在不同region)          不同的服务在不同 region         方案: 1个 LB + 1个 URL Maps + 多个 backend service + 多个 backend (分布在不同region)          不同服务,不同版本在不同region         方案和上面一个一样, 同一个服务的不同版本视作不同的服务.            例题:     Which of these networking tiers is recommended if you want to use Global HTTPS Load Balancing?         Premium network tier             premium 是高级的意思, 就是用最新的LB , 不用标准的(Standard network tier)                     How many HTTPS Load Balancing Backends would you need to support one version each of three different microservices, each with two MIGs in two different regions?         6个             一个微服务就表示一个backend service       每个服务在两个region中分别部署一个 MIG(backend) , 一共是 6 个                     Service Directory:  谷歌提供的服务发现服务 帮助微服务之间互相发现彼此 一个地方 publish, discover, connect 服务 可以在混合云中运行 Managed Service 通过 DNS, HTTP, and gRPC client libraries 支持多种开发语言, 提供 Rest/ RPC 的 API 输出日志     Audit log   request/response log   Eventarc:  谷歌的事件处理服务 遵循了 CloudEvent 标准     支持多语言, 混合云,   标准类库和工具   可移植性强    理解为事件总线, 集中 上游 - 谁来触发 event     Pub/Sub   Cloud Storage   Cloud Functions   Cloud IoT   Cloud Memorystore   Cloud Audit Logs (间接的, GCE 和 GAE可以先把log 发送到 Audit log, 然后由 audit log 触发event)   . .     下游 - 谁来处理event     Cloud Functions (2nd gen)   Cloud Run   GKE services   . .    12345678gcloud eventarc triggers create my-pub-sub-trigger --destination-run-service=$SERVICE_NAME --destination-run-region=$REGION --event-filters= type=google. cloud. pubsub. topic. v1. messagePublished gcloud eventarc triggers create my-audit-log-trigger --destination-run-service=$SERVICE_NAME --destination-run-region=$REGION --event-filters= type=google. cloud. audit. log. v1. written  --event-filters= serviceName=storage. googleapis. com  --event-filters= methodName=storage. objects. create DevOps CICD and SRE: CICD:  CI: continus integration     代码开发完后, 持续运行测试和打包       CD: continus deployment/ delivery     CI 之后, 持续部署到测试环境和生产环境    CICD 中要做的事情     静态代码分析         Sonar Lint , 开源代码分析工具          Runtime 检查   测试         单元测试     整合测试     系统测试     Sanity and Regression testing           CICD 的工具     CICD 的工具有的是 Google Cloud 的服务, 有的是要在 Google Market Plance 中找到, 并进行安装   Cloud Source Repositories         代码管理, 相当于 github     优势             To keep code private to a Google Cloud project       To reduce work                     Container Registory         保存 自己的镜像 image          Jenkins         CI 工具          Cloud Build         打包工具          Spinnaker         多云部署平台 (CD)             支持 GCE, GKE, GAE 和其他云平台       支持多种部署策略       快速发布应用                      Binary authorization     在 continus deployment 的环节的安全控制, 保证部署可信的Image 到GKE or Cloud Run 的容器中, 在部署过程中会对image 进行签名验证, 只有可信的image 才能部署到指定环境中   步骤         给所有 GKE Cluster (开发, staging 和 生产)配置 Binary authorization policies (二进制认证 policy)     认证后只有经过签名的 image 才能进行部署     设置 CI/CI pipeline, 让 attestations (验证) 作为 Pipeline 的一部分,     开发人员的image 没有经过 验证, 不能直接上生产了.           Infrastructure as Code:  通过配置文件 (比如YAML) 来对infrastructure 进行部署, 配置, 修改, 管理 可以跟踪 infra 的变化 可以通过配置文件快速复制 infra IaC 包括了软硬两部分     平台硬件资源管理 (Infra Provisioning)         可以部署虚机, 数据库, 网络, 存储等     开源方案 - Terraform             Terraform can be used as an infrastructure management system for Google Cloud resources.        将模板部署在规模较大的环境中 以便根据需要快速创建任意多个相同的应用环境       要使用Terraform 你需要使用 HashiCorp Configuration Language (HCL) 创建一个描述环境组件的模板文件 然后Terraform会使用该模型 来确定创建所描述环境所需的操作       如果你需要更改环境 可以修改模板 然后使用Terraform来更新环境以进行更改       你可以在Cloud Source Repositories中 存储Terraform模板                GCP 方案 - Google Cloud Deployment Manager             用Deployment manager 创建资源, 可以用它管理 Terraform       Yaml 是配置文件                 有版本控制                      自动处理依赖关系       部署遇到错误可以自动回滚       免费使用, 为部署的硬件资源付费```YAML       type: compute. v1. instance name: my-first-vm properties:   zone: us-central1-a   machineType: «MACHINE_TYPE»   disks:   - deviceName: boot     type: PERSISTENT     boot: true     autoDelete: true     initializeParams:       sourceImage: «SOURCE_IMAGE»   networkInterfaces:   - network: «NETWORK»     # Give instance a public IP Address     accessConfigs:     - name: External NAT       type: ONE_TO_ONE_NAT```                     配置管理 (Config Management)         安装适合的软件和工具到硬件资源上     开源工具:             Chef       Puppet       Ansible       SaltStack        SRE:                              Site Reliability Engineering 体现系统性能和可靠性的四个黄金信号(Four Golden Signals)     latency 延迟时间         请求应用的响应时间, 是否足够快          Traffic 流量         用来衡量系统的请求量的大小          Saturation 饱和度         用来衡量资源资源的容量消耗的程度          Errors         衡量系统健康程度           是 DevOps 的升级版本 , 专注的事情比以往的更多的指标     availability   performance   efficiency   change management   monitoring   emergency response   capacity planning    四个衡量维度 (metrics)     SLI - 服务级别指标         在一个时间范围内的, 可衡量的服务参数             A time-bound measurable attribute of a service       比如                     SLO - 服务目标         就是几个 9          SLA - 服务协议         是一份agreement , 签字了就是合同的一部分.           Error Budget (错误预算)         是一个数值, 是 100% 减 SLO , 结果可能是 0. 01 , 或者 0. 00001     用来衡量 SRE 团队部署的速度 (错误越少, release 速度越快越频繁)           最佳实践     处理负载过量 (exceed load)         一些企业由于突然太过成功导致应用的访问流量暴增(success disaster)     解决方案             Load Shedding (卸载部分流量)                 通过 API 访问限制         将时序类型的数据丢弃一部分                      Reduce Quality of Service (降低服务质量)                 原来的实时 response 改成 hardcode 的内容                                    Peneration testing (渗透测试)         模拟黑客攻击, 寻找安全漏洞     可以是黑盒测试或白盒测试          Load Testing (压力测试)         模拟真实场景的流量     模拟突发流量     工具: JMeter, LoadRunner, Locust, Gatling etc          Resilene Testing (伸缩测试)         当系统的某个部分损坏了, 整个应用仍然可以进行服务的能力     几种测试方法             Chaos Testing (Simian Army) 混沌测试                 随机把机房的某个部分弄坏 (想象一个有武器的猴子进了机房, 可能会随便按按钮, 扯网线)                      在应用的某层进行压力测试       测试断网                 比如 VPN, Cloud Interconnect 断掉                      灾难测试                 比如指定时间把整个 Data Center 停电                                    Marketplace (Cloud Launcher):  相当于 app store 可以安装     操作系统 - Linux 等   应用环境 - LAMP 等   Web 应用 - WordPress 等   企业应用 - SAP Hana on GCP (商业方案, 需要付费)   数据应用 - Cssandra 等   CICD 工具 - Jenkins 等   …   Release Management:  发布管理的目标     零 downtime   同一时间, 生产只有一个版本   最小化承恩   新版本上生产之前要测试    最佳实践     小而快的发布   尽量自动化   处理新发布版本的问题         分析日志和查看监控     回滚, 在另一个环境检查问题           几个发布方法     Recreate 重建发布         卸载掉原来的版本, 用新版本代替     特点:             发布需要downtime , 影响客户正常访问       回滚的话需要更长的 downtime       过程非常快       新版本不能向后兼容时(应用和数据), 可以用这个方法       不需要额外的 infra                     Canary 金丝雀发布         先发布新版本到部分实例上, 测试好后再在全部实力上发布新版本     特点             没有downtime, 对用户影响小       有问题就回滚       新版本需要有向后兼容能力.        不需要额外infra                AB 测试             是基于金丝雀的发布方式       要看一下用户对两个版本的不同反应       最终留下客户喜欢的那个版本                     Rolling 滚动发布         和金丝雀基本一样, 就是需要先设定好一个滚动升级的机制, 比如每次新版本替换生产的实例个数或百分比     特点             没有downtime, 对用户影响小       是自动发布过程, 但需要提前setup 好       有问题就回滚       发布速度慢, rolling 需要一段时间       新版本需要有向后兼容能力.        不需要额外的infra                     Rolling with additional batch         和 rolling 基本一样, 但是需要额外的一个/几个 实例     将新版本先发布到新增加的实例上, 没问题了再将新版本 rolling 到其他实例上     特点 (对比 Rolling):             不会影响生产上的任何正在处理用户访问的实例                     Blue Green 蓝绿发布         增加额外的实例环境发布新版本, 两个环境(和版本)并行, 测试后撤掉原来的环境, 保留新环境为生产环境     特点             没有downtime       在两个环境值之前切换和回滚       新版本要有向后兼容能力       需要很多额外的infra       配置有些复杂       要避免支付等场景使用蓝绿发布, 因为可能会引起重复的支付交易订单                Shadow 测试             是基于蓝绿发布的       第二个环境的流量是 shadow 第一个环境的流量                      多个 Google 服务都可以应用不同的发布方式     MIG (Managed Instance Group)         Canary     Rolling     Blue Green          App Engine         Recreate     Canary and AB Testing     Rolling          GKE         Recreate     Canary     Rolling     Blue Green          Identify and Security: Container Analysis:  对容器进行自动安全扫描Artifact Registry:  image 镜像安全管理KMS:  Hosted key management service 要启动 KMS API, 并创建一个 Key Ring     Key ring, 钥匙戒指, 算是一个KMS 的总钥匙?   Key rings group keys together to keep them organized.    先创建好 key ring, 再在下面创建 keys (Customer-managed key)   创建好 Key 之后, 在创建一个新的compute engine 的 instance 的时候, 可以在选择add new desk 的时候, 就可以选择刚创建的Customer-managed key 进行加密. (默认是用google 自己的Google-managed key)    数据状态分为     Data at rest         存在某个地方的数据          Data in motion/ transit         传输中的数据          Data in use         使用中的数据     比如内存中的数据           需要为at rest, in transit 的数据进行加密 加密类型     Symmetric key encryption 对称加密         用同一个key 对数据进行加密和解密          Asymmetric key encryption 非对称加密         用公钥加密, 私钥解密     公钥分享给别人, 私钥自己留着           Cloud KMS     Google 的 key management service   KMS 用于给google cloud 的所有可加密服务提供加密   分为三种         Google-managed key             Google 创建和管理       我们不需要做任何管理和配置                Customer-managed key             我们在 KMS 中创建和使用       Use key from KMS                Customer-supplied key             可以自己的 key, 放在 KMS 中进行管理和使用       Provide your own key                     IAM, Service Account (服务账号), ACL:  Identity and Access Management (IAM)     IAM 有继承关系, project &gt; bucket &gt; object         最低级的资源的 Policy 是它和它的祖先的所有 policy 的合集          用户或者应用都需要访问 Google Cloud 资源         IAM 可以提供服务: 配置谁, 访问什么资源, 访问什么内容, 可以做什么action     Authentication - 正确的用户     Authorization - 正确的访问权限          Identify 可以是:         GCP User     A Group of GCP Users     一个 GCP 应用     一个 on premise 应用     未认证的外部用户          提供的控制包括 (对单个用户)         执行一个 single action     针对一个特定的 cloud resource     来自某个特定的 IP address     在一个特定的 time window           为一个同事赋予访问 storage bucket 的过程     概念         Role             包含一些 permission (提供对某些资源的某些actions)                 一个role 可以有多个permissions                      Role 中并不定义 member/who                 Role 定义 “can do what”                      分三种类型                 Basic Roles (现在google 已经不推荐了, 因为范围比较广, 如果给一个人设置了editor, 那么他可以编辑所有Project 下面的服务, 包括 GCE, GKE, Cloud Storage 等所有服务)                     Owner                         view + create + update + delete + billing                                  Editor                         view + create + update + delete                                  Viewer                         view                                  Billing Admin                         view + billing                                                   Predefined Roles                     Google 提前定义好的           比如:                         Storage Admin (roles/storage. admin)                             storage. buckets. *               storage. objects. *                                        Storage Object Admin (roles/storage. objectAdmin)                             storage. objects. *                                        Storage Object Viewer (roles/storage. objectViewer)                             storage. objects. get               storage. objects. list                                        Storage Object Creator (roles/storage. objectCreator)             storage. objects. create                                  每个 Role 都有的 permission                         resourcemanager. projects. get             resourcemanager. projects. list                                                   Custom Roles                     我们自己定义的 Roles 来弥补 perdefined roles 不能满足的情况           比如定义一个人, 除了可以 editor GCE 和 GKE 之外, 不能干别的事情           只能应用在 Org 和 Project level, 不能在 folder level 应用           一般用来设置指定的服务的访问权限                                                  Policy             指定 哪个 member/group/service account/workspace or cloud identity domain/外部用户 有什么 role                 一个member 可以有多个role, 是在policy 中定义的                      指定有什么条件, 比如 Which Resources?, When?, From Where?                     步骤         选择一个合适的 role - (ex. Storage Object Admin)     创建 Policy, 把同事和这个role 绑定           Service Account 服务账号     为应用提供访问 GCP 资源   比如, 给一个 VM 访问Cloud Storage 的权限,而不用给VM创建一个个人账号   service account 也是资源, 如果某个人要访问, 也需要给一个role (editor or viewer) 才能访问   可以跨项目访问         In project B, add the service account from Project A and assign Storage Object Viewer Permission on the bucket          Service account 有一个唯一的email 地址, 但不需要password         有公钥私钥     不能通过浏览器登录          服务账号的类型有三种         Default service account             当我们使用某些服务的时候, 系统会自动创建一个账号       一般不推荐我们使用                User Managed             用户自己创建的 Service account       推荐我们使用自定义的                Google Managed service account             GCP 用来代替用户对服务进行管理的       我们不用管这种服务账号                     服务账号在几个场景中的创建过程         场景 1: VM -&gt; Cloud Storage             创建一个 Service account 给 VM       把Cloud Storage 的 IAM role assign 给 VM 实例                 系统会自动创建 Google Cloud-Managed Keys , 并用于 assignment         只要不删除 Service account, VM 就可以一只访问 cloud storage         这个 Role 也是 IAM Role, 把这个 IAM role 给 service account                               场景 2: on-prem -&gt; Cloud Storage             我们不能直接把service account assign 给 on-premise 的 App       创建一个 service account       创建一个 service account User Managed Key       让 on-prem 的应用访问 key file                 本地设置一个环境变量 export GOOGLE_APPLICATION_CREDENTIALS= /PATH_TO_KEY_FILE                       本地应用调用 Google Cloud Client Libraries , 就可以找到环境变量, 并访问 Cloud Storage                场景 3: Google Cloud APIs (Short Lived)             因为访问时短暂的, 使用 OAuth2. 0 或者 OpenID 或者 JWTs token 就可以了                             ACL (Access Control Lists)     定义了谁可以访问 Bucket, 以及访问什么level   和 IAM 的区别         IAM 只能允许访问一个 Bucket 中的所有 objects             Uniform 的形式访问 Bucket                ACL 可以设定访问哪些 Objects             Fine-grained 的形式访问 Bucket.                      IAM 允许用户访问 Object, 但是 ACL 没有允许, 用户也可以访问 Object.     Signed URL     让外部用户(无google cloud 账号的用户) 临时的访问 Bucket 中的 objects   创建流程         1: Create a key (YOUR_KEY) for the Service Account/User with the desired permissions     2: Create Signed URL with the key: gsutil signurl -d 10m YOUR_KEY gs://BUCKET_NAME/OBJECT_PATH           用 Bucket 创建一个静态网站     创建一个bucket, 上传web 页面         bucket 的名字要和 DNS 中的域名保持一致     要有index. html 和 error. html     在bucket 中的 permission中 增加 allUser , 并赋予 Storage Object Viewer 权限     bucket 就会出现 public URL 地址           Policy 的 描述文件  123456789101112131415161718192021222324{  bindings : [   {      role :  roles/storage. objectAdmin ,      members : [        user:you@in28minutes. com ,        serviceAccount:myAppName@appspot. gserviceaccount. com ,         group:administrators@in28minutes. com ,        domain:google. com      ]   },   {      role :  roles/storage. objectViewer ,      members : [        user:you@in28minutes. com      ],      condition : {        title :  Limited time access ,        description :  Only upto Feb 2022 ,        expression :  request. time &lt; timestamp('2022-02-01T00:00:00. 000Z') ,     }   } ]}    用 gcloud 命令123456789101112131415gcloud compute project-info describe #- Describe current projectgcloud auth login #- Access the Cloud Platform with Google user credentialsgcloud auth revoke #- Revoke access credentials for an accountgcloud auth list #- List active accounts# gcloud projectsgcloud projects add-iam-policy-binding #- Add IAM policy bindinggcloud projects get-iam-policy #- Get IAM policy for a projectgcloud projects remove-iam-policy-binding #- Remove IAM policy binding gcloud projects set-iam-policy # - Set the IAM policygcloud projects delete #- Delete a project# gcloud iamgcloud iam roles describe #- Describe an IAM rolegcloud iam roles create #- create an iam role(--project, --permissions, --stage) gcloud iam roles copy #- Copy IAM RolesIAM, Organization, Billing Account, ID manager:  IAM 中的几个概念     Google Account         一个人的账号, 一个 email 地址          Service Account         一个应用账号, 提供访问一个服务所需要的权限          Google Group         每个group 有一个对应的email 地址     group 下面可以有多个 Google Account     group 是跨project 的     policy 可以应用到 group 上          Workspace         原来叫做 G suite     提供google 多种产品的 API , 比如 Google Chat, Gmail, Calendar, Meet 的 API     Workspace 下面的都是 SaaS 服务           GCP 的资源层级     Organization &gt; Folder &gt; Project &gt; Resources   每个层级都可以单独设置 Policy, policy 向下继承   由较低级别的策略实施的 IAM 策略可以 覆盖在更高级别定义的策略(IAM policies that are implemented by lower-level policies canoverride the policies defined at a higher level. ) ?   组织是一个公司         org 有一些特殊权限的role, 比如 org. policy. administrator 专门负责修改权限     默认用gmail 创建的GCP 是不带 org 的, 需要自己去创建 org, 主要的两个原因是:             When you want to centrally apply organization-wide policies       When you want to create folders                     Folder 往往是一个部门         Folder 下面可以有 子Folder 和 projects     Folder 可以设置 Policy, 被下面所有的 Folder 和 Projects 继承          Project 是一个项目的一个环境(开发/测试/生产)         为一个项目的不同环境创建不同项目     可以为要共享的资源创建共享目录     更容易为该环境的使用人员分配统一权限     所有的 GCP 资源都跟某个 Project 关联 (project 可以关联所有的 GCP 资源)          Resources 是 project 下的一个服务    Billing Account (付费账户)     一个org 下面可以有多个 billing account         初创公司可以只有一个biiling account     大企业建议按组织的架构设计 billing account, 比如 IT, HR          一个 Billing account 下面可以有一个或多个项目   billing account 下面包含付费详细信息   可以为每个billing account 设置 budget, alter and export         比如设置 400 块钱的 budget, 然后设置在 200 块钱, 320 块, 400 块的时候进行 alert 提醒             alert 可以发送到                 email         pub/sub                               可以为已经花费的详细信息导出数据报表             可以在 Big Query 中进行查看分析       可以把历史的 export 存放到 cloud storage 中长期保存                      Organization policies     是跨项目的, 全组织级别的 policies   比如:         允许或者不允许在某个 region 创建资源     全组织不允许创建 Service Account          需要有 Organization Policy Administrator 这个 Role 才能进行编辑   Policy 是可以继承的, 从 Org &gt; Folder &gt; Project &gt; Resource         Policy 可以在任何一个层级进行设置     Resources 会继承所有父层级的所有 policies             这个resrouce 上的有效 policy 是它和它的所有父 policy 的 并集 (Union)       父层设置的 Policy, 子层是不能进行限制和修改的的                 比如 Org policy 设置了不能创建 VM, 那么 folder level 和 project level 就算设置了可以创建 VM 也没用.                                     提示:         IAM 是 Who          Org Policy 是 What    IAM 最佳实践     最少权限         按需提供权限, 不要给额外的, 不需要的权限     不建议给 Basic Roles (Owner/Viewer/Editor) 权限          分离权限         敏感的服务, 可以考虑把权限分给两个人, 每人负责一部分, 比如deploy 一个人, release 新版本 一个人          持续监控         用 cloud audit log 来审计 IAM policy 的使用 和 Service Account 的 key 的访问情况          尽量多的使用 Group         组中有多个用户, 可以绑定 role 和 group , 为多个用户赋予相同权限           Identity management (谷歌云的 ID 管理)     普通用户         可以通过注册时使用的 Gmail 账号 可以作为 Super Admin 来访问 Organization, Folders, Projects          企业用户:         Option 1: Google Workspace             用户用企业自己的域名邮箱登录谷歌云       企业通过 Worksapce 管理用户, 组       把 Org 和 workspace 整合                Option 2: Federation             用户用第三方的 Idp (Identity provider) 登录谷歌云                 步骤                     用户登录时跳转到第三方 Idp 进行登录认证           认证完成后, 再跳转回 google, 同时SAML assertion (断言) 会发送到 Google Sign-in           好处 - 不需要保留用户登录的敏感信息到 GCP 上, 满足一些企业的合规要求.                             第三方 Idp 包括                     AD (Active Directory)                         Google Cloud Directory Sync 可以同步 Google 和 (on-prem)AD 之间的用户和组的数据             单向同步 (on-prem to GCP)             同步之后, 就可以用 SAML 进行 SSO 登录                                  AAD (Azure Active Directorty)                         AAD 可以整合 on premises 的 AD                                                   Federation 的场景                     使用 IDaaS (比如 Okta) 作为 Idp, 进行 Single Sign On 登录           使用 AD 作为 Idp 进行 SSO 登录           使用 AAD 作为 Idp 进行 SSO 登录                                                        IAM Member 和 ID 的场景     所有用户都有 G suit 账号, 我们创建了一个新的项目(生产) , 需要给运营团队提供访问权限.          方案             给运营团队创建一个组, 给这个组提供可以访问项目的权限                     所有用户都有 G suit 账号, 我们创建了一个新的项目(生产) , 需要给一个团队成员提供访问权限.          方案             和上面的方案一样. 但如果只是一次性的临时访问, 就位这个人单独提供必要的 role 的访问权限                     给一个外部的审计人员提供可以查看一个项目的所有资源, 他不能进行任何修改         方案             给审计人员提供 Basic Roles 的 roles/viewer role                     项目 A 的一个 VM 上的应用需要访问项目 B 的 cloud storage 的 bucket.          方案             在 project B 中 给 Project A 的 VM 设置可以访问的 role                      OAuth2. 0 和 SAML 的不同场景   1.  提到 Restful API 的 authorization and authentication, delegated authorization 那就是 OAuth 2. 0   2.  想要用 google/youtube 账号登录一个 smart TV app, 就用 OAuth 2   3.  提到 Identity management , 企业内部软件放到 GCP 上, 要用企业内部 ID 进行 SSO 登录 , 就是 SAML   4.  要选择一个 protocol 来进行 SSO 登录, 那就是 SAMLIdentity Aware Proxy (IAP):  提供登录认证授权服务 当您要对应用和资源强制执行访问权限控制政策时，请使用 IAP 比如对指定的用户,组授权访问资源 比如您可以允许员工但不允许承包商使用某一资源，或者只允许特定部门使用该资源。 IAP 可以使用 Google Cloud Directory Sync 与 Active Directory 或 LDAP 服务器同步 后的用户和组数据进行登录. Identity Platform:  为客户的应用提供登录认证服务, 可以称为 CIAM 和 IAM 区别     IAM 是提供用户访问谷歌云里的服务的认证机制   CIAM 是为用户的应用提供认证的机制    主要功能     Authentication &amp; authorization for web &amp; mobile apps (iOS, Android, . . )   Multiple authentication methods         SAML, OIDC, email/password, phone, social - Google/Facebook/Twitter/. .           Features: User sign-up and sign-in, MFA etc.    是从 Firebase Authentication Legacy 演化而来的   完美集成了 Identity-Aware Proxy    使用 IAM 和 CIAM 的场景     An Application on a GCE VM needs access to cloud storage         Cloud IAM - Service Account          An enterprise user need access to upload objects to a Cloud Storage bucket         Cloud IAM          I want to manage end users for my application         Identity Platform          I want to enable “Login using facebook/twitter” for my application         Identity Platform          I want to create user sign-up and sign-in workflows for my application         Identity Platform           和 Service Account 的区别     service accoutn 是 GCP 内部的服务之间互相访问的权限机制   Cloud Armor:  是一个防止 DDos 网络攻击解决方案库, 收集每年的 OWASP Top 10 attacks, 并提供保护机制Secret Manager:  存放数据库密码, API key credentialsSecurity Command Center:  用 Security Command Center 分析 infra 的安全     功能         维护 compliance     发现风险 (对现有的project, 资源…)     可视化风险          可以对多个 GCP 服务进行安全管理    Network Intelligence Center:  用 Network Intelligence Center 监控网络, 检查网络拓扑和架构Operations and Monitoring:  Cloud Operation 原来的名字是 Stackdriver, 包含了     Cloud Monitoring   Cloud Logging   Cloud Trace   Cloud Debugger   Cloud Profiler   Cloud Error Reporting    Cloud Monitoring:       需要先建立 workspace (orgnization) 才能监控多个项目   可以监控服务的 metrics         CPU     Network     Process matrics     Disk traffic     Uptime          可自定义监控指标 (custom metrics)   需要在 VM 上安装 Cloud Monitoring agent   可以创建 dashboard 来视觉化监控内容和结果   可以配置 Alert 来提供报警         Condition     Notification     Documentation          可监控 GCP 的多个项目 和 AWS 服务   OpenTelemetry:  不是谷歌云的服务 Opentelemetry 一个遵循 CNCF 的, 跨平台的, 开源监控服务 Steps to use OpenTelemetry:1: Add OpenTelemetry libraries (for your specific language) to your project2: Instrument your code to export telemetry 下游:     Google Cloud Monitoring   Google Cloud Logging   Google Cloud Trace    用 X-Cloud-Trace-Context header 放指定参数, 来跟踪某个请求     To force a specific request to be traced, add an X-Cloud-Trace-Context header to the request.    Cloud Logging:  可以从所有的服务中获取实时日志进行分析     大多数Google managed service (无服务)都是自动发送日志到 Cloud Logging   VM 是通过 安装 Logging Agent (基于 Fluentd) 想 Cloud Logging 发送日志         建议所有 VM 都安装 Logging Agent          On-premises 的服务器或者虚机可以通过下面两种方式发送日志         Blue Medora 的 BindPlane tool (推荐)     Cloud Logging API           可提供日志的获取, 存储, 搜索, 分析, 监控, 报警 可以自定义 log 的 metrics (Cloud Logging lets you define metrics based on your logs. )  无服务 主要功能     Logs Explorer - Search, sort &amp; analyze using flexible queries   Logs Dashboard - Rich visualization   Logs Metrics - Capture metrics from logs (using queries/matching strings   Logs Router - Route different log entries to different destinations    审计日志     提供         对什么服务     做了什么操作     对什么资源进行了audit     谁进触发的日志           日志可以导出到以下服务中, 进行后续操作     Pub/sub   BigQuery   Cloud Storage    日志保存     普通日志: 默认 30 天, 最大 3650 天 (10 年)   管理日志: 默认 400 天    Access Transparency Log (访问透明日志):          提供 GCP 团队访问我们的服务的日志 (只有 Gold 级别的客户才会有这个服务)      Cloud Audit Logs (审计日志):           四种             Admin Activity Logs                 以管理员身份登录的用户进行的操作日志         比如 VM 的创建, 打补丁, 修改 IAM 权限         比如 修改bucket or object                      Data Access Logs                 数据访问日志         比如 VM 的 Listing, Image 的 listing         比如 Bucket 和 object 的 修改和查看, 比如 BigQuery 的 query 查询                     如果为了审计要求, 要记录一个bucket 中的所有的活动行为                         为 Bucket 开启 data access audit logging                                                                System Event Audit Logs                 非人为的系统级别日志         比如虚机自动重启, GCP 系统维护                      Policy Denied Audit Logs                 用户或者Service account 访问被拒绝                                     日志的路由 (Routing)        Log 从不同的来源都要经过 Routing   路由通过查看规则决定将什么来源的日志, 需要经过什么样的过滤/筛选, 最后发送到什么地方   Sinks 是路由规则   Log 经过路由可以保存到:         Locked sinks (默认的日志存放方式)             _Required logs bucket                 存放管理事件, 系统事件, Access Transparency logs         保存 400 天 (不能改)         免费, 不能删掉bucket                      _Default logs bucket                 其他类型系统日志         保存 30 天 (可改成 1 天到 10 年)         不能删, 但是可以disable                               User-managed Log Sinks (用户自定义的日志导出, 长期存放的下游服务)             Cloud Storage       BigQuery       Pub/Sub                      日志使用场景     VM 的 trouble shooting         安装 Cloud logging agent 把日志导到 Cloud Logging     在Cloud Logging 导出的日志中搜索线索          用 BigQuery 查询和分析日志         安装 Cloud logging agent 把日志导到 Cloud Logging     创建 BigQuery 数据库     在 Cloud Logging 中创建 Sink, 把log 在按照规则导出到 BigQuery          保留日志给外部审计使用         在 Cloud Logging 中创建 Sink , 把log 导到自己创建的bucket 中     在 Bucket 中为审计员创建 Storage Object Viewer role     也可以用 Google Data Studio 看分析图形数据          Stackdrive log:  就是 Cloud log 原来的名字 收集各种日志, stackdriver is google’s recommended tool for monitoring and debbuging 场景:     测试 灾难计划    Stackdriver logs对比 (Admin) Activity logs     (Admin) Activity logs 是用来查看系统被用户使用的情况的         以管理员身份登录的用户进行的操作日志     比如 VM 的创建, 打补丁, 修改 IAM 权限     比如 修改bucket or object          Stackdriver logs 可以看系统级别的事件         可以从所有的服务中获取实时日志进行分析和debug          Cloud Trace:  相当于浏览器的网页加载过程分析工具, 像甘特图一样展示所有元素的加载时间和依赖关系.  它可以告诉你     • How long does it take to handle a given request?   . Why is it taking so long to handle a request?   • Why do some of the requests take longer than others?   • What is the overall latency of requests for the application?    分布式跟踪系统 通过对应用的tracing , 获取应用每个步骤的延迟(读取)时间数据, 并进行分析, 找出性能瓶颈 通过 Cloud Trace API 可以跟踪很多服务     Compute Engine   Cloud Run   Cloud Function   GKE   App Engine (Flexible/Standard)   …    支持的语言包括     C#, Go, Java, Node. js, PHP, Python &amp; Ruby    Cloud Debugger:        可让您在任意代码位置检查应用状态，既不需要停止正在运行的应用，也不会导致应用性能下降 部署，包括测试、开发和生产。只有在捕获应用状态时，调试程序才会在请求延迟时间内增加不到 10 毫秒的时间 Take snapshots of variables and call stack No need to add logging statements 支持的语言包括     Python、 Node. js、 Java、 Go、 Ruby、 PHP 或 . NET Core    Cloud Profiler:        系统性能瓶颈 的剖析工具     分析 CPU ,内存   从源代码中剖析, 找到性能瓶颈   可以分析运行在不同平台环境的应用, 包括         GCP     AWS,Azure,…     on-premise          功能包括         Profiling agent (获取剖析数据)     Profiler interface (查看数据报表)          支持的开发语言         Java     go     python     node. js      Error Reporing:                 实时获取生产环境的异常问题 把所有的 error 集中在 console 中管理 使用 Firebase Crash Reporting (放在手机上跟踪app crash 的) 获取 iOS 和 Android 的崩溃信息 可以看到error 详细信息     时间,频率, 影响的用户账号登    特支持开发语言包括     Go, Java, . NET, Node. js, PHP, Python, and Ruby   OS Patch Management:  给 VM 的操作系统机型升级管理的 包含两部分     Patch compliance reporting         which provides insights on the patch status of your VM instances across Windows and Linux distributions. Along with the insights, you can also view recommendations for your VM instances.           Patch deployment         which automates the operating system and software patch update process. A patch deployment schedules patch jobs. A patch job runs across VM instances and applies patches.           Cloud Operations for GKE:  You can view a cluster’s key metrics, such as CPU utilization, memory utilization, and the number of open incidents.  You can view clusters by their infrastructure, workloads, or services.  You can inspect namespaces, nodes, workloads, services, pods, and containers.  For pods and containers, you can view metrics as a function of time and view log entries. 场景:  跟踪对微服务的一个请求     Cloud Trace    确定一个微服务的 异常 (error)     Error Reporting    再生产上 一步一步 Debug 一个问题     Cloud Debugger    要看一个请求的日志     Cloud Logging    为了审计要求, 要记录一个bucket 中的所有的活动行为     为 Bucket 开启 data access audit logging   Complaiance and Regulations 合规:  谷歌云支持合规 用谷歌云的用户也需要确保自己在云上的应用合规 客户如果要保证合规, 需要和 google 签署一份 BAA (Business Associate Agreement) 谷歌云支持多种认证和标准     ISO/IEC 27001         security controls , 管理信息风险          ISO/IEC 27017         information security controls , 云服务          ISO/IEC 27018         云的个人隐私数据安全, critical components of cloud privacy - personally identifiable information (PII)          ISO/IEC 27701         国际隐私标准          PCI DSS         Payment Card Industry (PCI) Data Security Standards (DSS)     PCI DSS: PCI 支付卡的数据安全标准     用 tokenizer service 加密信用卡中的敏感信息             tokenizer service 是用来对信用卡的敏感信息做对应的token, 有了这个 token 就不用保存信用卡的敏感信息了, 也就合规了.                 22 年, GCP 整体都满足了 PCI 的合规要求, 但是客户仍然需要自己配置环境, 让自己的应用满足 PIC 合规          SOC 1         审计, 供应商的服务          SOC 2         审计, 服务供应商的安全, 可用, 隐私, 保密.           COPPA         儿童线上隐私保护 Children’s Online Privacy Protection Act          HIPAA         健康保险便携义务 Health Insurance Portability and Accountability Act     Reference: https://cloud. google. com/security/compliance/hipaa/          GDPR         欧洲数据保护 General Data Protection Regulation           HIPAA 最佳实践     应用 IAM 最佳实践   开启 Cloud Storage buckets 的 版本功能   开启 Autid log, 并把log 存到 Cloud Storage 中   不要对 PHI 的信息 缓存到 Cloud CDN    PCI 最佳实践     碰到有支付的应用, 为支付环境创建一个单独的账号和环境         隔离其他环境          限制对这个环境的访问         Follow “principle of least privilege”          控制进和出的流量         inbound: 通过 firewall rule 控制, 只允许             https       同一个内网网段访问       办公室的网络(给审计和管理用)                outbound: 只允许             https       反馈给 third-party payment processor                应用             用 compute engine 和 GKE 来跑应用                 不建议 App Engine 和 Cloud Function ,因为他们不支持 outbound cloud firewall rule                                    创建 HTTPS 的 Load Balancer, 并绑定SSL 证书   使用 harden (成熟的, 官方的) Linux Image   只安装必要软件   自动化 DevOps, 少人工干预   用开源软件进行安全管理, 定期升级安全补丁   开启         VPC Flow Log     Access Transparency Logs     Firewall Rules Logging     Configure Monitoring Alert          Log 保存到 Cloud Storage, 或 BigQuery 进行分析   开启 Cloud Data Loss Prevention 权限来限制数据访问   Migration to GCP 云迁移和整合: 云迁移:  云迁移的几个阶段     Phase 1: Assess the workloads to be migrated         评估要迁移到云上的内容             对要迁移的应用进行整理和分类       做一些实验和 POC 验证       计算迁移成本       确定要迁移的应用和先后顺序                 可以基于:                     Business value           Teams           Dependencies           Refactoring effort           Licensing and compliance needs           Availability and reliability requirements                                                       Phase 2: Plan the foundation         设置云端环境             设计 资源org 层级, 配置 IAM, 设计网络拓扑和连接(GCP 内部连接, 云和 on-premise 连接)       安全, 监控和治理计划       组建迁移团队                     Phase 3: Deploy the workloads         迁移应用和数据             数据迁移                 评估成本, 时间窗口, 实时和批量迁移, 数据安全                      应用部署       进行自动化                 Automate configuration management with Ansible, Chef or Puppet         Automate build and deployment using Jenkins, SonarQube, Cloud Build or Spinnaker         Implement Infrastructure as Code using Terraform or Deployment Manager                                    Phase 4: Optimize your environment         迁移后进行优化             提供日志, 监控和报警支持       增加无服务比例       通过 autoscaling 优化成本                      几种云迁移方式     Retaining: 不迁移(保留在 on-premise)   Retiring: 淘汰掉老旧不用的服务   Rehosting: 原来应用啥样还啥样, 直接放到云上, 换个环境   Replatforming: 对原有应用简单调整, 比如进行容器化   Repurchasing: 对原有应用进行云改造, 比如换个云数据库   Refactoring: 重构, 不如做成微服务, 无服务    两个云迁移案例     MS SQL Server 迁移到谷歌云的步骤如下:         Create a Cloud SQL for SQL Server instance     Move backup of your database to Cloud Storage     Import the database into Cloud SQL for SQL Server     Validate the imported data          容器迁移到谷歌云的几个选项:         迁移到:             App Engine flexible environment                 不能定制底层 VM 环境, 但马上就可以用, 很少的环境管理负担, 可伸缩性强, 但不能缩到零                      Cloud Run and Cloud Run for Anthos                 不能定制环境, 但是可以马上就用, 没有环境管理负担, 可伸缩性强, 可以缩到零                      GKE and Anthos clusters                 比较麻烦, 需要自己配置管理 Cluster, 但是比较灵活, Highly scalable                      Compute Engine                 不推荐, 啥都要自己配置, 太麻烦                                    迁移大量数据的计算:  例如 迁移200T的数据， 带宽是100Mbps , 需要多长时间     按 1Gbps 与 1T 是 3 小时 -&gt; 6000 小时 -&gt; 250 天    数据迁移的时间表      数据迁移的方案选择     aws 到 GCS 建议用 Storage Transfer Service   GCS 到 GCS 建议用 Storage Transfer Service   on-prem 迁到 GCS 带宽够用, 1T 以下的数据迁移 , 用 gsutil   on-prem 迁到 GCS 带宽够用, 1T 以上的数据迁移 , Storage Transfer Service   on-prem 迁到 GCS 带宽不够用, 花费时间不满足业务要求, transfer appliance   Cloud Scheduler:  google cloud 的 cron job scheduler 底层是一个 Google App Engine 的 cron 服务 使用 Unix cron 格式 自动重试功能 前提:     需要先在项目中建立一个 GAE    案例     scheduler 发送信息到 pub/sub   GCE 的多个实例从 pub/sub 中读取信息, 然后进行处理   Cloud Emulators:  模拟的开发环境 (sandbox) 用于在无法连接到 GCP 的时候, 开发人员可以在自己的机器上模拟 GCP 进行应用开发 支持模拟:     Cloud Bigtable   Cloud Datastore/ Firestore   Cloud Pub Sub   Cloud Spanner   Managed Service for Microsoft Active Directory:  也是 Active Directory Federation Service (AD FS) For single sign-on synchronize users and groups from Active Directory to Cloud Identity 将您的本地 AD 网域连接到云端Pricing Calculator:  价格计算器 , 计算 GCP 中不同服务的价格 支持 40 多种服务 注意, 这只是计算器, 并没有直接和我们的项目中的已经使用的服务绑定Hybrid and Multi-cloud: Anthos:  混合多云的集成开发环境 相当于 OpenShift, 混合多云版本的 GKE (GKE on-prem + GKE on-GCP)     主要运行 K8S   既可以在GCP 上管理 K8s, 也可以在 on-premise 上管理 K8s    集中的配置管理 (Git) 多集群(multi-cluster)管理 提供 Service Mesh (based on K8s Istio)         课程提供的解释:     Run Kubernetes clusters anywhere (cloud and on-premises)   Config Mgmt - Central policies - Kubernetes API, Service Mesh, Access control   Service Mesh (dashboards, logging, monitoring, distributed tracing)         Anthos Service Mesh 是一个工具套装, 用来监控, tracing, logging, 深度了解服务的 performance, 以及 performance 如何影响其他的流程, 发现存在的问题.              例子: 通过Anthos Service Mesh 监控一个服务的 latency, 超过阈值就报警                 安装 Anthos Service Mesh         在 console 中定义 Service Level Objective(SLO)         基于 SLO 创建 alerting policy                                    CI/CD - Watch for updates in the Git repository and applies changes to all relevant clusters automatically    API: API Management Tools:  有多种 API 管理工具, 包含     Cloud Endpoint   API Gateway   APIgee Edge   Cloud Endpoint Cloud API Gateway 分布式 API 管理系统 提供 API console, hosting, logging, monitoring, and other features 可以使用任何支持 API 标准的 API 可以支持 GCE, GAE, GKE, 支持的终端     IOS   Android   Javascript    API Gateway       full namaged API Gateway 提供定义良好的 Rest API 来包住backend servicesApigee (Edge):  管理 Rest API 生命周期     设计   安全   发布   分析   监控   scale         on-premise     google cloud     hybrid cloud          caching with Cloud CDN    提供 develoer portals     方便开发人员 管理 API, 获取 API key    案例:     为老旧应用抽象出 API 对外服务   把已有资产进行 API 暴露   perform business analytics and billing on a customer-facing API         把商业分析和 Billing 包装成面向客户的 API 暴露出来          Transcoder API:  Optimized files for delivery 提高 transcoding 的效率 数据源和目的地都可以是 Cloud StorageVideo Intelligence API:  Scene-level video annotation 实时流媒体的视频智能分析Game: Game Server:  提供 Agones 集群协调管理服务 提供无缝的多人游戏体验平台好的架构设计:  Resiliency: 弹性, 挂了部分, 服务还能用     用 Cloud Monitoring   安装 logging agent 发送日志到 Cloud Logging   给 VM 开 Migrate VM Instance 和 Automatic restart   健康检查   用 LB 放在 VM 前面    Availability: 99. 99%     99% 代表 7 小时一个月可以挂   99. 9% 代表43分钟一个月可以挂   99. 99% 代表一个月有 4 分半可以挂   99. 999% 代表一个月有 26 秒可以挂    High Avaibility     多 region 支持的服务, 一个region 挂了也没事   用 LB 分配流量   用 health check 检查出问题的实例, 不在给问题实例分配流量   给 VM 的 On host maintenance 配置 Migrate VM Instance         当计算引擎执行定期基础架构维护时，它可以将您的 VM 实例迁移到其他服务器(同一个zone的)，而不会停机     不支持 GPU 类型 和 Pod 类型 的 VM          给 VM 配置 Automatic restart (当VM 发生非人为的关闭后, 会自动重启)    Scalability: 伸缩, 随着流量/用户/数据的增减, 可以增减服务器     横向伸缩: 增加实例个数, 配上 LB + auto scaling         单zone     一个region 的多 zone     多个region 的多 zone          纵向伸缩: 增加单个机器的性能    lower costs: 知道怎么省钱     Sustained use discount 持续使用的折扣         VM 用的久且有持续流量, 那么满了一个月, 那么从用量的 25% 开始逐渐打折, 一直到月底打到 7 折     这个是自动发生的, google 帮我们算.      只有 GCE 和 GKE 才可以打折     某些机型, 比如 E2 和 A2 不打折     通过 App Engine Flexible 和 Dataflow生成的 VM 不打折          Committed use discounts 提前预定好用量的折扣         预定1-3 年     最高可以达到 3 折     某些机型, 比如 E2 和 A2 不打折     通过 App Engine Flexible 和 Dataflow生成的 VM 不打折     适用场景: 大量用户的稳定网站.           Spot (preemptible vm) 临时用用的机器         临时的机器可以用于那种数据不怕丢的计算, 不需要实时进行的计算     最高可以打到 2 折, 最便宜的一种机型     可能随时被 google 收回     有 30 秒的警告     没有 SLA 保障     不会自动重启     300 美金的免费账号用不了     Preemptible 和 Spot 的区别主要是             Preemptible 有 24 小时的提前通知                     Billing 账单         google 是按秒算钱的     实例停了不算钱, 但它上面的存储会算钱     最好创建 Budget alert 提醒我们别花超了          网络流量费用         ingress 免费     egress             egress 到 google 服务免费 (youtue, map )       同一个 region 之间的谷歌服务 egress 免费       不同 region 之间 egress 付费                             安全     使用 Firewall Rules   尽量用内部 IP 地址   如果有合规要求, 用 Sole-tenant nodes   使用自己做的 custom image 来生成 VM      架构师要知道的东西:  业务需求     降低成本         managed service     autoscaling     premptible vms     总成本(TCO - total cost of ownership)来源:             licensing cost (软硬件)       计算服务成本       存储成本       网络成本(线路成本, 数据的 ingress, egress)       人员成本(开发, 测试, 运维, 业务)       其他                 事故成本         第三方API 等                               CAPEX，全称为Capital Expenditure，即资本性支出, 如果在 GCP, 那就是采购云的一次性支出     OPEX，全称为Operating Expense，指的是企业的管理支出, 如果在 GCP 那就是持续的订阅支出          加快开发/创新速度         devops(CI/CD) SRE     微服务?          合规   应用和架构透明   数据智能    技术需求     功能性需求, 比如技术部门要求:         容器化     linux     容器协调     自动scaling     NoSQL     大量数据存储, 且低成本          非功能性需求         High Availability             Compute Engine                 MIG, AutoScaling, Health Checks (Auto healing), GLB, Global &gt; Multi-Regional &gt; Regional &gt; Zonal                      GKE                 Multi master, Regional clusters with pod and cluster autoscaling                      Bigtable                 Place clusters in different zones or regions                      Cloud Firestore/Datestore                 Use Multi-region locations                      Cloud SQL                 Use HA configuration (regional)         read replica 不能作为 HA                      网络连接                 Dedicated interconnect &gt; Partner interconnect &gt; VPN Have a backup connection.                       Managed Services - like App Engine, Cloud Functions, Cloud Storage, Cloud Firestore/Datastore, BigQuery                Scalability             CE                 用 MIG 和 template + LB         UMIG 不能自动伸缩                      GKE                 用 pod 和 cluster 自动伸缩                      persistent disk                 可以伸缩, 比如增加硬盘大小, 增加硬盘数量                      local SSD                 不能自动伸缩                      无服务                 Cloud Storage, App Engine and Cloud Functions are serverless (Auto Scaling)                      数据库                 Pub/Sub, BigQuery and Cloud Datastore are serverless (Auto Scaling)         BigTable, Cloud Spanner, Cloud SQL, Dataproc are NOT serverless ,                               Durability     Security             Cloud KMS                 对 传输的数据或者 at rest 数据进行加密         公钥私钥                      Cloud Armor                 是一个网络攻击解决方案库, 收集每年的 OWASP Top 10 attacks, 并提供保护机制                      Secret Manager                 存放数据库密码, API key credentials                                    谷歌云 Architecture Framework:  有四个原则     运营完美         高效的运行, 管理和监控交付的商业价值     策略             自动 build, 测试和部署       监控商业指标       执行灾难恢复测试                增加软件开发和发布的速度             小而快的发布       对静态代码分析和安全扫描       使用                 Cloud Source Repositories         Container Registry         Cloud Build                               监控系统和业务健康度             监控和日志                 Cloud Monitoring         Cloud Logging         Cloud Debugger         Error Reporting         Cloud Trace         Cloud Profiler                      SLI, SLO, SLA                灾难恢复 Dister Recovery             创建 DR 恢复方案       定义 RTO, RPO       考虑 网络, 计算, 数据, 带宽, infra       定期进行灾难测试       DNS 来切换 primary 和 backup       定期对 VM 的持久磁盘进行snapshot 并拷贝到其他region       开启 Live Migration, 确保 VM 在维护的时候也能正常提供服务                     安全, 隐私和合规         对安全控制, 隐私, 进行计划,确保合规     策略             仅提供最小的权限       自动部署敏感任务       进行安全监控                Authentication and authorization             IAM policy                 提供必要的role 给用户         使用 Org Policy Service (组织都允许什么)         使用 Cloud Asset Inventory (跟踪资产目录)         使用 Cloud Audit Logs (audit IAM Policy 变化 和 Service Accounts)                               计算安全控制             尽量使用私有 IP (避免外网访问)       使用 harden VM image (没有乱七八糟的软件)       使用 Shielded VM (避免远程攻击, privilege escalation, and malicious insiders)       开启 GKE 集群 的 Node 自动 upgrade                网络安全             用 custom VPC (不要用default VPC)       每个 project 对应一个 VPC (相互隔绝)       使用 firewall rules 控制进出的流量       用 Security Command Center 分析 infra 的安全       用 Network Intelligence Center 检查网络拓扑和架构                数据安全             谷歌云默认会把存放在云上的数据进行加密       Cloud Storage                 给 bucket 使用版本 (敏感数据)         bucket 中数据进行生命周期管理, 可以降低成本         合规的数据可以用 Bucket Lock         临时对外提供的数据用 Signed URLs                      使用其他数据类服务, 用 audit log       开启 Access Transparency Logs                     可靠性         设计可伸缩, 高可用, Self-healing(自动修复)的架构     定义 SLI, SLO, SLA 和 Error budget     监控和日志             Monitoring, logging, tracing, profiling, debugging etc.                 设计高可用和伸缩             设计跨region 架构来实现 failover       尽量 横向 scale       预测高峰流量, 并进行提前计划                事故管理             减少 mean time to detect (MTTD 平均故障发现时间): 及时通知团队       减少 Mean Time to Mitigate (MTTM 平均故障缓和时间): 临时措施       减少 Mean Time to Recovery (MTTR 平均故障修复时间): 彻底修复       延长两次故障间隔时间 Mean Time Between Failures (MTBF): build 可靠系统       记录故障原因和修复记录, 以及lessons learn                     性能和成本优化         Use GPUs and TPUs 增加性能             GPU 用来提高 ML, 视觉的性能       TPU 用硬件加速 ML                尽量使用无服务, managed service             Cloud Run, App Engine, Cloud Functions, Dataproc and Dataflow                分析成本和优化             Cloud Billing 看账单, 导出账单                 BigQuery 可以分析账单         Google Data Studio 对账单视觉化                               几种打折             Sustained use discounts (长时间使用的打折)       Committed use discounts (预定时长的打折)       Preemptible VMs (用临时vm 运行不重要的工作任务, 可以打折)       用 Cloud Storage Lifecycle 管理数据, 节省成本                     Managed service / Serverless 无服务:  一些名词     IaaS (Infrastructure as a Service)         比如: Compute Engine          PaaS (Platform as a Service)         比如: App Engine             是serverless                     FaaS (Function as a Service)         比如: Cloud Function             是serverless                     CaaS (Container as a Service) Serverless         比如: GKE, Cloud Run             GKE 也有 serverless       Cloud Run是serverless                      无服务     不需要自己去设置 VM, LB, Disk 的服务   不需要自己在虚机上安装相关的软件   上来就能用.    VM , LB, Disk 对用户都是透明的   默认提供了 auto scalling, HA   一般是付钱就能用, 按请求数, 用多少付多少   比如:         AWS 的 Lambda     GCP 的 function           Serverless NEG (network endpoint groups)     一个 NEG 定义了 一组 backend endpoints for a load balancer.    A serverless NEG is a backend that points to a Cloud Run, App Engine, Cloud Functions, or API Gateway service.    一个 NEG 可以代表         1 个或者一组 Cloud Run 服务     1 个或者一组 Cloud Function 服务     1 个或者一组 App Engine 服务     1 个或者一组 API Gateway 服务          NEG 前面放一个 LB   可以把 NEG 比作 无服务版本的 MIG    Serverless VPC Access     Serverless VPC Access 是一个私有访问选项, 允许你通过一个 internal IP 让一个 无服务(serverless) 连接到 VPC 网络.    VPN tunnel 连接的on-premise 环境(包含的database) 也算是 VPC 网络, 让 Serverless 服务访问 on-premise 的服务(比如数据库)   Workesapce:  为使用 GCP 的组织和组织的员工提供便利的服务 比如, 日历, 文档, 邮件等.  方便组织的开发人员/员工可以远程高效开发/办公 workspace 里面的应用都是SaaS 的服务命令行工具 - Cloud Shell (GCloud):  谷歌云提供四种方式和云进行交互     Web Console/ Mobile Console/ Admin console   CLI   API   SDK    cloud shell 是 google 默认给我们提供的一个虚机     5G 默认 磁盘 放在 $HOME 目录   我们自己在目录下创建的的文件只在 访问的 session 期间存在   20 分钟不用 shell, 会就被自动terminate 掉   120 天不活跃, 目录可能都被删了   可以在这个shell 上通过 SSH 登录其他的 VM    通过命令行工具可以和管理谷歌云的服务     create   delete   update   read    有一些谷歌云服务有特别的命令行     Cloud Storage - gsutil   Cloud BigQuery - bq   Cloud Bigtable - cbt   Kubernetes - kubectl (in addition to Gcloud which is used to manage clusters)    使用方法     Mac 上 安装 Gcloud SDK   网页上用 Cloud Shell    官方中文介绍以及相关命令12345678910111213gcloud init # 初始化 gcloud, 包括允许 gcloud 使用我的账号认证信息, 并设置 configurationgcloud config list # 列出我的账号的可配置参数gcloud compute instances list ## 语法## gcloud 组 子组 动作 . . . gcloud compute zones listgcloud compute regions listgcloud compute machine-types listgcloud compute machine-types list --filter= zone:us-central1-b gcloud compute machine-types list --filter= zone:( us-central1-b europe-west1-d ) "
    }, {
    "id": 17,
    "url": "http://localhost:4000/Agile-Project-Management-and-Agile-Delivery/",
    "title": "Agile Project Management and Agile Delivery",
    "body": "2022/06/03 - 用了敏捷这么多年, 一直没有机会把自己的一些管理心得写下来. 因为前不久在为一个客户做敏捷项目的时候客户希望我能给客户做一个敏捷 Workshop , 所以才有机会让我回顾之前的敏捷项目, 不管是做的好的部分还是不好的部分, 都整理出来, 和大家一起分享. Agile Project Management and Agile Delivery[TOC] 资源: User Story 定义和模板 Scrum Guide 官方中文版 Key concepts: 中文版的一张图片介绍 Scrum  Scrum 简单来说就是 11 个元素, 分成三类:  3 个角色:     产品 owner   Scrum Master   交付团队    5 件事     Sprint - 下面四件事情的容器   Sprint Planning   Daily Scrum   Sprint Review   Sprint retrospective    3 个武器     Product Backlog   Sprint Backlog   Increment - Sprint 的增量交付物     User Stories    As a  (who) I want.  so that.      +story points + acceptance criteria.  Should be independent, negotiable, valuable, estimable, sized appropriately, testable (INVEST)  Story points  Story points are a relative unit of measure to assess whether a user story is big or small. You can use different scales for story points (. G. 1, 3 or5; tshirt sizes s, m,, xl, xxl)Product Backlog  An ordered list of everything that might be needed in the product (also known as Master Story List) The product owner owns this artifact, which is where everything and anything that may be needed for the product (now or in the future) is stored. Sprint Backlog  The set of Backlog items selected to be delivered over a period of time The development team owns this artifact, which contains the plan for the current sprint. Velocity  The number of story points delivered over a sprint 每个迭代一个敏捷团队完成的 story point 数量, 如果一个迭代, 4 个人的团队可以完成 20 个 story pints. 那么后面大概就是每个迭代 20 个 的 velocitySwimlanes  A visual representation of stories (y axis) Vs status on the kanban/agile board: To Do; Doing; Done (x axis). Swimlanes let the team see progress on individual stories, while highlighting specific areas that need attention. Minimum Viable Product (MVP)  The bare minimum product that meets the client’s expectations Includes all must haves and excludes all nice to haves (no bells and whistles). Release  Comprised of several iterations or sprintsSprint  A period of time in which the team will work on a defined set of user stories The container event within which the other four events occur. The sprint should be one month or less in duration. increment  交付物, 每个迭代的增量产品, 成果Agile Principles:  Customer satisfaction by early and continuous delivery of us Welcome changing requirements, even late in development ful software Working software is delivered frequently (weeks rather than months) Close, daily cooperation between business people and developers Proiects are built around motivated individuals, who should be trusted Face-to-face conversation is the best form of communication (co-location) Working software is the principal measure of progress Sustainable development, able to maintain a constant pace Continuous attention to technical excellence and good design.  Simplicity-only build what is really essential.  Self-organizing teams Regular adaptation to changing circumstance. 翻译  通过我们及早和持续的交付使客户满意 欢迎不断变化的需求，即使是在开发软件的后期 可工作软件经常交付（几周而不是几个月） 业务人员和开发人员之间密切的日常合作 Proiects是围绕着积极进取的个人建立的，他们应该被信任 面对面交谈是最好的沟通形式（共址-坐在一起办公） 可工作的软件是衡量进度的主要标准 可持续发展，能够保持恒定的步伐 持续注重技术卓越和良好设计。 简单 - 只构建真正必要的东西。 自组织团队 定期适应不断变化的环境。The Agile Team: 三种角色 Product Owner  Represents the or is the end customer Defines what is valuable and priorities (owns the backlog)Scrum Master    Facilitates team meetings, removes team impediments, liaises with the scrum team     product owner and ensures the team remains on track  Delivery Team  The people in charge of execution or that are also part of the projects implementation (e. G. Developers, testers, business analysts, etc.  Defines what user stories are part of a sprintTools: Burn down chart  A chart that shows the remaining work to be done, progress over time and the teams velocity (points delivered in a sprint). On the X axis you have time  (sprints) and on the Y axis effort (story points)Kanban/Agile board  A visual pull system to track work and progress in a logical/basic flow (from left to right): To Do&gt; In Progress&gt; In Review or In QA (optional)&gt; DoneTrellow 看板可以被设计成如下类型. 可以单独放一个看板用于存档完成的工作, 每隔一段时间(自然月, 或者是一个 Iteration 或者是一个 Sprint, 就把Done 这个 list move 到存档的看板中, 重命名为那个迭代的名字或者一个日期名字, 比如 Done December 2021 (Sprint11)),  TODO, Doing, QA, Done, Backlog, Parking Lot( 一些站会讨论出来的额外话题, 需要站会后 去单独讨论) Discovering, Planning, Implementation, Closure Team member names, Donepoker planning Rituals - 仪式一个也不能少: Sprint planning  Meeting where the team defines which user stories they Will work on in an upcoming sprint -Delivery Team and Scrum Master.  Happens at the beginning of the sprint when the Scrum team gathers and creates a plan for the sprint. Daily stand-ups  15 min daily meeting where people stand up near the Kanban board and share what they did yesterday, what they are doing today and impediments (if any) -Delivery Team and Scrum Master When the development team inspects where they are in relation to the sprint goal and adapts their work plan for the next 24 hours. Sprint Review(Demo/Showcase)  Meeting where the team presents what they have delivered over a sprint to the product owner-product Owner, Delivery Team and Scrum Master.  At the end of a sprint, the Scrum team and stakeholders gather and collaborate to inspect the increment and adapt the product backlog. Retrospectives  The last event in a sprint, where the Scrum team meets and decides how to improve in the next sprint.  Meetings done after sprints to review, revise and adjust (similar to a lessons learned, but in real time, not just at the end of the project) -delivery Team and Scruin Master. 要问的三个问题     What went well   What didn’t   What should you do differently in next sprint   Agile Myths:    Agile is anti-documentation   Agile is anti-planning Agile is undisciplined Agile requires a lot of rework Agile is anti-architecture Agile doesn’t scale Agile solves everything Agile is only for IT projects翻译 谣言  敏捷是反文档的 敏捷是反规划的 敏捷是无纪律的 敏捷需要大量的返工 敏捷是反架构的 敏捷无法扩展 敏捷解决一切 敏捷仅适用于IT项目启动一个敏捷项目之前要准备的工作: Before you start doing Agile . Start with Sprint O (Planning)    Define who will be part of the team and which role each person will play   Agree with the team on days and times for the Agile rituals.  比如 Sprint plan, standup meeting, Sprint Review, Retrospectives 的时间和日期   Get your physical or digital tools ready (e. G Kanban board,     Make sure you have done the due diligence required for the project (e. G business case, budget, key requirements, etc. ) yes, you still need it, just keep it lean     Determine any constraints that are part of your project (budget, time, business cycle, etc. )   Define which days you will release, test and sprint (e. G. Releases or push to production will be done on a Thursday morning). 如何面对做不完的任务和不够的时间:    You make a list&gt; feature or what needs to be done (a. K. A. User stories )     You size things up estimate the effort required for each user story by assigning a unit of measure (a k. A. Story points)     You set some priorities&gt; put most important story points at the top of the list (backlog), which will be the first you take for the upcoming sprint.     You start executing &gt; working through user stories (delivering value) over a period of time (a k. A. Sprints )     You update the plan as you go &gt; you reflect on what worked and what didn’t and adjust (a k. A. Retrospectives)  Scrum 失败的原因: 失去目标 - Lacking Goals:  让 Scrum 团队知道我们的产品目标以及产品如何影响客户 公司目标转化给产品目标, 产品目标转化给 Spinrt 目标 目标可以被 measurable 目标要customer-focused 每个 Sprint 需要有一个目标, 并且在 restropective 会上阐述本次 sprint 的 increment 是否达到了 sprint goalTaylorism 泰勒主义:  泰勒主义主要思想就是最大化的发掘每个人的工作潜能. 被有些人理解为 - 现代奴隶制Table 1. Taylorism vs. Scrum       Taylorism   Scrum         Workers only know how to do the specific tasks they’ve been assigned; they don’t have or need a big-picture view of what their organization is trying to accomplish and are not encouraged to broaden their skill sets.    Work is performed by cross-functional teams that have all the skills they need to get the job done. These teams are supported by leadership, and high levels of trust are leveraged between leadership and Scrum teams to make decisions quickly and deliver increments of working software to customers frequently. Workers are encouraged to broaden their skill sets and collaborate.        Managers plan work without input from the people who perform the work.    Planning happens at varying levels across all the Scrum roles. Management is invited in to inspect the team’s work during sprint reviews and to collaborate with the product owner, so that the team can take management and stakeholder opinions into consideration as they figure out what to do during the next sprint.        Management tries to make the work as predictable as possible by precisely managing resource utilization with exact estimates.    Scrum teams manage their time and focus as they plan their work. The development team is responsible for resource utilization. They use estimates to trigger conversations within the Scrum team and with management—not to make the work as predictable as possible.        Management optimizes workers’ performance using metrics and measurements.    Scrum teams use metrics to optimize outcomes that benefit the customer.        Management uses money and rewards as primary motivators for performance. Workers are motivated to achieve rewards and avoid punishment (extrinsic motivation).    Scrum team members have a goal they are trying to achieve, the autonomy to achieve it, and a comprehensive set of skills to accomplish their goal. They also have opportunities to learn from each other. Team members are motivated to perform their work because they enjoy what they do and feel a sense of personal accomplishment (intrinsic motivation).    团队出现 泰勒主义的迹象  Scrum框架的机械实现，而没有真正接受其原则和价值观。 Scrum团队成员认为Scrum只是微观管理的新方式。 Scrum团队成员之间没有有意义的协作迹象。 Scrum团队正在产生低质量的增量。 人们仍然以他们的忙碌程度来衡量，而不是他们的工作成果。 Scrum团队无法在每个冲刺 （sprint） 中交付增量产品。 回到过去的做法，但用新的名字称呼它们。Trust is Missing: Scrum 中的信任  The product owner trusts the development team to create a done product increment by the end of every sprint.  The development team trusts the product owner to provide them with a clear product vision.  Development team members trust one another to do their best and support one another.  Management trusts the Scrum team and removes any impediments to delivery. 提高团队信任的方法  Shorten your sprint length. Instead of a four-week sprint, try a one-week sprint. The development team will have fewer product backlog items to focus on, the product owner will have stakeholders in the sprint review even more frequently, and collaboration will happen in shorter intervals.  Focus sprint planning on setting a sprint goal that has a true impact on the customer. This helps inspire the team and gives them something real to work toward.  Use the daily scrum as an opportunity for the development team to inspect their progress toward their sprint goal. Celebrate progress and promote opportunities for Scrum team members to support and help one another. This helps increase trust within the team.  Create opportunities for the Scrum team to collaborate with real customers. The sprint review event is perfect for this. Who better to talk about the impact of the team’s work than the people who are actually affected by it? Introduce or reemphasize the importance of the Scrum values. If you and your team keep the Scrum values in mind, then empiricism will shine. Coach’s Corner - 教练角: 如果没有一个 Scrum master 委员会, 大家的 Scrum 是松散的, 需要有一个类似 PMO 的地方, 定期让大家一起研讨 Scrum 最佳实践. Coach’s Corner 可以做的事情如下  Gather all the Scrum masters in your organization.  Discuss with them how far your organization has come in terms of transitioning to the Scrum way of doing things and how much is still left to do. If it’s helpful, reference the Taylorism vs. Scrum info we presented in this chapter.  Have everyone in the group spend five minutes on their own answering the following question: What’s within my control that I can change to get our organization 15% closer to where it needs to be? Put people in groups of two to four, and have each person take three minutes to share their 15% Solution with their group. Make sure that the people who aren’t sharing are actively listening and not giving feedback or advice.  Finally, have each team member spend five minutes sharing their idea(s) again—but this time encourage the other team members to ask questions and give feedback to each 15% Solution. 定期回顾 Scrum 的价值: Scrum 有五个价值, 需要定期和团队回顾, 是否我们遗忘了这些价值  Commitment can transform a team. It’s a promise to yourself, your teammates, and your organization to do the very best work you can. If everyone on your Scrum team is committed to delivering a done increment of valuable product each and every sprint, you can accomplish great things together.  Focus allows us to do our very best. Valuing focus means that we give people the time they need to think about their work. After all, creativity is hard enough without being constantly interrupted. Allowing development team members to focus just on one product, the current sprint, and the current sprint goal gives them the best chance of succeeding. Encourage the product owner to focus on the future value of the product while you, the Scrum master, focus on upholding Scrum.  Openness is the core of transparency, which is what makes Scrum work. If the members of your Scrum team (and the people who work with your team) aren’t open with each other and the wider organization, they can’t solicit honest feedback or adapt their work accordingly. You need to be open and honest, even when you’re struggling or there’s a tough issue to address. If you aren’t, transparency will suffer.  Respect creates a feeling of safety. Being open with others can be scary and admitting when you’re stuck is hard, but respect makes these actions easier. A high-performing Scrum team is built on mutual respect, and honest discussions create the safety needed to tackle difficult issues. Respect helps a team gel, grow, and learn together.  Courage is the linchpin of the other Scrum values. It takes courage to commit, to focus amid distractions, and to be open to new ideas. And it takes courage and faith in your teammates to count on having respectful interactions when you need to discuss problems. 翻译:  承诺：Scrum大师致力于仆人的领导。你通过影响力而不是头衔来领导。您的承诺是致力于Scrum框架，并确保Scrum团队颁布和理解规则。 焦点：Scrum大师的重点是团队围绕工作自我组织的能力。当存在自我组织时，团队有权尽其所能。您专注于培养协作文化，并帮助团队度过健康的冲突和困难情况。 勇气：成为Scrum大师有时很可怕。你会做一些事情来保护你的团队，但并不总是感到安全。你必须勇敢地行动起来，对伤害团队的做法说不。有时，向权力说真话需要勇气。在为Scrum团队和更广泛的组织服务时，请依靠勇气透明地行事。 尊重：作为一名Scrum大师，你正在与那些可能害怕你领导的变化的人一起工作。永远尊重那些人。尊重他们过去的工作，他们的成功，以及他们今天在学习和颁布Scrum的旅程中的位置。 开放。Scrum大师对他们观察到的事情和他们对事情进展的感受持开放态度，但他们也愿意听取团队的反馈。引导大家对 Scrum Value 认识的一些问题  Why do courage, commitment, focus, respect, and openness matter? Ask people to give examples of situations where specific values helped them.  How do we act on the Scrum values? If your team members aren’t using the values to guide their actions, they risk performing uninspired, mechanical Scrum.  What happens when the Scrum values aren’t present on our team? One example: without respect, team members may lack the courage to be open with one another. 在 Retrospective meeting 上问下面的问题, 让大家重新认识 Scrum Value  When is it difficult to live the values of courage, commitment, focus, respect, or openness? What gets easier when we embrace the Scrum values? Which Scrum value(s) helped us the most this sprint? Is there a specific value that you think we especially need to work on within the team?两种让大家了解 Scrum Value 的方法  During your next sprint retrospective, write each value across the top of a whiteboard. Then ask the team to come up with ways that they applied each value during the last sprint. You should also ask how the values were violated. Use the responses to generate a discussion about each value. Over time, this will create a common understanding about how each value applies to the team. If your team is struggling with embracing the Scrum values, you can perform this exercise during every retrospective. If, on the other hand, your team is consistently working well together, you may not need to perform it very often (though you should still do it occasionally, just to keep the values fresh in everyone’s minds. ) If your team is more action-oriented, you can try framing each Scrum value as a product backlog item in the product backlog. Here is a story format you can use to get you started: “As a ***, I want to __*** in order to **_. ” Use this format to make these “value stories’’ actionable. For example, “As a Scrum team member, I want to show up on time to Scrum events in order to show my teammates that I respect them. ”Product Owner - PO: 一般来说一个产品只能有一个产品经理. :    单个产品如果有多个产品经理, 会带来各种利益冲突     产品所有者委员会带着相互竞争的利益来到谈判桌前：技术、部门、客户、政治、组织等。     只有一个产品所有者，开发团队知道什么工作对产品真正重要，以及他们应该关注什么。Scrum团队内部和利益相关者都对产品现在在哪里以及未来的走向有着整体的看法，因为有一个人指导它。唯一的产品所有者收集利益相关者和客户的反馈，并将潜在更改纳入产品积压。决策不会因相互竞争的利益而停滞不前。  以下是一些关于产品所有者角色的重要事实，供您在阅读本章时记住：  The PO role should be performed by one person per product, not a committee.  The PO manages and is responsible for the product backlog.  The PO works relentlessly to maximize the value of what the development team is working on by ordering the product backlog.  The PO is in charge of creating and maintaining the product vision.  The PO uses Scrum as a catalyst to inspect the product and adapt its direction based on customer and stakeholder feedback.  The PO keeps tabs on the marketplace and adapts the product with it.  The PO works with stakeholders to gather opinions and ideas, but is empowered to make final decisions. 产品经理必须是全职的, 不能由其他角色的人员兼任. : 一些产品经理需要全权负责的在 Scrum 框架之外的工作  Evaluating competitor products Assessing customer information Building a road map Managing the budget Implementing value-based metrics Gathering stakeholder opinions产品经理没有足够的权利 / 或者拥有太大的权利: Scrum Master 需要帮助产品经理争取到足够的权利来履行产品经理在敏捷开发中要做的所有工作 一个总司令类型产品经理的特征  决定了开发团队要做的事情, 参加每日晨会, 并且要求 Scrum Master 让开发团队满负荷运转 一个人的意见代表了公司的所有利益相关者Scrum Master 要帮助产品经理得到适当的权利, 但也要帮助 PO 分担压力, 让他能放松. 如果权利过于集中, 产品就会缺乏透明度. PO 和 Scrum Master 是一个人: 有时因为预算或者管理者的无知, 希望PO 和 Scrum Master 是一个人. 甚至架构师或者开发人员来担当产品经理 但是PO 和 Scrum Master 的职责不同, 两个角色由一个人担任会有冲突 SM 和团队的对话:  开发团队：维护Scrum，并确保团队针对经验流程控制进行优化。 利益相关者：创建对复杂工作领域的理解，并向他们展示进行经验流程控制是多么必要。 管理：合作进行Scrum成功所需的组织变革。产品所有者和团队的对话:  开发团队：产品的愿景和方向，以及产品所有者如何将愿景和方向转化为产品积压。 利益相关者：与产品方向合作并保持积极参与。 管理：完成预算编制过程。Scrum大师对产品所有者的支持工作:  促进产品所有者和开发团队之间的互动。 帮助产品所有者识别并与利益相关者合作。 与产品所有者合作，找到进行适当产品积压管理的方法。 寻找方法，使产品积压对开发团队和利益相关者透明。 向产品所有者教授Scrum。 指导产品所有者进行敏捷产品管理。因此, 产品 owner 和 Scrum Master 需要两个全职的人员担任 缺乏产品愿景: Scrum Master 要和 PO 一起, 创建一个愿景 教练角: 帮助产品 owner 创建一个 PO 交互图 Work together to create a Product Owner interaction map:  On a whiteboard, create four quadrants. Label them “Stakeholder,” “Customer,” “Development Team,” and “Management” (respectively).  On individual sticky notes of the same color, answer this question: “What interactions is the product owner having with this group right now?”. Place each sticky note in the appropriate quadrant. An example might be placing a sticky labeled “product backlog refinement” in the development-team quadrant.  On different color sticky notes, answer this question: “What interactions is the product owner not having (or not allowed to have)?” Again, place each sticky note in the appropriate quadrant. These items are impediments that are preventing your product owner from making value-based decisions. Why aren’t those interactions happening? What is the first course of action you can take to change this? In other words, what is the first impediment we can remove?Product Backlog: 产品backlog 是Scrum团队对其产品的所有了解，以及他们打算在任何给定时间构建和交付什么。这是路线图、产品愿景和执行计划。竞争对手们很想得到它，但前提是Scrum团队建造得很好。 产品 backlog 应该是不断被优化的, 优先级被重新排序的, 根据市场的变化进行调整的, 逐步被实现的. 一个产品, 多个 backlog: 在某些情况下, 一个产品产生了多个 backlog, 比如单独为 bug 创建一个 backlog, 或者为一个产品的不同开发团队建立不同的 backlog. 这样会带来灾难 这样做的坏处  对于需要完成什么，没有整体的看法，因此团队将开始分歧，导致产品不再具有凝聚力，甚至可能无法发挥作用。 每个团队将以自己的方式构建产品积压，使整体产品愿景和目标变得毫无用处。 基础架构在团队之间将不一致，导致技术债务和需要重新设计不连贯的功能。 团队之间的依赖性不会被考虑，从而导致指责和冲突。 试图整合所有不同团队的工作将为冲刺增加不必要的复杂性和时间。 没有一个团队对产品的质量和价值对利益相关者负责。一个产品应该只有一个 PO 和 一个 Backlog 如果有多个 Backlog, Scrum Master 应该做如下这些事情:  Work with your product owner to gather every product backlog. This sounds a little silly, but there are likely many spreadsheets, notebooks, and napkins in your organization with important product information and backlog items. Check with the development team(s). They probably have lists of technical debt and “things we intend to do later,” captured somewhere, that should also be included in the product backlog.  Ask the product owner to merge all of these lists into a single product backlog. The new backlog will be sprawling and seem overwhelming at first, but that’s okay. You’re about to prune it into something more manageable.  Delete anything that’s older than six months, as you likely won’t ever do the work for those features. And if that work is important, it will reappear via feedback from customers and stakeholders. Understandably, your PO may be really nervous about truly deleting any PBIs. But trust us, it’s better for everyone in the organization to axe PBIs that are old and stale. The PO may be tempted to create a separate list of “deleted” PBIs “just in case,” but as we discuss in the next section, that’s a bad idea. The product backlog (singular!) should be the one and only source of truth for the project.  Group similar items together and help your product owner create new product backlog items, if needed. Refine and rework these items as you merge them together to create a unified product backlog.  Help the product owner order the newly built product backlog. Discuss what needs to be worked on next in order to deliver the most value in the next sprint.  Facilitate a product backlog refinement session with the Scrum team(s). Helping the team(s) re-establish a holistic vision of the product is important. A refinement session or two can help get everyone back on the same page and allow them to see what has changed now that all of the work is visible.  Have your product owner share the updated product backlog during the next sprint review. 太多或者太少的 PBI (Product Backlog Items): 太少的 backlog 导致产品缺失了愿景和目标 太多的 backlog 导致愿景不清晰, 不具体, 增加过多的沟通成本 好的 backlog 足以定义您产品的短期、中期和长期未来：  Short-term items (a. k. a. stories) should have the most detailed requirements. It’s good to have two to three sprints worth of such items that are ready for the development team to bring into a sprint. These items should contain enough detail that the development team feels confident bringing them into a sprint, but not so much detail that there’s little flexibility in the outcome.  Mid-term items (a. k. a. features) are things that will be accomplished 3-6 months from now. They should have enough detail for the development team to know the direction that the product is heading. Invest minimal time here. Remember, PBIs are emergent—things will change before you get to these items. Rough estimates and sizing are perfectly fine for these PBIs.  Long-term items (a. k. a. epics) can be vague. They should describe what the product might contain in the distant future. These items are too large to be finished within a single sprint, are often unclear, and will need multiple refinement sessions to become actionable. 细化产品 backlog: 产品 owner 和交付团队要定期(每个 Sprint 一次或者多次)将一些 需求(Feature or Epics) 进行细化, 工作包括  添加详细信息 评估工作量 (optional) 分解 归属的 Sprint (optional)仅在必要时估算: 您估计得越晚，离工作就越近，估计就越相关。有些人认为，在软件开发等复杂职业中工作时，准确的估计是不可能的，也是浪费时间。虽然很少能够准确估计事情的进展，但出于两个关键原因，创建估计仍然值得：  估算允许产品所有者在权衡功能的预期价值与构建该功能的预测努力时破译PBI的投资回报。 估算为开发团队提供了详细信息，可以帮助他们确定可能需要什么才能将该项目带入冲刺。例如，它可以帮助他们确定单个PBI是否可以（或应该）分为多个项目。关于工作的对话通常比实际估计更重要。不要走得太远:  如果您正在和开发团队讨论几个月内无法处理的产品backlog，请停止——您是在浪费时间 一般保持 2 个 Sprint 的清晰 backlog 就差不多了 留一些关于实现需求的对话空间给开发团队, 让他们有一些想象的空间, 模棱两可让开发团队有机会谈论完成PBI的更好方法和找到创造性的解决方案PBI 的格式不一致: PBI 建议的字段包括:  Title Description Acceptance Criteria Estimate Value一个例子: Title: 作为一名Scrum培训师，我想为Scrum培训课程创建一个新列表，以便潜在学生可以获得有关该课程的信息。 Description: Scrum培训师需要能够输入和编辑以下信息：  课程名称 课程描述 Scrum教练姓名 课程地点（地址） 课程开始日期 课程结束日期 注册课程的链接（目前通过第三方支付处理器处理）Value: 查找和注册课程的高能力-与收入的直接链接估计：开发团队估计此PBI的五个故事点。当前平台支持大多数请求的功能。 一成不变的 产品 backlog: 有的 PO 给所有的 backlog 都设定了, 而且不希望做任何变化. 随着外部环境的变化(市场, 竞争对手, 用户习惯, 科技变革, 政治因素等等), 我们需要不断的改变自己的产品 只有 water flow 的项目才不希望变化, 而敏捷项目是要拥抱变化 我们需要不断的迭代, 然后在客户和市场面前验证, 然后再不断改进,迭代 Frustrated Stakeholders: 一个 Stakeholder 问产品 owner, 之前提的一个需求什么时候完成, PO 说按照现在的趋势, 估计要等 6 个迭代后实现(12 周后), 但是到了12 周, 这个需求被其他中间插进来的需求给 delay 了.  所有这个 Ctakeholder 感觉被骗了. 更好的回答方式: “鉴于对该功能的估计，并假设我们不添加任何其他产品backlog item，我们预测大约有六次冲刺。这也假设我们的其余估计是准确的，在我们完成工作之前，我们不会知道这一点。我很乐意与您坐在一起，重新评估该功能的价值主张，看看它如何符合该产品的路线图。” 这样说的好处:  传统上，团队被要求同意，然后承诺进行估算。我们的答案不同：它包括“预测”一词而不是“承诺”，以强调缺乏确定性。 它提到积压的产品将发生变化。 它提醒每个人工作的不可预测性，以及估计可能是错误的。 产品所有者提议重新审视作品的价值主张，这表明根据价值订购积压产品是灵活的。计算某个功能哪个 Sprint 能开发完成的预测方法:: 表3。产品积压样本       类型   故事点估计         功能A   4       功能B   5       缺陷1   6       功能C   2       功能D   8       功能E   5       缺陷2   1   利益相关者想知道功能D何时准备就绪——换句话说，他希望根据该功能的交付情况进行预测。如果开发团队的速度（这是一种花哨的“容量”）为每次冲刺10个故事点，那么他们需要多少冲刺才能交付功能A？使用简单的数学，我们知道，只要遵守PBI的顺序，就需要大约三次冲刺才能交付该功能。 没有排列先后顺序的 Backlog:    先后顺序 - 是哪个先做,哪个后做. 这个是 PO 来决定的, 包含了优先级的因素和产品功能的依赖逻辑.     优先级 - 是重要紧急程度, 只能说明哪个PBI 更重要紧急, 是从需求方的角度来看的, 没有产品功能的依赖逻辑   Product backlogs should be ordered, not prioritized.  A clearly ordered product backlog creates transparency to stakeholders and helps the dev team know exactly what they should be working on at any given time. 使用 Order, 而不是 Prioritization 的理由: There are some really good reasons for preferring ordering over prioritization:  The term “priority” implies a level of certainty that we simply don’t have in a complex environment. If your product owner gives a PBI a priority of 1, that implies that the item will get done, and your stakeholders will hear that implicit message loud and clear. And why shouldn’t stakeholders assume that PBI will get done—it’s of the highest priority! If ten PBIs have a priority of 1, how does the development team know which one they should work on first? In short, they don’t. But if PBIs are ordered instead, the development team knows exactly which ones to work on, and in what order.  If your product owner has multiple top-priority PBIs, he should know which one delivers the most value to stakeholders—and therefore, he should be able to order the items appropriately. If the PO doesn’t know which PBI is most valuable, how can he possibly know whether the sprint should be funded in the first place?教练角 - 帮助 PO 找到最优价值的 PBI , 并进行排序 的 workshop: Min Space 方法 邀请利益相关者、开发团队成员、客户、在组织中担任领导职务的人以及对产品或项目结果感兴趣的其他人。  Have the participants form groups of 4-7 people, and provide them with materials for rapidly capturing ideas, tools such as markers and sticky notes, and space to post the ideas (either on a wall or flip chart).  Display the current product backlog so that everyone can see it, and then pose this question: “Which features in our product backlog are needed in order to have a successful product release?” Ask each person to write down (one per sticky note) as many must-do product backlog items and must-not-do product backlog items as they can in 2-3 minutes. When time’s up, ask the groups to consolidate their individual lists, eliminating duplicates. At this point, each group’s list of must-do PBIs will likely be quite large.  Have the groups aggressively test all the items on their must-do lists by answering this question about each item: “If we delivered all of our must-do PBIs except this one, would we still have a successful product release?” If the answer is “yes,” then the group has to delete that PBI from their must-do list. Encourage the teams to be ruthless when answering this question. This step can take around 15-20 minutes, depending on the size of lists. Allow more time as needed.  Have the groups compare their work. Ask each group to spend a few minutes sharing their whittled-down, must-do PBI list with everyone, and have them discuss the trade-offs and decisions they made to get to their current list.  After each group has presented their lists, have everyone work together to consolidate their lists into one master must-do PBI list that is as short as possible.  Have everyone discuss the master list and work together to help the PO order these vital PBIs. 开发团队: 假设有下面的这些开发团队在我们公司使用敏捷, 但是失败了 表4。团队组成       团队编号   团队能力         1   数据库和基础设施       2   网络服务       3   网络服务       4   前端和用户体验       5   前端和用户体验       6   前端和用户体验       7   前端和用户体验   缺乏必要的技能: 瀑布式项目的开发团队组成, 他们是顺序进行项目开发的  分析 设计 开发 测试 部署Scrum 开发团队需要拥有所有必要的技能，以创建每次冲刺的增量 无需依赖团队以外的任何人, 因为他们是自给自足的——他们不必依赖其他人来取得进展。 开发团队需要具备一切必要的能力，才能实现这一目标。这通常被称为跨职能团队。 分析一个团队是否需要其他技能集的快速有效的方法是绘制从开始到生产所要完成的 backlog iteam 所需的工作流程, 比如:  分析/精炼 UX/UI Design 编码 测试 部署如果发现了 Scrum 开发团队缺失某项技能, 就需要补齐这块短板 That’s Not My Job 这不是我的工作:  Scrum团队的开发人员可能会编写代码、开发测试用例、创建用户界面设计、编写客户文档等。 无论单个团队成员的专业领域如何，整个团队都有一个单一的责任：在冲刺结束时提供增量。 Scrum 开发团队是共享问责制 Scrum 的开发人员/设计/测试等人员不用向他的垂直部门经理汇报 团队中没有一个角色是英雄。开发团队一起成功或失败 开发团队中的所有人都是向Scrum 团队进行汇报的. 对交付工作的定义: Scrum 开发团队初期交付的成果和后期交付的成果会有一些不同  Now: Our definition of done as it stands today. What are we capable of right now? For instance, in our earlier example, this section would contain the following:     Code has been reviewed by someone other than the person who wrote it.    The PBI has been tested by someone other than the person who developed it.    The PBI has been deployed to the development environment.    The PBI has been tested by someone besides the code reviewer or the person who wrote it.     Next: What do we plan to do next to advance our architectural practices and make our definition of done more strict? Again, pulling from the earlier example:     Automated tests have been written.    The code passes the continuous integration build.    The functionality is sitting in the staging environment, ready for deployment.     Future: What things do we imagine being in our definition of done once we’re capable of them in the future? Here are some possibilities for our example scenario:     The PBI passes all automated tests (unit, integration, and functional).    The PBI has been reviewed by the entire Scrum team.    The PBI has been shipped into production.    Revisit this exercise frequently during sprint retrospectives to help evolve the team’s definition of done. 每个人都为自己: 尝试让开发团队的个体之间有更多的交互, 只有团队有更多的沟通和协作, 才可以保证每个 Sprint 的高质量完成 一些尝试  结对变成 代码 review 分享开发成果和经验,心得 先完成工作的开发人员帮助没有完成的开发人员共同开发 限制每个 Sprint 的工作量, 留出时间让大家进行交流 开发人员帮助测试人员一起测试Wait Your Turn- 等轮到你: 开发团队的各个角色在迭代开发的时候可能会有相互等待的情况, 比如开发等设计, 测试等开发, 运维等测试等. 如何让开发团队不用相互等待  尽早的让PO 把架构和 UI/UX design 在 Sprint 之前就准备好, 让开发人员不需要等待 Sprint 开始的时候测试人员就需要准备测试用例和环境 运维和上线的事情最好由开发人员搞定, 不需要额外找一个不能开发只能运维的人员 开发人员要具备测试的专业能力, 相互进行测试团队太大了: 在过去的一年半里，开发团队从10人增加到29人。这让很多工作变得复杂. 不透明, 不可控  开发人员个性不同, ：英雄、消防员、流浪汉，随你便。等等, 沟通变得困难 晨会时间边长, 多数人在等待别人发言, 却和自己无关 让一个 Sprint 的内容变得过多, 管理困难, 透明度变差 产出增量变大, 增加测试时间, 增加上线风险解决方法  Scrum Guide建议开发团队由三到九人组成，虽然不是强硬的规则, 但是我们仍然建议3-9 人 一个大团队, 要去掉多余的人, 留下那种多面手, 全栈工程师.  如果必须要拆分成多个 Scrum 团队, 要确保每个小团队都可以独立的自我运转并在每个迭代可以看到增量.  通过让团队自主进行重组，他们立即开始将自己更多地视为一个团队，而不是一个个人集合。 要求每个团队给自己取一个有趣的名字，如超级英雄力量或电子游戏。教练角: 开发团队和 Scrum 的角色和责任  作为一名Scrum大师，记住开发团队拥有他们工作的所有权，而不是你，这一点非常重要。 Scrum大师很容易忘记他们是仆人领袖，而不是任务主管。 开发团队对其流程缺乏自主权是不健康的，也适得其反。如果开发团队缺乏责任, 尝试下面的方法  Gather the development team together. Ask each person to silently spend one minute brainstorming as many team improvements as they can think of (process, technical, and so on), and to write each idea on a separate sticky note. To get them inspired, try offering a phrase for them to complete, such as “In order for us to become a rock-star development team, we have to…” After the one minute is up, ask them to pair up with another person and spend two minutes collaborating and writing down yet more improvement ideas together.  After these two minutes are up, have the pairs share an idea one at a time by shouting it out and placing it on a whiteboard.  Ask the group to look for patterns, and if any emerge, group the related sticky notes together. Discuss all the topics for five minutes, paying specific attention to the patterns.  Lastly, give each person three sticker dots. Tell them to put the dots on what they think are the most pressing items. They can place all three dots on the same sticky note or distribute them across various sticky notes. Catalog the results and make them visible in the team area. Use the dots to help prioritize: Address the items with the most dots first. Choose one or two of these improvements to implement in the next sprint, and then revisit the list during the next sprint retrospective. Look carefully at the list for signs of impediments, and if there’s something going on that the development team can’t resolve on their own, be sure to take action. 敏捷大师: 一个被动的 Scrum Master 是常见的失败因素 做一个仆人领袖意味着什么？  仆人-领导哲学意味着，最重要的是，为他人服务是领导力最重要的方面。仆人领袖将他人的需求放在首位。仆人领袖的主要兴趣是帮助和支持他人成长和繁荣，因为这是促进个人和团队成功的最佳方式。这与传统领导力形成鲜明对比，传统领导力是积累权力和权威的首要问题。Scrum大师的角色是：  通过将Scrum原则和实践应用于团队成员、利益相关者和组织中其他人之间的互动，确保团队和组织理解并实施Scrum。 根据需要促进Scrum活动，以帮助每个人遵守Scrum角色和规则。 通过指导团队成员如何接受和践行Scrum价值观，为Scrum框架注入生命。 充当团队的仆人领导，而不是团队老板。 以影响力和同理心领导，同时关注Scrum团队成员和他们所服务的客户的需求。 激发旨在提高产品质量和项目成果的变化。 向组织体现Scrum价值观。我的团队没有人知道Scrum: 如果不确定团队是否了解 Scrum ,那就找个晨会, 画个图给大家, 让大家填 Scrum 的 11 要素的便签, 填不上, 就由教练来写, 然后由团队来贴, 并由贴的人给大家讲解为什么  请您的团队在图表上放置他们选择的任何便签，并讨论他们放在板上的物品的重要事实。 一旦第一个人分享了他们的事实，请进一步阐述所选便签，并提供额外的见解和上下文。 不要立即纠正错误的答案。事实上，在整个练习过程中，不要碰一个便签。 如果团队需要提示，请询问是否有问题。 你可以很容易地花两三个小时来锻炼这个练习。如果这似乎对一次会议来说太长了，每天分成半小时的会议，直到所有便签都放在正确的位置。超级英雄 Scrum Master: SM 有的时候会忍不住直接给 Scrum 团队提供问题的解决方案或决定, 而不是引导团队自己做出尝试和决定, 这样会带来一些伤害:  No ability to experiment: When a Scrum master “solves’’ all the team’s problems, the team won’t learn how to experiment. A Scrum master must constantly inspect and adapt practices. Some changes work—others don’t. Dictating answers robs the team of these lessons.  Scrum team members withdraw: Apathy sets in when a Scrum master mandates solutions. A disengaged team can lead to silos of knowledge and individual actors instead of a gelled and cohesive Scrum team.  Whole-team concept is compromised: Every member of the Scrum team contributes to the product and code base. If a hero Scrum Master is solving all the problems, the Scrum team becomes dependent on this hero behavior. Coaching others to solve issues and impediments can help teams grow, mature, and find success together. 一些想法:  Scrum大师英雄主义在短期内似乎有帮助，但随着时间的推移，负面影响会放大。 看到问题, 如果团队没有发现, 不要自己指出来, 而是引导团队成员去发现并纠正 鼓励团队做出决定, 成功后做出表扬 如果团队似乎没有就他们的工作做出决定，请集思广益，以团队的方式纠正决策中的不平衡。轮值的 Scrum Master: 显然，Scrum大师是一个忙碌的人，需要一套专业技能才能成功。Scrum大师是观察者。 你的工作是观察团队：他们如何工作和互动。您还可以关注外部组织如何与Scrum团队互动。 如果这个职位是轮值的, 任何人都可以填补该角色, 如果Scrum大师为团队带来了一些特定的技能，这些技能需要时间和精力来发展。如果Scrum主角色持续轮换，您将无法了解许多社交线索、信息点和机会, 这些技能将断裂 Scrum Master 需要有技术背景吗?  根据《Scrum Guide》，不，在某些情况下，技术技能可能会分散注意力——特别是如果Scrum大师发现技术工作比辅导、促进、培训、指导、消除障碍和随之而来的组织变革更有趣。 另一方面，技术技能对Scrum大师来说可能是一个好处。如果Scrum团队坚持技术决策，如果Scrum大师有一些开发经验，他或她可以更快地解决这个问题。但开发团队拥有构建产品的“方法”，他们拥有团队内部交付工作所需的所有技能。 成为Scrum大师需要如此多的不同技能，以至于在评估候选人时，技术水平几乎属于要寻找的末尾。Scrum Master有三个级别的服务：对产品所有者、开发团队和组织  您对产品所有者的服务始于帮助有效管理产品backlog 的排序。这包括创建按业务价值排序的PBI。这方面的技术和实践各不相同，但您花在这样做上的时间有助于PO优化业务价值。您还负责教产品所有者如何规划产品发布并获得投资回报。做得好，为产品所有者服务直接有助于增加利润。 开发团队也需要您的帮助。Scrum大师消除障碍，教授技术和团队技能，保持自我组织，打破筒仓，鼓励合作，指出技术债务问题，并为Scrum活动提供便利。结果：一个更有效的开发团队。 最后，你也为组织服务。通过向组织提供敏捷辅导，您不仅可以查看团队如何开展工作的大局，还可以查看更广泛的组织如何影响您的团队。您发现了Scrum团队内外使产品交付更加困难的问题和障碍。通过解决这些问题，您正在提高Scrum团队的有效性。随着组织中有效团队数量的增加，这些团队提供的价值也可能增加。兼职或者轮训的 SM 可能做不到持续和连贯的对组织, 开发团队和 PO 的支持 So Many Impediments, So Little Time - 障碍太多, 时间太少: 有一种失败是 Scrum大师没有有效地处理超出团队能力的障碍 这些障碍可能包括:  测试服务器已经停机两周了。 Stage 环境并不反映生产环境，但DevOps团队太忙了，无法处理它。 组织要求代码审查由开发团队以外的人员执行，团队已经浪费了等待其他团队完成这些审查的时间。 架构团队花费了大量时间批准新的编码模式，这阻碍了他们取得进展的能力。有一个方法是创建 障碍 Backlog  找到一个Scrum团队和利益相关者可见的地方，在那里您列出了阻碍团队的所有障碍。 按优先级或严重程度列出它们，并定期提供有关影响和进展的最新信息。 与您的利益相关者、产品所有者和开发团队一起查看障碍。促进这项工作，使团队能够尽快解决这些障碍。 强有力的促进对于这项工作的运作至关重要, 包括想出创造性的方法来收集团队的建议, 并将建议提炼成可操作的改进方法。 让任何人都可以使用便签和记号笔来添加障碍 Backlog，也可以贴上他们提供的解决方案的建议. 但不要自己碰任何东西！ SM 在看到某个障碍并想亲自解决它之前, 参考如下步骤:  想想这是真正的障碍，还是只是一个有变通办法的临时障碍。 如果这是一个障碍，想想团队是否可以自己解决。 如果您确定障碍超出了团队的修复能力，那么是时候采取行动并帮助消除障碍了。可怕的 Scrum Lord (暴君): 这些Scrum大师对懈怠的团队成员大发脾气。他们分配任务，设置截止日期，并管理任意指标。 本来应该是仆人的领导, 被指挥和控制的欲望所取代。 Scrum团队应该自我管理并自我组织。在Scrum中，使用武力效果不佳。 带来的伤害:  质量下降：强加指标和不切实际最后期限让团队被迫降低质量以满足新需求。在这些类型的情况下，质量是首先要做的事情。对于一个团队来说，很容易决定，为了在不切实际的最后期限前完成，他们必须停止编写单元测试。或者更糟糕的是，完全停止测试。   技术债务增加：不断承受压力的Scrum团队不太可能做出正确的决定。随着时间的推移，质量和架构方面的一系列妥协将导致技术债务增加。具有讽刺意味的是，随着系统变得太脆弱和复杂，无法轻松改变，额外的技术债务将降低速度。   士气低落的团队：Scrum的核心是增强开发团队的能力。自组织从软件开发中移除了传统的指挥和控制杠杆。通过强制任务分配和技术指导重新添加这些杠杆会削弱Scrum开发团队拥有工作的能力。这是扼杀士气和导致员工留用率大幅下降的绝佳方式。Scrum明确地将权力还给了开发人员。尊重这一点！ 变成 Scrum Secretary (秘书): 感觉 Scrum Master 要变成秘书的迹象有:  Scrum 活动中的会议速记员 早上给团队带咖啡, 晚上给团队定晚餐SM 的职责不是养活团队或者成为秘书 把这些工作交还给团队, 他们能管理好自己 担任Scrum 清洁工/管理员 (Janitor): 当 SM 帮助Scrum 团队 执行 Sprint 的时候, 可能一不小心就变成了清洁工/管理员  亲自设置看板工具 讲解 backlog User Story, 或者创建一个新的 亲自画燃尽图, 给看板贴贴纸, 并移动贴纸这些活动都不属于Scrum大师。开发团队管理他们的工作，无论是在数字工具中还是在物理板上。 Scrum Master 对这些内容应该是 Read only 底线，不要碰物理板或工具！ 教练角: 成为一名专业的Scrum大师是一生的旅程。 打印 Scrum Guide , 每个月阅读一次, 并做一些心得笔记 Management 管理: Scrum Master 面对的三群人, PO , 开发团队 和组织 组织包括了很多人  管理者/经理 业务团队 C level …. 管理者/经理可能会因为 Scrum 让开发团队自我运转而感到害怕, 害怕自己原有的领导权被分散掉 这导致有些管理者可能并不希望 Scrum 成功, 甚至悄悄运作, 让 Scrum 失败 管理者控制预算，拥有招聘和解雇权，并可以利用组织内的政治关系来挖掘和抵制与Scrum相关的变化 SM 的一个重要工作就是管理组织, 包括  You help management plan the company’s Scrum adoption.  You lead and coach people (including managers) during the adoption process.  You help everyone (yep, including managers) understand how empiricism and Scrum work.  You serve as a change agent, provoking changes in the organization (with management support) that improve the way your Scrum team works. 不知道经理想要什么/ 没有得到经理的信任: 经理们有很多顾虑，包括交付、可预测性、利润和预算。他们实际上不在乎冲刺、回顾、故事要点或Scrum的任何日常细节。 这可能会导致经理和Scrum团队之间的严重脱节, 经理正在试图弄清楚他们如何融入这套新规则。 SM 需要弄清楚管理层需要您做什么, 这将帮助SM 与经理建立起信任的关系 如果我们还不知道经理想要什么, 就去和他聊聊, 问问他们需要你做什么.  在这次谈话中, 经理们想从我们这里获得什么？   我可以做什么来帮助经理获得他们想要的东西？   我可以利用哪些Scrum实践来实现这一结果，同时仍然满足管理层的需求？ 我需要哪些数据才能发挥作用？ 我怎么知道我成功了？通过成为管理层的合作伙伴，您可以增加Scrum团队和更广泛组织之间的信任。 信任让我们的生活更轻松。 ### 想一次对话就解决问题: 如果您正在就您提议的更改与经理进行首次互动，请慢慢来。  问问经理他们需要你做什么。 就对他们来说很重要的问题进行另一次对话 然后，你的工作是出现在下次会议上，满足他们的需求。随着时间的推移，您将与该经理建立关系并建立信任。一旦您赢得了他们的信任，您可能会有机会建议对当前流程或实践进行小幅更改。 请记住，从介绍到建议改变公司流程——即使是小的改变——是许多对话过程中需要发生的事情，而不仅仅是一次。 不对经理的需求好奇: 有些经理试图阻挠 Scrum , 他们为 Scrum 设置各种障碍, 并试图对Scrum团队应该自己处理的领域（如架构、设计和测试）保持控制和监督 看起来经理在故意和我们作对, 其实并不是. 如果遇到抵制变革的经理, 请保持好奇心, 找个机会了解他的顾虑是什么, 以及他的需求是什么. 先不要急着给出你的回应, 做出回应之前花点时间, 不要试图立即反驳他们刚才说的话。 再多问一些问题，以了解他们试图实现的目标的核心。 比如 “听起来很有意思, 你为什么会有这样的想法/问题” 保持好奇心（通过提问而不是做出判断）是你如何履行仆人领袖的角色。 更好、更快、更便宜——谁在乎呢？: 老板：  听着，你可以随心所欲地向我兜售Scrum理论，但如果Scrum没有比我们目前做得更好、更快、更便宜，我们就不会投资了。我们应该保持好奇, 为什么老板会这么问, 到底 Scrum 让工作变得更好,更快,更便宜的一些原因：  Scrum创造了根据频繁反馈做出新决定的机会。这比过去重视完成一套要求而不是与客户合作的做法要好。 Scrum比其他方法或框架更快。它更快地为客户提供价值，因为频繁的反馈有助于您知道问题何时发生，这让利益相关者更快地获得投资回报。对于Scrum团队来说，了解产品构建时的挑战和风险也更快。 Scrum更便宜。Scrum做得好，可以防止您交付客户不想要和不付费的东西——换句话说，浪费。经理应该在 Scrum 中做什么?: 正常运转的 Scrum 团队  产品所有者完全有权就产品做出战略和战术决策。 Scrum主控负责维护Scrum框架；为开发团队、产品所有者和组织提供服务；并消除阻碍Scrum团队的任何障碍。 开发团队决定如何最好地完成工作，并构建潜在的可交付产品的高质量增量。问题在于，这种分散的决策方式可能很难让经理们适应。许多经理喜欢就产品和工作方式做出决定。经理们往往不愿意失去这种决策权，他们可能会感到紧张，事实上，《Scrum Guide》甚至没有提到经理。 那么经理为Scrum团队做什么呢？事实上，很多。经理：  为Scrum团队设定高水平的组织目标。这些目标澄清了团队工作的目的。明确的目的激励团队，从而实现高效工作。 帮助确保组织中的每个人都朝着共同的目标而努力。如果各个团体不都朝着共同的目标和结果保持一致，一个组织将无法获得Scrum的全部好处。 定义Scrum团队在里面工作的边界。管理层制定组织标准、惯例和政策，可以帮助人们集中注意力并清楚地了解他们应该如何开展工作。 保持自我组织。经理应该捍卫团队的自我组织能力，并决定如何最好地完成工作。例如，托德咨询的一个组织的一位主任试图要求每个开发团队以同样的方式创建估算。开发经理为每个开发团队评估他们认为最好的方式的权利而奋斗。经理最终获胜，保留了开发团队做出能够自我组织的决策的能力。 消除组织障碍。经理应与Scrum大师合作，努力尽快消除组织障碍。例如，如果您的团队只有一个工具许可证，这对他们正确测试应用程序至关重要，该怎么办？您可以联系您的经理，寻求加快采购流程以获得额外许可证的帮助。 处理人力资源职责。这些包括政策合规性、福利、职业发展、培训以及与满足团队成员需求相关的所有其他事情。通过一些实验性的尝试来解决经理的担忧 在与对Scrum持怀疑态度的经理交谈时可以尝试的问题示例：  我们如何将您的担忧转化为实验？ 我们尝试一些实验会有什么风险？ 过去我们可以借鉴哪些有效的经验？教练角: 需要定期和经理们进行一些沟通互动活动 你多久没和公司管理团队合作了？如果已经有一段时间了，也许是时候举办一个活动了，不仅会引发一些启发性的讨论，还可以帮助你和管理层团队创建一个障碍清单。你可以这样做：  将管理团队聚集到一个墙上有足够的空间的大房间里。给墙壁的一个区域贴上“为什么是Scrum？” 让每个参与者用便签写下他们对以下问题的答案：“我们为什么要采用Scrum？” 在每个人都写下答案后，让每个人都大声朗读答案（一次一个），然后在“为什么是Scrum？”中发布答案。空间。 一旦每个答案都发布，请大家共同努力，将答案分组到共同的主题中。虽然每个组织都不同，但可能会出现一些常见的模式。例如，您可能会看到一组与可预测性或交付相关的便签。 查看分组，并选择与交付最相关的便签。例如，您可能会看到一个关于代码质量、提高开发技能、了解客户的需求、更快地向客户提供软件或省钱的便条。 把这张便签放在墙上标有“交付”的新区域，然后问：“为什么交付很困难？” 给参与者一分钟时间在便笺上写下尽可能多的想法，每个便笺一个答案。 请参与者找到一个合作伙伴，并在两分钟内比较他们的列表，并继续扩展他们。你想让他们在单独的贴纸上想出尽可能多的想法。它们还应该消除任何重复。 让每对人找到另一对一起工作，然后让这四人一组花四分钟比较并添加到他们的想法列表中。让他们用新的便签写下他们想出的每个新想法。同样，让他们消除任何重复。 让每个小组在将他们的想法放在您创建的“交付”区域时大声朗读出来。如果另一个小组听到他们也写下了一个想法，请让他们揉碎笔记以消除重复。 祝贺该小组创建了一份交付障碍清单。Thinking in Sprint: 所有必要的Scrum事件（冲刺、冲刺规划、每日Scrum、冲刺审查和冲刺回顾）都是时间框定的。每个活动的持续时间都应该根据您的环境量身定制。以下是《Scrum指南》关于每个活动应该持续多长时间的指南：  Sprint - one month or less Sprint Planning - a maximum of eight hours for a one-month sprint, usually shorter for shorter sprint lengths Daily Scrum - a maximum of 15 minutes regardless of sprint length Sprint Review - a maximum of four hours for a one-month sprint, usually less for shorter sprint lengths Sprint Retrospective - a maximum of three hours for a one-month sprint, usually less for shorter sprint lengthsSprint 是一个经常被误解的工具, Sprint 可以带来如下好处  The sprint time box creates focus on doing only the things that are essential to completing the sprint goal, and getting features released to your customers. Any distractions that are not related to your sprint goal become an easy “no’’—you don’t have time to waste.  Working and thinking in sprints will expose many problems with the way your Scrum team currently works. Every bottleneck, inefficient process, and ineffective practice will come to light as the team tries to get a feature completed, tested, integrated, and deployed by the end of the sprint. This provides excellent opportunities for your team to identify and resolve impediments that could have gone undetected or unresolved if you were using another framework or methodology.  The sprint represents a commitment to Scrum. By working in sprints, the team has decided to provide multiple opportunities to inspect and adapt progress toward the sprint goal. This commitment allows the team to get feedback frequently and maintain alignment with the customers’ needs.  Working in sprints gives your business partners transparency into the team’s progress, and empowers the stakeholders to manage cost and release schedules with real data, clear information, and product increments to inspect at the end of every sprint.  Sprints give your team members opportunities to let go of limiting beliefs about what is possible and to try new approaches. While this may be uncomfortable at times, with the help of the Scrum master (you!), they can move through these situations and grow as developers and team members. The momentary discomfort is worth the long-term benefits. 我们需要一个特殊的冲刺: 很多时候, 团队还是在使用瀑布的开发方式,但是却给这些方式冠以了Scrum 的名字, 如果出现下面这些名字, 就需要提高警惕了  Sprint Zero - Also called architecture or planning sprints, the intent of this type of sprint is to create a robust plan, often in the form of a product backlog, that the team can use to get started.  Design Sprints - The result of these sprints is a design output such as user experience or architecture. There’s no business value delivered during these sprints, only a vision of what future business value might look like, such as user interface mockups or a database design.  Development Sprints - Sprints where the only thing achieved is writing code. All other tasks, such as design, testing, and integration, are put on hold or handled by other teams.  Test Sprints - Sprints where code written by another team is tested. Bugs and issues that are found are reported back to the team that created them.  Integration Sprints - The output is code merges or attempts at completing integrations with other systems that the system is dependent upon.  Hardening Sprints - These are for code clean up, bug fixes, integration, testing and anything left over, to make sure the system is ready for production.  Release Sprints - Often used as the final phase, a release sprint is where software that has been designed, written, and tested meets the infrastructure team for deployment into production.  Bug Sprints - After code is shipped into production and bugs start rolling in, these sprints are designed to patch and fix them, and then deploy those fixes back into production. 弄清楚为什么您的团队要求专门的冲刺是从Scrum练习中删除这些事件的第一步。以下是一些可能导致团队请求特定、专业冲刺的常见潜在问题：  Sprint zero是一个团队希望在开始前进行太多分析的标志，这在第一次冲刺期间对学习不open。团队还使用冲刺零来暂停软件开发过程，以赶上测试、技术债务，或只是让一周更容易充电并为下一次冲刺做准备。 设计冲刺暗示了一个团队在处理不完整信息时感到不舒服。软件开发世界中的一个严酷事实是，您无法完美规划复杂性。您将在整个冲刺过程中进行实验、学习和适应。 测试、集成和硬化冲刺是试图在开发周期结束时将质量塞进产品中。质量必须是Scrum团队的首要考虑因素。开发团队每天在冲刺中测试、集成和改进产品——而不仅仅是在最后。Scrum要求团队每次冲刺都会输出潜在的可释放增量，这需要测试。 发布冲刺让Scrum团队有时间将最新的产品增量转移到生产中，并验证一切是否仍然正常工作。由于手动部署和回归测试实践，团队通常需要这种分阶段的方法。每当团队手动部署产品时，他们都不是花在构建企业想要的有价值的东西上。为了帮助您的团队克服这些过时的做法并提高效率，请作为一个团队规划您的发布流程，并寻找自动化重复和复杂任务的方法。 Bug冲刺是质量低下的标志。放慢速度，减少冲刺中积压的产品，加强完成的定义，并停止创建错误。有时你必须放慢速度才能更快地交付。让我们更改冲刺长度: 冲刺的长度应该在 1 个月或者之内  在开发过程中保持冲刺持续时间的一致性可以建立节奏。 一致的时间框还有助于团队向产品所有者和利益相关者提供一致的预测。 一旦冲刺开始，它的长度就无法改变。 我们不会因为更快地完成所有工作而缩短它们，也不会因为我们没有完全完成而延长它们。但我们离得太近了！我们不能在冲刺中增加几天来完成最后一个积压的产品吗？  不。即使您已经完成了两周的冲刺，并且没有完全完成产品增量。这是一个困难的地方，但你必须去冲刺retrospective ，讨论冲刺期间发生了什么，以及为什么你没有完成的产品增量。无论发生什么，我们都不会超过时间限制。 没有失败的冲刺，只有我们可以与利益相关者一起学习的不良结果。延长冲刺的时间的问题:  连续延长一天会增加朝着错误方向前进的风险 让那些真正影响了交付的障碍没有被暴露出来, 如果不解决，成本、延迟和风险将加剧。 如果你要求的额外一天还不够呢？你见过多少次“再多一天”变成“再多一个月”？ 如果您延长冲刺时间，您将剥夺团队改进流程、实践和协作的机会。“正确的”冲刺长度是多少    当团队决定冲刺长度时，您首先应该考虑PO 多久会被利益相关者和客户要求进行一次反馈。在高度复杂的环境中，过长的反馈循环可能会有风险。     您的团队需要讨论各种冲刺长度选项的产品和技术影响，并确定他们在没有反馈的情况下可以舒适地工作多久，以及利益相关者在不检查最新产品增量的情况下可以舒适地工作多久。当然，冲刺时间还必须在技术上可行，这样才能完整完成“完成”的工作。     您的团队决定的周数远不如选择背后的推理和导致该决定的协作讨论重要。在您的团队决定“恰到好处”之前，请确保您平衡了风险，经常为客户提供价值，以及技术可行性。  Scrum的会议太多了: 有的时候 Scrum 团队会被会议所淹没, 可能的原因 和解决办法  违反时间框：如果您的Scrum活动超过分配的时间，您需要认真研究为什么团队无法在预定时间内实现活动目标。 多个团队中的人：被分配到多个团队的开发人员必须参加每个团队的Scrum活动。考虑单团队任务。 Bad facilitating: The events are within the time box but the team isn’t making adaptations. For instance, if people aren’t engaged in a sprint review or sprint retrospective, perhaps it’s time to change the format.  日历上仍然保留着旧会议：这是最常见的问题。每个Scrum活动都旨在取代传统会议。如果您的开发团队仍然被他们的日历困住了，是时候减少一些会议了。如何面对额外会议的经验:  邀请利益相关者参加 Sprint Review meeting, 向他们介绍我们的迭代成果和规划, 回答他们的问题和困惑 随时欢迎领导来了解 Scrum 的进展情况, 为他们展示我们的 report, 但是这部分时间要由 PO 来完成比较合适, 不要影响开发团队的节奏.  大家都可以访问的工具和高度透明的报表底线：帮助您的团队成员清理日历并开始工作。 取消一个 Sprint: 产品Owner 有权利取消一个没完成的 Sprint, 但是这是在迫不得已的情况下才能做的. 只要 Sprint 足够短, 应该完成它. 产品所有者确实有权取消冲刺；然而，该决定通常是与Scrum大师和开发团队协商做出的，因为它发生了如此戏剧性和破坏性的事件。 就像不必要地取消冲刺一样，继续永远无法带来价值的冲刺也会浪费时间。因此，当情况发生变化时，即使你以前从未取消过冲刺，也许也值得讨论。 严格按照需求开发: PO 在 Sprint 之前应该尽量细化需求, 并和开发团队进行充分交流, 让开发团队清晰的了解要被开发的内容 但是如果在迭代过程中, 开发团队不再和 PO 针对需求进行互动, 那么也是不正常的事情. 无论开发团队在任何时候对Sprint Backlog 中的内容有疑问, 都应该第一时间找到 PO 进行交流. 教练角: 处理那些Scrum 之外的事件的方法  在Sprint 期间, 让团队成员留意非 Scrum事件的活动，如额外会议、延迟、中断、拦截和障碍。随着这些活动的发生，让团队把它们写在便笺上，并把它们放在一个公共区域。目标是尽可能多地捕捉这些活动。 在 Spring 结束后的 Retrospective 会议上吧这些便签拿出来, 让他们进行分类, have the team sort the resulting sticky notes using the MoSCoW (Must, Should, Could, Won’t) method:     Must continue - things that absolutely need to continue, or the success of the product will be threatened.    Should continue - things that should continue and which might affect the product (negatively or positively).    Could continue - things that we could keep doing, but that don’t affect the product.    Won’t continue - things that are a waste and that we shouldn’t continue.     一旦便签分类完毕，让团队后退一步，考虑活动中突出的内容。问他们以下问题：     有什么应该移动的吗？如果是这样，为什么要移动它？   有什么重大分歧吗？如果是这样，我们该怎么做？   我们可以优化“必须继续”的活动吗？   对于“应该”和“可以”的活动，它们中的任何一个应该移动到“必须”还是“不会”？   我们的大多数活动都在“必须”列中吗？它们真的是我们必须继续做的事情，还是我们只是觉得它们是？   我们如何停止“不会继续”的活动？   有什么艰难的决定吗？   根据这个练习，我们将在下一次冲刺中实施的一两个项目是什么？   我们什么时候应该重温这个列表？   Sprint Planning - 冲刺计划: 冲刺计划活动是冲刺中的第一个活动，整个Scrum团队都参加。输出是Sprint Backlog（包含预测和计划）和冲刺目标，团队可以自行组织。 经常遇到的情况是  Sprint Plan 会议上大家陷入了讨论需求的情况, 往往几个小时还在讨论某个需求到底是怎样的.  PO 没有很多时间提前准备, 导致花费大量时间在会议上讨论需求或者画线框图冲刺规划中的讨论有两个方面：  我们在建造什么——产品所有者准备了排序的产品backlog。PBI应该有足够的细节，开发团队可以将其视为candidate，可以被选入冲刺backlog。细节水平取决于你情况的背景。在某些情况下，一个模糊的想法可能已经足够好了。其他人将要求在冲刺前大幅完善PBI。 我们将如何构建它——随着开发形成冲刺积压，他们合作讨论了手头问题的方法。冲刺期间将出现更多的工作——意识到这一点很重要。我们希望开发团队对他们即将实现的目标充满信心，相信这是可能的，并接管它。他们制定的计划应该足以在冲刺开始时将他们从地面上抬起来。意味着确定性的充分详细实施不是目标。冲刺积压由计划和预测组成，完全由开发团队拥有。马拉松计划活动: 团队在 Sprint Plan 中花费了大量时间搞清楚一些 Backlog 的需求细节以及如何实现 常见问题  PO 没有提前细化 PBI, 导致大量的讨论拥有健康的产品backlog需要两个组成部分：  产品所有者根据情况适当管理产品backlog。这可能意味着他们积极参与添加、更新、删除和订购积压的产品项目。或者这可能意味着他们已经下放了其中一些责任，同时仍然对积压工作保持全面问责。 开发团队通常花在冲刺精炼积压产品的时间不超过百分之十。这些活动包括添加细节、估计、分解、讨论实施计划，以及执行为即将到来的冲刺做准备所需的任何其他产品积压活动。我们称从开发团队收到此类TLC的产品积压项目为“就绪”PBI。产品积压项目应该有多少细节？  在产品积压中，底部和下部的项目通常是模糊的、注定要在未来发生的大变化。当您朝着顶端前进时，项目变得更加清晰，更适合实现。但一个积压的产品项目应该被视为“就绪”多少细节？ PBI应该有足够的细节，以便开发团队可以以高度的信心开始工作，相信他们可以在冲刺结束时交付它。提前太多的细节可能会让你做错事。细节太少，团队可能无法在冲刺期间有效地处理该项目。如果开发团队觉得他们有足够的信息来实现PBI，并且可以在冲刺期间完成，那么它就是“就绪”。没有 Sprint goal 的 Sprint Plan: 目前的冲刺没有总体目标。当冲刺产生不令人满意的结果时，产品所有者和开发团队经常争论不满意。 冲刺目标是什么？  冲刺目标是一个描述冲刺目的的明确目标。它概述了产品的业务需求或功能。这是Scrum团队在冲刺期间与客户的联系。它创造了透明度，是开发团队和产品所有者之间的范围谈判工具。只要你有一个冲刺目标，那么产品所有者和开发团队对彼此都有明确的期望。考虑一下 Sprint Backlog - 一个在线宠物用品商店的团队 表5。Sprint积压示例       PBI #   类型   头衔         118   Feature   添加数字支付-Apple Pay       119   Feature   添加数字支付 - Google       120   Feature   添加数字支付 - Facebook       121   Feature   添加数字支付-亚马逊       124   Bug   狗主人建议展示猫用品。       126   Feature   添加发货集成-联邦快递   我们可以为这次 Sprint 制定一个冲刺目标 - “添加数字在线支付形式。” 谁创造了冲刺目标？  冲刺目标是冲刺规划的输出。这是产品所有者和开发团队之间的协作努力。作为Scrum大师，您应该帮助您的团队创建一个冲刺目标。让团队透支: 很多 企业都希望团队能在一个 Sprint 中做尽可能多地工作, 但这会造成    产品质量低，因为完成项目的竞争成为最重要的成功因素。     由于冲刺成为小型“死亡游行”，员工被视为“资源”，而不是人，士气低落。     由于“清除盘子”成为需要解决的问题，而不是取悦客户，因此对产品的预期价值缺乏所有权。     没有创新的空间，因为有很多事情要做，而这个列表填补了——或超过了开发团队完成它的时间。     团队的对话不是关于有效和交付优秀产品的对话，而是关于最大限度地提高个人效率。     团队过于疲劳  冲刺完成率是一个很好的指标吗？  冲刺完成率是一些组织用来查看团队是否在冲刺期间完成所有冲刺积压项目的指标。它在确定Scrum团队的有效性方面没有价值。一个团队可以完成所有冲刺积压项目，但提供低质量的工作，不会给产品增加任何价值。相反，一个团队无法定期完成整个冲刺积压，而是提供高质量、有价值的增量。底线：冲刺完成率是无用的，您不应该使用它们。技术债务越来越大: 如果因为要做一个需求而放弃重构已有的不良代码并不是好的建议 建议把重构代码并做更完善的自动化测试作为 backlog 中的工作, 而不是不断的在不良的代码上累加功能 请在冲刺规划期间提出以下问题：  我们有没有办法让那个地区更友好地编码？ 是否有涵盖现有功能的自动化测试？ 是否值得成对地合作，以找到一种方法使代码更健壮？ 我们可以做些什么来降低此代码的脆弱性？与其最终导致积压一堆“重构”项目的产品，不如在技术债务进入对话时立即承担技术债务。如果有技术债务，请将其解决纳入提供某种业务价值的产品积压项目中。不要忽视它，否则你最终可能会陷入债务中。 教练角: Sprint Backlog: Scrum 团队要拥有 Sprint Backlog 的控制权, 而不是管理层, 否则可能会产生很多问题.  管理层会不断的追加需求到 Sprint Backlog 中, 导致Sprtint不可控 管理层会进行微观管理, 让 Scrum 开发团队没有 ownership, 开发工作马虎, 士气低落Sprint Backlog 的所有者是开发图案丢 Product Backlog 的所有者是 PO, 管理层可以和 PO 讨论增加 Product Backlog, 但不是 Sprint Backlog 开发人员被 Burn down:  Sprint烧毁图表是一种互补的Scrum实践，开发团队可以使用它来检查他们的冲刺积压。我们认可不太漂亮的燃尽图, 因为我们可以从中找到问题, 并加以调整/纠正 但我们如果只看到这些图表相关的不良行为远远大于好处, 把它作为 KPI 来衡量团队的绩效, 那么这已经偏离了燃尽图的初衷 为了让燃尽图好看而可能会迫使开发团队弄虚作假. 冲刺以大量未完成的工作结束，这些工作被标记并在冲刺评论中显示为完成。Sprint Backlog 和相关的燃尽图等都属于 Scrum 团队. 管理层应该理解和尊重 我们对冲刺烧毁图表的痴迷导致了一些负面行为, 如果必要, 不用燃尽图也没关系, 保持物理看板的重要性, 它更能让团队时刻了解 Sprint Backlog 的完成情况, 尽管有的时候团队会认为物理看板和电子看板是重复劳动.  物理看板的重要性:  想象一下，如果你们都站在看板前，同时积极改变和管理冲刺积压，那么每天的争球会好得多。 物理Scrum板确实有助于提高透明度。如果经理们想知道发生了什么，他们可以来看看看板。（这也适用于数字Scrum板——它们也提高了透明度。） 当您一直在处理艰巨的任务或产品积压项目时，将便签从“执行”列实际移动到“完成”列真的很令人满意。 与浏览数字工具不同，浏览物理看板是再次检查开发团队成员在做什么的更简单、更快的方式。鼓励开发团队使用物理Scrum板是放松管理层对冲刺烧毁图表的痴迷的第一步 过度承诺 Sprint Backlog:  我们经常听到Scrum团队和组织说，Sprint Backlog 是一项承诺——冲刺积压中的所有内容都必须在冲刺结束前完成， 这种对冲刺积压的思考方式表明，一个组织渴望确定性，但在复杂的项目中并不存在确定性。 例如, 表面上似乎只需要几个小时就能修复的错误。但当你深入研究代码时，你会发现它比你想象的要复杂的多, 修复需要几天甚至几周 这就是复杂工作的本质. Sprint 开始后, 我们可以对 Sprint backlog 进行调整, 但是 Sprint 目标不能改变 如果开发团队发现 Spint 过程中因为调整 backlog 而影响了目标, 这时必须找 PO 进行沟通 谁更新看板(物理或者电子的): 有的时候开发团队并不会主动去更新看板上的贴纸的位置来反应最新的状态. 只能是 SM 催促他们去更新, 或者 SM 自己来更新 Scrum Master 更新看板是错误的, 因为  这样做实际上降低了冲刺积压的透明度 它变成了一个由Scrum Masnter而不是开发团队拥有的状态板 这对开发团队的自我组织能力产生了负面影响，因为他们对自己工作的自主权变小了谁更新Sprint Backlog/看板 - 不要成为 Scrum 警察  开发团队中的任何人都可以更新冲刺积压。开发团队以外的任何人都不应该更新冲刺积压——即使是你，Scrum大师也不应该更新。Sprtint Backlog 的 owner 是开发团队, 开发团队有义务让 Sprint 足够透明, 那么就应该及时的更新 borad 开发团队使冲刺积压对整个Scrum团队透明的一些原因：  在daily scrum期间，透明的冲刺积压有助于开发团队了解他们在实现冲刺目标方面取得了多大进展。 它让Scrum团队的每个人都知道事情的发展。 它提供了一种识别完成冲刺目标的潜在障碍的方法。 对于拥有不同技能组合的开发团队成员来说，这是一种有用的方法来协调工作。 它有助于防止开发团队成员被其他开发人员和Scrum团队以外的人员的问题打断。 它在Scrum团队和组织其他成员之间建立信任。当您遇到一个没有更新冲刺积压的开发团队时，请扪心自问以下问题：  您是否要求开发团队更新冲刺积压只是为了更新它？ 开发团队作为一个团队是否具有凝聚力和良好的工作氛围？ 您是否要求他们更新冲刺积压，因为利益相关者要求您提供有关开发团队工作的信息？ 开发团队是否理解在冲刺积压中创建透明度的重要性？ 开发团队是否害怕如果他们的工作太透明，每个人都能看到会发生什么？每日投影仪更新: 常常有这种情况, SM 或者开发 leader 每天早上打开投影仪, 调出电子看板, 询问每个开发人员的 backlog 进展情况, 并进行现场更新. 开发团队成员死气沉沉地盯着投影仪，等待轮到他们说话。很少有人主动更新冲刺积压和看板。 这种情况的明显问题:  SM 不应该是更新 Sprint backlog 的人 开发人员依赖 SM ,而丧失了责任感如果投影仪更新已经发生了, 尝试  不用投影仪, 而改用物理看板, 让大家在更新完自己的 backlog 之后再开晨会, 如果不更新, 那就不开晨会等待外部依赖: 一个案例: 一个 Scrum 团队的代码review 需要外部的架构团队帮助完成, 但这成为了一个瓶颈和不可控因素  无法准确预测工作的完成时间 降低 透明度要解决这个问题的方法是:  把Sprint 过程中的依赖步骤都画在看板上, 增加依赖步骤的透明度  和组织讨论, 如何将外部依赖的能力变成团队内部的能力. 因为我们 Scrum 团队应该具备所有完成 Sprtint Backlog 和实现 Increment 的能力教练角: 一些重点概念的回顾  开发团队拥有冲刺积压。 冲刺积压需要透明，以便开发团队可以正确检查，并在必要时调整计划以实现冲刺目标。 这种透明度可能会让一些开发团队感到害怕，因为它们可能仍然习惯于Scrum前的做事方式——这可能没有涉及Scrum要求的那么多透明度。 作为Scrum的大师，我们可以通过利用透明度来促进将改善开发团队工作流程的变革，并承诺我们将与管理层合作，解决透明度提高所发现的任何障碍，从而缓解这些担忧。 让开发团队使用看板来管理他们的冲刺积压。Daily Scrum 每日站会/每日争球: 每天的争球是开发团队计划他们未来24小时内如何共同努力，朝着冲刺目标取得进展的机会。 简而言之，这是一个带有15分钟时间框的协作规划会议 但现实是很多 Scrum 团队都在开会的时候引入了错误的方法, 下面是一些反面的例子  经理带领开会/ SM 带领开会 进行问答式对话 批评式对话 讨论细节的对话 超过了 30 分钟每日Scrum是Scrum团队健康状况最可靠的指标之一。如果你知道该寻找什么，你会发现开发团队在日常争斗中透露了很多关于自己的信息。仔细观看每日争球可以告诉你足够的答案：  这是一支自组织起来的团队吗？ 团队是否在工作中进行协作？ 团队是否对工作有共同的责任感和所有权？ 开发团队是专注于结果（冲刺目标）还是单个任务？ 这是一家拥有冲刺积压的授权团队吗？每日Scrum作为状态会议: Daily Scrum 要说的三件事  我昨天做了什么来帮助开发团队实现冲刺目标？ 我今天将怎么做来帮助开发团队实现冲刺目标？ 我是否看到了阻碍我或开发团队实现冲刺目标的障碍？但这些问题让团队成员专注于自己是个人，而不是团队的一部分。当重点是个人状态更新时，您已经失去了每日争球的大部分价值。 在这种类型的日常争斗中，协作通常很低。问题仍然隐藏，障碍得不到解决，团队成员永远不确定它们的进展如何。您失去了透明度，并将冲刺目标置于风险之中，因为重点是个人状态，而不是开发团队在冲刺积压项目或实现冲刺目标方面的进展。 要注意, 以上三点都是围绕Scrum 目标, 大家就像一个篮球队 , 每个队员在讲的的是如何为争取胜利各自所做的努力, 而不是个人的表现和汇报. 每天的争球让每个人都有机会就需要做些什么来提高成功的可能性达成一致。 上面的是哪个问题并不是必须的, 我们有事可以尝试换一批问题  有什么卡住了吗？ 如果有什么问题卡住了，我们如何共同努力来完成这项工作？ 谁需要帮助？ 我们今天需要完成的最重要的事情是什么？ 我们如何增加最重要的事情完成的几率？另外, 建议大家使用物理看板来开 Daily Scrum, 下面这些点可以帮助我们  专注于工作。使用您的团队板来指导对话。对于每个产品积压项目，讨论障碍、进度和团队今天将处理的任务。向多个团队成员寻求见解。这样做将使每个人都能共同了解正在进行的工作，依赖性在哪里，以及如何减轻任何风险。 让看板保持最新状态。您的组织中有人想要真正的状态报告。通过鼓励开发团队保持看板的最新状态，您帮助创建一个可以向他人提供宝贵信息的工具。管理层可以走过看板，一目了然地看到事情的进展。这种工作的透明度是与利益相关者建立信任的绝佳方式。 让团队自己主持(找个团队的开心果主持, 不用 leader 主持)。如果开发团队以外的人影响了日常的争球，请让他们离开，让开发团队的一名成员为讨论提供主持。你会惊讶于一个新声音可以帮助揭示什么。如果问题在于你，那么要么强迫自己保持沉默，要么干脆不参加。（我们在下一节中会详细讨论这个问题。） 让开发团队关注冲刺目标。冲刺目标的目的是提醒团队他们为什么要构建当前的软件增量。随着日常争吵中出现新的工作、想法或决定，请使用冲刺目标作为指南，以过滤噪音，并让团队专注于高度优先的工作。Scrum大师不是日常Scrum的焦点。事实上，他们甚至不必参加活动。但是，如果你每天的争球变成了状态会议，你可能会发现自己处于活动的中心。如果您注意到开发团队正在与您交谈，而不是相互交谈，是时候做出调整了。 每周两次状态会议?: 一些团队成员可能会抱怨日常变化不大，他们宁愿有15分钟回来。其他人可能会感到尽可能多的功能交付压力太大，以至于他们会声称自己太忙了，无法参加另一次会议。许多Scrum团队希望每隔一天甚至更少频繁地举行一次。  请记住，每天的争球是为开发团队成员准备的。这是他们的活动，他们需要它。在为期两周的冲刺过程中，事情可以而且确实会迅速改变。如果开发团队不频繁停下来检查他们的进度，他们可能会错过及时调整工作以实现冲刺目标的机会。 每日的争球是一个拐点。这是一个动态的重新规划会议，团队会考虑整个团队的进展、变化、障碍和新见解，并决定下一步该怎么做。如果团队不希望开每日 Scrum , 那么  请问他们需要改变什么才能让它有价值。使用这些想法来提出团队可以进行的实验，以尝试改善他们的日常混战促进和实践。 首先重申日常争声的目的。提醒团队成员，他们的部分承诺是遵循Scrum框架。 然后征求团队成员的意见，如果他们不每天见面以协调工作，会发生什么。这是一个探索团队经历的绝佳机会，看看他们能否找到解决根本问题的方法。并非所有的声音都被听到了: 在每日晨会上, 只有部分人参与到交流当中, 其他一些人总是沉默 作为Scrum大师，您应该确保听到每个声音并考虑多种观点的方式促进日常Scrum。特别是在您与新团队合作时，您需要根据各种观点来模拟这种倾听和接受能力。 例如，假设您有一个团队成员主导了日常的争球。如果这种情况经常发生，请向其他团队成员询问他们的观点，并保持空间 使用1-2-4-All 来进行您的日常争球，以便听到所有声音。工作原理如下：  让开发团队成员花一分钟默默地思考以下问题：“您认为有什么事情可以让我们在团队冲刺目标上取得了进展？” 接下来，让他们花两分钟的时间找另一个伙伴一起讨论，并从第1步中的自我反思中汲取见解。 一旦他们和伙伴一起探索了这些想法，让这对伙伴再形成四人行，并花四分钟进一步分享和发展这些想法，特别注意他们在讨论中注意到的相似之处、差异和模式。 最后，问团队“在谈话中，有什么想法真正脱颖而出？”让每个小组与整个Scrum团队分享一个想法。在 1-2-4-all 中, 每个人都有机会分享他们的观点，让别人听到他们的声音，同时感到安全地分享他们的想法和他们在日常争斗中学到的东西。 手指(5 指)投票 - 是对某一个想法有多赞成的方法.  你可能会问团队：“你对我们有望实现冲刺目标有多大信心？”告诉他们思考这个问题，并提出1到5级的答案。然后让每个人都显示代表他们反应的手指数量。 五根手指：是的！我100%与价值陈述保持一致。 四指：我强烈支持价值陈述。 三根手指：在提供支持之前，我想讨论一下价值陈述。 两根手指：我反对价值陈述。   一根手指：不！绝对不是！值语句是错误的。   接下来，让1和2s谈谈为什么他们的信心很低。问他们：“是什么阻止了你投票支持？”经过初步讨论，Scrum团队成员可以再次投票，看看他们是否比以前更一致。这项技术有几个好处：  低级仪式：做起来很简单——你只需要一个问题和一个拳头。之后，由团队成员来讨论他们的分数，并努力实现对齐。 鼓励参与：没有参与的人没有承诺加入Scrum团队。五拳让所有团队成员都参与对话。1和2s有一个非对抗性的机会来表达他们的担忧，让团队一起学习。 让“不”变得更容易：“不”是一个令人难以置信的难说词，特别是当你不得不对队友说的时候。五拳让Scrum团队成员有机会不同意被问的问题，而不会令人不快。这是创建开放和诚实对话的关键。 创建对齐：对齐的团队是一个强大的团队。投票的目标是让团队合作并收敛商定的数字。花时间解决1和2s让每个人都参与其中。 引导行动：对齐过程往往会找出Scrum团队成员缺乏信心的原因。例如，也许自动化构建不那么自动化，或者架构中可能存在一个重大缺陷，使冲刺目标变得困难。了解这些障碍为团队提供了一份清晰的行动项目清单。五拳是一种技术，你可以在感觉到球队缺乏信心时使用它。当前冲刺的状态、实现冲刺目标的可能性，甚至冲刺积压的状态，都是它的绝佳话题。希望你会发现这些数字真的比你的Scrum团队参与的丰富对话和他们最终采取的行动要。 毫无进展的任务(PBI): 当观看开发团队每天进行争球时，您通常会注意到被阻止的任务——这些任务是团队没有取得任何进展的任务。有时，这些任务似乎粘在团队董事会上：它们只是不会移动。更糟糕的是，日常争球的结果可能无法解决被阻止的任务。  日常争斗的全部意义在于他们讨论任何障碍，并作为一个团队制定克服这些障碍的计划。 当任务和故事似乎卡住了，没有人说话时，是时候担心团队为什么不直言不讳了。 开发团队对其进展负责，其中的一个重要部分是尽快作为一个团队指出和解决障碍和障碍。一个比较好的方法就是跟踪 PBI 的年龄  从 Sprint 开始后的第一个晨会结束后, 给每一个任务点一个点 之后的每天晨会后, 对没有状态变化的 PBI 再点一个点, 有状态变化的不需要点点.  关注点最多的 PBI, 和团队讨论如何能够加速被卡主的 PBI当任务或故事卡住，而做这项工作的人没有寻求帮助时，你可以打赌团队没有践行Scrum价值观。尝试与您的团队讨论Scrum价值观。当团队成员停止提供和寻求帮助时，询问他们缺少哪些价值观。使用此值列表开始对话：  Commitment: The members of the development team are committed to following the rules of Scrum, and to helping one another deliver a high-quality increment of valuable software.  Focus: The sprint goal creates focus for the development team. It’s a guidepost that the team can use to make rapid decisions. For example, the team may have to decide whether to accept additional work during the sprint. If the new work puts the sprint goal in jeopardy, the answer is a clear no.  Courage: It takes courage to raise an impediment to the team, to ask for help with a story that has you stumped, or to raise a concern about the direction the team is going. Development team members must show courage and act transparently for the betterment of the team, especially when they realize that they’re wrong and need to pivot quickly to realign to the sprint goal.  Openness: The team members need to be open with one another to keep their progress toward the sprint goal transparent. Otherwise, the team won’t be able to inspect and adapt. Openness allows team members to both offer and ask for help. Openness also helps us listen to all voices and opinions so that the team has ownership of its decisions.  Respect: The daily scrum is intense. All the work is out in the open so the team can inspect and adapt. Team members may need to ask for help or have differing opinions. Without respect for each other and for everyone’s abilities, empiricism and self-organization just aren’t possible. 惩罚迟到: 一旦日常的Scrum开始，一些Scrum大师就会锁上会议室的门。似乎通过为迟到创造后果，你正在帮助团队成员按时出现。但关门、晚点罐子和让人们迟到时唱歌或跳舞等惩罚都是嘲笑行为。公开羞辱不是Scrum大师的工具之一。 一个愚蠢的舞蹈无法解决问题。您需要更深入地挖掘，并与团队合作，以了解正在发生的事情。 团队是否能够决定何时何地举行每日争球？这是他们的活动——让他们选择如何举办 参加每日 Scrum 对于开发团队的一些成员来说，这可能感觉效率低下，但日常争球产生的结果——对齐、专注、承诺和目的——对高性能团队至关重要。 45分钟的Scrum: 每日争球的时间框是15分钟。如果您的开发团队花费的时间超过此限制, 那么要考虑的第一个问题是：有什么问题？  团队成员太多了吗？ 团队是否在日常争斗中试图解决开发问题？ 产品所有者是否利用这段时间提供有关产品积压项目的详细信息？等等……为什么产品所有者会参与日常争球？一旦你掌握了时间的流向，你就可以开始做一些更正了。我们已经讨论了使用物理团队板的问题。仅凭这项技术就可以让你的日常争光重新集中注意力。 保持话题的方法:    如果团队偏离了话题或试图解决问题，请尝试使用停车场(Parking lot)：每日争球后可能讨论的事项列表, 然后，开发团队的成员可以决定如何清除停车场的物品（自我组织），并继续他们的一天。   如果您还没有团队看板顶部的冲刺目标，现在是发布它的绝佳时机。 每当球队偏离话题时，团队成员都可以指出冲刺目标，以提醒他们保持正轨。 当团队在日常争球中偏离话题时，一个很好的问题是：“这次对话如何有助于我们朝着冲刺目标取得进展的计划？”教练角: 您可以在下一次Daily Scrum中尝试以下几点：  在你下一次每日争球开始时，花几分钟提醒团队它的目的。 将自己从开发团队成员圈子中移除，专注于冲刺目标，并帮助避免Daily Scrum 转移到状态会议。 如果您需要在日常争球中参与，特别是与新团队一起参与，请尝试将自己限制在开放式问题上，以促进透明度，鼓励检查冲刺目标，并通过为当天制定新的计划来促进适应。 偶尔跳过参加每日争球，看看会发生什么。希望开发团队仍将举办活动，协调他们的工作，并制定当天的计划，帮助他们实现冲刺目标。 如果开发团队成员以外的人参加日常争球，请让他们停下来练习积极的倾听技能。日常争斗是为了开发团队。 通过设定时间、确定地点和决定如何最好地衡量他们在冲刺目标方面取得的进展，指导开发团队拥有日常的争球。提醒开发团队，他们拥有团队看板、冲刺积压，以及他们如何监控进度。鼓励他们保持进展透明。 提醒您的团队接受帮助。我们都陷入了困境。不要呆滞不上。勇敢点。他们的队友在那里互相帮助。Product Increment 产品增量(Scrum 完成了的交付物): Product increment  Scrum 结束后, 完成了的交付物, 是PO 和开发团队在 Scrum 启动之前就一起确定了的产品增量 增量是在冲刺中完成的所有PBI的总和 是经过测试的, 有足够的健壮性的 每次冲刺都有完成产品增量是开发团队所需的核心任务之一, 开发团队致力于创建优质产品并交付已完成的增量 开发团队还专注于向产品所有者交付和服务。造成没有办法完成增量的原因很多  新团队成员加入 其他人离开公司 开发人员被分配到多个项目 没有冲刺目标 冲刺计划很差 冲刺期间没有改进会议 产品积压项目要么太大，要么不清楚 产品所有者缺席 障碍堆积如山 技术债务正在减缓团队的速度 开发团队并不具备交付已完成增量所需的所有技能 团队成员不一起工作您的Scrum团队需要随时提供经过测试、集成、可部署和可用的产品版本。目标是每隔几天提高一次增量，同时保持其随时可用且可能可release的状态。如果产品所有者想在接到通知后立即发布最新增量，那应该是一项低风险和简单的任务。 我们尚未定义“完成”: Scrum要求每个增量都是可Release (发布/释放)的 每个人对完成的定义可能都不一样 PO 定义的完成可能是上线, 并可以让客户正常使用, 开发者的完成是开发完成, 测试的完成时测试完成…. 下面是一个定义”完成”的例子: (使用它。修改它。更好的是，与您的团队集思广益，创建您自己的完成定义！)  所有代码都已开发完毕。 它符合团队的编码标准。 单元测试是书面和通过的。 代码已签入源代码控制存储库。 没有编写代码的开发团队成员已经满足并确认了验收标准。 持续集成构建正在通过。 产品所有者看到了产品（增量）。 所有代码都已部署到Stage 环境。只要 PO 希望, 随时都可以向生产发布新功能 当开发团队说他们“完成了”时，产品所有者清楚地知道已完成的工作处于什么状态。 将代码始终处于可释放状态，使产品所有者在选择何时发布时具有很大的灵活性 在许多组织中，在团队按需发布产品之前，必须消除许多障碍。作为Scrum大师，消除这些障碍是你的工作。 舍弃质量来满足上线日期: 有的时候在组织的压力下, 团队被要求提交尚未”完成”的增量到生产, 这是有巨大风险的 因为未完成的工作——开发团队声称已经完成，但还剩下更多工作——从来都不是增量的一部分。 未完成的工作永远不应该投入生产。在我们的示例场景中，如果团队将未完成的工作投入生产，可能会对组织的运输和接收系统产生可怕的后果。 为什么我们不能发布未完成的工作？  每个增量都会添加到所有之前的增量中，并经过彻底测试，确保所有增量一起工作。这条规则直接来自《Scrum Guide》。未完成的工作不符合该标准，因此它永远不会发布。 考虑一下，如果你破例了这条规则会发生什么。如果您忽视质量并允许将工作发布到生产中，您如何知道您的软件对客户的影响？您不会——直到为时已晚，客户报告您的产品质量下降。发布未完成工作的短视决定增加了团队失败的风险。这将成为一个习惯，质量会下降，您的客户会注意到。这根本不值得冒险。如果非要迎合上线日期  在截止日期前，请与产品所有者一起重新查看您的产品积压项目。努力与他们合作，试图了解什么是真正必要和可能的。与团队合作，您可以尝试找到将PBI切成更小的部分的方法，这些碎片仍然有价值，可以在冲刺中发货。 可以谈判的是在范围上的变化, 砍掉一些不是真正对上线日期有价值的 PBI, 或者将PBI 分解后上线部分. 不要尝试讨论降低质量. 教练角:  如果还没有”完成”的定义, 请使用我们在本章中讨论的练习来捕获并发布您的第一个定义，供每个人查看。 无论您是自检查和调整了完成的定义以来的一次冲刺还是十次冲刺，请使用您的下一次回顾围绕开发团队对质量的承诺开始对话，并利用这段时间确保完成的当前定义支持该承诺。 如果在交付团队无法单独解决的高质量产品增量方面存在障碍，请立即将这些问题提交管理层。您需要列出障碍清单，并计划如何尽快解决这些障碍。 开发团队成员拥有他们的工作方式。已完成的定义反映了这种所有权。在下一次冲刺审查会议上，花点时间向您的利益相关者介绍已完成的定义。向他们展示质量很重要。这可以使未来的权衡讨论更顺利。Sprint Review:  Sprint Review 是开发团队向利益相关者展示 increment 的会议. 被邀请的人员不仅仅是 PO, 还包括其他和 increment 有利益关系的人员.  Sprint Review 增加了产品的透明度, 受众狭窄的 Sprint Review 可能导致该公司浪费了大量资金，无法解决该项目旨在解决的问题。 Scrum团队Review 增量并调整产品backlog。Scrum团队与利益相关者合作，review发生了什么，并决定下一步该怎么做。 如果只是邀请了 PO, 可能在产品战略和目标等某些方面没有办法得到及时和直接的反馈利益相关者不参与: 将利益相关者排除在冲刺审查之外是该项目失败的关键。您的产品所有者可能希望推迟有关项目范围以及利益相关者愿望和需求的对话，但这不是正确的做法。这种缺乏透明度可能会杀死产品。 我的利益相关者是谁？  利益相关者是指对产品开发工作的结果有既得利益的任何人。从因项目而改变工作的员工到批准为项目提供资金的人，利益相关者可以来自组织的各个角落。他们是谁取决于产品的性质。 产品所有者应该了解他们的利益相关者是谁，并搜索了组织的每一个角落，以确保他们适当定义并包括了每个人。此外，请记住，利益相关者可能会在发展努力的整个生命周期内发生变化。产品所有者偶尔应该重新查看他们的利益相关者名单，并检查是否有人应该添加到该列表中或从中删除。 产品所有者在任何特定时刻都必须知道谁是利益相关者。作为Scrum大师，请与您的产品所有者合作，确保他们已经从组织的每个角落识别出利益相关者  产品所有者应该对冲刺审查期间没有足够的利益相关者代表在房间里保持警惕，就像他们拥有太多一样谨慎。 Scrum的一个重要组成部分是在组织与Scrum团队之间建立信任。让利益相关者参加冲刺审查有助于建立信任, 特别是在产品开发工作的早期如果您的产品所有者在识别利益相关者时遇到问题，请尝试以下练习：  与您的产品所有者——也许还有他们已经确定的一些利益相关者——坐在一起，并尝试想想组织中对您正在构建的结果有既得利益的每个人。在单独的便签上写下每个人的名字。如果您陷入困境，请考虑以下问题：     钱来自哪里？   谁的工作会因为这个产品而改变？   由于我们正在构建的东西，谁可能会与客户进行不同的互动？   如果他们不知道这个项目发生了什么，谁可能会生气？    一旦你的头脑风暴减慢，休息一下。 重新召集时，在白板或活动挂图上创建以下三个类别，然后将每个便签放在适当的类别中：     Sprint Review需要的人：这些人最需要有关产品的信息。如果这些人不检查增量并提供反馈，Scrum团队将无法对积压的产品进行明智的更改。   随时了解进展的人：这些人不需要每次冲刺都检查增量，但他们确实需要随时了解团队的进展。   监视这：这些人不需要频繁更新，但应该定期收到有关项目的更新。最好与他们核实，看看他们希望多久收到一次这些更新。   产品所有者作为评委:  冲刺审查不是产品所有者应该像拿着小木槌的法官一样行事的事件 这种行为是开发团队和产品所有者之间缺乏沟通的症状，并在开发团队内部滋生蔑视。如果您看到这种反模式的发展，是时候找到改善沟通的方法了。 开发团队和产品所有者应在整个冲刺过程中进行沟通，以便开发团队的工作透明，PO可以在当前冲刺中提供有关PBI的清晰度。没有这种对话，产品所有者最终可能会在冲刺审查中感到惊讶，利益相关者可能会同样甚至更惊讶如何解决:  如果你发现 PO 在Sprint 进行阶段没有出现过, 那么需要让开发团队找 PO 聊聊, 确保PO 意识到这点 如果你发现开PO 在在 Product Review 的时候对开发团队说的一些事情感到意外, 那么 SM 要和开发团队聊聊, 确保他们在整个冲刺过程中随时向产品所有者通报团队的进度。你的工作是在开发团队和产品所有者之间建立积极的关系。如果这两个角色不能有效协作，Scrum就无法很好地工作。频繁的沟通让每个人都很开心，意见一致。 展示未完成的工作: PO 和开发团队不要为了取悦他人而试图夸大他们的进步, 没有完成的工作, 就算可以展示也尽量不要展示, 留到下次完成了所有的测试和部署再进行展示, 否则只能给利益相关者带来过高的期望和失望 避免在冲刺审查中展示未完成的工作的诱惑，否则您可能会设定错误的预期。 展示未完成的工作，使其听起来团队取得了比实际成就更多的成就，这不值得潜在地削弱利益相关者对团队的信任。 诚实确实是最好的政策, 如果团队在冲刺期间遇到问题，请直截了当，并与利益相关者合作，试图消除任何障碍。 只演示功能, 不讲背后的原因以及项目的方向: 利益相关者应该知道团队实现的冲刺目标背后的原因以及项目的方向。增量检查不应仅涉及演示团队自上次冲刺审查以来构建的功能。 为了让Scrum团队收到有价值的反馈，帮助他们以知情的方式调整产品积压，并确保每个人（利益相关者和团队成员）都真正了解项目方向，团队需要在冲刺审查中创造更大的透明度。例如，这里有一个agenda 可以帮助冲刺审查成为真正的协作会议：  产品所有者对产品的愿景的解释 审查发布计划和预算 这次冲刺目标的说明 回顾团队构建的工作和产品增量 评估当前的客户分析和数据 讨论团队目前面临的障碍 与利益相关者合作，完善积压的产品 总结我们下一步要去哪里，以及我们在这次活动中发现了什么 一个团队什么时候应该准备冲刺审查？  Scrum指南没有回答这个问题，但团队可以通过多种方式为冲刺审查做准备。理想情况下，冲刺审查议程在整个冲刺过程中都会出现。您的团队可能会决定在冲刺审查之前举行一次简短的会议，以点头和交叉点头。产品所有者可能想调整议程，因为他们非常了解利益相关者。您，Scrum大师，可以建议新的方法来促进活动，使其尽可能有更多的互动。无论您的团队如何决定制定议程，重要的是要做好一些准备，以使活动井然有序和富有成效。团队中有一个”英雄”: 另一个场景：在冲刺审查期间，每个开发人员轮流展示他或她在冲刺期间单独工作的内容。利益相关者被更外向的开发人员所吸引，并将他们视为团队的英雄。更内向的开发人员很难解释他们的工作，也很难快速说话，这样他们就可以“下台”。  在冲刺审查中听到“我”一词表明开发团队的自我组织能力功能失调，团队是作为一个个人集合而不是一个真正的团队运作的。 “我”心态在团队中创造了英雄，并让利益相关者认为一名团队成员的贡献比其他团队成员更大。 利益相关者将把特定任务的责任分配给单个开发团队成员，而不是整个Scrum团队。 创建集体所有权需要“我们”心态。在冲刺审查期间，谁应该谈谈？  Scrum团队中的每个人都应该有机会发言，以确保Scrum的每个角色都有足够的代表性。重要的是，做这项工作的人有机会展示它。作为一名Scrum大师，这可能是一个很好的机会，可以指导和指导那些在观众面前说话不舒服的团队成员。开发团队可以做很多事情来帮助促进团队成员之间的合作和协作。以下是一些你可以尝试的：  在冲刺期间限制冲刺积压项目的WIP（进行中）。 确保团队成员有一个清晰简洁的冲刺目标来自我组织。 尝试配对编程。 尝试暴徒编程，这是一种整个团队同时处理同一事物的技术。 禁止团队使用“我”一词。不景气的 Sprint Review: 这种反模式的一些表象:  利益相关者在 Review 期间做其他的工作, 直到团队讨论他们感兴趣的事情 另一些利益相关者根据他们认为团队何时会讨论与他们相关的信息而进入房间 还有一些人完全停止参加 利益相关者称该活动为“无聊”。 利益相关者不愿表达他们的意见如果利益相关者感到无聊或不感兴趣，他们缺乏参与将导致遗漏意见或想法。然后，该项目可能会面临风险，因为团队尚未收到决策所需的反馈, 客户可能会发现产品在生产中存在问题，他们可能会发现产品缺少重要功能 协作冲刺审查意味着Scrum团队从利益相关者那里获得想法和建议，以便团队可以改变产品方向，利益相关者能够尽早提出任何问题。 你如何避免这种情况呢？  冲刺审查议程应该改变，以反映Scrum团队的当前情况，这种情况经常变化 产品所有者应以吸引所有人参与的协作方式促进冲刺审查。 作为Scrum大师，您必须与产品所有者合作，并教他们如何领导讨论，以充分利用冲刺评论 留意安静的参与者，并以非侵入性、协作的方式让他们参与进来。 尽最大努力征求所有与会者的意见，这样就不会有人的想法被忽视。尝试促进技术和议程，让团队成员和利益相关者参与每次冲刺审查。谁在冲刺审查中记笔记？    我们经常看到Scrum团队在没有记录活动期间发生的事情和大家发表的冲刺评论。     产品所有者负责更新和调整积压的产品，但这并不是说产品所有者是唯一应该做笔记的人。     事实上，每个人都应该在冲刺审查中记笔记。我们合作过的一些最好的Scrum团队在冲刺审查后不久就坐下来比较笔记。这可能会在短跑规划中临时发生，也可能在审查结束后，在回顾之前发生。在收集了每个人的观点后，产品所有者可以相应地调整产品积压。  跳过它: 当Scrum团队没有实现冲刺目标时，一些团队选择取消冲刺审查，以避免尴尬或惩罚。 取消冲刺审查以避免不良情绪是对组织缺乏信任的迹象。推迟坏消息只会在最终报道时使新闻变得更糟。 进行冲刺审查正是Scrum团队在出现问题时需要做的事情，因为这将有助于他们在组织中建立信任 Scrum团队出于多种原因跳过冲刺评论：  他们没有实现冲刺目标。 他们只需要多一点时间就能实现冲刺目标，并将在下一次冲刺后举办活动。 只有一两个利益相关者可以参加。 一些团队成员不在办公室。 该组织有“清洁板块”的心态（有关此反模式的更多信息，请参阅第10章，冲刺规划），团队没有完成整个冲刺积压。 有一个障碍减缓了团队的速度，他们没有什么可展示的。无论取消的原因是什么，重要的是要记住，没有失败的冲刺。 每次冲刺都是一个了解产品、您的组织和您所做工作的复杂性的机会。 正如我们在本章前面所讨论的，冲刺回顾不仅仅是一个演示。Scrum团队应与利益相关者一起评估市场状况的变化、产品积压状态以及他们遇到的任何问题。 跳过冲刺审查会降低产品开发工作的透明度。您失去了与利益相关者一起检查增量的机会， 无论其完整性如何。放弃这个检查点会给项目增加风险。 不要取消冲刺审查，无论你认为它可能为Scrum团队带来多大的容易。无论多么困难，都要举办活动。 作为Scrum大师，由您来确保此事件发生。如果您和您的团队最终担心冲刺审查会不舒服，请尝试将以下内容添加到您的议程中：  这些活动发生在冲刺期间：… 这些障碍阻碍了我们：… 我们面临的复杂性是…… 目前，我们的产品积压包含… 发生的事情是否改变了产品的未来？现在在令人不舒服的冲刺审查中面对音乐远比跳过音乐并冒着团队可能朝着错误方向前进的风险要好得多，这会导致未来更糟糕的冲刺审查。体验勇气的Scrum价值，并进行每次冲刺审查，以便您可以与利益相关者合作，克服团队遇到的任何问题。 站立的鼓掌:  我们需要保持冷静, 不要被一次成功的 Sprint Review 冲昏了头脑 Sprint Review 不是旨在惩罚或奖励Scrum团队的活动。相反，这是一个利益相关者检查发生了什么、提供反馈和决定下一步该怎么做的机会。 如果利益相关者确实在冲刺审查期间奖励或惩罚Scrum团队，那么您，Scrum大师，必须立即提醒利益相关者他们为什么参与冲刺审查，并将注意力重新集中在增量和团队下一步应该做什么上。 如果团队专注于获得奖励，这将让Scrum团队和利益相关者之间为了项目的利益在冲刺审查中进行艰难的对话。一旦养成获得额外奖励的习惯, 会严重影响 Scrum 的 value教练角: 冲刺review是Scrum团队和利益相关者的合作活动。 这是一个与团队外的人挤在一起评估增量状态的机会。 作为Scrum的大师，我们必须不断找到新的方法来提高人们对正在发生的事情的透明度，以便我们让Scrum团队有机会就接下来会发生什么做出明智的决定。 为了确保您的冲刺Review 尽可能有价值，在下次冲刺review之前，请问自己以下问题：  合适的人是否参与了冲刺审查，以便以知情的方式调整积压的产品？ 清楚我们为什么要像这次冲刺一样吗？ 参加冲刺审查的每个人都知道Scrum团队为什么要构建此产品吗？ 我们是否会让利益相关者对我们的真实处境有任何误解？ 整个Scrum团队是否对冲刺期间发生的事情保持一致？ 我们review的议程和上次一样吗？Sprint Retrospective - 持续改进的经验总结会: 每次冲刺回顾都是Scrum团队重申其持续改进承诺的机会。 团队利用时间反思  团队合作方式 质量 定义“完成” 并寻求方法来改进我们每次冲刺潜在可发布产品的方式/工具/技术实践。如果不公开参加这个活动，也没有找到改善每次冲刺的方法，你的产品、质量和合作的人将受到影响。 如果我们作为一个团队不合作、帮助、支持和挑战自己来改进，我们将错过一个改善客户服务方式的关键机会。 大家都不愿意来: 有的时候大家觉得做的不错, 不用来 ; 或者觉得做的很差, 不好意思来 ; 或者就是觉得浪费时间 冲刺回顾是整个Scrum团队的一项活动。它应该包括  所有开发团队成员 产品所有者和 Scrum大师 请记住，此活动仅适用于Scrum团队 - 当团队以外的利益相关者或经理参加时，这使团队更难有效协作。让一群来自不同背景、文化和经验的人合作已经够难的了。添加另一层困难无济於事。每次。团队可以改进的唯一方法是整个团队参与这项活动。 冲刺 retrospective 什么时候应该发生？    它应该总是在冲刺review 之后，但在下一次冲刺计划之前发生。这是因为，如果您尚未review 增量以及您如何与利益相关者合作，那么您尚未完全review 冲刺。这可能会导致不适合您情况的适应。     就持续时间而言，《Scrum指南》指出，冲刺回顾性时间箱最多三个小时，为期一个月。根据您的冲刺长度相应调整该时间框。  肤浅的承诺: 有的团队在 retrospective 中做出了改进的建议和计划, 但是却在后面的 Sprint 中忽视这些改进的建议  团队成员必须真正致力于团队在回顾中确定的改进。 团队成员必须相互承诺进行改进。 忽视这些承诺清楚地表明，团队没有接受持续改进的心态。如果您的团队陷入了创建肤浅承诺但没有完成这些承诺的陷阱，您可以使用一些技巧来尝试解决这个问题  在下一次回顾中，首先询问成员团队是如何处理上一次 restropectrive 中承诺的的改进的。 如果团队没有取得进展，那么你需要谈谈原因。团队是否忘记了在冲刺规划期间预留完成改进项目所需的时间.  可以考虑把改进作为一个 PBI 放在下个 Sprint 中 来进行跟踪无论如何，团队需要进行公开和相互尊重的讨论，以帮助团队成员在下一次冲刺中致力于改进。 多少个改进是合理的？  一个雄心勃勃的团队可能会发现他们希望在下一次冲刺中进行的许多改进，但承诺过多可能意味着没有一个真正实现。请记住，在冲刺期间，改进会影响团队的能力。承担一两个以上的改进项目可能不会导致产品所有者和利益相关者期望的业务结果。Scrum团队应该只选择一些他们认为可以在下一次冲刺中实施的关键改进。作为一个团队，为每次冲刺确定正确的改进数量。毫无意义的改进: 团队在 Retrospective 中提出了一些改进建议  玩得开心 每周去吃一次午饭 写更好的代码 更准确地估算 协作协作我们要思考一下这些建议是可行的吗? 实现他们会让下一个 Sprint 变得更好吗? Retrospective 是关于检查  阻碍团队的人、 关系和 技术实践。让我们回顾一下团队提出的项目，并添加一些您可能提出的后续问题，以深入研究这些问题：  玩得开心     有人在扼杀所有乐趣吗？   是什么阻止了这个团队认为工作很有趣？   上一次在冲刺回顾中，球队被感谢是什么时候？    每周去吃一次午饭     大家都相处得很好吗？   我们吃午饭要解决什么？   我们沟通得不够好吗？    写更好的代码     我们对质量的定义是什么？   我们在写糟糕的代码吗？   考虑到我们现在所知，架构仍然合适吗？   我们应该重新审视我们对“完成”的定义吗？    更准确地估算     我们为什么要估算？   我们错误地估计了什么？   我们的错误估计是否设定了不切实际的期望？   是否有障碍或不良做法影响我们的估计能力？    协作协作     我们在互相帮助吗？   开发团队和产品所有者之间发生了什么？   我们错过了合作的机会吗？   我们正在适当地完善积压的产品？   团队必须深入研究关系和/或技术问题，以发挥其最大潜力。 回顾应该像冲刺审查一样严肃和专业 只有50%的人参与讨论: 谈论和分享意见对某些人来说可能很容易, 对于另一些人可能很难 讨论主题像“我们能做些什么来变得更好？”这样敏感时，一些参与者可能会非常不舒服。 然后，会议只能由少数人主导，让会议室其余部分的想法和意见保持沉默。如果房间里有强烈的声音或个性造成冲突，与会者可能会进一步”闭嘴”。 SM 需要帮助团队成员找到自在的方式来分享坦诚的意见和接收关键反馈 请务必尝试不同的方法，以确保每个团队成员都能被听到。 例如，您可以通过发布调查然后共享结果来匿名获得团队的建议并保持团队安全 海星回顾，是了解Scrum团队在冲刺期间可以改变工作方式的好方法。以下是你的工作：  在白板或活动挂图上画一颗星星，并标记五个部分：开始、“停止”、“更多”、“更少”和“保持”向您的团队解释，各个部分应该包含以下问题的答案：     开始：团队希望在下一次冲刺中开始做什么，以改善他们合作和交付产品的方式？   停止：哪些事情没有给团队带来价值，应该在下一次冲刺之前停止？   更多：我们做得好，我们团队需要更多的事情？   更少：哪些活动消耗的时间和精力比它们创造的价值还要多？   保留：我们需要保留哪些实践、实验或想法，因为我们已经看到它们会产生我们希望保留的积极变化？    请参与者为每个领域产生尽可能多的想法。给每个参与者几分钟时间将他们的评论放在黑板上，并大声朗读出来。 当所有领域都已填补后，请就团队共享的内容进行简短的讨论。指出趋势和模式。 让每个人都投票给一个开始项目和一个与他们产生最大共鸣的停止项目。 探索获得最多选票的开始和停止项目，然后生成可测量的更改或实验，以便在下一次冲刺中尝试。冲刺Retrospective 讨论的结果应该在哪里显示？    在这个问题上没有明确的共识。一些团队将结果作为产品积压项目进行跟进，因为这些改进增加了增量的价值。其他团队将他们展示在团队室，在那里他们每天进行争球。无论您的团队做出什么决定，请确保您的改进显示在团队被提醒并可以采取行动的地方。     谨慎地将追溯结果存储在管理层可以看到的公共工具中。根据对组织的信任程度，这可能会降低团队对他们遇到的问题持开放和诚实的意愿。另一方面，公开分享这些结果可以帮助其他正在与类似问题作斗争的团队。  跳过它: 当面临外部压力或关键截止日期时，一些Scrum团队将取消冲刺Retrospective。因为团队需要时间工作而取消回顾是功能障碍的症状。在这些情况下，回顾正是团队所需要的。 你如何说服你的团队举行回顾，即使他们觉得没有时间进行回顾？以下是团队试图跳过回顾时可以问的一些问题：  我们感受到的压力来自哪里？ 我们取消回顾换回来的时间——这是成功和失败的区别吗？ 我们面临的最后期限是怎么来的？ 我们的架构是否稳定且适合产品？ 技术债务在我们建设的东西中显而易见吗？有时你必须偷偷摸摸地回顾 不得已的 mini Sprint Retrospective, 如果团队没时间开会, 那就和团队成员进行简单的交流, 获取需要赶紧的地方. 以下是我从这种情况中学到的东西：  这次交流花了五分钟，球队在下一次冲刺中有所改进。 有时，我们必须聪明地与不成熟的Scrum团队合作。 牢记冲刺回顾的目的。在这个简短的互动中，我们很快得到了一个很好的改进想法。认领胜利，继续前进。总结:  因为没有足够的时间进行冲刺 Retrospective 永远不是借口, 回顾永远不应该取消 这是Scrum团队检查他们为什么没有时间和适应的机会，这样他们就不会再有这种感觉了。 取消回顾就是在伤口上撒盐，随着时间的推移只会变得更糟 根据《Scrum Guide》，回顾的时间表是为期四周的冲刺需要三个小时，投诉会议: 例如，Scrum团队通常抱怨管理层以及他们如何“得不到管理”。这不是对组织中人们的感同身受的观点。  Scrum 团队有的时候会感觉到 外部世界可能看起来很奇怪、荒谬，甚至落后于时代。 从而产生一种“我们对他们”的心态。出于这些和其他原因，冲刺回顾可能会螺旋式地演变成投诉会议。 不要让团队在回顾期间发泄管理问题，而是问问他们可能对经理的行为有什么贡献。建议:  和影响 Scrum 的外部组织进行沟通, 了解到底发生了什么导致抱怨, 并试图去建立同理心和理解, 并视图解决问题 继续抱怨而不采取行动，会加剧紧张局势，并阻碍该组织成功采用Scrum的能力。 我希望我能在回顾期间提供便利，而不是抱怨。 作为一名Scrum大师，你应该将抱怨和指责视为危险信号。 你需要意识到抱怨的危险。它营造了一种消极和无助的文化，问题没有得到解决，而只是讨论。 引导对话回到Scrum团队如何拥有他们正在讨论的障碍或问题。教练角: 为了让 Retrospective 更有意义, 尝试和团队定期讨论下面这些问题  Is our quality getting better? Do we feel like a team? Are we living the Scrum values? Has our definition of done become more strict? What’s slowing us down, both technically and organizationally? How is the product owner/development team relationship? How are our relationships with people outside the Scrum team? Are we proud of the work we’re doing?如果使用正确，冲刺回顾是帮助您的团队以积极的方式前进的绝佳工具。至关重要的是，它们要发生，并在结果上富有成效，为团队带来坚实的改进 "
    }, {
    "id": 18,
    "url": "http://localhost:4000/Prepare-for-AWS-Certified-Solutions-Architect-Associate-Certification/",
    "title": "Step by Step for AWS Certified Solutions Architect Associate",
    "body": "2022/04/23 - 想要快速了解 AWS 服务的最好方法就是去获得相关的证书. AWS 提供了各个方面的证书可以方便我们掌握提供不同方向的技能与服务能力, 包括 账户管理, 网络, 安全, 计算, 存储, 数据管理, 灾备迁移等. 我在本文中将把我自己获取 AWS Certified Solutions Architect Associate 证书过程中所积累的经验分享给大家, 让我们开始吧 AWS Certified Solutions Architect Associate[TOC] Well-Architected Framework: 5个支柱: 卓越运营:  设计原则     执行运营即代码:在云中，您可以将用于应用程序代码的工程规范应用于整个环境。您可以将整个工作负 载(应用程序、基础设施)定义为代码，并使用该代码进行更新。您可以将运营流程写成代码(脚本)， 并通过事件触发来自动执行这些脚本。通过以代码形式执行操作，您可以减少人为错误并实现对事件的一 致响应。   频繁进行可逆的小规模更改:将工作负载设计为支持组件定期更新。以较小增量进行失败时可逆的更改 (尽可能不影响客户)。   经常优化运营流程:在使用运营程序时，要寻找机会改进它们。在改进工作负载的同时，您也要适当改进 一下流程。设置定期的实际演练，以检查并验证所有流程是否有效，以及团队是否熟悉这些流程。   预测故障:执行“故障演练”，找出潜在的问题，以便消除和缓解问题。测试您的故障场景，并确认您了解 相应影响。测试您的响应流程以确保它们有效，并确保团队能够熟练执行。设置定期的实际演练，以测试 工作负载和团队对模拟事件的响应。   从所有运营故障中吸取经验教训:从所有运营事件和故障中吸取的经验教训，推动改进。在多个团队乃至 组织范围中分享经验教训。   安全性:  设计原则1在云中实现安全性有七个设计原则:          健壮的身份验证体系:实施最小权限原则，并通过对每一次与 AWS 资源之间的交互进行适当授权来强制 执行职责分离。集中进行身份管理，并努力消除对长期静态凭证的依赖。       实现可追溯性:实时监控和审计对环境执行的操作和更改并发送警报。为系统集成日志和指标收集功能， 以自动调查并采取措施。   在所有层面应用安全措施:利用多种安全控制措施实现深度防御。应用到所有层面(例如网络边 缘、VPC、负载均衡、每个实例和计算服务、操作系统、应用程序和代码)。   自动实施安全最佳实践:借助基于软件的自动化安全机制，您能够以更为快速且更具成本效益的方式实现 安全扩展。创建安全架构，包括实施可在版本控制模板中以代码形式定义和管理的控制措施。   保护动态数据和静态数据:将您的数据按敏感程度进行分类，并采用加密、令牌和访问控制等机制(如适 用)。   限制对数据的访问:使用相关机制和工具来减少和消除直接访问或人工处理数据的需求。这样可以降低处 理敏感数据时数据处理不当、被修改以及人为错误的风险。   做好应对安全性事件的准备:制定符合您组织要求的事件管理和调查策略和流程，做好应对事件的准备工 作。开展事件响应模拟演练并使用   可靠性:  设计原则1在云中实现可靠性有五个设计原则:          自动从故障中恢复:通过监控工作负载的关键绩效指标 (KPI)，您可以在指标超过阈值时触发自动化功能。 这些 KPI 应该是对业务价值(而不是服务运营的技术方面)的一种度量。这包括自动发送故障通知和跟踪 故障，以及启动解决或修复故障的自动恢复流程。借助更高级的自动化功能，您可以在故障发生之前预测 和修复故障。       测试恢复过程:在本地环境中，经常会通过执行测试来证明工作负载能够在特定场景中正常运作。通常不 会利用测试来验证恢复策略。在云中，您可以测试工作负载的故障情况，并验证您的恢复程序。您可以采 用自动化方式来模拟不同的故障，也可以重新建立之前导致故障的场景。此方式可以在实际的故障发生以 前揭示您可以测试与修复的故障路径，从而降低风险。   横向扩展以提高聚合工作负载的可用性:使用多个小型资源替换一个大型资源，以降低单个故障对整个工 作负载的影响。跨多个较小的资源分配请求，以确保它们不共用常见故障点。   无需再预估容量:本地工作负载出现故障的常见原因是资源饱和，即对工作负载的需求超过该工作负载的 容量(这通常是拒绝服务攻击的目标)。在云中，您可以监控需求和工作负载利用率，并自动添加或删除 资源，以保持最佳水平来满足需求，而不会出现超额预置或预置不足的问题。虽然还有很多限制，但有些 配额是可控的，其他配额也可以管理(请参阅“管理Service Quotas与限制”)。   管理自动化变更:应利用自动化功能对基础设施进行更改。需要管理的变更包括，对自动化的变更，可对 其进行跟踪与审查。   性能效率:    设计原则   1在云中实现性能效率包括五个方面的最佳实践:       普及先进技术:通过将复杂的任务委派给云供应商，降低您的团队实施高级技术的难度。与要求您的 IT 团 队学习有关托管和运行新技术的知识相比，考虑将新技术作为服务使用是一种更好的选择。例如，NoSQL 数据库、媒体转码和机器学习都是需要专业知识才能使用的技术。在云中，这些技术会转变为团队可以使 用的服务，让团队能够专注于产品开发，而不是资源预置和管理。   数分钟内实现全球化部署:您可以在全球多个 AWS 区域中部署工作负载，从而以最低的成本为客户提供 更低的延迟和更好的体验。   使用无服务器架构:借助无服务器架构，您无需运行和维护物理服务器即可执行传统计算活动。例如，无 服务器存储服务可以充当静态网站(从而无需再使用 Web 服务器)，事件服务则可以实现代码托管。这 不仅能够消除管理物理服务器产生的运行负担，还可以借由以云规模运行的托管服务来降低业务成本。   提升试验频率:利用虚拟和可自动化的资源，您可以快速利用各种类型的实例、存储或配置执行对比测 试。   考虑软硬件协同编程:了解如何使用云服务，并始终使用最适合您工作负载目标的技术方法。例如，在选 择数据库或存储方法时考虑数据访问模式。   成本优化:    设计原则   1   在云中实现成本优化有五个设计原则:         践行云财务管理:为获得财务上的成功并加速在云中实现业务价值，需要投资云财务管理/成本优化。您 的组织需要投入时间和资源增强自身在这个新的技术和使用情况管理领域中的能力。与安全性或卓越运营 能力类似，您的组织需要通过知识构建、计划、资源和流程来培养能力，从而成为一家具有成本效益的组 织。   采用消费模型:仅为所需计算资源付费，并可根据业务需求而非复杂的预测增加或减少使用量。例如，开 发和测试环境通常只需要在每个工作日运行八个小时。您可以在不需要时停用这些资源，从而实现 75% 的 潜在成本节约(40 小时对比 168 小时)。   衡量整体效率:衡量工作负载的业务产出及这些产出的实现成本。使用这种衡量方式了解您通过提高产出 和降低成本获得的收益。   不再将资金投入到无差别的繁重任务上:AWS 会负责繁重的数据中心运营任务，例如安装、堆叠和驱动服 务器。它还消除了使用托管服务管理操作系统和应用程序的运营负担。因此，您可以集中精力处理客户和 业务项目而非 IT 基础设施。   对支出进行分析和归因:使用云，您可以更轻松地确定系统的准确使用量和成本，从而将 IT 成本透明地分 摊到各个工作负载拥有者。这有助于衡量投资回报率 (ROI)，并让工作负载拥有者能够据此优化资源和降 低成本。   几个定义: Availability:  determined by percentage uptime, in 9s     一堆9    例子：     S3 standard 是 4 9s   S3 IA/ Inteligent tier 是 3 9s   S3 OZ IA 是 2. 5 9s   High Availability:  The system will continue to function despite the complete failure of any component of the architecture     部分组件挂了，系统继续用   Fault Tolerance:    The system will continue to function without degradation in performance despite the complete failure of any component of the architecture      部分组件挂了，系统继续用，且性能不打折      几个例子:      不是FT（fault tolerance）的例子 (貌似都部署在了EC2上)         Virtual Private Gateway     Nat Gateway     ElastiCache     RedShift     EBS     RDS Multi-AZ             包含了 Aurora                ECS on EC2     EC2          符合 FT的例子         S3     DynamoDB     Aurora serverless     API Gateway     CloudFront     Route53     ELB     Lambda             包含了 Lambda@Edge                     Redundant:  multiple resources dedicated to performing the same task     多个资源干一个活   Durability:  数据在系统中保持，不会因为硬件的故障而丢失。 可以通过备份，replica ， 跨AZ replica 方式提高 durability 例子：     S3 standard 是 11 9s   S1 OneZone IA 也是 11 9s   Resilience:  使⽤⽹络资源的系统，或者被⽹络资源使能的系统在⾯对不利条件、压⼒、攻击或者损害的时候所展现出来的预测、承受、恢复和适应能⼒。 统计的对象是 Availability 和 DurabilityScalability:  the ability of a system to increase resources to accommodate increased demand. This can be done vertically or horizontally, and is not necessarily automated.      有按需（水平or垂直）增加资源的能力，不一定非要自动的。   比如：         EBS Volume, 只能加不能减     其他的只要是能 Elastic 的， 一般都是Scalable          Elasticity:  the ability of a system to increase and decrease resources allocated (usually horizontally) to match demand, and implies automation.      有按需（通常是水平）自动增加或减少资源的能力   比如：         EBS Volume IOPS ， 可加可减     只要是能 Elastic 的， 一般都是Scalable          SLA:  到这个网址去找     aws. amazon. com//sla   AWS Service Scope:  不同的服务在AWS 上有不同的scopeAZ Scope:  EC2 instance EBS NAT Gateway RedShift Node RDS Instance 以上这几个也同时满足 HA 但不是FTRegion Scope:  S3 Bucket DynamoDB Table SNS Topic VPC CloudWatch Alarm 以上这几个也是支持FT 的 ELB EFS Security GroupMulti-Region Scope:  S3 Cross-region replication RDS Cross-region read replication DynamoDB Global TableEdge Location:  Route 53 Hosted Zone CloudFront Distribution WAF Filtering Rule Lambda@Edge几个数字: 传输率: 100mbps 的传输速度 一天可以传输 1T 的数据1Gbps = 10x100mbps 的传输速度 一天可以传输10T 账户管理: IAM: IAM:   user/group policy 可以attahe 到user /group 上   policy      Policy Generator 可以帮助创建policy, 填写表格后帮你创建json 格式的policy   Policy simulator , mapping policy and services 验证policy 是否是你希望的那样。      Policy JSON 里面的元素          Effect： Allow or Deny           Action: 可以干啥， 比如访问S3的*           NotAction: 啥都行，除了这个XXX           Resource: 干谁， 指定一个S3 Bucket 的ARN           Conditions： 约束条件，                比如限制某个IP段能不能访问              aws:SourceIP                      限制可执行的区域，比如只有这个区域才能run instance              aws:RequestedRegion                      限制只有数据部门的人有读写权限              aws:PrincipalTag/Department:”data”                      不允许删除，除非打开了MFA Delete              BoolIfExist:aws:MultiFactorAuthPresent:false                      password policy MFA Access Key IAM Roles     IAM Roles 是临时 （temporary ）权限， 通过不同的方法进行使用，   主要是给各种 AWS service 提供不同权限的临时访问授权      Security Tools      credential tool (account level, audit 用途)   列出该账号下所有用户和他们的credential 状态    access advisor (user level)     可以看到有哪些服务权限授予了这个用户， 并且可以看到什么时候用户对这些服务进行了访问      IAM Permission Boundaries 权限边界      是在一个Policy 的JSON中设置两个Statement, 互相之间有约束。   两个statement 是取交集， 没有交集就没有权限   At this time, only IAM User supports permission boundaries, which does limit usage somewhat.    如果两个statement 有冲突， 那么Deny&gt; Allow   流程         先设置一个大的权限边界（Permission Boundary），比如拥有所有的S3, CloudWatch, Ec2 的任意操作权限。     然后在设置一个给用户的小的边界（IAM policy），只可以创建用户的权限。     两个权限没有交集， 所以这个用户最后就是没有任何权限          权限边界可以在User 和 role 使用，但不能在group 使用   和 IAM Policy 的区别         Permission boundaries are limits, and IAM policies define actual tasks that can be allowed or denied.           案例：         说一个公司的新员工被赋予了 full access to DynamoDB， 而且还一不小心删了生产的一些表， 怎么解决才能避免以后在发生。             使用permission boundary 去限制给所有员工设置DynamoDB 最大的可以从IAM获得的权限 - 不允许删除生产的数据等                     STS - Security Token Service:  ﻿给AWS资源授权临时和受限制的访问 ﻿可以理解成 sudo 命令 ﻿Token 有效期是1小时（需要refresh）   ﻿AssumeRole 角色扮演      ﻿返回你的皮给请求者      ﻿AssumedRoleWithSAML      ﻿返回皮，带个SAML， 让用户可以拿着SAML 去登录      ﻿AssumedRoleWithIdentity      ﻿返回皮， 和第三方IdP(facebook login, Google login …. )   ﻿AWS不鼓励用这个，而是鼓励用Cognito      ﻿GetSessionToken      ﻿返回个带有session 的token,用来MFA 用。      ﻿考试      ﻿题中如果提到了跨账号访问, 提到了 AssumeRole, 那么答案就是 STS   Identity Federation:  Federation 让外部用户可角色扮演一个role 访问AWS服务，不需要提前创建一个IAM user SAML 2. 0 Federation     把SAML 和 Active Directory/ADFS 整合到一起的Federation    考试     题中如果提到你有一个移动应用, 想让用户访问他们自己的 S3, 那么最好的方式使用 Cognito Identify Federation .    Custom Identity Broker Application:  ﻿如果提到了 on-premise identity provider, 但是不支持 SAML 2. 0 , 那么就是 BrokerCognito:  让移动，web 用户可以直接访问AWS 资源 比如     让移动app用户可以通过登录Facebook 账号来访问S3 bucket   AWS Directory Service:    就是在AWS 上管理微软的AD     三种          AWS Managed Microsoft AD,          在AWS建一套AD, 让AWS 和 on-premise 进行trust沟通              AD Connector          AD 存在了On-premise 上， 通过proxy 的方式在AWS 和 on-premise 沟通              Simple AD          没有 on-premise, 只有AWS， 和AWS上创建的AD              技巧：          如果提到了on-premise 的 AD 账号访问 AWS 并支持 MFA, 那就是Managed AD     如果提到了on-premise 的 AD 账号访问 AWS 并通过代理的形式访问, 那就是 AD Connector     如果提到了直接用 AD 账号访问 AWS , 没有on-premise 的 AD 账号, 那就是 simple AD          Root 用户可以做， 但是 AdministratorAccess 不能做的事情：:  change account name or root password or root email address, change AWS support plan, close AWS account, enable MFA on S3 bucket delete, create Cloudfront key pair, register for GovCloudOrg: Organization:  可以管理多个account organization 只能有一个master accoutn ,多个 member account org 支持两种账户，     一种是master account 创建的账户，   另一种是standalone 的账户    提供single payment method 把所有账单算在一起来计算折扣     Consolidated Biling         多个账号都用的服务， 比如 Shield, 只要付一份钱就可以了     Consolidated Billing across all accounts - single payment method           Multi Account Strategies     给每个部门， 或者成本中心， 或者开发、测试、生产单独创建账号。 有效隔离资源    Organization Unit (OU) ，     是组织的下一级单位， 是member account 的组   主账号管理多个OU, 每个OU 下面再分多个member 账号   OU 里面可以套OU   主要有三种类型         Business OU             分财务， IT,Sales 等                Env OU             分开发，测试，生产等                Project OU             分项目1， 项目2 等                      Service Control Policies (SCP)     让OU 下面的所有账号可以干什么事情或者不能干什么事情， 默认什么都不可以干， 必须设置一个 explicit Allow   Does not apply to the Master Account         通过SCP 可以限制org 中的每个account 中的用户和 role 访问特定AWS 服务、资源、API 。还可以设置特定的条件来缩小范围             SCPs 影响所有attached 到的账号的member account 的users和roles, 包括member account 的 root user       如果一个user or role 有一个被赋予了具有特定访问能力的 IAM 权限， 这个权限没有被SCPs 允许或者明确拒绝了， 那么这个用户就不能执行这个特定访问权限                SCP 设置的 policy 可以 override 管理员账号的权限     SCPs 不影响：             service-linked role (服务相关角色) 是直接链接到 AWS 服务的一种独特类型的 IAM 角色。service-linked role 由服务预定义，包括该服务代表您调用其他 AWS 服务所需的所有权限       SCPs do not affect the root credentials, either console or API.                      比如：         限制这个OU下面所有账号不能使用 EMR这个AWS服务     因为当地的个人信息保护法（PCI）, 所以要强制关闭某个服务          父子OU 有继承关系， 比如， 父OU 绑定的SCP 在子OU中会被继承                   在组织之间迁移账户:  先删掉原来账户里的member account 新组织给账户发送一个邀请 接受新组织邀请 删掉旧account(master account 才需要这一步)RAM: Resource Access manager (RAM):  RAM 的目的是让一个org 下的多个子账号分享资源， 避免浪费 分享资源不需要额外付费 把自己AWS的资源分享给其他AWS 账户， 几个账户可以共享资源，一起工作。 可以是自己的org 的， 也可以是其他org 的 网络是共享的， 多个账户的服务之间客户互相通信     需要使用private IP   需要打开安全组的设置    可以分享的内容：     VPC Subnets         可以分享subnet内的所有的资源（比如同一个org 下几个子账号的所有EC2 实例）（只能分享给同一个ORG下的account）， 几个账户可以共享一个VPC 的Subnet， 在其中一起工作     不包括Security 和 VPC     participant 可以在subnet 下面创建和管理他们自己的资源， 比如在subnet 下面创建一个自己的S3bucket.      participant 不能修改和删除不属于他自己的资源     例子：一个org 下同一个region 的多个子账号分享EC2 实例             通过 RAM (Resource Access Manager)创建一个VPC -&gt; 分享它的一个或者多个subnet 给那些子账号-&gt; 子账号把EC2 都放在subnet 中，大家就可以公用EC2 了。                     Route53 Resolver Rules   Transit Gateway   License Manager Configurations   VPC Sharing (Shared service VPC):  (Resource Access Manager 的一部分) 来 share 同一组织下的多个账号的一个或多个子网 ，     VPC Sharing, 分享的是下面的Subnets, 而不是VPC 自己。   RAM 就是在多个AWS账号之间分享AWS资源的，   Transit Gateway 使用了 RAM 功能   SSO: AWS Single Sign-on:  单点登录， 多个网站/域名都是在一个点登录，登录后访问多个域名/应用不需要重复登录     案例：         IBM 一个账号登录就可以使用Slack, Box, Office365, Teams, Webex,     客户已经有了AD, 客户的员工可以通过设置 AWS SSO 来访问客户在AWS 中的多个账户。           集中管理SSO 去访问多个账户和第三方应用 和 Cognito （assumeRole/STS）的区别     Cognito 是登录AWS 实例提供的服务的时候使用第三方的账号登录， 比如用facebook 登录   单点登录是一个点登录后就可以访问多个网站    集成和Organization 支持     SAML 2。0   Active Directory    集中了Permision 和 CloudTrail 的管理网络边缘: Route53: DNS 概念:  A, AAAA, CName, Alias Zone File: 存放DNS记录的地方 100% up time     Route 53 has an uptime SLA of 100%    Domain Name Server (DNS Server) ： 解析域名服务器（官方，私人）Route 53:  health check, for aws service（end point）     目的是做auto failover   checker 是15个global 的 检查服务， 3个说健康就健康， 小于3个就不健康。   返回 状态码200 、300是健康   public resource only (有 public ip的service)         private hosted zone 的资源可以让 health check 检查 private host zone 的 资源的 cloud watch alarm          路由和防火墙需要允许 route 53 进来   Calculated Health Checks 是让health check 去检其他的健康检查结果的汇总， 父子关系的健康检查   private host zone 需要通过 clout watch alarm 来检查，然后把结果暴露给health checker   当 Route 53 运行状况检查的主记录失败（并且配置了 DNS 故障转移）时，预期的行为是什么？         任何后续 DNS 记录都会返回辅助记录，直到主记录再次通过运行状况检查。           100% availability SLA Record Types （记录类型）     A – maps a hostname to IPv4   AAAA – maps a hostname to IPv6   CNAME – maps a hostname to another hostname         The target is a domain name which must have an A or AAAA record     Can’t create a CNAME record for the top node of a DNS namespace (Zone Apex)， 比如abc. com就不行， www. abc. com 就可以。 Alias 可以解决这个问题          PTR - 你告诉我一个IP, 我告诉你一个域名   Alias Record Target         将同一个 Route53 的 一个域名转到一个AWS 资源（包括域名）上去， 但是前提是域名必须是账号中的Route 53 自己管理的域名。 Alias 不能管理第三方的域名     Alias target 支持除了EC2之外的很多服务 , ELB, Cloud Front, API Gateway, Beanstalk, S3, VPC, Global Accelerator.           NS - Name Server, 是AWS提供的DNS服务器列表， 让用户把自己的域名的Hosted Zone 里面设置这个列表，就可以找到自己在AWS里的服务    RecordsTTL (TimeTo Live)     记录被客户缓存的时间， 过期了客户又要去route 53再问一遍   不能设置    Hosted Zone, 放记录的容器     public hosted zone, 告诉外部客户如何找到AWS里的服务（服务要含public ip）   private hosted zone, 告诉aws 服务如何找到其他aws服务 (服务要含private ip)         DNS 中设置一个private hosted zone 就可以解析内网的域名， 将一个域名指向一个内网的IP地址     一般会指向一个VPC             如果VPC 要能够接收流量， 就需要设置两个配置                 打开                     VPC 的 DNS hostnames - 设置一个内部域名           DNS resolution for private hosted zones. - 可以被解析                                                        route policy （你告诉我域名， 我告诉你去哪个IP, 这里的IP可以是任何alias target）     simple - 你告诉我一个域名，我告诉你一个（或多个IP，客户随机挑一个）   weighted - 转发给多个IP地址为不同的权重， 0% - 100%   latency - 用户去哪个IP最快就给他哪个IP   geolocation - 用户在那个国家， 就去这个国家的IP   geoproximity         在 geolocation 的基础上， 给同一个国家内的多个IP设置 -99到99的权重， 越高半径范围越大。 用 Traffic Flow Policy 来记录               multi-value - 一个域名可能会有多个IP, 请求说要多个 ， 那就返多个    用Route 53 解析从其他域名商买得域名     Create a Hosted Zone in Route 53   Update NS Records on 3rd party website to use Route 53 Name Servers   通过Driect Connection 连接的on-premise 和 AWS VPC 之间互相之间解析DNS 的方法:  在Route 53 上创建一个inbound endpoint, 在 on-premise 上的DNS 解析器 （resolvers）可以把DNS请求发送到53的 endpoint 上 在Rout 53上创建一个 outbound endpoint, 53就可以通过这个endpoint 把 aws上的流量forward 到 on-premise 网络希望Route 53 可以在网站不可用的情况下跳到一个静态错误页面， 需要的最简单可以实现的方案是：:  设置Route 53的 active-passive (主动-被动) 的 failover routing policy. 当 Route 53 健康检查发现ALB 不健康， 流量会被转到静态错误页面（放在S3 bucket 上）CloudFront: CloudFront:  CDN 网络, 加速边缘侧的下载 216个 edge locations CloudFront + S3 的方案比纯S3 的方案有更低的成本（更省钱） 安全     防DDOS攻击， 整合Shield, WAF   外部暴露HTTPS, 内部HTTPS talk   OAI (Origin Access Identity) + S3 bucket policy 保证S3数据源的安全， 不会把源暴露出去         在CloudFront 上设置OAI ，不暴露S3源的真实地址     设置S3 Policy, 只允许CloudFront 访问， 不允许外网直接访问          Field Level Encryption         在 edge 用非对称加密（有公钥私钥）用户的敏感数据， 比如信用卡数据     最多加密10个字段     公钥放在Edge location, 私钥放在EC2 实例上           TTL , 有个cache 的时间段， 过期了需要回到origin 去读 Origins (CloudFront 的数据源)     S3 Bucket 中的文件   Custom Origin (http) 自定义源 (必须有public ip, 切SG需要允许edge location 的IP进行访问)         EC2实例     ALB     S3 Website     any http backend you want          Multi-origin         根据客户请求的path 不同而去不同的origin, 比如 /api/ 就去ALB, /*就去S3          Origin Group         为了HA, 比如两个EC2、 两个S3 都可以组成一个 Origin Group。     CloudFront 访问的是Origin Group, 当一个Ec2 挂了也没事。           Geo Restriction , 对某些发起请求的国家的用户进行限制（版权或者合规的原因）     white list   black list   “国家” 是使用第三方的 Geo-IP数据库    S3 Cross Region Replica for CloudFront     想要最佳的效果， 必须要在你想做CDN的国家设置一个read only replica.     Signed URL/ Signed Cookie     比如给付费用户下载视频的时候提供一个临时的URL, URL1分钟后就过期。         背后的逻辑： 想要创建一个signed URL 给客户且不允许客户访问源的方法有一下几个步骤             创建一个CloudFront 用户可以让CloudFront 调用 OAI 来访问S3 源       配置S3 允许CloudFront 可以通过OAI 访问源的文件 （这时，还没有URL地址可以让外部用户访问）       需要通过Lambda 来创建一个URL       通过CloudFront signed URL 来限制只有付费用户可以访问文件                     过期时间可长可短（分钟到年）   一个url 对应一个文件         用户通过signed URL只能访问一个文件          一个 cookie 对应多个文件         用户通过signed Cookie 能访问多个文件          对比S3 Pre-signed URL, 主要区别是场景不一样， 一个是为了加速下载并给特权账户使用， 一个是临时使用S3的一个文件。   CloudFront Price Classes:  Price Class All : 所有region 都打开，性能最好， 钱也是哗哗的 Price Class 200: 去掉最贵的几个国家， 其他的都选 (最贵的往往是一些偏僻的地方， 比如印度) Price Class 100: 只选最便宜的几个国家（往往是最发达的几个国家）CloudFront 和 Global Accelerator的区别:  有，没有Cache的区别 静态和动态的区别 HTTP 和UDP/TCP 的区别CloudFront 和 S3 Transfer Accelerator 的区别:  他们都支持加速静态文件 但是1G以内， CloudFront 效果更好 1G 以上， S3 TA 效果更好CloudFront 和 VOD, Live streaming video:  You can use CloudFront to deliver video on demand (VOD) or live streaming video using any HTTP origin. One way you can set up video workflows in the cloud is by using CloudFront together with AWS Media Services. 错题总结 - Cloudfront 的正确描述：:  CloudFront 可以基于文件类型来路由到多个oregion （Multi-origion）     比如图片类型就路由到 一个 S3地址， 其他文件路由到另一个地址    使用 field level encryption 保护敏感数据 使用 origin group， 设置primary和secondary origins 来进行高可用和 failoverAWS Global Accelerator: AWS Global Accelerator:  让一个全球化的应用可以被世界各地的用户访问 提升了应用的 performance 和 availability 利用Edge Location 到应用实例之间的AWS 内部网络加速， 让全球客户可以快速访问到应用。 可以降低AWS 客户网络的复杂性 可以被加速的对象     Elastic IP,   EC2 instance,   ALB, NLB    AGA 和 ELB配合     ELB 是region scope. 如果需要跨region ,需要借助于Aws Global Accelerator (AGA) 的 跨region 能力；在AGA中创建endpoint group， 把每个region 的 ALB 包含进去。   ELB provides load balancing within one Region, AWS Global Accelerator provides traffic management across multiple Regions […] AWS Global Accelerator complements ELB by extending these capabilities beyond a single AWS Region, allowing you to provision a global interface for your applications in any number of Regions. If you have workloads that cater to a global client base, we recommend that you use AWS Global Accelerator. If you have workloads hosted in a single AWS Region and used by clients in and around the same Region, you can use an Application Load Balancer or Network Load Balancer to manage your resources.     加速原理     AWS Global Accelerator 提供了两个静Anycast IP (公共IP) (所有服务器用同一个IP, 客户会路由到最近的一个/延迟最低的那个) 给到了应用   客户请求的域名会被翻译成这个IP, 给到客户， 客户找到最近的一个Anycast IP   Anycast IP 将流量转给Edge location   Edge location 通过AWS 内网将数据转给应用。    因为Anycast IP不会变， 所以不影响客户的缓存。 Health Check , 可实现全球的灾难恢复 Security     只有两个公共IP需要暴露   AWS Shield 可以防止DDos 攻击    场景     游戏， VOIP电话， 蓝绿测试，UDP, MQTT, IOT,   需要静态IP地址，   需要全球的快速 failover   两个IP   Anycast IP   CloudFront 和 Global Accelerator的区别:  有，没有Cache的区别 静态和动态的区别 HTTP 和UDP/TCP 的区别案例：: 一个国际应用的一个region 的 ELB 挂了， 降低网络延迟并提供automitic failover cross aws region 的方案是：  设置 AWS Global Accelerator ， 增加 endpoint 去 让用户访问其他地理位置     AGA 有两个IP地址， 可以让你增加或删除 origins, AZ, Region是， 且不会降低应用的availibility.    通过手动调整endpoint 路由的流量和权重，如果一个endpoint 有failure , AGA 马上几秒钟就会自动的将流量redirect 到一个健康的 endpoint 上   通过AGA 你可以：         整合AGA的两个静态IP地址与AWS服务（NLB, ALB, EC2, Elastic IP, ） 加速     把endpoints在AZ/region 之间移动， 不需要修改DNS 和 应用 （两个IP是同一个IP地址）     通过给 endpoint group 配置一个 traffic deal percentage 来 dial (拨叫) trafic up/down 到一个指定的region, 可以实现测试性能和发布一个更新 （蓝绿部署）     控制流量权重分配到每个endpoint。          Snow Family: Snow Family:  type     Snowcone         8TB          Snowball         80TB     Snowball Edge Compute Optimized 和Snowball Edge Storage Optinmized 都支持 Storage Clustering (存储集群).           Snowmobile         100PB = 100 * 1024TB     AWS 建议 10 PB以上的数据用Snowmobile 来传输           Challenge     无宽带， 低带宽， 网费贵，网络不稳定    场景     把数据运回来   边缘计算（人工智能，数据分析等）， 算好了把结果拿回来         用EC2 或者 Lambda计算           使用流程     console上申请Snowball, 快递到你家   安装 snowball client/AWS OpsHub 到自己的服务器上         AWS OpsHub, 安装在客户电脑上的软件， 用于管理Snow 设备          连接snowball, 开始拷贝数据   快递回AWS   创建一个 snowball job 数据先导入到S3, (不能直接到Glacier) , 在用lifecycle rule 把数据搞到glacier 中 (同一天)    AWS DataSync     Snowball/Snowcone 连上网了，就可以AWS DataSync 来把数据导入到AWS    Edge Computing     All: Can run EC2 Instances &amp; AWS Lambda functions (using AWS IoT Greengrass)   Long-term deployment options: 1 and 3 years discounted pricing   网络: VPC: VPC - Virtual Private Cloud:  ﻿每个region 最多5个VPCs   ﻿每个VPC 最多5个 Cidr      ﻿每个Cidr 最小范围 /28 (16个IP)   ﻿每个Cidr 最大范围 /16 (65536个IP)    ﻿VPC 只支持IPV4 ﻿VPC 如果跨多个AZ， 那么需要支付额外的az间流量通信费VPC中需要付费的网络设备：:  EIP NAT Gateway     按小时和流量付费    VPN Gateway     按小时和流量付费    Peering connection     按小时和流量付费    interface endpoint     按小时和流量付费   (gateway endpoint 免费)    Transit gateway     按小时和流量付费   更贵一点    Trafic mirroring     按照对应连接的的ENI 个数按小时付费    VPC flow logs     按照流量付费   VPC regional 网络间的付费:  同一个AZ 之间的流量传递，除了public IP 之外都不要钱。 AZ 之间的数据传输需要付费 Egress （访问互联网）需要付费 Ingress 免费 Region 之间传输数据需要付费 使用 CloudFront 来优化S3 可以省钱     S3 到 CloudFront 这部分的通信免费   VPC security group:  使用VPC 安全组可以控制网络流量访问你的文件系统 （EFS 等）CIDR:  IP 地址段     /32 代表1个IP（2^0）=1   /31 代表2个IP （2^1）=2   /n 代表 (2^n)   /24 代表最后一个地址段可用, 256 个IP   /16 代表最后两个地址段可用 , 256x25个IP   /8 代表最后三个地址段可用   /0 代表全地址段可用    https://www. ipaddressguide. com/cidr 可以帮忙计算Cidr 的IP个数 Private IP 可以用的地址段     10. 0. 0. 0/8   172. 16. 0. 0/12 (AWS VPC 默认地址段范围)   192. 168. 0. 0/16   subnet:  subnet 绑定到某个AZ上， 定义一段CIDR 每个subnet 会有5个IP地址被预留     Subnet CIDR 中的前4个和后1个 (以10. 0. 0. 0/24为例)         10. 0. 0. 0 - subnet 网络地址     10. 0. 0. 1 - VPC 的 Router     10. 0. 0. 2 - Mapping AWS 提供的DNS     10. 0. 0. 3 - 保留位，未来可能会用     10. 0. 0. 225 - Network Broadcase Address 广播地址。 因为AWS 不支持广播， 所以也是保留位。          比如：         需要29个IP 地址， 需要用/26 ， 因为/26 有64 -5 = 59 个； 而/27 是32-5 = 27 ，不够             27 = 27                      区分public subnet 和 private subnet     pri sub - 这个subnet 中的资源通过绑定的route table 没有被连接到internet gateway, 那就是 private subnet   pub sub - 这个subnet 中的资源通过绑定的route table 被连接到internet gateway, 那就是 public subnet         还需要让EC2 实例有public IP, 并且实例所在的AG 允许， pub sub 下面的实例才能访问互联网          Internet Gateway (IGW):  在VPC level 让VPC 内的资源 （e. g. EC2 实例） 可以访问互联网 是AWS managed service, 自动伸缩， 高可用， 容灾， 不用我们管 IGW 是的最佳描述是 Fault Tolerant     An Internet gateway is a fault-tolerant virtualized resource with no visibility from the customer perspective.     IGW 是完全免费的     The Internet gateway, while it does not accrue any charges directly, does allow traffic to reach outside networks, and that throughput can possibly be charged based on the destination.     一个VPC 只能对应一个IGW 需要配置Route Table ， 让Public subnet 在Route Table 中指向IGW 才能连接互联网 Internet Gateway 有两个作用     作为route table 的目标， 让流量可以访问互联网   进行网络地址翻译 （Network Address Translation （NAT））         public subnet 下面的实例通过路由连接IG， 先被IG 进行了 NAT， 然后连接互联网。     private subnet 下面的实例先连接pub subnet 下面的NAT 设备进行NAT, 然后再连到pub subnet 下的IG 连接互联网           IGW 没有啥安全功能     The Internet Gateway has no built-in features for monitoring, whitelisting, or blacklisting traffic that passes through it.    Bastion Hosts 堡垒机:  IGW 解决了public subnet 应用可以访问互联网的问题 但是外网用户无法访问（ssh） Private subnet 中的资源(ec2 实例) , 需要一个堡垒机/跳板机 （放在public subnet ）作为跳板， 跳到private subnet 中来访问资源 安全组配置     Bastion Host的安全组需要打开（开放给外网访问者的IP , 端口是22）   私有子网的实例的安全组需要允许bastion host 访问    高可用的堡垒机方案是：     创建一个public NLB，该 NLB 链接到由 ASG 管理的堡垒主机所在的 EC2 实例上         Create a public Network Load Balancer that links to EC2 instances that are bastion hosts managed by an ASG          NAT Instance - network address translation （这个过时了， NAT Gateway 是代替者）:    用来让private subnet 的实例可以访问外网（但是外网无法访问private subnet 中的资源）     底层是EC2     NAT 必须放在public subnet 中     需要在EC2中关闭 Source/Destination Check     NAT 需要绑一个 Elastic IP     private subnet 的 route table 需要配置， 让private subnet traffic 流向 NAT     NAT 访问外网时， 会隐藏掉 EC2 的IP, 而是把NAT 的 Elastic IP 暴露了给外网     缺点          NAT instance 没有高可用/伸缩功能           和外网通信的带宽依赖于EC2的类型           需要手动设置Security Group.          Inbound: 允许private subnet 的 HTTP, HTTPS , SSH 访问     Outbound: 允许HTTP/HTTPS 访问 internet             优点/特色      NAT instance 支持 port forwarding   安全组可以和一个NAT instance 关联（NAT instance 是一个EC2 ， 所以外面可以套一个SG）   NAT instance 可以做堡垒机（hastion server）   NAT Gateway - NATGW (老师上课讲的是这个):  是NAT Instance的代替者 The NAT gateway is charged by the hour and for traffic throughput.  IPV4 only, IPV6 是用Egress-only internet gateway 实现 让private subnet 中的EC2 实例可以通过NATGW 访问 internet 支持更高的带宽， HA, 容灾， 不需要管理 ， serverless NATGW 在指定AZ中创建， 使用EIP NAT 必须放在public subnet 中 NATGW 不能被与它在同一个subnet 中的EC2 连接， 只允许非自己的subnet 的EC2访问 需要先有一个IGW, (private subnet → NATGW → IGW) 无需安全组 private subnet 的 route table 需要配置， 让private subnet traffic 流向 NATGW NATGW 访问外网时， 会隐藏掉 EC2 的IP, 而是把NAT 的 Elastic IP 暴露了给外网 NATGW 默认在一个AZ中是高可用的， 如果需要 支持 容灾 （fault-tolerance）, 需要在多个AZ中创建 NATGW NATGW和 NAT Instance 的区别  NAT gateway 不支持 port forwarding NAT gateway不能作为堡垒机 NAT gateway没有安全组 NAT instance 可以作为堡垒机, 因为他是一个 EC2 实例 NAT instance 可以有安全组, NAT instance 支持port forwarding NAT Gateway 是high availible, 但不是fault tolerant.      The NAT gateway is replaced by AWS if it fails, but there is a short disruption of service during the replacement.    VPC中的DNS解析:  比如public subnet 的一个 EC2 需要请求 google. com , 需要AWS提供域名解析服务。 决定 VPC 的 DNS 解析是否需要通过 Route 53 进行     是 （默认）         请求 Amazon DNS Server 169. 254. 169. 253 或者 VPC 预留的DNS 解析IP地址 xxx. xxx. xxx. 2          否         需要自己创建一个costom DNS server , 用来解析           DNS Hostname     EC2 实例的域名 ，有private 和 public         如果需要让EC2 有一个 public DNS Hostname, 需要在创建VPC 的时候打开 enableDnsSupport = true 的选项             不开启的话实例只有一个私有DNS Hostname       开启后， 每个EC2 实例都会带两个DNS Hostname, 一个 public hostname, 一个 private hostname                如果是要解析私有域名， 需要在Route53上同时打开enableDnsSupport = true 和 enableDnsHostname = true的选项             比如                 当EC2 实例想访问 web. mycompany. private的时候， 如果Route53 的 Private Hosted Zone 记录了这个域名对应的内网IP, 会就返回这个IP 给EC2实例， EC2 就可以访问内网中的这个IP （另一个EC2 实例）了                                    NACL 和 Security Group:    NACL - Network Access Control List          可以理解成Subnet 这一层的安全组           每个NACL 对应一个Subnet           NACL 对应的rule 会应用到所有subnet 中的ec2中， 不管EC2 如何设置SG           和 SG 的区别                NACL 是 stateless （能进来不一定能出去；能出去不一定能回得来）                 SG 只有 Allow 没有Deny, NACL 既有allow 也有 deny                 SG 是 stateful (只要能进来，就能出去； 能出去，就能进来)                 例子：              通过EC2实例 http访问某个外网IP地址; 这个外网IP 通过SSH访问我们的EC2实例                 NACL inbound rule 和 outbound rule 都需要单独设置允许。比如SG 更加严格         SG 只要设置inbound rule 或者 outbound rule 就可以了；                                        NACL Rule          每个rule 就是一个allow/deny 一个CIDR 的规则     每个rule 有一个 号码（1-32766）， 数字越小优先级越高     如果两个规则有冲突，那么数字小的那个胜出     建议每条rule 的number 间隔是100     刚创建的NACL 默认Deny 一切地址             因为新创建的NACL 有一条 number 是 * 的 deny rule , deny 了 0. 0. 0. 0/0                         Default NACL          这个Default NACL 是在创建VPC 的时候系统自动创建的default subnet 自带的NACL     Default NACL 允许一切 inboound 和 outbound 的访问的     不要修改它          NACL 和 Ephemeral Ports:    Ephemeral Ports 是当客户端与一个目标服务进行通信的时候，客户端对应的操作系统临时创建的一个端口， 目的是可以让目标服务返回信息时可以把这个端口号带回来， 让操作系统把response 和 request 对应起来。     当两个subnet 之间的服务相互调用的时候， 那么request 和 response 都要分别通过2个 NACL .      比如EC2的web 调用 Mysql ,         request 的 target port 是 3306, src port 会带一个随机的端口号（1024-65535）。     response 的 target port是1024-65535 ， src 就是mysql 的 3306          VPC Reachability Analyzer:    网络诊断工具， 用来诊断VPCs 两个 endpoint 之间的网络连接性     它通过网络配置信息建立一个模型， 然后模型判断是否连通 （不会真的发送信息）     比如两个EC2之间的通信需要经过Instance A → ENI A →SG A →SGB →ENI B →Instane B.      Reachability Analyzer 会找出失败的环节并图像化的展示出来。   VPC Peering:    两VPC的对等互联 (CIDR 不能重叠)      可以跨账户   可以跨Region      连接后效果是感觉两个VPC 就是一个VPC     按照流量带宽，按小时计费。     A，B，C 三个VPC ， 需要三个Peering   A VPC peering connection is a networking connection between two VPCs that enables you to route traffic between them using private IPv4 addresses or IPv6 addresses. Instances in either VPC can communicate with each other as if they are within the same network. You can create a VPC peering connection between your own VPCs, or with a VPC in another AWS account.      The （VPC Peering Connection） traffic remains in the private IP space. All inter-region traffic is encrypted with no single point of failure, or bandwidth bottleneck.       两个 VPC Peering 完成后， 还需要设置两个 subnet 下面的 Route Table 可以让VPC下面的EC2之间互相通信      不同账号的VPC peering 完成后， 在一个VPC 下面的EC2 安全组中可以设置allow 另一个VPC 的账号下的服务直接访问自己      VPC Endpoints:  允许您向其他 AWS 客户公开私有应用程序，而无需将应用程序公开到互联网，也无需建立 VPC 对等连接。 通过 Private Link 实现 AWS 对外的服务都会有一个Public url。 只支持在 region level， 不能跨region. 如果想跨region , 需要 peering connection Fault tolerant     The VPC endpoint resource is defined as a horizontally scaled, redundant resource, which means it is fault tolerant.     Endpoint 用于让EC2 实例可以通过内网直接访问DynamoDB, S3， CloudWatch , SNS 等AWS服务， 而不是通过EC2 →NATGW → IGW → DynameDB public url 的方式 好处是节省公网流量， 提高网络响应速度 EC2 → VPC Endpoint (Private Link) → DynameDB 需要设置 route table 确保DNS 解析设置没问题 有两种VPC Endpoint     Interface Endpoint         给Endpoint 帮一个 ENI 网卡（带private IP）, 两个服务通过IP 通信     支持EC2 访问大部分AWS 服务 （除了S3 和 DynamoDB）          Gateway Endpoint.          在VPC Endpoint上提供一个gateway,且需要在route table 中设置这个gateway 作为一个tag.      只支持 S3 和DynameDB          不管是interface 还是 gateway endpoint , 都是通过proxy 把流量连接到API endpoint上的    案例     公司希望通过SQS 解耦底层应用， 但是担心通过internet 访问SQS 底层绑定的组件有风险。         解决方案是： 用VPC endpoint 访问SQS          VPC Flow Logs:  获取进入IP 的流量， 包括     VPC Flow Logs   Subnet Flow Logs   ENI Flow Logs   还会收集 ELB, RDS, ElastiCache , RedShift, NATGW, Transit Gateway 等的网络信息    Flow logs 发送到两个地方     S3 - 用Athena 查看   CloudWatch Logs - 用CloudWatch Logs Insights 查看    Flow logs 语法主要部分     Versioin   srcaddr &amp; dstaddr 源和目标地址   srcport &amp; distport 源和目标端口   action (allow or deny)    题目案例 -（通过Flow Log 的 Action字段（allow /deny） 来troubleshooting 判断是 SG 的问题还是NACL 的问题）     Incoming request (to EC2)         如果inbound 被 reject 了， 那么 NACL or SG 都可能有问题     如果inbound accept ， outbound reject 了， 那一定是NACL 的问题（因为SG 是stateless， 不会阻止outbound）          outcomming request (from EC2)         如果outbound 被 reject 了， 那么 NACL or SG 都可能有问题     如果outbound accept ， inbound reject 了， 那一定是NACL 的问题（因为SG 是stateless， 不会阻止inbound）          Site-to-Site VPN:  当客户私有Data center 需要连接VPC 的时候，就需要建立一个 s2s VPN connection     客户的DC 需要部署一个 Customer Gateway (CGW)         可以是软件应用或者是硬件设备             CGW （with public IP）可以直连VGW       CGW (with private IP) 可以线连接客户的NAT (public IP)， 再连VGW                     VPC 需要部署一个VPN Gateway (VGW)         需要设置Subnet 的Route Table ,允许CGW 的IP访问     需要设置EC2 的 SG ， allow ICMP protocol 的inbound 访问, 才能在on-premise 的机器上ping EC2 instance           关键词     IPSec   VPN CloudHub:  客户有多个数据中心，每个数据中心都建立了VPN /DX连接 客户的多个 VPN /DX 连接到 AWS 的同一个(Virtual Private Gateway )VGW , 就是VPN CloudHub 好处     多个数据中心可以互联了   一条VPN不稳定，可以通过其他VPN 进行弥补， 提高整体稳定性    需要设置VPC 的 dynamic routing 和 route table 题目：     客户有多个DC, 都连了 VPN 到 AWS, VPN 供应商不稳定， 需要创建一个backup connection, 通过公网连接多个DCs ， 方案就是 AWS CouldHub         理由： CloudHub 是多个VPN 连接到同一个VPC , 并且让多个VPN之间也可以互相访问， 有效解决了某个VPN网络不稳定的问题。          总部用了Direct Connection 连接到AWS VPC, 分公司用 site-to-site VPN 连接到AWS VPC, 如何让总部和分公司互联互通。方法是AWS VPN CloudHub         CloudHub 让 多个通过site to site VPN, direct connection 连接同一个VPC 的 on-premises环境可以互通          VPC 连 VPC (Endpoint Service ):  暴露你的VPC 中的服务给其他的VPC Peer 是对等双向的； Endpoint 是单向的     方案3： 通过Endpoint Service 实现         建立一个Private LINk,     访问端的VPC 建一个ENI     服务端的VPC 建一个NLB, 两端 就可以通过VPC Endpoint 来进行访问。          方案1 ： 将服务暴露到外网， 谁都能访问。         比较复杂， 且会有额外流量费用          方案2 ： VPC之间建立 VPC Peering connection. （Peer 是对等双向的； Endpoint 是单向的）         一旦VPC 比较多， 要建立的Peering 数字会大幅增加          Transit Gateway 交通枢纽 Gateway:    如果多个VPC 需要整合到一起， 需要建立多条Peering connection , 比较复杂，     可以用过Transit Gateway 作为交通枢纽， 所有VPC、DXGW, VPN connection 都连到 Transit Gateway      hub-and-spoke (start) 星型结构      cross region     VPC transit gateway are charged hourly and for traffic throughput ， 挺贵的     Cross account sharing      做了Transit Gateway 还不够， 想要分享资源， 需要通过 RAM 做一个 shared service VPC   Resource Access manager (RAM) 将一个账号下的资源通过Transit Gateway 分享给其他账号， 具体请看 Resource Access manager 的介绍      需要设置路由表来决定到底谁（vpc）和谁(vpc)可以沟通     关键词      IP Multicast         IP 组播， 将IP数据包传输到一个组播群（Transit Gateway）中，所有该组的成员都可以收到这个数据包的信息。           VPC Traffic Mirroring 流量镜像:    是VPC 的一个服务， 建立一个Traffic Mirroring,     将 VPC 中通过 ENI 的流量， 通过 Traffic Mirroring （可以过滤filter traffic）把流量镜像到一个 NLB 或者 ENI      NLB 下面 是 Auto Scalling Group , 下面是EC2      获取流量      源： ENI   目的地： ENI 或者是 NLB      源和目的地 可以是同一个VPC ，也可以是不同的VPC ,      VPC之间镜像， 需要建立Peering      场景：      流量内容检查   网络风险监控   trouble shooting      题目：      问如何获取VPC 内通过IP 的流量         答案是 VPC Flow Logs, traffic Mirroring,          IPV6 in VPC:  IPV4 默认打开（关不掉）， IPV6 默认不打开     打开后EC2 实例最少拥有一个Pirvate IPV4 和 public IPV6    IPV6 都是public IP 地址 题目：     如果没有办法launch EC2 实例， 是因为IPV4 被用光了， 解决方法就是给subnet 增加一个新的Cidr IPV4 地址段   Egress-Only Internet Gateway （EIGW）:  只出不进的、针对IPV6 的Internet Gateway Egress-only internet gateway 是在VPC level， 并不是在public subnet 里面 类似NAT Gateway ， 帮助private subnet 中的有IPV6 的instance 访问外网用的     IPV 4 的 instance 还是访问 Nat gateway 去访问外网    需要更新Route Table, 让private subnet 将流量转到Egress-only internet gateway 上     ::/0 表示所有外网的IPV6 的地址    VPC Console Wizard 支持的内容:  VPC with a single public subnet VPC with public and private subnets (NAT) VPC with public and private subnets and AWS Site-to-Site VPN access These three options are valid configurations supported by the Amazon VPC console wizard.      不支持 VPC with a private subnet only and AWS Site-to-Site VPN acces   不支持 VPC with public subnet only and site-to-site VPN   ELB: ELB:    暴露唯一的入口 - DNS给下游的target group (EC2 instance, IPs, container, ALB(上游需要是NLB)) . NLB 多提供一个static IP      要想让实例看到客户的IP 需要通过header 的 X-Forwarded-For 来透ALB传递过来      health check (特定端口和网页路径（router），检查200)     stickiness (CLB and ALB only)      cookies         别用 AWSALB, AWSALBAPP, AWSALBTG             ELB 流量分配：      ELB 可以分配流量给同一个AZ下面的4个实例   ELB 不能分配流量给不同的两个 region 西面的4个实例（每个region 两个实例）   ELB 可以分配流量给同一个region 下面两个AZ的实例 （每个AZ 两个实例）      cross-zone (cross az)      打开cross-zone功能的LB ,不管每个az有多少个instance, 都可以让每个instance 获得同样的流量分配   没有cross az的LB, 每个az均匀分配， 但是下面的instance 就不均匀了   ALB 默认开启， NLB和CLB默认关闭    cross-region     ELB 是region scope. 如果需要跨region ,需要借助于Aws Global Accelerator (AGA) 的 跨region 能力；在AGA中创建endpoint group， 把每个region 的 ALB 包含进去。      Security Group     Type          CLB (classic) L4&amp;7           ALB L7 ,                对外暴露的是一个DNS (URL)                 用二级域名、路径、参数来路由                 port mapping for dynamic port                 ALB 可以通过Cognito 来给要访问你的应用的用户做安全认证              Cognito User Pool 可以让用户通过知名的社交应用的账号（idPs）来登录我们的应用                         NLB L4          对外暴露的是一个public IP 地址，允许外部应用访问这个IP地址     one static ip per az, support assign elastic ip     NLB 后面可以是Instance ID 作为target             流量通过被primary network interface 指定的 primary private IP 地址被路由到实例                         GLB(gateway ) L3          流量需要进到AWS的第三方应有跑一圈， 然后再出AWS              keyword in the exam: GENEVE, Port 6081, Transparent network gateway       target group:  可以是EC2实例、容器task、Lambda、private ip     这里的IP 一定是 Private IP    ALB health check 是在 target group levelSNI Server Name Indication:  解决了一个LB下面有多个web 服务需要提供多域名证书的问题。 方法是在SSL握手的时候就要客户提供域名， 然后根据域名找到对应的SSL证书 ALB，NLB onlyConnection Draining (Deregistration Delay):  可以设置当一个实例被发现不健康之后 或者 被terminate 之前， 是否还转流量给他，以及要等多久在terminate 1 and 3,600 seconds (the default is 300 seconds) 案例： 一个税务软件运行在EC2 实例+CLB + ASG 上， 软件需要花费10分成才能给出请求者答案。 当ASG scale-in 的时候，如何做才能不中断请求者？， 答案是设置 Deregistration delay (connection draining) 为10分钟，让ELB在实例终止前停止发送请求，确保实例有时间可以完成所有未计算完的税务任务。     默认当ASG 要scale-in 并下线一个实例的时候， ELB 会在实例终止前300秒（5分钟）就不会发送请求给他了， 但是这个例子可能还有一个没完成的计算（10分钟）在运行中   通过把 Deregistration delay 设置成10分钟， CLB 就会在实例terminate 的10分钟之前停止发送请求给实例。 10分钟后ASG terminate 这个实例。   ASG auto scaling group:  设置 min, max, actual size 来让实例个数随着请求的多少来增减 Unified ASG 可以管理的服务包括：     EC2   Spot fleets   DynamoDB   Aurora Read Replica   ECS on Fargate         不包括RDS, RedShift             launch configuration/template 负责提前预设好需要加载的实例的配置信息      Launch Configuration (旧的): 如果需要update ASG, 需要重新建一个launch configuration. 原来的改不了了。   Launch Template (新的)         可以有多个版本     可以建on-demand or spot instance           通过CloudWatch alarm 监控 ASG 的实例的性能指标 比如CPU 来决定scale out/in ASG的IAM roles 也会分配给它创建的新的EC2实例 LB 把一个实例设置成了unhealthy, ASG就会terminate 这个实例 ASG下的实例，不管什么原原因被terminate 掉了， ASG都会自动创建一个和原来的实例一样的新实例   Scaling policy      Dynamic Scaling policy         Target Tracking Scaling, 保持CPU40%, or 保持1000个请求数             擅长处理突然的大流量导致的实例性能下降                Simple/Step Scaling, CPU大于70%加两个， 小于30%减一个     Step scaling applies “step adjustments” which means you can set multiple actions to vary the scaling depending on the size of the alarm breach. When you create a step scaling policy, you can also specify the number of seconds that it takes for a newly launched instance to warm up.      Scheduled Actions, 周五10点加10个实例             设置 desired capacity 的数量为10       也可以设置 range 为 max=xxx, min=xxx. 但如果题目指定的是固定数字，就需要用 desired capacity.                 如果两个policy 都触发了， 会 launch 实例数量最大的那个 policy ，不管是 scall-out 还是 scall-in          Predictive Scaling , AWS AI 帮你增减         AWS recommends leaving Predictive Auto Scaling in forecast-only mode for at least 48 hours , but each application is unique, and some applications might require longer.      案例：             EC2 实例带有ASG, 后面是DynamoDB, 因为每周2次但是不定期的高流量，需要如何设置ASG                 Predictive Scaling, 可以帮组我们学习流量曲线，自动伸缩                                       Metrics 参数      CPUUtilization ; CPU使用率   RequestCountsPerTarget: 每个ect2 Group 的请求数   Average Network In/Out, IO 是否平均   Custom metric ， 客户自定义的监控指标， 比如应用请求数据库的次数等。    Scaling Cooldowns . 发生scaling 之后需要个冷静的时间， 这个时间内不要再做scaling   ASG Default Termination Policy :      如果有多个AZ，从最多的实例的那个AZ下手    在最多实例的AZ中挑一个 launch configuration 最老的那个下手   对于不健康的实例的处理逻辑      ASG 启动了一个新的 saling activity, terminate 掉不健康的实例，然后再创建一个新的实例代替被terminate 掉的实例         有一个不健康了， 那么就干掉他，然后换个新的     ASG 干活是一个一个来的（顺序是先干掉，再增加），不会同时进行             对于rebalancing的处理逻辑(对人为干预的情况进行rebalancing)      当有人手动删除了实例，就会造成unbalancing   因为AZ unbalanced, ASG 会自动补偿（compensate）重新平衡（rebalancing），ASG 会launch 两个新的实例，不会在performance or availability 这块妥协。         因为2个实例被devops团队干掉了， 所以补充两个新的     Rebalancing 的顺序是 先增加， 再干掉，不会同时进行。           Lifecycle Hooks： 当正在进行scale in/out 的时候， 可以设置一个hook, hook 可以调用额外的程序， 比如对意外被terminate 的实例在删掉之前写一些日志来trouble shooting   ASG的 HA      就算ASG 跨了3个 AZ，最小的HA 依然是2个实例。   当我们设置ASG 最小的 capacity 是2 时， ASG 会将两个实例分到不同的AZ中。 当流量上升，ASG 会将第3个实例放到第3个AZ中， 当流量下降后，还是会保持2个实例。   一个实例有问题，但是没有被ASG terminate 掉的可能性:    Grace period      ASG 在启动一个新的实例时， 会用 HealthCheckGracePeroid 设置一个时间段，让ELB 不要检查这个实例的健康状态， 直到grace peroid 过期   grace period 的目的是让实例启动过程中可以把所有的服务启动起来后，再接收ELB的流量   Grace Period 这段时间内，就算实例的状态不正常， 被ELB 检查出来了， 也不会被terminate 掉      Impaired status      如果一个实例在 impaired status( 受损状态，需要时间自我修复 ) 状态， ASG 不会马上terminate 掉它， 会等几分钟， 等待它自我修复。      实例未通过 ELB 运行状况检查状态 （The instance has failed the E   LB health check   status）      ASG 默认情况是使用CloudWatch Alarm 监控 target group 的 健康情况 . 不会使用ELB 的 health check 结果去terminate 一个实例。 因此，ASG 不会终止未通过 ELB 运行状况检查的实例。   一个实例有问题，ALB 删了她，但是ASG 没有换一个新的上来的问题：:  ASG 在用基于EC2 的健康检查， ALB 在用基于ALB的健康检查。     想要避免题目中的问题，就需要把ALB 和 ASG 都设置成 ALB Based health check   Direct Connect (DX): Direct Connection (DX):  客户有钱，有混合环境， 且需要高带宽、稳定的网络连接 1Gb and 10Gb throughput AWS 在一些国家地区提供了有专线（当地电信供应商提供）的 location ,比如香港有1个，台湾有2个 在客户的私有网络和AWS VPC 之间建立一个独享的私有网络     网络需要建立在 特定的 Location 上    通过DX, 可以同时访问AWS上的public 和 private resource 需要在VPC 上建立一个 VGW (Virtual Private Gateway)     virtual private gateway 是一个 highly available 的服务 ，有 n个9s. 通过硬件backup 在同一个region 的两个data center, 符合HA，但是不是 fault tolerant    支持IPV4 和 IPV6    DX type      Dedicated connection （1-10G 的带宽）         向AWS 提交申请， AWS再去找当地电信合作伙伴完成          Hosted Connection （50m， 500m … 10g 选项）         向电信商提交申请             建设好DX 一般要1个月以上， 等不急的就不要用了     Encryption      DX 因为是专线内网连接，默认不加密传输数据   可以通过 DX + VPN 的方案来给传输的数据进行加密      DX Resiliency      支持客户data center 数量 增长         每个date center 可以建立一个新的 DX 去连接所在的Region 的同一个AWS VPC          支持DX 的高可用         同一个DX location可以建设两个connection ， 互相backup          最大化的resiliency         设置两条DX, 与多于一个DX location 的不同设备进行终端连接             Opt for two separate DX connections terminating on separate devices in more than one DX location.                      Direct Connect Gateway:  让客户的date center 通过DX 连接不同Region 的多个VPC ， 需要使用Direct Connect Gateway 多个VPC 之间设置了Direct Connect Gateway     客户先通过DX 访问到第一个VPC ,然后再通过DX Gateway 访问其他region的VPC    网络成本 （xxx$/每GB）:    同一个AZ下的实例间内网进行通信 free     不同AZ下的实例通信      内网 0. 01   外网 0. 02      不同 Region 的实例通信      外网 0. 02      Egress &amp; Ingress (出口流量/进口流量)          AWS 只对Engress 计费。 Ingress 基本上是free           尽量减少engress 的流量          比如用户要从AWS RDB请求100M的数据到客户端进行计算， 优化方法是把计算在AWS内部做完， 把结果返回给客户端             把能预先做的在VPC内部做掉， 只把结果发出去                         S3          Ingress 0 ,     Egress 0. 09             S3 transfer Acceleration +0. 04 ~ 0. 08                         S3 Cross Region Replication          Egress 0. 02              S3TA （Transfer Acceleration） 只有成功了才收钱， 不成功不收钱           CloudFront          Egress 0. 085             省钱方法      尽量用 private IP 通信   尽量在同一AZ中用内网处理大流量的数据通信（但会有HA的问题），不要跨AZ   EC2 访问 S3/Dynamo DB的时候要通过 Endpoints， 不要走公网（EC2→nat gateway → internet gateway → internet → S3）   把数据计算在内部做完， 把结果发出去   监控: CloudWatch: CloudWatch:  必须装了CloudWatch agent 才能监控   CloudWatch 给每个AWS服务都设置了 Metrics     需要在每个AWS服务（比如EC2）上安装一个CloudWatch Agent, 才能把日志传回来     Metric      是一些变量， 比如CPUUtilization, Networkin …   属于某个 Namespace   Demension 是 Metric的属性(最多10个)， 比如         instance id     environment     …          标准Metric         监控频率             默认5分钟刷一次standard Metrics       开启 Detail Monitoring 可以每1分钟刷一次标准Metrics                     Custom Metrics 自定义指标         比如监控内存，磁盘，登录用户数等非标准Metrics     使用 API PutMetricData 来设置     监控频率 （间隔时间）             可以使用 StorageResolution API 设置， 两个参数                 标准60秒         high resolution: 1 秒                      需要确保EC2的时间是正确的                     Metrics filter expression (过滤表达式) 可以触发 CloudWatch Alarm , 比如发现error 就报警      Dashboard      CloudWatch 是 global service, 所以可以设置dashboard 看多个account 和 region 的数据   可以设置dashboard的时区   可以设置刷新间隔10s 到 15m   可以分享给非AWS账户， 但需要通过Cognito SSO 登录查看      CloudWatch Logs ,      通过监控、收集其他服务产生了日志   Log Groups   Log Stream   可以定义Log 失效时间， （永不过期， 30天等）   Subscription Filter         一个用于过滤日志的 instance, 通过Metrics filter expression (过滤表达式) 进行过滤。          CloudWatch Logs Aggregation (日志整合)         可以把不同region 的账号产生的cloudwatch logs 进行整合。 整合后再发给下游服务          上游（被监控对象）         EC2 (log默认不会发送到CloudWatch, 需要安装CloudWatch Agent, 并设置EC2权限)     SDK     CloudWatch Logs Agent （old）     CloudWatch Unified Agent (new 支持更多的Metrics)     Beanstalk     ECS     Lambda     VPC Flow Logs     API Gateway     CloudTrail     Route53          下游 （CloudWatch logs 可以发送给:）         S3     Kinesis Data Stream     Kinesis Data Firehose     Lambda          CloudWatch Alarm:  用于触发报警， 单位包括     抽样 sampling   %   Max   Min   etc.     3个主要的目标     EC2   EC2 Autoscaling   SNS    CLoud Watch Alarm直接就可以重启，停止， 启动， terminate EC2， 不需要额外其他服务参与。 Cloud Watch （Alarm） 直接就可以调用SNS 给管理员发通知， 不需要调用Lambda 服务。CloudWatch Event:  可以被触发的事件 (夹到AWS 服务当中) 比如：     EC2实例启动，CodeBuild失败，   可以和CloudTrail 集成来夹到 API call 当中    可以放到Cron 中， 定时启动 事件触发后， 会有一个Json文件发送到target 服务中     Target 服务包括         Lambda, ECS task     SQS, SNS, Kinesis,     Step Functions, CodePipeline, CodeBuild     SSM, EC2 Action          EventBridge: EventBridge:    是CloudWatch Event 的升级版     触发来源          Default event bus : 默认事件总线， 就是CloudWatch Event （AWS 服务）触发events           Custom Event bus: 定制事件总线，我们自己的应用触发Event           Partner Events： 第三方事件， 从 SaaS 服务或第三方应用接收events          第三方应用包括             Zendesk       DataDog       Segment       Auth0       …                        EventBridge Schema Registry     EventBridge 是唯一可以和第三方SaaS partner 整合的 event-based 服务，是SaaS 和 AWS 服务的沟通桥梁      AWS 服务包括 SNS, Lambda, SQS， Kynesis KDS KDF等等      考试：      看到有SaaS， 第三方 ，feed, 要解耦， 那么就选Event Bridge   CloudTrail: CloudTrail:    如果有人删了一个资源， 就去CloudTrail 查     CloudTrail 为AWS账户提供 治理，审计， 合规     可以查看账户的历史事件，包括      Console   SDK/API   CLI   IAM User/IAM Role (AWS服务的调用)      可以把log 发到 CloudWatch Log or S3     跨Region     默认保留90天     上游      CloudTrail Events      下游      展示在Console 让管理员看到(只 能保存90天）   存到S3（怕删，就放这， 用Athena）, or CloudWatch log   发到Event Bridge 中 （让别人知道）   CloudTrail Events:  CloudTrail Event 是 Cloud 的上游，最终会将Event 传给CloudTrail 。 分三种     Management Events, 管理型事件         对服务进行的操作， 比如删除S3 Bucket     默认会被记录          Data Events, 数据型事件         对数据进行的操作， 比如删除S3 Bucket 中的一个object.      默认不会被记录          Insights Events         帮助我们检查账户中的不寻常的操作记录， 尤其是Write Events     发现不正常就触发CloudTrail Insight Events          AWS Config: AWS Config:  查看所有AWS服务的配置信息和历史记录 给AWs资源做审计和合规     是验证配置的合规， 而不是人的合规。    记录所有的配置和变化 一些常问的问题     是否有没有限制的SSH访问安全组   bucket 是否有外网访问   ALB 之前的配置变化是什么    可以设置将变化发给SNS 可以跨Region 可以将配置数据存到S3中， 通过Athena分析 Config Rule,     是验证规则   利用AWS 对超过75个服务的config 设定规则         比如             验证是否所有的EC2 都用了t2. micro       验证EC2 的 84端口是否对外暴露                     不会阻止action 的发生   Config Rules- Remediation         config rule 虽然不能阻止action, 但可以将发现的问题发送给 Auto-Remediation Action 处理， 将配置改回原来正确的样子          Config Rules - Notification         可以将发现的问题发送给 EventBridge/ SNS 进行处理           Pricing: no free tier     $0. 003 per configuration item recorded per region   $0. 001 per config rule evaluation per region   CloudWatch, CloudTrail, Config 的区别:  CloudWatch 基本上是监控系统性能指标 CloudTrail 是对人的监督。 是监控AWS账号在Console, CLI, SDK/API 对AWS服务操作的跟踪记录。 Config, 是对服务的配置信息的监督。 记录配置的变化， 通过rule 来检查资源的合规性， 是对资源的检查， 不是对人。 例子 （ALB）     CloudWatch 监控ALB 的连接进程数   CloudTrail 监控谁通过API call 修改了ALB 的配置   Config 通过rule 监控ALB 的配置是否被修改   X-Ray: X-ray X光:  可以跨账户调试和跟踪数据， 并在一个账户中集中可视化 AWS X-Ray 可帮助开发人员分析和调试生产、分布式应用程序，例如使用微服务架构构建的应用程序。 使用 X-Ray，您可以了解您的应用程序及其底层服务的执行情况，以识别和排除性能问题和错误的根本原因。 X-Ray 在请求通过您的应用程序时提供端到端的视图，并显示应用程序底层组件的地图。 您可以使用 X-Ray 跨 AWS 账户收集数据。 X-Ray 代理可以承担将数据发布到与其正在运行的帐户不同的帐户的角色。 这使您能够将数据从应用程序的各个组件发布到中央帐户。 案例：     提到一个多业务部门的公司， 每个部门以自己的AWS账户， 希望可以跨账户调试和跟踪数据， 并在一个账户中集中可视化， 最好的方案是 ： X-ray （x射线） ， 跨账号跟踪调试数据   CloudTrail, CloudWatch, VPC Log, AWS Config, X-Ray 的区别:  CloudTrail 是跟踪人的行为的； CloudWatch 是监控AWS服务的性能指标的； VPC log 是跟踪进出VPC的网络IP的 AWS Config, 是对服务的配置信息的监督。 记录配置的变化， 通过rule 来检查资源的合规性， 是对资源的检查， 不是对人。 X-ray - 跨账号跟踪调试数据， Debug。安全: KMS: KMS:    提供 Encryption 服务     每次请求KMS 只能加密4k的数据， 如果需要加密更大的数据， 需要使用 Envelope Encryption     两种类型的Key,     第一种：Aws Managed Service keys      AWS 已经创建好的key, 不需要我们知己创建， 直接就可以用来去加密EBS, S3, RDS 等      第二种：Customer Master Key (CMK)          自建的Key          CMK 不能直接被删掉，会进入pending deletion 状态。 7 days up to a maximum of 30 days. The default waiting period is 30 days。 误删除可以找回来              分如下两种           Symmetric （AES-256 keys） 对称加密                一个key 进行加密和解密                 只能通KMS API 来使用这个key                 场景：              AWS 内部不需要外部用户访问的数据，可以用对称加密                         Asymmetric (RSA &amp; ECC key pairs) 非对称加密          两个key, 一个公钥，一个私钥     加密用公钥， 解密用私钥     公钥可以下载，私钥通过KMS API 访问     场景：             在外部的用户不能使用KMS API, 就需要用对称加密， 下载个公钥加密， 然后到了AWS 再用私钥解密                     KMS Key Policy:  控制访问KMS Keys, 类型     default KMS key Policy                   custom KMS key policy         定义可以访问KMS Key 的用户和角色     定义谁可以管理key     在跨账号访问KMS Key的时候可以用          KMS Automatic Key Rotation:  自动key 刷新     只在客户管理的CMK 使用   开启后，每隔1年刷新一次   刷新后Key ID 保持不变   老的key不会被删掉， 还会保留， 可以解密原来用老的key 加密过的内容    手动key 刷新     刷新时间自己定   新的key 有另一个不同的ID         最好新老key 使用同一个 alias , 避免修改代码          分享Encrypted 内容给其他account:  分享了用KMS (CMK)加密过的内容给其他账号， 也需要把 KMS CMK 分享给人家 场景     EBS   SSM Parameter Store: SSM Parameter Store:  用于存储机密信息     最简单的方法： 也可以通过把用户名密码放在环境变量中加密保存，然后在通过run time 来读取并解密。   Parameter Store 是更高级的保存方法    是serverless 服务 使用KMS 进行加密 有version 功能 可以给CloudWatch 发 Notifiction 存放方式 例子展示     /my-department/         my-app/             dev/                 db-url         db-password                      prod/                 db-url         db-password                                     Parameters Policies （针对高级参数）     可以设置TTL, 过期后提醒需要对机密信息进行更新   会发送notification 给 cloudWatch Event   Secrets Manager: Secrets Manager:  存放机密信息， 比如数据库的用户名密码等明文配置信息 比 Parameter Store 更适合存放数据库的密码 和 SSM Parameter Store 很像，区别是     强制在xx天后进行更新   帮你自动更新机密（用Lamdba）   已经整合了RDS (mysql, postgresql, aurora)   使KMS 加密    考试关键字     如果考试中问到RDS 的密码需要保密管理， 那么就是 Secrets Manager   CloudHSM: CloudHSM - Hardware Security Module:  KMS 使用软件进行加密 CloudHSM 是AWS提供的， 使用专用的硬件模块来管理加密密钥并完全控制它们 让客户通过CloudHSM 硬件来完全自助的管理自己的加密key 客户的应用需要安装CloudHSM client softward 对应的AWS 加密服务是 SSE-C, 因为CloudHSM 是管理自己的key , 而不是自己加密，加密解密还是在AWS上用SSE-C ClouHSM 可以和其他AWS 存储服务集成，比如EBS, EFS 等 complete control of encryption key lifecycle management.  immediately remove the key material and audit key usage independently of AWS CloudTrail should integrate with other storage services that will be used on AWS CloudHSM HA     一个CloudHCS client, 两个CloudHCM (服务端)   Firewall Manager: Firewall Manager:  AWS Firewall Manager 是一项安全管理服务 允许您跨 AWS org 中的账户和应用程序集中配置和管理防火墙规则。 它与 AWS org集成 支持的服务     AWS WAF   AWS Shield Advanced 保护   VPC 安全组   AWS Network Firewalls (网络防火墙规则)   Amazon Route 53 Resolver DNS Firewall rules   WAF, Shield, Security Group, Firewall Manager 的总结:  WAF 为 ALB, API Gateway, CloudFront设置规则 Shield 为 ALB, CLB, EIP, CloudFront 防止DDOS Security Group 为VPC 内 EC2, 带ENI 的资源 提供 inbound outbound 的限制（allow） FirewallManager 是在org level 对整个公司的防火墙做统一配置管理AWS Shield: AWS Shield:  防止DDos 攻击的工具 分两种     AWS Shield Standard         免费， 可以防御网络层 Layer3, Layer4 的 SYN/UDP 攻击          AWS Shield Advanced         3000刀每月， 可以针对EC2, ELB, CloudFront, Global Accelerator, Rout53进行重点防御     可以和AWS 的 DDOS Response Team 直接联系           可防御对象包括     Route53   CloudFront   ALB   CLB   EIP   WAF: AWS WAF - Web Application Firewall:    保护web 应用避免恶意攻击（layer 7 for http ）     web应用层保护     WAF 可以部署在      ALB   API Gateway   CloudFront      定义了 Web ACL   (Access Control List)      ACL 是WAF 部署在服务上的具体配置   ACL 的 Rule 包含了         IP 地址     HTTP Header     HTTP Body     URI String          可以预防的攻击包括         SQL Injection     Cross=site scripting (XSS)               通过 Geo_match 来blok 一些国的访问   通过 Rate-based Rule 防止DDOS 攻击 （overwhelming attack ((过载攻击))）（通过统计并发的事件）      例题：      应用部署在了EC2上， 包含了敏感个人信息， 需要防范各种恶意攻击。 公司选用了 WAF. 正确使用 WAF 的方案是？         创建一个CloudFront distribution for EC2, 部署 WAF 到 CloudFront 来提供必要的安全保证             WAF 可以在 edge 端就完成安全防护，危险不会触及到实例。       WAF 也可以运行在ALB 和 API Gateway 上面s                     关于允许或者不允许某些国家的访问:  需要block 掉某些国家的访问， 用WAF, 和 CloudFront Geo Restriction ， NACL 都可以     WAF 是从VPC 的角度来block, 通过IP的方式 block 某个国家， 通过白名单允许某个国家。   Geo Restriction 是从edge 的角度block . 不属于VPC. 如果题目中没有提到CloudFront 、edge 之类的字眼， 那就选WAF 吧   NACL 是禁止IP地址进入   关于block 一个国家，但又需要允许这个国家的一个IP的访问: 公司的应用跑在AWS上 ALB + EC2 + WAF , 希望可以block 两个国家的访问应用， 但还需要其中一个国家的一个开发人员可以访问应用。 可以捆绑的两个解决方案是：  Use WAF geo match statement listing the countries that you want to block Use WAF IP set statement that specifies the IP addresses that you want to allow through 解释：     WAF geo match - 把想要block 的国家名字放进去就可以了   WAF IP set - 把想要允许的IP放进去就可以了   GuardDuty: GuardDuty - 人工智能保安:  人工智能的的威胁发现， 保护AWS account 主要监控网络层的用户访问 需要输入数据源包括     CloudTrail Logs   VPC Flow Logs   DNS Logs         好记的方法就是进大门(DNS log)， 进小门(VPC log)， 进房间（CloudTrail Events）           发现异常后， 通过CloudWatch Event rule 发送notification 给 SNS 或者 Lambda 处理 可以保护 加密货币–Cryptourrency AttacksInspector: Inspector:  针对EC2 实例的应用安全分析工具 应用实例层的安全保护 可以分析OS 漏洞, 网络访问 需要在instance 上安装 Inspector agent 分析后，会得到一个report , 列出所有的弱点 可以发送通知给SNS Inspector 的 security assessments 可以检查 EC2 上的 vulnerabilities (弱点/漏洞)Macie: Macie:  人工智能产品， 发现隐私数据并对数据进行保护 数据层面的安全保护 PII (personal identifiable information) 主要服务对象是S3 Bucket信息流: SNS: SNS:    通过订阅模式把一个消息发给多个接收者     pub (Topic) &amp; sub     Topic 是 SNS 消息发送者      可以有10万个topic   类型         standard     FIFO             进入topic 的消息是按顺序进入， 出去也是按顺序被订阅者接收到       主要目的是为了让SQS FIFO 作为订阅者可以按顺序处理消息       Standard SQS 不能订阅 FIFO SNS       SNS Fifo 有去重功能， 两种方法                 由publisher 给每个消息指定deduplication ID         由SNS FIFO 通过MD5 message content 来自动生成deduplication ID                      关键字：                 Odering         Deduplication (去重)                                       Sub          可以有1千万个订阅者，           会一次性拿走所有在SNS中的消息           Message Filtering     可以帮助订阅者过滤掉不需要的消息          场景： 发送订单到SNS, 后面接fan out 到SQS, 每个SQS 通过Message Filtering 获取不同类型的订单分别进行不同类型的处理             上游（发送信息给SNS）      CloudWatch Alarm   ASG notification   S3 bucket event   CloudFormation state change   etc. .       下游 （订阅者）      SQS   http/https   lambda   email   SMS   mobile notification   如果使用SNS来解耦，最 appropriate 的 订阅者是：         SQS     http/https             Encription      in-flight HTTPS API   at-rest KMS, client-side encription      Security      IAM plolicy 控制访问SNS API (user leve)   SNS Access Policies ( 和 S3 Access Policy 一个原理) （service level）         支持跨账号访问SNS     控制其他服务发送信息给SNS Topic的权限          fan out 模式:  原理     一个SNS 后面接一堆SQS   每个SQS 接收到同样的消息， 但是每个SQS 负责处理不同的任务    好处     完美的decoupled 模型   把一个工作分解成不同任务， 并行处理， 加快速度   防止数据丢失         没用Fan out 模式， SDK 发送信息给多个SQS 是轮询方式， 没轮询完挂了，就会造成有的SQS 没收到信息。     用了Fan out 模式， SDK 发个信息给SNS, 所有SQS 都会收到SNS的推送， 不会丢数据。           需要让SQS access policy允许 SNS 发送信息给它 Enhanced Fan Out 模式     Kinesis + SNS + SQS   SQS: SQS:  capacity     自动伸缩， 不需要担心吞吐量   SQS 可以支持对不同吞吐量的微服务，不同处理速度的微服务进行解耦， 增加了 failt tolerance    pricing     请求次数+数据传输量 按月收费         次数：100万个以后按次收费，越多单价越便宜     传输量：进不收费，出在10T以后收费，越多单价越便宜           Producer &amp; Consumer 发送者、接收者     可以增加Consumer 的数量来增加处理的吞吐量， 比如用ASG 里面的EC2 作为 consumer 处理SQS中的消息。   通过CloudWatch (Queue Length Metric &amp; Alarm) 通知ASG 自动添加EC2    decouple applications API     SendMessage API   DeleteMessage API    类型     Standard Queue         消息默认保存4天， 最多保存14天     最大消息size 256K     不限消息个数和吞吐量     消费者一次最多取10条     标准SQS不能直接修改成为FIFO， 只能删掉，建个新的FIFO     只有Standard SQS 支持 S3 Events, Fifo 不支持 S3 Events          FIFO         first in first out     吞吐量限制在300/s     但如果使用批量（batch）的方法，可以每秒做10次（max）批量操作, 就可以达到3000/s             假设需求是要每秒1000次， 那么就设置4个批量（300*4=1200) 就可以满足需求了。                以. fifo 结尾     如果要把standard queue 改成 Fifo 需要先把原来的删了，建个新的fifo     场景：             需要顺序处理的消息                      Encryption     in-flight , 使用 HTTPS API   at-rest, 使用KMS keys,也接受客户的Client-side encryption    Security     IAM policies 让用户访问 SQS API （user level）   SQS Access Policy (类似S3 bucket policy) （service level)         允许其他服务（SNS, S3 …）访问SQS     可以允许跨账号访问SQS           Message Visibility Timeout     consumer获取消息后，默认30秒内其他人看不到，30秒后没处理完会被其他consumer 看到。 可以考虑用ChangeMessageVisibility API延长 timeout 时间   timeout 时间太短可能会造成同一个消息被处理两次    场景：     处理流程很长的程序， 可以解耦， 分成前后端两部分， 中间用SQS处理   比如视频处理应用   Dead Letter Queue:  相当于queue垃圾桶 消息保存在DLQ的时间可以设置成14天比较好 场景     一个message 被处理多次没有成功， 达到阈值（MaximumReceives）次数后会被扔到DLQ中   用于debug 一些特殊消息   Message timer (message level):  可以设置某个message 到了queue之后在固定时间后再被消费者看到。     默认是0秒，最大是15分钟   Delay Queue （producer 触发） (queue level):  message 进入queue 之后一段时间， consumer 才能看到并接收 所有的message 到了这个queue 都要等待同样的一段时间才能被消费者看到 默认是0秒， 马上可以看到, max 是15分钟 可以在queue中修改默认值 producer 可以通过参数 DelaySeconds来修改默认值SQS 有 shot polling 和 long polling. :  shot polling 是默认的， 消费者定期去拿消息， 没有拿到就返回。 long polling 是消费者去连接SQS， 并等待一段时间， 拿到数据返回(或者超时返回) ， 更详细解释看下面Long Polling SQS 的付费机制是消费者的连接次数， 所以long polling 可以在更少的连接次数中获取数据， 所以更节省成本Long Polling （consumer 触发）:  consumer 可以在message 还么到到queue之前就在那里等着 好处是高效的处理消息， 降低延迟 可以在queue中打开设置选项。 时间范围 1-20 秒 consumer 可以通过 WaitTimeSecond 来设wait 时间 SQS Temporary Queue:    Request-Response messaging pattern 是一种双向Queue 的设计模式, 通过SQS Temporary Queue 来实现   SQS Temporary Queue Client 是个服务， 可以实现这个模式 两个服务之间需要请求和回复 producer 可以指定response 的queue name consumer 处理完message之后将结果发送到指定的queue.  多个producter 可以分别指定自己要接收的queue的名字， 多个consumer 操作完分别回给不同的producer 如果有多个 producer , 需要一个request queue, 多个 response queueSQS 与 ASG 的设计模式:  模式1     SQS 后面接一个ASG , 里面是一堆EC2 实例   给 CloudWatch指定 Costom Metric(自定义的指标) ， 比如queue length, 实例个数等   CloudWatch Alarm 发现指标超出阈值， 发送信息给ASG    模式2     Queue 两边都是ASG   上游和下游个数需要匹配:  如果SQS 有多个数据源（比如1个IoT设备），如果也希望有10个 consumer 对应， 就需要每个IOT 设备提供一个unique ID, 通过 SQS FIFO 让消费者可以根据unique ID ,并且按顺序来提取数据Kinesis: Kinesis:  实时收集、处理、分析流数据 每条数据包含     Partition Key, 就是数据ID         同一个partition key 的数据会被同一个shard 来处理， 其他shard 不会处理          Data Blob, 就是数据内容本身    场景     application log   website click stream, IOT data   DMS 将数据从S3 导入到KDS    四个组成 部分     Kinesis Data Stream         获取，处理和保存数据流          Kinesis Data Firehose         读取数据流到AWS 数据存储          Kinesis Data Analytics         通过SQL or Apache Flink 分析数据          Kinesis Video Streams         获取，处理和保存视频流           Kinesis Agent     kinesis agent 是一个独立的java 程序，   部署在需要读取数据的client side, 作为 kinesis 服务的上游， 可以写入到KDS or KDF    Shard     KDS 是有多个Shard 组成的   吞吐量 - 1M/每秒/每shard， or 1000 message/每秒/每shard   通过增加 Shard 的数量提高 Kinesis 的吞吐量   一个 shard 流量过高, 可以拆成多个 shards   多个 shard 流量过低, 可以合并   Kinesis Data Stream:  数据进入就不能删，可放置一年， 数据可以被反复处理 每秒可以处理1M 的数据进入/每个 shard,     每秒1000条message/每个 shard    每个shard 单独计费，增加shard 就加钱     可以通过batch 的方式增加吞吐量还不用多花钱    上游的数据来源 - Producer (1m/s)     DMS   applications   AWS SDK/KPL(Kinesis Producer Library)   Kinesis Agent         基于 KPL 开发的           下游 - Consumer (2m/s)     apps , KCL (Kinesis Consumer Library)   Lambda         可以写到 S3, Redshift, opensearch, dynamoDB, 其他任何地方     可以作为 简单版本的 ETL     可以作为 triger, 发邮件, 通知等          AWS SDK   Kinesis Data Firehose   Kinesis Data Analytics   Kinesis Connector Library (比较老的方案, 现在基本都是用 lambda 和 firehose 代替)         可以写到 S3, Redshift, opensearch, dynamoDB          3rd party libraries , 比如:         Spark, log4j, Appenders, Flume, Kafka …          去重         producer 在数据中加一个 unique ID     consumer 可以在最终的数据存储的地方进行去重检查           场景     kinesis data stream 是处理数据流的, 用例包括         将不同源的记录路由到一个记录处理器， 并保持顺序     多个应用读取同一个数据流     存储几个小时的数据，然后供其他应用消费。 Kinesis Data Stream 默认存24小时的数据， 最长可以存储最长365天的时间。          Kinesis Enhanced Fan Out: New game-changing feature from August 2018.  用 KCL 2. 0 和 Lambda 实现(Nov 2018) 每个 Consumer 从每个 shard 获取 2 MB/s 带宽的流量 可以同时有 20 consumers , 就意味着可以把带宽流量扩大到 40MB/s 每个 shard No more 2 MB/s limit! Enhanced Fan Out: Kinesis pushes data to consumers over HTTP/2 可以减少 70 ms 的延迟Fan out 和标准consumer 的对比 Standard consumers:     Low number of consuming applications (1,2,3…) -   Can tolerate ~200 ms latency   Minimize cost    Enhanced Fan Out Consumers:     Multiple Consumer applications for the same Stream   Low Latency requirements ~70ms   Higher costs (see Kinesis pricing page)   Default limit of 20 consumers using enhanced fan-out per data stream   Kinesis Data Firehose:  serverless ，自动伸缩，无需人工干预， 不存在因为scale 限制写不进去， 需要扩容的问题。     full managed service,   near-real time   读取数据到 S3, OpenSearch, Splunk    上游 (1m/s)，和KDS 一样的, 还多了     KDS   CloudWatch logs/events   AWS IOT   SNS    中间， KDF 可以和Lamdba 结合, 对数据进行 data transformation 下游 （batch write）     S3   Redshift (数据仓库)   ElasticSearch   HTTP endpoint   3rd party data store         datadog     splunk     new relic     mongodb           Firehose 每1分钟 （1 minute） 可以把数据写入到下游存储中(比如 S3) 所有操作失败的数据可以保存到 S3 bucket 中 ,( 正常数据也可以转存到 S3) Spark 和 KCL 不能从 KDF 中读数据 场景：     日志分析流， 获取不同类型日志进行分析，并存储在永久的存储中， 可以供后期做分析用   Kinesis Data Streams vs Firehose 的区别:  Streams     Going to write custom code (producer / consumer)   实时 (~200 ms latency for classic, ~70 ms latency for enhanced fan-out)   手动伸缩 (shard splitting / merging)   存数据 for 1 to 365 days, replay capability, multi consumers   Use with Lambda to insert data in real-time to OpenSearch (for example)    Firehose     Fully managed, send to S3, Splunk, Redshift, OpenSearch   无服务, Serverless data transformations with Lambda   准实时 (lowest buffer time is 1 minute)   自动伸缩   不存数据   Kinesis Data Stream 和 SQS 的区别:  Kinesis Data Stream:• Data can be consumed many times• Data is deleted after the retention period• Ordering of records is preserved (at the shard level) – even during replays• Build multiple applications reading from the same stream independently (Pub/Sub)• “Streaming MapReduce” querying capability (Spark, Flink…)• Checkpointing needed to track progress of consumption (ex: KCL with DynamoDB)• Provisioned mode or on-demand mode   SQS:• Queue, decouple applications• One application per queue• Records are deleted after consumption (ack / fail)• Messages are processed independently for standard queue• Ordering for FIFO queues (decreased throughtput)• Capability to “delay” messages• Dynamic scaling of load (no-ops)   不同的使用场景 SQS use cases:• Order processing• Image Processing• Auto scaling queues according to messages. • Buffer and Batch messages for future processing. • Request Offloading Kinesis Data Streams use cases:• Fast log and event data collection and processing • Real Time metrics and reports• Mobile data capture• Real Time data analytics• Gaming data feed• Complex Stream Processing• Data Feed from “Internet of Things”Cloudwatch 订阅过滤器 与 Kinesis:  cloudwatch 是 流式的日志系统     可以把日志流到         KDS,     KDF,     Lambda          案例: 准实时         CloudWatch Logs -&gt; Subscription Filter -&gt; KDF (分支 -&gt; Lambda 做数据转换) -&gt; OpenSearch          案例: 实时         CloudWatch Logs -&gt; Subscription Filter -&gt; Lambda 代替 KDF 并做数据转换 -&gt; OpenSearch          案例: 分析         CloudWatch Logs -&gt; Subscription Filter -&gt; KDS -&gt; KDA - &gt; Lambda 进行一些任务处理          可以使用 cloudwatch filter   可以通过 AWS CLI 开启   Kinesis Data Analytics:  可以理解为一个在KDS/KDF 的中间的分析环节 关键词：     time series analytics   real-time analytics   real-time dashboard   real-time metrics    上游     KDS   KDF    中间， KDA 用 SQL 下游     KDS   KDF   Kinesis Data Analytics 和 Athena 的区别:  Kinesis Data Analytics 和 Athena 都用SQL, 都把数据存到S3 Kinesis Data Analytics 分析实时数据流 Athena 分析历史数据kinesis 中的数据的排序:  场景：     100个卡车的GPS 定位数据传送到Kinesis 来分析卡车的位置   同一个卡车发送的定位数据需要按顺序被处理，如果先后顺序不对， 行走路线就是来回跑了。   一共有3个 shard    处理逻辑     每个卡车有一个unique id 作为partition key (只有这样，才会正确排序)   每个partition key 会被同一个shard 来处理   100个车的partition key 分别送到了3个shard 中， 大概每个shard 会处理1/3 的所有车辆，且服务的车辆对象固定不变    考试会让你从 Kinesis Data Stream 和 SQS FIFO 中选一个，需要知道怎么选 考试会问为啥没有排序正确， 就是因partition key 没有放对应的 user idBatch messages:  可以在不增加成本的情况下让Kinesis Data Stream消息并行的发送（http 请求）， 这可以最大化的利用现有shard的情况下来提高性能。是节省成本的理想方案Amazon MQ:  为了让on-premise 的 MQTT， AMQP， STOMP， Openwire, WSS 等开源queue 可以迁移到AWS, 且不用 re-engeering , AWS 提供 Amazon MQ 是代替SQS / SNS 的方案 ， 有他们基本功能 ， Queue 和 Topic     Amazon MQ has both queue feature (~SQS) and topic features (~SNS)    Amazon MQ HA     通过EFS mount 到两个Amazon MQ Broker 来实现 failover.     场景： 公司希望把 开源Queue/通知 迁移到AWS , 那就用 Amazon MQ 关键词     message broker   计算: Beanstalk:    paas 服务， beanstalk 负责iaas （ec2 asg, elb, rds, health check and ect. ）, 我们负责applicaiton代码, 和对系统的配置     beanstalk 本身不收费， 产生的底层iaas     环境类型      web 服务器环境， 用于网站开发   工作线程环境， 用于内部工作      配置预设      单一实例   单一实例-spot   高可用   高可用-spot   自定义      工作步骤          创建 环境                EC2, ASG, ELB, RDS,….           选择应用名称， 开发语言级版本（比如php）， 上传代码， 点击开始创建     创建application 过程中会自动创建             target group       security group       Auto Scaling group (asg 会自动创建ec2)       ELB       Cloud Watch       部署 EC2 实例                 包括webserver 和 RDS                      application available       health check 检查完毕状态从pending 改成 ok       完成后就多了一个application 和一个环境， 点击application url就可以看到php congratulation 页面了       要修EC2,RDS 需要去ec2， RDS, ASG, ELB, SG 要去 对应的consule 界面进行管理       删除环境会把所有的都删掉                     EC2: EC2:    user data (run with root user, only first boot)      dynamic configuration. 把100个需要配置的EC2 的user data 写到一个地方， 作用是避免重复劳动，不能加快部署速度。      security group      only have allow rule for IP CIDR or security group   实际是绑定网卡的， 和EC2是多对多   Stateful，能进来就能出去   默认不能进， 随便出   time out 是 security group 的问题， connection refused 是EC2的问题   22,21,80,443,3389   inbound rule         允许上游SG/IP(or range) 的inblound on 自己的 port             purchase options          on-demand          On-demand instances are not bound by any contract or agreement and can be modified at any time, including on termination, without any penalty.              比如： 一个 instances that can be resized at any time in order to execute performance testing                         reserved (1or3 year, 70% discount)           spot instance (90% discount) , 关闭的方法是先关掉 spot request, 再terminate spot instance          spot fleets 是on-demand和spot 组合的，自动获取spot并用了最低的价格     Spot Fleets 是一堆Spot instances, 通过ASG 进行部署的， 用于最大化的HA。              dedicated host          对物理机有绝对的拥有权。 可以自己决定如何部署instance.      有 host ID， 可以被应用直接访问     在dedicated host 上可以部署VMs, 只支持两种purchae options             on-demand       reservation                关键词             自己有license                         dedicated instance                Dedicated instance 可能会和同一个AWS account 的其他instance 分享同一个物理机器，但是dedicated instance 所在的物理机不会让其他账户进行访问。                 关键词              single tenant hardware       isolatiing instance                            scheduled (scheduled reserved instance)         Scheduled Reserved Instances: Scheduled Reserved Instances (Scheduled Instances) enable you to purchase capacity reservations that recur on a daily, weekly, or monthly basis, with a specified start time and duration, for a one-year term. You reserve the capacity in advance, so that you know it is available when you need it. You pay for the time that the instances are scheduled, even if you do not use them.              重启机器，public ip会换掉  placement group:  cluster (same az, same rack)     Cluster placement group 是所有EC2在同一个AZ同一个Rack上， 所以网络延迟最低（10G带宽）    spread (HA, max 7 instance per group per az)     Spread placement group, 所有的实例都不在一个物理硬件上， 支持最大化的高可用    partition (hadoop, cassandra, kafka, max 7 partition per az)     Partition placement group, 每个AZ里面可以有很多机器， 适合HDFS，HBase， Cassandra， Kafka 的场景   ENI:  一个ENI 可以从一个EC2换相同AZ下的另一个EC2 一个ENI可以有一个Public ip, 一个主private ip 多个secondry ip.  一个private ip只能绑定一个elastic ip 最大吞吐量 25GbpsElastic IP:  普通的public IP， 停止后再启动，IP地址会变。 如果想要一个固定不变的IP地址，就需要申请一个Elastic IP 一个EIP只能一次attach 到一个实例上 可以把EIP从出问题的实例上remapping 到 正常的实例上 EIP不是一个好的架构选择     Instead, use a random public IP and register a DNS name to it   Or, use a Load Balancer + private IP   ec2 instance hibernate:    将启动时需要加载的内容从内存里放在磁盘文件上， 下次启动直接读文件到内存中， 加快启动速度     场景：      需要一个进程一直不停， 需要保存ram状态， 需要加速启动速度   EC2 上有个内存数据库，希望重启后数据仍然保留      限制条件： C,R,M 3，4，5 系列， ram &lt; 150g, encrypted EBS only, on-demand and reserved instance. max 60 days hibernate  EBS:  可以在同一个az的不同ec2之间插拔   目前最大支持64T 容量     多EBS 对一 ec2 的关系      特例是EBS Multi-attach         只能在同一个az下实现一个ebs 对多个ec2 实例， 只有io1,io2 可以     Provisioned IOPS SSD volumes(io1, io2)             by default, root volume 在ec2 terminate的时候也会被删除         实例没有运行         通过console，API,CLI 直接修改DeleteOnTermination 参数为 False          这个实例还在运行， 怎么修改这个错误呢？         通过命令行来设置 DeleteOnTermination 参数为 False             通过snapshot的方式进行备份（可跨az）     type          gp2,gp3,io2,io3(最快), st1, sc1(第二慢), standard (最慢)          EBS Standard 最便宜。存储上限是1T     SC1 适合 cold storage data set     ST1 适合high throughput     gp2 （general purpose）是launch EC2的默认选择，有中等偏上的IOPS能力，价格比其他SSD类型都低。     EBS io2 (PIOPS) Block Express可以最大化IOPS, 是下一代的EBS 存储方案架构, 最贵              gp， io，instance store 是ssd,可以做boot volume           价格对比          standard &lt; SC1 &lt; ST1 &lt; GP2 &lt;IO1/IO2 &lt; instance store     既要为预留的size 付费，又要为读取的流量付费。（EFS 只为流量付费）             encryption      可通过snapshot 的方式对未加密的EBS加密， 是在copy snapshot的时候进行加密的   EBS volume 可以被设置成默认就被加密   关于加密了的 EBS 正确的描述是：         数据在volume 和 instance 之间流动时是被加密的     Snapshot 是被加密的     volume 中的数据是被加密的             EBS 的 RAID      Use RAID 0 when I/O performance is more important than failt tolerance   Use RAID 1 when fault tolerance is more important than I/O performance         EBS 支持标准 RAID 的配置，包括RAID 0 和 RAID 1             RAID 0 是需要性能更高的时候使用， 但是有磁盘损坏无法修复的风险       RAID 1 是两个磁盘互为备份，但是性能差一些。                     EC2 instance store:  better i/o 方案 比ebs 目前最大支持10G 容量 重启ec2， 数据就丢了 通过备份和 replica 可以解决 是个硬件设备 intance store 最大也就支持3. 8T (不支持太大的存储) 如果这个EC2 实例挂的是 Instance Store 的 volume, 那就无法被Cloud Watch Alarm 触发来进行自动的 recovery.  正确的描述     不能把一个instance store 从一个实例上 detach 然后再attache 到另一个实例上。   如果给一个挂了 instance store 的EC2做 AMI， instance store 上的数据不会被保留   其他的正确答案 （不正确的改成正确的）         Instance store 是一个物理硬盘(This storage is located on disks that are physically attached to the host computer)，不是网络存储(network storage)     只用在launch 一个新的实例的时候可以指定他的instance store. 重启的时候不能指定，因为重启后instance store 的数据就丢了     不管你是stop, terminate or hibernate, instance store 的block 都会被重置，数据会丢失。          AMI:  特性     通过加密的snapshot 拷贝一个 AMI , 不会产生一个非加密的snapshot   你可以跨Region 拷贝一个 AMI   你可以在不同的AWS账号间分享AMI    有三种， public ami (aws 提供)， own ami, marketplace ami az1 的 ec2 创建一个ami，copy 到 az2(可选择加密选项) 在az2 恢复成另一个一样的一个ec2 同一个AMI 只能在同一个az创建实例, 不能 在另一个az创建实例 Golden AMI ， 黄金AMI, 把本来要创建实例后需要做的软件安装， 依赖安装鞥工作提前做到AMI中， 作用是加快部署速度，快速启动实例EC2 Metadata:  可以通过URL http://168. 254. 168. 254/latest/meta-data/ 获得；也可以通过CLI 命令行获得 包含了EC2 的基本信息， 比如 IAM, HostName 等EC2 Instance Auto Recovery:  EC2 实例的恢复 用CloudWatch alarm 来自动回复受损 （impaired）EC2 实例 一旦没有问题， 会恢复和以前一样。一个恢复完的实例就等同于原来的实例     私有IP   共有IP   Elastic IP   Metadata   Placement Group    这里要注意，一个正常的EC2 实例，重启后IPV4 地址会变掉     但是因为 auto recovery 恢复的实例， IPV4地址不会变   EC2 Enhanced Networking (SR-IOV):  更高的带宽， 更高的PPS(每秒packet) , 更低的延迟Elastic Fabric Adapter (EFA):  是Elastic 织物适配器 更高的计算性能和网络带宽     Elastic Fabric Adapter (EFA) 是一个网络设备， 可以attach 到 EC2上， 来支持 HCP 和 机器学习应用。 EFA 还提供 允许应用和硬件接口直接通信， 提高网络通信速度。   The EFA exhibits both the highest throughput and the lowest latency of any network interface in AWS.    Spot Fleet:  Spot Fleets allow us to automatically request Spot Instances with the lowest price Spot Fleets = set of Spot Instances + (optional) On-Demand Instances Spot Fleet stops launching instances when reaching capacity or max cost 可以用 Spot fleet 提交一个一次性的任务， 比如2个小时的大数据计算， fleet 会启动一堆 spot instance 来执行任务， 执行完成后spot instance 会被terminate, fleet 也会结束。 也可以在fleet 中使用 on-demand 实例。spot 有两种类型:    可以是one-time 的，     也可以设置成 persistant , 只有设置成了persistant, 才有下面 stop/interrupted 这些故事          如果 spot request 还在， 那么如果spot instane被中断了(interrupted), spot request 就会被再打开          spot instance 如果是被stop 了， 只有instance 再次启动后， spot request 才会被再打开              当你cancel 了一个 active spot request, 这并不会terminate 对应的instance          如果要关闭 instance, 需要手动关闭。              spot blocks 设计成不会被打断（interrupted）          Spot block 是被设置成可以执行固定时间的、不会被打断的 instance 。 时间可以设置成1-6个小时。           Spot instance, Spot blocks, Spot Fleets 的区别:  Spot instance 是一种EC2 实例可行，可以被打断（提前2分钟通知），能省90%的钱。 Spot Block 是不会被打断的Spot instance,一次可以使用1-6个小时 Spot Fleets 是一堆Spot instances, 通过ASG 进行部署的， 用于最大化的HA。EC2 实例的tenancy:    一共有三种 tenancy, 分别是      default - 实例在分享的硬件上运行   dedicated (dedicated instance) - 实例在 single-tenant hardward 上运行（只给你用，但是你不能配置它）         但是可能会分享同一个账户的多个实例在一个硬件上运行          host (dedicated host) - 实例在dedicated host 、隔绝的、可以配置的服务器上运行 （只给你用， 你可以随便配置）         一个实例就在一个硬件上， 同一个账户的其他实例也不会在这个硬件上运行。             tenancy 之间的转换规则      可以把实例的 tenancy 从 dedicated 改成 host   可以把实例的 tenancy 从 host 改成 dedicated   不能从default 迁到 host   不能从 default 迁到 dedicated   不能从dedicated to default   不能从 host 迁到 default      ASG 的launch configuration 和 VPC 都可以设置 tenancy.     ASG 中设置的 launch configuration 的 tenancy 类型是和 VPC 的 tenancy 相关的      如果launch configuraiton 的 tenancy 设置成了default, 那么就会继承VPC 的 tenancy      如果launch configuration 的 tenancy 设置成了非default ， 那就会覆盖VPC 的 tenancy  容器: ECR:    AWS 容器 Images (镜像)管理平台     底层通过S3 进行管理     安全      IAM role   Vulnerability scanning 弱点扫描   version      image lifecycle  AWS Fargate:    AWS Serverless 无服务容器平台   Fargate 是一种无服务技术，让Docker 可以脱离EC2 直接运行在AWS 上面， 它支持EKS 和 ECS   Fargate 默认是支持跨az的，不需要单独配置multi-az      ECS with Fargate   EKS with Fargate    看ECS的介绍ECS: ECS:    AWS 提供的Docker 容器管理平台     是基于EC2的， 要自己维护EC2     这里真正能执行任务的容器是task     task 有自己的role （policy）用于给外面的服赋予权限对自己进行访问     ECS launch type          EC2          在ECS Cluster 中放一个ASG           在 ASG 下面部署多个EC2 实例，              EC2 实例同时也ECS Cluster 当中                EC2 实例是分布在不同的AZ当中的，实现了HA           每个实例上有一个 ECS agent 和 多个ECS Tasks              ECS agent 负责和容器外部的 ECS 服务通信， 对ECS tasks 进行管理       每个 task 没有自己的 IP 但是有一个被动态分配的 port                好处             Predictable cost, regardless of the number of running containers                 When deploying ECS onto EC2, there is no charge for the ECS management layer, and the customer can take advantage of cost discounts by using reserved instances.                                         Fargate          serverless, 直接把Docer运行在AWS上， 不需要EC2 了, 简单了     需要给每个task 配一个ENI (网卡)，每个task 就有了一个unique private IP     如果容器一直在Fargate上运行，那么从长期的角度来看，Fargate 要比 EC2 版本的ECS 要贵很多。             安全      IAM Role for ECS， 有两层roles         EC2 instance role , 是给ECS agent 用的， 可以对外进行通信用的比如对外获取镜像， 发送日志到cloud watch等     ECS task role , 允许每个EC2里面的task 有自己的role, 这个role 可以允许task 访问S3, DynamoDb 等服务             存储      ECS 容器的Volume 方案是EFS         不管是EC2 中的 task 还是 Fargate中的 task, 都可以直接 mount 到 EFS 上     因为 EFS 支持 multi-az, 所以在任何 az 的 task 都可以 mount 到 EFS 上， task 默认就是跨 AZ 的     Fargate + EFS是完美的 Multi-AZ 无服务容器 + 存储方案          ECS 和 ALB:  ALB 对外暴露域名或 IP, 对内直接连接 不同 instance 上的 tasks EC2上 的 tasks     每个 task 有一个被动态分配的 port (比如 36789)   Dynamic Port Mapping 可以让 ALB 转发流量到多个 ECS Tasks 中, 解决了每个 task 的端口不一样的问题.    需要在 ALB 的安全组中进行设置， 允许 任何 port 的EC2安全组进入    Fargate 上的 tasks     每个 port 都分配了一个ENI , 带有唯一的 private IP   所有的task （ENI）都使用同一个 port (比如80)   需要在 ALB 的 安全组中设置， 允许 带有 task 的端口号的所有 ENI 可以进入   ECS 和 AWS Event Bridge:  EventBridge 是 serverless 服务， 让一种无服务调用另一种无服务。 当需要让 S3 的 Event 可以调用 ECS task 来处理任务时， 需要用 EventBridge 来调度task 需要给 task 的 role 设置权限允许 访问 S3 和 dynamoDB.  场景     客户上传一个图片， 希望通过容器里的服务创建缩略图， 并将数据记录的 DynamoDB 中。   ECS Scaling:    通过 Cloud Watch metrics（ECS CPU usage）检测发现超过阈值 ， CloudWatch alarm 通知ASG 增加 tasks 数量。      如果是 EC2 版本的 tasks, 还可以增加EC2 的数量 （optional）   ECS Rolling Update:  比如， task 的 python 应用需要升级， V1 升到 V2, 一共100个 task ，要轮换的替换现有V1 保证服务一直对外提供 通过设置min ， max 的 百分比来对所有 task 轮换升级     min 50%， max100% , 就是先换掉 50个v1，加50个v2, 再换掉另外50个v1，再加50个v2, 保证总数最少被不少于50， 总数最大不超过100个   min 100%, max 150%, 就是先增加50个v2， 再减掉50个v1, 再加50个v2, 再减掉50个v1. 保证总数不少于100， 不大于150   ECS 访问 S3 的权限:  给S3 建IAM Role + 把这个role 作为 task 的 taskRoleArn     Create an IAM role with S3 permissions, and then specify that role as the taskRoleArn in the task definition.    EKS:  Amazon’s Kubernetes     Pods 相当于instance   notes 相当于tasks    是 serverless 是ECS 的替代方案 除了API被AWS 改写了， 其他的和开源版的kubernetes基本一样 场景：     公司已经使用了K8s, 希望迁移到AWS, 就选EKS   存储: S3: S3:    bucket - 装文件（object）的桶          名字要全球唯一, 小写字母数字组合           multi-region level          When configuring S3 cross-region replication, need manually copy existing objects.              Enabling cross-region replication does not copy existing objects by default, and so it is up to the customer to perform this task       cross-region replica 需要打开 version                         S3 Standard is described in the SLA documentation as being designed for 4 9s availability and 11 9s durability.          11 9s durability 需要对object 做三次 replica              versioning          在启动version 之前就已经存在的文件， 他们的第一个版本是 null     Versioning 一旦开启就不能关闭了， 且只能暂停一次。             Object      S3://my-bucket/folder/subfolder/my_file. txt   ﻿key 是整个url, 其中my_file. txt是 object name, /folder/subfolder 是 prefix   最大5T, 超过5g 需要multi-part upload， 建议超过100m 就 multi-part upload   Meta data 元数据, 就是给上传的文件定义一堆key value， 用来描述文件类型， 语言， 和其他自定义的任何定义文件的信息， 可以通过CLI获取   tag 标签, 一堆key value , 方便之后用于文件的生命周期管理   S3 object 只属于上传他的账号（创建者）， 如果是其他用户/应用上传的文件， 就算是S3 owner 也看不到（没有访问权限）。      encryption ,      S3 Glacier 默认就是SSE 加密at-rest 的数据的       bucket/文件 加密 4种          SSE-S3 , key 的管理和加密过程由aws 完成， 看不到key, 也不能参与管理             设置header “ x-amz-server-side-encryption:aes256”                SSE-KMS, key 的管理和加密过程由KMS完成, 可设置rotation policy             设置header “x-amz-server-side-encryption”: ”aws:kms”       KMS is a shared-tenancy, region-scoped service that uses resilient architecture to protect your encryption keys                SSE-C, key 的管理在客户端， 加密在服务端             必须用HTTPS       key 需要在header 中提供                Client Side Encryption - key 的管理和加密解密都在客户端              in flight 加密          SSL or TLS     S3 网站如果需要TLS 加密的话需要使用 default s3. amazonaws. com TLS certificate.               强制bucket 对内容进行加密的方法                在policy中设置， 如果api请求的header 中没有加密的key value, 那么就拒绝                 Default Encryption 设置项              可以给bucket 打开Default Encryption 选项， 打开后上传的未加密文件会自动被加密                        Security          IAM policy (user level)           Object/Bucket ACL （resource level）          如果没有显性deny, 那么只要是IAM policy 允许或者 object/bucket policy 允许， 这个用户明访问s3资源              可以跨账号访问           encryption          Glacier 默认支持加密， 包括了 数据加密和传输加密（data at rest and in-transit）              MFA delete          没那么容易删 object version 或suspend version on bucket          需要开启version         bucket owner/root 才能开关bucket 的 MFA     only support CLI              Pre-signed URLs          给临时想访问的人          通过CLI or SDK 创建 url         下载url 通过CLI     上传url 通过SDK     默认3600秒有效期， 通过TIME_BU_SECOND 参数设置     URL的使用者权限继承了创建url的人得权限              logging          S3 access log (对人)     存在另一个bucket 中， 可以通过Athena 来进行查询分析             不要去监控logging bucket                CloudTrail to log API calls （对应用）          S3 access log 只知道谁访问了S3, 但不知道谁改了配置。      Block public access      可以在bucket 中设置中禁止外网访问bucket   可以在account level 设置      S3 website      给bucket 打开配置， 并上传一个index. html 和 error. html   给bucket 的policy 开通外网访问权限   S3 网站如果需要TLS 加密的话需要使用 default s3. amazonaws. com TLS certificate.       S3 CORS header （基于S3 website)      （Cross-Origin Resource Sharing） 跨域资源共享   A 站 的网页要显示 B 站的图片， 需要先去B拿允许，A使用 CROS Headers (ex: Access-Conrol-Allow-Origin) 再去请求资源   在Bucket 中设置一段json ,授权请求方可以访问， Json内容包括         AllowedHeaders     AllowedMethods     AllowOrigins     ExposeHeaders             Access Point      类似SG的功能， allow 某个VPC 或者所有 internet 的访问， 并为之设置 policy.       Consistency policy      S3保证app 调用API 写入信息后，立即读取就可以读到写入的内容。      S3 Replication      必须启动versioning   两种方式(CRR &amp; SRR)         CRR , cross region replica             场景是：合规要求， 跨账号的 replica                SRR, Same region 的 replica             场景： 需要一个用来分析日志的bucket                     replica 创建好后，老的数据并不会同步到replica, 只有新的object 才会同步到replica.    主S3 的 delete 标志也会被同步到replica, (选项可以被关闭)   但是删除的版本ID不会被同步， 防止可以删除   replication 的复制不能是链状的， 只能是两个bucket 之间的， 比如只能是a→b, 不能是a→b→c   S3 Storage Classes:    Type      General   IA         4 9s     场景 - backups, 灾备          One Zone IA         可以重建的数据， 第二备份     2. 5 9s S3中最低的     场景：可重建的缩略图，xx天后可删除          Glacier         场景：xx天后，用户可以等待xx小时获取数据     Glacier 默认支持加密， 包括了 数据加密和传输加密（data at rest and in-transit）     Retrieval Mode             expedited 版要1-5分钟       standard版要3-5个小时       bulk版要5-12个小时                     Glacier Deep Archive         Retrieval Mode             standard 版要12个小时才能读到数据       bulk 版要48个小时才能读到数据                     Intelligent Tiering         AI 基于数据使用情况每个月做一次自动扫描分析和自动分层。     S3 intelligent Tiering class 是为了优化成本而设计， 可以自动迁移数据到成本优化的访问层。 主要分为两层             频繁访问层       不频繁访问层。                        LifeCycle policy/Rule      将object 在满足某个条件后从一个 存储类型转到另一个存储类型 ， 或者删除   rule 可以应用于某个过滤条件         比如：创建一个 lifecycle policy, 45天后通过 prefix 把/images 放到 S3 Standard IA          只能从高往低转， 不能从低往高转         object 在从standard 到 IA （或者从 IA 到 one zone IA）状态最少要30天后才能转型到另一个状态     从 standard 到 Glacier/Glacier Deep Archive 没有限制，当天或者过一天就可以转                S3 Analytics      对当前存储分类进行分析， 并给出建议， 比如应该把一些object 放到 IA 等   不支持One Zone 和 Glacier   可以和LifeCycle Rule 配合一起使用，用S3 Analytics 分析好了之后就用 LifeCycle Rule 给文件做规划      Athena      serverless   对S3 的对象做分析的工具   使用标准的SQL 语句分析S3上的文件   场景         BI, 分析，报表     VPC 的 Flow logs, ELB logs, Cloudtrail trails and etc.      后面可以再接一个Amazon QuickSight 服务。          S3 Performance:  S3 的性能是可以同时支持5500个读操作/秒，同时支持3500个写操作/秒 提升性能的方法     Multi-part upload, 提高并发，加快速度   S3 Transfer Acceleration,通过网络优化加快速度（ 从AWS Edge介入AWS 内网专线到达S3 ）   S3 Byte Range Fetches, 分多块并行下载， 加快下载速度。也可以用于只下载大文件的一小部分   S3 Select &amp; Glacier Select,         客户端发送带 Where(SQL) 语句的请求给S3, S3 在服务端执行语句， 把过滤后的结果的object 返回给客户端， 减少传输size , 加快下载速度。          Prefixes （前缀法）         S3://my-bucket/folder/subfolder/my_file. txt     ﻿key 是整个url, 其中my_file. txt是 object name, /folder/subfolder 是 prefix     可以通过在bucket 中建立多个 prefix 的方式并行并行处理读写， 成倍增加吞吐量             比如增加10个 prefix 可以把吞吐量提高到 35000 的写， 55000的读                S3 没有限制 prefix 的个数，          S3 Event Notification:  比如上传了一个图片， 就触发一个推送 触发的对象可以是     SNS   SQS   SNS -&gt; SQS   Lambda   Event Bridge -&gt; ECS   S3 Requester Pays    默认是bucket owner 付费     requester pays      是owner负责 storage 的费用   请求数据的人付下载数据的费用      requester 必须是AWS 用户， 不能是非AWS账号用户  文件锁， 两种:  Glacier Vault Lock 保险箱锁     应用于 WORM （write once and read many）模式 。 只允许写一次， 之后就再也不能修改/删除这个文件了，可以随便读。   放在S3中的 object 和 对应的policy 可以被 vault lock 锁上， 没人可以修改这个文件以及对应的权限了。   用于合规和特殊数据保护    S3 Object Lock     应用于 WORM （write once and read many）模式 。 只允许写一次， 之后就再也不能修改/删除这个文件了，可以随便读。   把一个对象的一个版本锁住一段时间   Modes         普通模式 - 限制一个版本一段指定的时间     Legal mode, 限制一个版本，没有指定时间     Governance mode ， 不允许大多数用户修改，少数用户可     Compliance mode, 比Governance mode 更严格， 包括 root 在内， 谁也不能改， 时间段也不能缩短          S3 Storage Gateway:  S3 是 AWS私有的技术， 不能在on-premise 服务器上应用 Storage Gateway 用于bridge on-premise 和 AWS S3 的数据传输 有三种Storage Gateway ， 都要安装在客户的服务器上     File gateway         用于on-premise应用将数据文件存储/备份到AWS上     通过IAM Role 访问S3 bucket     **使用NFS 和 SMB **     和AD （active directory） 整合进行用户认证     关键字 - AD， NFS, SMB， aata file     案例：     公司的legacy 报表应用有上G 的. json 文件来自不同的数据源都存在了on-premise storage location， 没有办法承受继续增加的文件size.  公司并不想改变这个 legacy 应用， 希望可以把数据从 on-premise 应用更新到S3 上， 建议的方案是：             在 on-premise 上设置 file gateway,       配置数据源写. json 文件到file gateway 上。       让 legacy 应用读取 file gateway 的文件。       file gateway 可以把数据replica 到S3上                     Volume gateway         将基于云的iSCSI block storage volume 给on-premise 应用 使用             Volume Gateway provides an iSCSI target, which enables you to create block storage volumes and mount them as iSCSI devices from your on-premises or EC2 application servers. The Volume Gateway runs in either a cached or stored mode:       In the cached mode, your primary data is written to S3, while retaining your frequently accessed data locally in a cache for low-latency access.        In the stored mode, your primary data is stored locally and your entire dataset is available for low-latency access while asynchronously backed up to AWS.                 Volume Gateway 提供的服务是让on-primise 的应用/用户可以在本地访问网络磁盘， 通过 cached volume 把经常访问的文件缓存到用户本地， 而同时把所有的文件都备份到S3中。     关键字 volume, block storage， iSCSI          Tape gateway         用于把 tape backup 挪到 cloud.      关键字 VTL (Virtual Tape Library), Tape， tape           Harware Applianc     如果客户不让安装Storage Gateway软件， 那么让客户买个硬件 - hardware appliance , 它上面自带了storage gateway   关键字 - 没有on-premise virtulization   S3 Bucket Policy:  可以定义S3 被不同账号的用户进行访问。 Bucket policy 可以给不同账号分配权限来访问资源，可以带上若干条件     date condition - 比如访问时间   bollean condition - 比如是否是SSL   IP condition - 比如是否来自某个地区   string condition - 比如是否来自某个应用    想让外部用户访问S3 Bucket，并确保 bucket owner (并不是object 的创建者) 可以访问所有的object, 就需要创建bucket policy ，在上传文件的时候 grant bucket-owner-full-control 权限 案例     提到公司给客户提供S3使用的服务， 希望可以既有用户层面的跨账号访问能力， 又有账号层面的一个账号有多个用户的能力， 方案是 - 使用S3 Bucket Policy   S3 Sync:  S3 之间拷贝大量数据 （不能用Snowball 的前提下） 使用 S3 Sync 命令 可以将大量数据快速在S3 之间进行copy1aws s3 sync s3://DOC-EXAMPLE-BUCKET-SOURCE s3://DOC-EXAMPLE-BUCKET-TARGET 案例     公司把1P的数据通过Driect connection 方式拷贝到了一个AZ 的S3中， 现在希望把数据也拷贝到另一个AZ的S3，(不能用Snow Ball) 最好的方法是S3 sync 命令   S3 Bucket Object Retention (文件的保留时间):    可以通过两种方法          用Retain Until Date (一个指定的明确的过期日期) 给一个object 的 version 设置retention period          一个object 的不同的版本可以设置不同的 retention mode 和 periods              也可以通过bucket 的 default setting 中设置 retention period, 但只能设置duration , 不能设置 retain until date。          如果题目说了每个对象会有不一样的retention, 就不能bucket default setting  S3 的 consistent （一致性）:  S3 is strongly consistent for all GET, PUT and LIST operations.  当做完一个成功的写操作/overwrite/delete , 后续的读操作都会得到最新的版本的object 和 list S3 已经支持修改 tag, ACL, metadata 之后的读也会拿到最新版本的 objects 和 listEFS: EFS:  跨az 对ec2是一对多 (100+) 有单独的SG 比gp2贵3倍 最大吞吐量 3Gbps linux only(NFS 4. 1) , auto scale (pb) 通过 CloudWatch BurstCreditBalance metric 来检测EFS的性能是否在正常水平 encryption     Upon file system creation, check the Encrypted File System option.          在创建文件           type     performance mode (速度快)         MaxIO     General Purpos          throughput mode (吞吐量大)         provisioned throughput mode     bursting throughput mode             Provisioned Throughput Mode 是预留吞吐量模式，用于有明确的高吞吐量的预期的情景。 可以随时提升吞吐能力，但是降低吞吐能力需要等待超过24小时。                 如果您的文件系统相对较小，但也可以提供高吞吐率                      Bursting Throughput Mode                 是默认的EFS 吞吐量模式，平时维持低吞吐量，可以应付爆炸式的瞬间高吞吐量，然后再落回地吞吐量模式。         您存储的越多，您可以使用的吞吐量就越大                      AWS EFS 默认使用Bursting Throughput Mode, 但如果你的EFS有大量是的数据需要高频率的读写， 那么建议改成 provisioned Throughput mode.                       storage tiers (life cycle management 可以把不常用的文件放从standard放到infrequent access （EFS-IA） 省点钱)EFS IA (Infrequent Access):  可以通过给EFS 创建 lifecycle 来把EFS 转成EFS IA 来达到省钱的目的。 Choose Lifecycle Management file access policy to transition files to infrequent access (7, 14, 30, 60, or 90 days) using the AWS CLI, API, or Amazon EFS Management Console.      最多可以把文件创建90天后转成 EFS IA   EFS, EBS, S3 的价格对比:    EFS -      用多少付多少   $0. 3 / GB / Month      EBS      Provision（预定） 多少付多少   $0. 1 / GB / Month      S3      用多少付多少      $0. 023 / GB / Month     有可能EBS 会Provision 了很多G磁盘， 所以会很贵  EFS performance modes:  有两种性能模式     Max I/O 性能模式         高并发的应用和负载， 比如大数据分析， 媒体文件处理， 基因分析等          General Purpose 性能模式         用于一般的EFS 存储场景， 比如web 环境， CMS ， 主页目录， 一般性的文件存储。     如果创建EFS 的时候没有选择 performance mode, 默认就是 general purpose mode          FSx: FSX:    FSX用于加载第三方的文件系统， 包含Windows 等     FSx for windows file server      支持将microsoft Distributed file system replication(DFS / DFSR) 迁移到AWS FSX for windows file server       SMB （Server Message Block）是windows 的文件系统。 提到SMB 就应该是 FSx for windows file server       - FSX for windows file server 和 Storage Gateway (FSx File Gateway) 都支持 SMB - 题目提到的如果是如何迁移到AWS, 就是 Storage Gateway (FSx File Gateway) - 题目提到的如果是如何迁移后如何在AWS 管理, 就是 FSx for windows file server      FSx for Lustre, 是并行分布式文件系统, 支持   并行数据处理， 支持人工智能, HPC,      和 S3 无缝衔接， 把S3读到FSX中， 再把计算结果写回到S3中      提到 windows 文件系统迁移就是 FSx for windows file server, 提到 POSIX 和高性能 IOPS , 就是 FSX for Luster     文件系统部署选项      短期的用Scratch File System (性能更好 但是没有replica)   长期的用persistent file system, 有replica   数据库: ElastiCache: ElastiCash:  管理Redis 和 Memcached 通过内存数据库的高速响应缓解数库的大量重复的I/O工作 底层是EC2 需要改代码 用途     将频繁读取的数据放到cache中   放用户的session 数据 （可设置TTL）    场景     排行榜 （Redis Sorted Sets 功能， 随着新元素的加入，可以快速创建元素的排序）    Redis 和 memcache 区别     Redis 支持         multi az, auto failover     Read Replica to scale     Data Durability， 使用AOF 支持数据持久性(把数据存在硬盘上)     backup, restore          Memcache (多线程架构）支持         multi-node sharding     multi-threading     不支持             HA，persistent, backup/restore                      Security     security group   不支持IAM Database authentication (application/ aws user level)   Redis Auth 在创建Redis cluster 的时候可以设置 password/token, 让用户应用访问。这不是IAM level 的 auth.    Memcached Auth , 支持 SASL 认证   ElastiCache 没有什么 IAM policy， 只能 用于 API level 安全   SSL for flight encryption   ElastiCache Strategy:  Cache-aside 模式     读的时候先读缓存，没有就读数据库，读出来后放入缓存，同时返回响应   更新的时候，先删除缓存， 然后在更新数据库（缓存和数据库双写）    Lazy Loading/ Lazy Population     只有需要的时候才把数据加载到Cache中，这种策略可以节省缓存的size.  但是读的时候往往需要先从数据库中读，稍微慢点。   为什么是删除缓存而不是更新缓存， 这里就是懒加载的策略         因为当前的缓存可能会成为冷数据， 访问频率不高，没必要去更新，只有当下一次对该数据有读的请求的时候，采取更新缓存。这就是一种懒加载的思想。           Write Through     只要写到数据库，就写到缓存，这样比较费cache 的size，但好处是可以提供最快的读。   双写，对数据库写了， 就要写cache    Cache 的消除（Evictions）的几种途径     手动直接删除cache 中的item内容   内存满了，会把最近不使用的item 删掉   为item 设置一个TTL   ElastiCache Cluster Mode:  ElastiCache Replication     Cluster Mode Disabled         所有的数据都在一个Shard 里面     1个Shard 里面 有1个primary node, 可以有最多5个 replicas     primary node 负责读和写， 其他replica 负责读     数据是通过异步形式传输               Cluster Mode Enabled         数据跨Shard ， 可以多点写入     1个Shard 里面 有1个primary node, 可以有最多5个 replicas     可以跨AZ部署     每个Cluster 可以有最多500个nodes          RDS: RDS:    AWS 管理， Cannot SSH， 底层是EC2 + EBS Volume     auto backup, AWS 负责执行      daily full backup, 5 mins transation log backup   can restore 5 mins ago   备份默认存7天， 可延长到35天      auto storage scaling      aws 负责自动扩容db的容量   还剩10%， 还剩5分钟用完就自动扩   可以手设置 max limit for db storage   默认RDS Mysql 是不开启auto scalling 的, 需要手动打开 storage auto-scaling 设置      Snapshot      手动进行， 是backup的补充， 可以自定义保留backup 时间      Read Replica /读写分离 (用于scaling)      最多5个 read replica (aurora15个)   async （异步传输）         跨region花钱， 同region 不花钱          需要调整代码      Multi AZ (用于灾备， 不是用于scaling)          single dns name (统一入口， 下面是跨az 的 master 和 standbys)           3. 5 9s           sync (同步传输)          Sync 的解释：             Two-way communication between a client and an app or between two apps that require a response to each request                 比如 HTTPS, SSH, HTTP 都是同步的                                        auto failover (无需人工介入, 不用改代码）          当一个数据库挂了， Multi-AZ 可以快速在Primary 和 Standby 之间进行切换 （跨AZ）。     切换时间一般在60-120秒之间              当一个 multi-az 的 RDS 的 primary instance 挂了，会发生什么事情？                CNAME 记录会被更新，指向 standby DB.              single dns name (统一入口， 下面是跨az 的 master 和 standbys)       CNAME 可以将 一个域名跳转到转另一个域名       RDS multi-az 通过 更新CNAME 记录 来实现 auto failover ， 自动的将原来的RDS URL 指向standby DB       连接数据库的URL 并没有变化                      当 primary database 失败了， RDS 自动 启动 （initiate）failover to standby                    OS 升级维护的流程          RDS 先在 Standby 上 执行OS 更新 进行 维护工作， 然后等standby 启动后把standby 设置成 primary     然后再对old primary 进行维护， 然后把old primary设置成standby;              Multi-az database engine level 升级维护          部署了 multi-az 的 RDS DB 的任何engine level 的升级维护，都需要让 primary 和 standby 同时升级， 会有downtime ，直到升级完成。             downtime 要根据升级的规模大小而定                         single az to multi az的步骤 - 点击modify 按钮就可以了。 背后的逻辑          创建snapshot → 恢复成standby db 到另一个az → 开始sync             encryption          type          at rest (数据库本身)     done only first time create DB instance             kms aes-256       主没加密， 从也不能加密                in-flight (数据传输)             SSL                         加密操作 ：       create snapshot → copy and encrypt → restore to new → delete old   Transparent Data Encryption (TDE) 透明数据加密         只有 Oracle 和 Sql Server 支持     Only SQL Server and Oracle DB support TDE, which encrypts the data at a database level, on top of the storage layer and in the software itself.              Security      put in private subnet   security group   IAM policy (user level) &amp; Role (service level)   username &amp; password(admin level) in secret manager   IAM Database authentication (application level/user level)         mysql &amp; postgresql     应用程序/aws用户不需要用户名密码，而是拿着 IAM role 去RDS Server 那个token, 然后拿着token去连接数据库          题目：         哪种AWS Database 可以用 IAM Database Authentication 来配置 （选两个）             RDS Mysql &amp; RDS PostGreSQL       通过 IAM Database Authentication， 用户/应用就不需要用用户名和密码来访问数据库了， 只用authentication token 就可以了。       Token 的有效期是15分钟。       IAM database authentication works with MySQL, MariaDB and PostgreSQL.                      Aurora:    3倍postgresql, 5倍mysql     master + up to 15 replicas     Aurora Serverless (Aurora Cluster)      是Aurora 的无服务模式， 可以按照访问量的高低自动增加和减少 replica， 完全不需要人员干预。   是为OLTP（online Dtansactional Processing） 而设计的数据库模式， 可以scall down to 0, scall up to many.          OLTP - 联机事务处理过程(OLTP)，也称为面向事务的处理过程，一个步骤失败，都失败，所有步骤成功，都成功。             HA &amp; auto scale      都是自动的， 原生的   一个writer endpoint, 接一个master   一个 reader endpoint 连接 Load banance, 下面接auto scaling 的 read replicas   optional 可以有一个Customer endpoint, 用来指定某些高性能的read replica 用于特殊场景的读， 比如做分析报表。      Multi-AZ Failover          游戏公司使用了5个 Aurora multi-az read replica 并使用了 failover target. 5个 replica 被分配了如下 failover priority tiers 和 实例size.          tier-1 (16TB), tier-1 (32TB), tier-10 (16TB), tier-15 (16TB), tier-15 (32TB) .      发生了failover , Aurora 会 promote 哪个 read replica? 答案是 tier-1 (32TB)              每个Aurora replica 可以分配 priority tier (0-15, 数字越小优先级越高) 来支持 failover.           如果有多个replica 使用了相同的tier, 那么failover 时会promote size更大的那个。           如果tier 和 size 一样， 就随机挑一个          Multi-Master      为了能够瞬间处理Write Note的 failover   Improved query concurrency         When deploying multiple masters for a relational database, it is possible to direct a higher number of write queries to the underlying database because the compute and storage tiers are separate infrastructures.           每个note 都做 R/W, 互相之间做 replica      Cross Region          Aurora Global Database,          1 region 做 primary （R/W）     Up to 5 secondary regions (Read only)     每个 secondary region 可以有最多16个 read replicas     primary 的region 挂了， 立刻换到另一个region 作为 primary region     Recovery Point Objective (RPO) of 1 second and a Recovery Time Objective (RTO) of 1 minute.              Security      at rest , use KMS   in flight use SSL   IAM token (application level, 和 RDS一样)   cannot ssh      ML      Aurora 可以应用机器学习服务         SageMaker, Comprehend     场景， 产品推荐， 观点分析， 广告标签， 欺诈检测等          数据库端口:  PostgreSQL: 5432 MySQL: 3306 Oracle RDS: 1521 MSSQL Server: 1433 MariaDB: 3306 (same as MySQL) Aurora: 5432 (if PostgreSQL compatible) or 3306 (if MySQL compatible)应用程序访问RDS 需要的临时认证权限:  比如应用是 lambda ,不安全的方案是直接把用户名和密码配置到了应用中。更安全的方式是临时认证方案：     attach 一个 AWS Identity 和 IAM role 给Lambda   使用 IAM ahthentication 让Lambda 去访问 RDS   Neptune:    高性能图数据库， 用于处理复杂关系， 比如社交网络, 谁 like 了谁， 谁回复了谁     案例是Wikipedia , 知识图谱     HA      across 3 AZ, 15个 replicas   clastering      Backup , 连续backup 到S3     安全      KMS + HTTPS   IAM authenticaiton      关键字      Graphs   Athena:  无服务数据库， 使用SQL 语句 用于搜索S3中的数据 可以根据建立自己的表， 用于存储搜索的内容， 结果可以返回给S3 安全     IAM    场景     一次性SQL请求   无服务的 S3查询   日志分析   RedShift: Redshift:  基于Postgresql 相当与 Analytics / BI/ Data Warehouse 是一个 OLAP (online analytical processing) 分析数据的数据仓库（data warehouse） 支持高并发的请求 , 10倍 与其他data warehouse 有SQL 接口 和 AWS Quicksign ， AWS Tableau 集成 上游     Kinesis Data Firehose   S3 bucket   EC2 JDBC   DymamoDB   DMS   其他数据库    Node 是主要的计算单元     Leader Note - 负责计划和结果收集   compute node - 负责执行queries, 发送结果给leader note    Redshift 数据库表的存储方式是列式存储 安全     backup &amp; restore   IAM   KMS    RedShift Cluster 是HA但不是fault-tolerant     A RedShift cluster is designed for performance and low latency, not for resilience, and while it has multiple components, the loss of any one will generate a short outage while it is replaced.    Redshift Spectrum:  专门用来读S3 的数据的 需要先有Redshift cluster query 读取S3后 会放到上千个 Spectrum nodes中进行处理 处理后放到Redshift compute node 中， 最后回到 leader node 中 案例：     公司RedShift中的一部分历史数据迁移到了S3中， 分析师希望有一个每日的报告， 要使用这部分历史数据， 最好的方法是 Redshift Spectrum         使用 RedShift 的 Spectrum 功能， 将S3的历史数据映射到RedShift的Cluster Table中。 分析团队就可以链表查询了          Redshift durability: https://aws. amazon. com/blogs/aws/automated-cross-region-snapshot-copy-for-amazon-redshift 想要增加RedShift 的 durability, 就用 cross-region snapshots Redshift Enhanced VPC Routing:  为Redshift 优化的VPC, 方便通过 VPC 拷贝和上传数据， 不需要走外网。Redshift 和 Athena 的比较:  都是读S3 进行分析 Redshift 更快， 感谢indexesGlue:  ETL 工具 （extract, transform, load） 是个中间工具， 用于从数据源获取数据后进行转换， 然后再给分析的应用 上游 抽取     RDS   S3    下游     Redshift   Athena -&gt; QuickSignt   EMR    Glue Data Catelog     Glue Data Crawler（爬虫）去S3, RDS, DynamoDB, JDBC 里面去抓取元数据   将元数据写入到Data Catelog中的数据库和表中   Glue Jobs 会 对数据进行转换   将数据转换后发给 Athena, Redshift, EMR   ElasticSearch/OpenSearch:  新的名字是OpenSearch 可以把数据库中的数据进行索引， 提供快速的搜索服务 是对传统数据库的一个补充 提供cluster 集群服务 与AWS 进行整合的服务包括     Kinesis Data Firehose   AWS IOT   CLoudWatch logs    支持 multi-az 安全     Cognito   IAM   KMS , SSL   VPC   灾备和迁移: 灾备回复策略: 备份策略:  backup restore     等生产挂了，重新把备份恢复回去    Pilot Light - 小灯 data center 为主 阉割（critical infrastructure）版本运行在AWS （基础的EC2(stop)、read replica 的 RDS） 不介意RTO (recovery point objective)比较长 , 类似备份恢复机制     data center 挂了   启动EC2   Route53 将流量切到AWS 的EC2上    Warm Standby - 热备份 Data center 为主 mini 的完整版本运行在AWS上 （ELB + EC2 带了ASG + slave RDB） data center 挂了， route53 马上切到AWS， 根据流量情况auto scale up/down EC2 和 RDS 的 Replica Multi Site / Hot Site Approach - 热站 Active Active 模式 Data Center 和 AWS 都是生产 Route 53 会将流量分散到两边， 哪边挂了都不影响。 AWS Muli Region 没有 data center 了 服务分别部署在两个 Region 中 Active Active 模式 Route 53 会将流量分散到两边， 哪边挂了都不影响。RTO &amp; RPO:    RTO：      Recovery Time Objective   从宕机到恢复运营的时间段, 越长越不好。      RPO:      Recovery Point Objective   从宕机到恢复运营， 恢复了怎样的程度。 也许恢复到了宕机前1周的程度，或者恢复了5分钟之前的程度。 越长越不好。   灾难发生后如何快速恢复:  场景1： Goden AMI + 拷贝到 region     安装完软件后创建一个AMI ， 把AMI 拷贝到所有的region, （当灾难发生时）使用每个region 本地的AMI 进行灾难恢复         安装完软件后创建一个AMI的目的是做一个Golden AMI     把AMI 拷贝到每个region 的目的是可以让每个region 在灾难发生时快速使用本地的 golden ami 来快速回复。          DMS - Data Migration Service:  数据迁移服务 支持的迁移包括     Homogeneous migration 同质迁移   Heterogeneous migration 异质迁移    必须要创建一个EC2 来执行 replication tasks     EC2 是在源和目标库之间进行迁移动作    源     on-premise database         oracle, mss sql server, mysql, mariadb, postgresql, mongodb, sap, db2          azure Sql db   amazone RDS   Amazon S3    Target     on-premise database         oracle, mss sql server, mysql, mariadb, postgresql, sap          AWS RDS   AWS Redshift   AWS DynamoDB   AWS S3   ElasticSearch   Kinesis   DocumentDB    AWS Schema Conversion Tool (SCT)     用于在异质数据库做迁移时，对数据模型的 Schema进行转换    迁移步骤     在源库所在环境安装AWS SCT   SCT 将源数据库schema 转换成目标库的schema， 并创建目标库   DMS 的 EC2 实例执行 replication task, 将源数据库的数据导入到目标数据库。   SMS - Server Migration Service:  把on-premise 的服务器迁移到AWSDataSync: DataSync:  从 on-premise/AWS 移动大量数据到 AWS     也可以是AWS不同region之间 进行数据迁移    比命令行快10倍 和 DMS 不同， DMS 迁的是数据库， DataSync 迁移的是文件系统的数据 上游     NAS, NFS, SMB 文件系统的文件    下游 （没有EBS, 因为EBS 不是文件系统。 所以不会从EBS 同步数据，也不会向EBS 同步数据）     S3   EFS   FSx    replication task 可以定时进行，每小时/天/周 需要在on-premise 安装一个 DataSync agent 可以设置迁移时使用的带宽限制 案例     NFS/SMB Server → DataSync agent → AWS DataSync → AWS S3/EFS/FSx   VPC1 EFS → VPC1 EC2 instance with DataSync agent → VPC2 AWS DataSync → VPC2 EFS   迁移大量数据的计算:  例如 迁移200T的数据， 带宽是100Mbps , 需要多长时间     数据大小（M）/ 速度（Mb/s） = 时间(s)         1 Mbps = 1/8 Mb/s     1 天 = 86400 s     200 *1000 * 1000 / ( 100/8 ) = 160000000s = 185天           如果用Direct Connect (DX) 1Gbps     需要等一个月， 然后还需要18. 5天    用 snowball 迁     2-3 个 snowball   1周时间把数据搞到snowball 里面 + 运输 + 迁移完成   AWS Backup: AWS Backup:    AWS Backup 对AWS 的服务备份进行集中管理，做到符合公司的合规要求和数据保护。     创建好back plan 之后， 备份就是自动进行     流程      AWS Backup → 创建备份计划 → 执行备份 → 备份拷贝到 S3      支持的服务包括 (   不包含S3   )      FSx   EFS   DynamoDB   EC2   EBS   RDS   Aurora   Storage Gateway      支持 跨 Region 备份     支持跨 账号 备份     支持 即时恢复 （PITR point-in-time recovery）     按序， 定时备份     基于 tag 的 备份策略  Backup Plan , 可以在不同维度设置backup plan 的 policy:  频率 - 每12小时， 周，月， cronjob 备份转为cold storage - 永不， day, week, month, year 备份保留时间 - 永远， days, weekly, monthly无服务: AWS Cognito: AWS Cognito:    是AWS 的一个独立服务， 为客户提供认证管理服务     Cognito User Pools      为用户提供登录（sign in）服务   和API Gateway 进行整合   返回JWT (json web token)      Cognito Identity Pools (Federated Identity)      为客户提供AWS 认证， 以便让用户可以访问AWS 服务   场景： 让一个facebook 用户可以访问AWS 的S3 bucket      Cognito Sync      在用device 和Cognito 之间同步数据      Cognito 通过 AWS STS 生成temporary credentials 给客户（mobile app）， 客户拿着证书再去访问其他的服务（AWS S3）      场景不仅仅是S3, 还可以是 DynamoDB, Lambda 等等   API Gateway: API Gateway:  提供 restful API 给客户、其他AWS 服务     通过Lambda 提供 Put, Post, Get, Delete 等 Restful API 接口    客户可以直接请求API gateway, api gateway 再调用 lambda 对逻辑进行处理， 然后将结果写入到 DynamoDB 可以提供API 版本功能 可以生成 SDK 和 API 文档说明书（specifications） 可以Cache API 的 response 安全     通过SigV4 提供 IAM Credential 认证和登录   Custom (Lambda ) Authorizer         利用Lambda 来验证请求的 header 中带过来的 token     lambda 会使用OAuth/SAML等第三方认证进行验证     Lamdba 需要返回一个IAM Policy 给用户， 下次用户就用这个policy 访问 API          AWS Cognito User Pools         AWS Gateway 通过访问 Cognito验证token     cognito user pool维护用户在google, facebook 等网站的用户认证。           关键词     Swagger   Open API   SigV4 (Signature Version 4 (SigV4) , 把将身份验证信息添加到由 HTTP 发送的 AWS API 请求的过程. )   Customer Authorizer   Cognito , Google, Facebook login   三种方法来部署API Gateway - Endpoint type:    Edge-optimized (default) : for global client.      API Gateway 部署在某个Region ,但是通过CloudFront edge location来优化线路， 加快全球的访问速度      Regional - 只在一个国家部署。     Private - 只能在AWS 内部被VPC 通过ENI 访问  Lambda: Lambda:  virtual functions 优势     自动 scaling   deployed globally   Lambda 的concurrency setting on function 体现了Well-Architected Framework的 Performance efficiency and cost optimization    限制因素， （超过了下面这些限制就不要用lambda 了）     不能跨 region   每个Lambda 最多可以运行15分钟，如果需要运行1小时的应用，就别用Lambda了   内存限制128m-10g   环境变量 4k   磁盘使用的大小 512M   Lamdba 代码和依赖包大小 250M   Lambda 压缩包大小50M   并发的lambda 个数1000（可以联系管理员（AWS Support）增加）    和Lambda 整合的主要服务     API Gateway   Kinesis   DynamoDB   S3   CloudFront   CloudWatch/Event Bridge   CloudWatch Logs   SNS/SQS   Cognito   etc….     Pricing     Lambda 按照vCPU 和 Memory 的使用收费   同时 也对程序的运行时间收费         所以要在Lambda function 中设置超时退出功能，否则程序一直run 会造成多花没必要的钱。          可以选择多种支付方式         pey per calls     pay per duration          Lambda Edge:  应用于CloudFront When a Lambda function version is deployed to Lambda@Edge, it becomes a globally scoped resource.  可以用Lambda 在边缘侧辅助CloudFront 做一些逻辑处理     对到应用的请求进行过滤   修改 CloudFront request 和 response         收到user 的请求后修改 - viewer request     发送给origin 时修改 - origin request     得到origin响应时修改 - origin response     返回给user 时修改 - viewer response           应用场景     网站安全和隐私处理   搜索引擎优化   处理爬虫 （bot mitigation）   实时边缘侧图片转换   A/B 测试 at edge   用户认证/登录   用户行为分析   Lambda 最佳实践:  如果Lambda中有代码会被其他lambda重用， 那么最好的办法是做一个Layer， Layer是一个zip 压缩包， 类似于一个类库， 可以被很多lamdba调用， 避免重复code.  Lambda 可以被打包放在容器中 所有Lamdba的依赖包都应该和Lamdba一起打到一个包里面 不要过度配置lambda函数的超时来提高可以占用的内存大小， 否则会有意想不到的额外费用产生。 Lambda默认是属于一个VPC的， 一旦VPC-enabled for Lamdba, 就需要设置路由， 让它能通过NAT访问互联网。 Lamdba可以快速根据流量的情况自动伸缩， 所以最好设置一个CloudWatch Alarm服务, 当lambda发生伸缩的时候（ConcurrentExecutions or Invocations exceeds the expected threshold）可以及时通知团队Lambda runtime 支持的开发语言:  C#/. NET Go Java Node. js Python RubyLambda access policy:  Lambda 函数访问策略具有多个灵活性范围，并且可以在 Lambda 服务中包含不同的资源和版本。Lambda 函数访问策略包括     Function   Version   Alias   Layer version   DynamoDB: DynamoDB:  跨AZ, 的高可用，自动伸缩， No-Sql数据库 DynamoDB 不是in-memory 数据库 Table 是存数据的表     每个表有一个primary key, 创建的时候就需要指定   每个表可以设置sort key, 用来排序   每个表可以有无限个 items(rows)   每个 item 可以有attributes （column）, 随便多少，随便是什么。 可以是null, 最大400k.    DnyamoDB 的Item 有TTL 限制， 过期会被删掉   DynamoDB tables can be configured using either On-Demand or Provisioned mode.          on-demand 可以支持 highly resilient and capable of automatically scaling read and write capacity.            安全     IAM Role   DynamoDB pricing , Read/Write Capacity Mode:  DynamoDB 的付费方式决定了读写能力 两种付费方式     Provisioned mode (default)         可以测流量的时候用     可设置 auto-scaling     需要先设置好读和写的吞吐量     读的单位是RCU （Read Capacity union）， 比如每秒读5M     写的单位是WCU               On-Demand Mode         不可预测流量的时候用     用多少付多少，          DynamoDB Accelerator (DAX):  可以理解成DynamoDB 的缓存， 加速DynamoDB 对外read 响应的效率 DynamoDB Dax 不是 in-memory DB, 但是有 in-memory cache 功能 5分钟的TTL for Cache(default) DAX 可以提高10倍的DynamoDB 的performance DAX 将是透明的，不需要重构应用程序, 所以解决了热分区（hot partition problem）问题。DynamoDB Streams:  有点类似S3 Event Notification 功能， 当对DynamoDB 进行 create/update/delete 动作时触发事件， 就会以stream 的方式输出数据和其他AWS服务进行互动 item-level     发送到 Kinesis data stream   被lambda读取   被 kinesis client library 读取   etc…    例题：     客户要找一个可以自动伸缩， 高可用， 可以根据IoT的 data feed 随时处理data attributes over time(随时), 可以基于数据提供输出一个 持续的 stream 。 你建议的数据库是：         DynameDB          DynamoDB Global Tables:  两个table 分别在两个region 之间通过 two-way replication 进行数据同步 也叫做 active-active replication 应用可以read /write 到任何一个table 前提条件是要打开 DynamoDB Stream, DynamoDB Stream 会把 changlog 来在另一个region 中的DB中复制数据 好处     Lower latency of table access   DynamoDB - TTL:  每条item 都有一个过期时间的字段 DynamoDB 会定期扫描过期item, 找到就删掉 场景     通过只保留近期的数据来减少不必要的数据量   有法规要求， 只能保留xxx 天数据   DynamoDB - Transactions （事务）:  写两个表， 要么都成功，要么都失败 如果写第二个表的时候失败了， 第一个成功了也要被撤销。特色服务: CloudFormation: CloudFormation:    Infrastructure as code.     描述一个infrastructure, CloudFormation 就帮你把它创造出来了      要安全组   要两个 EC2 使用这个安全组   要两个EIP, 绑定两个EC2   要S3   要个ELB, 放在前面   ….       Stack      一个Stack 是一堆AWS 资源的集合， 你可以把它们看做是一个单位进行管理.    StackSet - 可以跨账号、跨Region deploy 相同的template   StackSet 可以用一个CloudFormation模板(template) 就可以给多个账号部署相同的AWS 资源   需要通过Org 的 admin 账号设置 CloudFormation 的 template         Template 保存在S3 上     带版本      要update 一个 模板，不能删改原来的， 只能建一个新的版本修改      模板中的一些关键参数      Resource - 自己的AWS 资源   Parameters - dynamic 输入变量   Mappings - static 输入变量   Outputs - 指已经创建出来的内容   Conditionals: 列出创建资源需要的依赖   Metadata - 元数据      CloudFormation StackSets            好处      快速部署和重建环境   自动出一个架构图   可以生成一个模板，方便重用， 避免重新发明轮子   还可能出文档   CloudFormation与BeanStalk区别:  都可以用来调用云资源和服务，也可以做相同的一件事情     都可以快速创建应用环境    他们的区别如下： CloudFormation面向的是开发者，BeanStalk面向的是应用程序 CloudFormation是为做某件事情而整合资源，BeanStalk是为某个应用程序而整合资源。 使用CloudFormation要比使用BeanStalk复杂得多，CloudFormation的配置模版是一份json数据，这个对于不懂编程的人来说很难应用！BeanStalk可以通过界面向导来配置。Step Function:  无服务虚拟工作流 配合Lambda ，Glue 等无服务一起使用 json 格式 包括的功能     顺序   并行   条件   超时   错误处理。。。。    可以整合的服务     EC2,   ECS,   On- premise server   API Gateway    最多可执行1年 场景     工作流   订单处理流程   数据处理流程   网页应用。。。   SWF - Simple Workflow Service:  和 Step Function 都是工作流服务 运行在EC2 上的工作流服务 功能和Step Funcing 类似 AWS 更推荐用 Step Funcing 做工作流 但是SWF有一些特色是Step Function 没有的     需要外部的信号干预流程   需要子进程把信息返回给父进程   EMR - Elastic Map Reduce:  做大数据计算分析用的框架 通过Hadoop Cluster 创建 底层是EC2 实例 （可以有上百个） 使用 Spot 实例进行自动伸缩 支持 Apache Spark, HBase, Presto, Flink… 关键词：     数据处理   机器学习   web 索引   大数据   OpsWork:  自动化运维工具 底层是 Puppet 和 Chef 和EC2, on-premise VM 配合 Chef / Puppet 介绍     Managing configuration as code   使用 Recipes 和 Manifests 来管理配置   和 SSM/Beanstalk/CloudFormation 作用类似， 但是 Chef/Puppet 是开源的    关键词     Puppet   Chef   WorkSpaces:  安全的云桌面 (Cloud Desktop） 是集成了on-premise 的 VDI (Virtual Desktop Infrastructure) 整合了微软的 AD 关键词     VDI   Desktop   AppSync:  在Web App 和Mobile 之间保存，实时同步数据 使用 GraphQL (Facebook 的移动技术) 和DynamoDB / Lambda 整合 实时订阅 离线数据同步 关键词：     在web 和 mobile app之间同步数据   GraphQL   Cost Explorer:  把在AWS 使用的成本视觉化，便于理解 创建客户报告分析成本和使用数据 跨账号分析cost 按天，月，等时间尺度分析 可选择做一个 Saving Plan 来优化成本 通过过去的使用预测未来12个月的成本 关键词     成本有点高   节省成本   预测未来12个月内的成本   CICD: CICD:    AWS 提供一系列工具来进行CICD     整体用 AWS CodePipeline 作为底层支撑， 上面包含了：          CodeCommit          Code 管理              底层是 GitHub           CodeBuild          Build Test              底层是 Jenkins           BeansTalk / CodeDeploy          deploy , Perversion              底层是EC2, CloudFormation       CodeDeploy:    AWS CodeDeploy 可自动将软件部署到各种计算服务，例如      EC2、   Fargate、   Lambda   和您的本地服务器。      您可以定义要执行的策略，例如就地部署或蓝/绿部署。  Compute optimizer:  找到计算资源的优化空间 往往配合Cost Explorer 一起使用Trusted Advisor - 可信的顾问:  优化性能和安全方面的建议 不能优化成本几个 advisor 的比较: Trusted Advisor (account level):  查看账号下服务的性能和安全情况， 并给出建议Access advisor (user level)  可以看到有哪些服务权限授予了这个用户， 并且可以看到什么时候用户对这些服务进行了访问Access Analyzer  分析哪些资源分享给了外部实体"
    }, {
    "id": 19,
    "url": "http://localhost:4000/Python-in-Container-02/",
    "title": "Python in Container Part Two",
    "body": "2021/07/26 - 在本教程中，您将学习如何创建描述简单 Python 容器的 Dockerfile 文件。 使用容器创建 Web 服务器、数据库构建、运行和验证 Django、Flask 并通用 Python 应用实现程序的功能. 最后还会学习调试在容器中运行的应用程序。 Python in Container[TOC] Build Container Image: 继续 Python in Container 01 的笔记 Dockerizing PyTest and Pdb - 例子: Python 的测试分为几种  Static Tests - Pylint , etc,     提供静态测试，   就是看看代码，分析代码，但不运行代码,   IDE 就可以进行静态代码的检查   和容器无关    Unit Tests - PyTest, Unittest, etc, 提供单元测试     非常适合用容器进行单元测试   生产环境应该是越干净，越小越好    Black Box Tests,     模拟用户行为，   创建内容，查看功能，   比如 selenimum   使用 PyTest 进行单元测试: 下载 multistage 的示例代码 1234git clone https://github. com/pythonincontainers/multistagecd multistagedocker build -t factors_flask:cython-multi -f Dockerfile. cython-multi . 下载测试用例的代码 123cd . . git clone https://github. com/pythonincontainers/simpletestscd simpletests创建测试镜像 并 生成容器 123456# 用上面的镜像作为基础镜像docker build -t factors_flask:tester -f Dockerfile. tester --build-arg BaseImage=factors_flask:cython-multi . # 创建容器，创建过程中就自动执行了测试脚本，并输出结果，并在当前目录下生成 html 版本的报告docker run -it --rm -v ${PWD}:/data factors_flask:tester浏览器打开目录中的静态 HTML 文件查看结果 使用 PDB 对 python 进行 debug: https://docs. python. org/3/library/pdb. html pdb 的使用方法是 python -m pdb xxx. py 123456# 创建基于 pdb 的镜像docker build -t factors_flask:debug -f Dockerfile. debug --build-arg BaseImage=factors_flask:cython-multi . # 创建基于 pdb 的容器，并进入酵母模式docker run -it --rm -p 5000:5000 factors_flask:debug# 进入 Python 的 debug 模式Django 的容器化 - 开发: 123456789101112131415161718192021222324252627282930git clone https://github. com/pythonincontainers/djangoimagecd djangoimage# 查看 Dockerfile. mydjango# ARG BaseImage=python# ARG ImageTag=3. 7. 3# FROM $BaseImage:$ImageTag# ENV PYTHONUNBUFFERED 1# ARG DjangoVersion=2. 2. 1# RUN pip install Django==$DjangoVersion# WORKDIR /codecd base_imagedocker build -t mydjango:2. 2-3. 7. 3 -f Dockerfile. mydjango . cd . . # 测试一下镜像是否可以正常创建容器docker run -it --rm mydjango:2. 2-3. 7. 3 django-admin --version# 输出 django 版本 2. 2. 1# 给 镜像起个别名docker tag mydjango:2. 2-3. 7. 3 django#创建容器并启动 django 项目, 会在本地目录创建一个 myproject 的目录# 就算容器运行后 --rm 删掉了， 但是本机的目录和文件还在docker run -it --rm -v ${PWD}:/code django django-admin startproject myproject# 再次创建容器，并设置端口映射，进入交互模式docker run -it --rm -v ${PWD}:/code -p 8000:8000 django bash# 进入容器cd myproject/# 启动项目python manage. py startapp myappexit通过 Powershell 定义一个佳作 dj 的 function 12345# 下面这个命令是通过 powershell 定义 一个 function, 方便后面的命令执行，但是 mac 不知道怎么定义# Function dj {docker run -it --rm -p 8000:8000 -v ${PWD}:/code django $args}# 进行数据迁移docker run -it --rm -p 8000:8000 -v ${PWD}:/code django python manage. py migratedocker run -it --rm -p 8000:8000 -v ${PWD}:/code django python manage. py runserver 0:8000访问 http://localhost:8000 可以看到 django 界面 修改view. py 增加 12345678from django. shortcuts import render# Create your views here. from django. http import HttpResponsedef index(request):  return HttpResponse( hello, world.  )修改 urls. py 123456789from django. contrib import adminfrom django. urls import pathfrom myapp import viewsurlpatterns = [  path('admin/', admin. site. urls),  path('', views. index, name='index'), ]重启服务器，访问 http://localhost:8000 可以看到 hello, world Django 的容器化 - 生产: Django 应用需要支持  支持技术包括应用服务器，数据库，缓存，Proxy 服务器 运行时环境架构 应用生命周期 更新和升级 缩放 服务质量 安全 高可用Django 应用的准备流程  django-admin startproject django-polls  创建项目 python manage. py startapp polls  创建项目的应用 Create the source code 写代码 python manage. py makemigrations polls  数据库初始化 python manage. py migrate  数据库迁移（创建） python manage. py createsuperuser 创建超级用户 python manage. py loaddata …  读取json数据到数据库中 python manage. py collectstatic  统计所有的状态，并放到一个目录下 python manage. py runserver … 启动 web 服务Python 容器的架构设计: 开发架构 测试架构 生产架构 示例代码 - 投票 demo1: 12git clone https://github. com/pythonincontainers/django-pollscd django-polls查看几个文件  Dockerfile. demo1  Dockerfile 文件 Mysite/settings. py 配置文件 Initial_data. json 初始化的数据ARG BaseImageFROM $BaseImageENV PYTHONUNBUFFERED 1WORKDIR /codeCOPY . . EXPOSE 8000RUN python manage. py makemigrations polls &amp;&amp; \  python manage. py migrate &amp;&amp; \  python manage. py loaddata initial_data. json &amp;&amp; \  python manage. py collectstaticENTRYPOINT [ python ,  manage. py ]CMD [ runserver ,  0:8000 ]创建镜像 demo1 , 并运行容器 1234docker build -t django-polls:demo1 -f Dockerfile. demo1 --build-arg BaseImage=django . docker run -d --name polls-demo -p 8000:8000 django-polls:demo1访问 http://localhost:8000/polls/ 查看投票 创建管理员用户: 三种方法 手动创建管理员用户，准备进行 demo2123docker exec -it polls-demo python manage. py createsuperuser# 输入 admin , admin@example. com, admin, admin访问 http://localhost:8000/admin/ 登录后台 自动创建 - 导出床架好的数据放在初始化数据中123docker exec -it polls-demo python manage. py dumpdata auth. user --indent 4# 将以下为输出追加保存到 initial_data. json 文件中docker rm -f polls-demo12345678910111213141516171819{   model :  auth. user ,   pk : 1,   fields : {     password :  pbkdf2_sha256$150000$nQfMOKShamoF$8ox6PWBZSlaxDDxzHOD0zhe4XcEYVEqARglRWuhUkCg= ,     last_login :  2021-04-28T14:32:47. 694Z ,     is_superuser : true,     username :  admin ,     first_name :   ,     last_name :   ,     email :  admin@example. com ,     is_staff : true,     is_active : true,     date_joined :  2021-04-28T14:31:38. 717Z ,     groups : [],     user_permissions : []  }}重新 build 镜像并 run 123docker build -t django-polls:demo1 -f Dockerfile. demo1 --build-arg BaseImage=django . docker run -d --name polls-demo -p 8000:8000 django-polls:demo1访问 http://localhost:8000/admin/ 登录后台 , 不需要手动创建超级用户 自动创建 - 在 Dockerfile 中将需要创建的数据用 python 命令执行看Dockerfile. demo2, 多了一行 RUN echo  from django. contrib. auth. models import User; User. objects. create_superuser('admin', 'admin@example. com', 'admin')  | python manage. py shell# 以上命令就是把 echo 出来的内容作为参数，让 python manage. py shell 来执行删掉刚才加到 initial_data. json 中数据，用 Dockerfile. demo2 建立镜像并运行容器 1234567# 删掉容器docker rm -f polls-demodocker build -t django-polls:demo2 -f Dockerfile. demo2 --build-arg BaseImage=django . docker run -it --rm -p 8000:8000 django-polls:demo2访问 http://localhost:8000/admin/ 登录后台 , 不需要手动创建超级用户， 已经在 build 和 run 的阶段就做好了 对创建的容器进行单元测试因为示例代码中有polls/tests. py ，用于进行单元测试 12345678910docker run -it --rm django-polls:demo2 test# 输出为# Creating test database for alias 'default'. . . # System check identified no issues (0 silenced). #. . . . . . . . #----------------------------------------------------------------------# Ran 8 tests in 0. 090s# OK# Destroying test database for alias 'default'. . . 运行 Django 和 Flask 的 应用服务器: Python 应用服务包括  Gunicorn (Green Unicorn) 绿色独角兽 uWSGI  全称 Python Web Server Gateway Interface Mod_wsgi  为 apache 定制前两个可以和任何 web proxy server 工作，最后一个是用来和 apache 工作的 Django app 前端的进化  Django development server application server proxy server (比如 Nigix) + application server AWS Load Balancer + Application Server CDN Server + Applicaiton server Kubernetes Ingress + Application server Istio Service Mesh + Application ServerGunicorn 的使用: 简单介绍 先创建 base image查看 base_image/Dockerfile. mygunicorn  基于 python 安装好 django 和 gunicorn 的镜像ARG BaseImage=pythonARG ImageTag=3. 7. 3FROM $BaseImage:$ImageTagENV PYTHONUNBUFFERED 1ARG DjangoVersion=2. 2. 1ARG GunicornVersion=19. 9. 0RUN pip install Django==$DjangoVersion gunicorn==$GunicornVersion创建 base image 123cd base_imagedocker build -t gunicorn -f Dockerfile. mygunicorn . cd . . 查看 Dockerfile. gunicorn ARG BaseImageFROM $BaseImageENV PYTHONUNBUFFERED 1WORKDIR /codeCOPY . . EXPOSE 8000RUN python manage. py makemigrations polls &amp;&amp; \  python manage. py migrate &amp;&amp; \  python manage. py loaddata initial_data. json &amp;&amp; \  python manage. py collectstatic &amp;&amp; \  echo  from django. contrib. auth. models import User; User. objects. create_superuser('admin', 'admin@example. com', 'admin')  | python manage. py shellCMD [ gunicorn ,  -c ,  gunicorn. ini ,  mysite. wsgi ] # 主要区别gunicorn -c gunicorn. ini mysite. wsgi 查看 gunicorn. ini 1234567bind =  0. 0. 0. 0:8000 workers = 2worker_connections = 1000timeout = 30user = Nonegroup = None创建 django 的 gunicorn 镜像, 并启动 容器 1234docker build -t django-polls:gunicorn -f Dockerfile. gunicorn --build-arg BaseImage=gunicorn . docker run -it --rm -p 8000:8000 django-polls:gunicorn访问 http://localhost:8000/polls/ uWSGI: 介绍 查看 base_image/Dockerfile. myuwsgi  基于 python 安装好 django 和 uwsgi 的镜像ARG BaseImage=pythonARG ImageTag=3. 7. 3FROM $BaseImage:$ImageTagENV PYTHONUNBUFFERED 1ARG DjangoVersion=2. 2. 1ARG uWSGIVersion=2. 0. 18RUN pip install Django==$DjangoVersion uwsgi==$uWSGIVersion 123cd base_imagedocker build -t uwsgi -f Dockerfile. myuwsgi . cd . . 查看 Dockerfile. uwsgi ARG BaseImageFROM $BaseImageENV PYTHONUNBUFFERED 1WORKDIR /codeCOPY . . EXPOSE 8000RUN python manage. py makemigrations polls &amp;&amp; \  python manage. py migrate &amp;&amp; \  python manage. py loaddata initial_data. json &amp;&amp; \  python manage. py collectstatic &amp;&amp; \  echo  from django. contrib. auth. models import User; User. objects. create_superuser('admin', 'admin@example. com', 'admin')  | python manage. py shellCMD [ uwsgi ,  uwsgi-http. ini ] # 区别在这里，启动命令的变化查看 uwsgi-http. ini 1234567891011[uwsgi]chdir = /codemodule = mysite. wsgi:applicationmaster = Truepidfile = /tmp/project-master. pidvacuum = Trueharakiri = 20max-requests = 5000http-socket = 0. 0. 0. 0:8000processes = 2创建 django 的 uwsgi 镜像, 并启动 容器 123docker build -t django-polls:uwsgi -f Dockerfile. uwsgi --build-arg BaseImage=uwsgi . docker run -it --rm -p 8000:8000 django-polls:uwsgi生产级别的数据库 - PostgreSQL:  目的是创建两个独立的容器， 一个是 数据库，另一个是链接数据库的 django 数据会独立于数据库存在于一个持久的存储上介绍: Python 对数据库的支持能力  Python Database API - PEP 249 SQLAlchemy Object-relational Mappers (ORM): PeeWee, PonyORM, SQLObject, Tortois ORM, Storm Native DB Engine Drivers: Mysql, ProsgreSQL, MongoDB, CouchDB, CassandraDjango 的数据库驱动  SQLite3 PostgreSQL Mysql and variants(MariaDB, etc,) Oracle Database Server数据库的设置主要在 settings. py 中 文档 下面是 对应的设置例子  ENGINE 需要提前设置好 USER 需要提前就存在 也可以通过环境变量来替换 hardcode 的配置信息123456789101112131415161718# SQLite3DATABASES = {  'default': {    'ENGINE': 'django. db. backends. sqlite3',    'NAME': os. path. join(BASE_DIR,'db. sqlite3'),  }}# PostgreSQLDATABASES = {  'default': {    'ENGINE': 'django. db. backends. postgresql', # 可以换成 mysql, sqlite2, oracle    'NAME': 'pollsdb',    'USER': 'pollsuser',    'PASSWORD': 'pollspass',    'HOST': 'db',    'PORT': '5432',  }}创建数据库所需的网络 和 posgresql 容器: 123456docker network create polls_net#docker run -d --name db --network polls_net -e POSTGRES_PASSWORD=myprecious postgres#docker exec -it db psql -U postgres# docker run -it --rm --network polls_net postgres psql -h db -U postgres手动创建的话参考下面的 postgresql 命令 12345CREATE USER pollsuser with password 'pollspass' CReATEDB;Create DATABASE POLLSDB WITH OWNER pollsuser;\l\du\q创建数据库容器  -v 可以将数据独立于存储123456docker rm -f db# docker run -d --name db --network polls_net -e POSTGRES_DB=pollsdb -e POSTGRES_USER=pollsuser -e POSTGRES_PASSWORD=pollspass postgresdocker run -d --name db --network polls_net -v polls_vol:/var/lib/postgresql/data -e POSTGRES_DB=pollsdb -e POSTGRES_USER=pollsuser -e POSTGRES_PASSWORD=pollspass postgres创建需要连接 postgre 的Django应用镜像和容器: 查看 Dockerfile. postgres ARG BaseImageFROM $BaseImageENV PYTHONUNBUFFERED 1WORKDIR /codeCOPY . . EXPOSE 8000RUN pip install psycopg2 # 是 postgresql 的 客户端RUN python manage. py makemigrations polls &amp;&amp; python manage. py collectstaticARG DjangoSettings=mysite. settings_postgresENV DJANGO_SETTINGS_MODULE=$DjangoSettingsCMD [ gunicorn ,  -c ,  gunicorn. ini ,  mysite. wsgi ]查看 mysite3/settings_postgres. py 12345678910DATABASES = {  'default': {    'ENGINE': 'django. db. backends. postgresql',    'NAME': 'pollsdb',    'USER': 'pollsuser',    'PASSWORD': 'pollspass',    'HOST': 'db',    'PORT': '5432',  }}创建镜像, 并启动容器，初始化数据并创建管理员用户 12345678910111213141516docker images gunicorndocker build -t django-polls:postgres -f Dockerfile. postgres --build-arg BaseImage=gunicorn . # 初始化数据库docker run -it --rm --network polls_net django-polls:postgres python manage. py migrate# 读取数据docker run -it --rm --network polls_net django-polls:postgres python manage. py loaddata initial_data. json# 创建管理员 # 这里用 admin, admin@example. com, admin, admindocker run -it --rm --network polls_net django-polls:postgres python manage. py createsuperuser# 启动web 服务docker run -it --rm --network polls_net -p 8000:8000 django-polls:postgres # 停止服务# 对容器进行单元测试docker run -it --rm --network polls_net -p 8000:8000 django-polls:postgres python manage. py test创建 PGAdmin 用来管理数据: 1234#docker rm -f db#docker run -d --name db --network polls_net -v polls_vol:/var/lib/postgresql/data -e POSTGRES_DB=pollsdb -e POSTGRES_USER=pollsuser -e POSTGRES_PASSWORD=pollspass postgresdocker run -d --name pgadmin --network polls_net -p 8080:80 -e PGADMIN_DEFAULT_EMAIL=admin@example. com -e PGADMIN_DEFAULT_PASSWORD=admin dpage/pgadmin4访问 http://localhost:8080/  登录用户名密码 admin@example. com admin 创建 数据库连接 db , db , pollsuser, pollspass 查看数据， 查看用户和权限删除数据库容器的步骤:    数据库容器删除了， 但是数据会保存在存储上，重建容器后，应用可以照常访问数据库的数据     数据库容器需要通过 kill 来删除， 确保数据可以有效的保存到存储中  123docker kill -s SIGTERM dbdocker rm dbdocker rm -f db生产级别的数据库 - MariaDB: 和 上一章的 postgresql 比较， 除了 setting. py 之外，其他 django 代码不需要做任何变化 MariaDB image 的说明 创建一个持久的存储, 快速创建一个 MariaDB 容器: 123456docker kill -s SIGTERM dbdocker rm -f dbdocker volume create maria_voldocker run -d --name db --network polls_net -e MYSQL_DATABASE=pollsdb -e MYSQL_USER=pollsuser -e MYSQL_PASSWORD=pollspass -e MYSQL_ROOT_PASSWORD=myprecious -v maria_vol:/var/lib/mysql mariadb# 默认的超级用户名是 root查看 Dockerfile. mariadb ARG BaseImageFROM $BaseImageENV PYTHONUNBUFFERED 1WORKDIR /codeCOPY . . EXPOSE 8000RUN pip install mysqlclient # 区别RUN python manage. py makemigrations polls &amp;&amp; python manage. py collectstaticARG DjangoSettings=mysite. settings_mariadb # 区别ENV DJANGO_SETTINGS_MODULE=$DjangoSettingsCMD [ gunicorn ,  -c ,  gunicorn. ini ,  mysite. wsgi ]查看 mysite/settings_mariadb 12345678910DATABASES = {  'default': {    'ENGINE': 'django. db. backends. mysql',     'NAME': 'pollsdb',    'USER': 'pollsuser',    'PASSWORD': 'pollspass',    'HOST': 'db',    'PORT': '3306',  }}创建链接 Mariadb 的 django 镜像和容器: 12345678docker build -t django-polls:mariadb -f Dockerfile. mariadb --build-arg BaseImage=gunicorn . docker run -it --rm --network polls_net django-polls:mariadb python manage. py migratedocker run -it --rm --network polls_net django-polls:mariadb python manage. py loaddata initial_data. jsondocker run -it --rm --network polls_net django-polls:mariadb python manage. py createsuperuser# admin@example. com, admin, admin# 启动 web 服务docker run -it --rm --network polls_net -p 8000:8000 django-polls:mariadb访问 http://localhost:8000/polls/ 使用 dj-database-url 来为 Django 建立一个适应多个数据库的方案: 查看官方文档  Support currently exists for     PostgreSQL,   PostGIS,   MySQL,   MySQL (GIS),   Oracle,   Oracle (GIS), and   SQLite.     用了 dj-database-url 只要修改 settings. py 就可以了查看 mysite/settings_universal. py 12345import dj_database_urlDATABASES = {  'default': dj_database_url. config(default='sqlite:////code/db. sqlite3')}查看 Dockerfile. db-universal ARG BaseImageFROM $BaseImageENV PYTHONUNBUFFERED 1WORKDIR /codeCOPY . . EXPOSE 8000# 下面一行是区别， 安装了所有可能使用的数据库的 python 客户端扩展，和 dj-database-urlRUN pip install psycopg2==2. 8. 2 mysqlclient==1. 4. 2 cx-Oracle==7. 1. 3 dj-database-url==0. 5. 0RUN python manage. py makemigrations polls &amp;&amp; python manage. py collectstaticARG DjangoSettings=mysite. settings_universal #区别在上面ENV DJANGO_SETTINGS_MODULE=$DjangoSettingsCMD [ gunicorn ,  -c ,  gunicorn. ini ,  mysite. wsgi ]Database_URL  创建能链接各种数据库的 django 镜像 和 容器 1234docker build -t django-polls:db-universal -f Dockerfile. db-universal --build-arg BaseImage=gunicorn . # 假设我们用 mariadbdocker run -it --rm --network polls_net -p 8000:8000 -e  DATABASE_URL=mysql://pollsuser:pollspass@db/pollsdb  django-polls:db-universal访问 http://localhost:8000/polls/ 生产级别的 web proxy server: proxy server rationale 理由  强大和精密(sophisticated)的 web server 支持 https 提供负载均衡本章要设计的架构 - 三个独立的容器， db, app, proxy server 本章要做的事情  建立数据库容器 建立应用 建立基于 nginx 的 proxy server 负载均衡练习先建立postgresql数据库容器: 1234567docker kill -s SIGTERM dbdocker rm -f dbdocker run -d --name db --network polls_net -e POSTGRES_PASSWORD=myprecious -v polls_vol:/var/lib/postgresql/data postgresdocker logs db# 看到 database system is ready to accept connections 就 OK 了建立 应用的 镜像和容器: 查看 Dockerfile. uwsgi4nginx  这次用了 uwsgi 替换了 gunicorn （去链接 nginx） 作为 base imageARG BaseImageFROM $BaseImageENV PYTHONUNBUFFERED 1WORKDIR /codeCOPY . . EXPOSE 8000RUN pip install psycopg2==2. 8. 2 mysqlclient==1. 4. 2 cx-Oracle==7. 1. 3 dj-database-url==0. 5. 0RUN python manage. py makemigrations polls # 区别， 并没有ython manage. py collectstaticARG DjangoSettings=mysite. settings_universalENV DJANGO_SETTINGS_MODULE=$DjangoSettingsCMD [ uwsgi ,  uwsgi-nginx. ini ] # 区别 用了 uwsgi 替换了 gunicorn 去链接 nginx查看 uwsgi-nginx. ini 1234567891011[uwsgi]chdir = /codemodule = mysite. wsgi:applicationmaster = Truepidfile = /tmp/project-master. pidvacuum = Trueharakiri = 20max-requests = 5000socket = 0. 0. 0. 0:8000processes = 2创建 django 镜像，初始化数据， 创建超级用户 12345678910# uwsgi 是一个自建的 base imagecd build_imagedocker build -t uwsgi -f Dockerfile. myuwsgi . cd . . docker build -t django-polls:uwsgi4nginx -f Dockerfile. uwsgi4nginx --build-arg BaseImage=uwsgi . docker run -it --rm --network polls_net -e  DATABASE_URL=postgres://pollsuser:pollspass@db/pollsdb  django-polls:uwsgi4nginx python manage. py migratedocker run -it --rm --network polls_net -e  DATABASE_URL=postgres://pollsuser:pollspass@db/pollsdb  django-polls:uwsgi4nginx python manage. py loaddata initial_data. jsondocker run -it --rm --network polls_net -e  DATABASE_URL=postgres://pollsuser:pollspass@db/pollsdb  django-polls:uwsgi4nginx python manage. py createsuperuser创建 app1 容器 123docker run -d -e  DATABASE_URL=postgres://pollsuser:pollspass@db/pollsdb  --network polls_net --name app1 django-polls:uwsgi4nginxdocker logs app1建立 Nginx 镜像和容器: 查看 Dockerfile. nginx  这里用了multi-stage buildFROM django as devWORKDIR /codeCOPY . . RUN pip install dj-database-url==0. 5. 0ARG DjangoSettings=mysite. settings_universalENV DJANGO_SETTINGS_MODULE=$DjangoSettingsRUN python manage. py collectstaticFROM nginx:1. 17. 0WORKDIR /codeCOPY --from=dev /code/static /code/staticCOPY mysite_nginx. conf /etc/nginx/conf. d/查看 mysite_nginx. conf 12345678910111213141516171819202122232425262728# mysite_nginx. confupstream django {  server app1:8000;}server {  listen   8000;  server_name _; # special  catch all  Server Name, please substitute with something appropriate  charset   utf-8;  client_max_body_size 75M;  # adjust to taste  location /media {    alias /code/media; # if Media Files are to be served  }  location /static {    alias /code/static; # if Static Files are to be served  }  # Finally, send all non-media requests to the Django server.   location / {    uwsgi_pass django;    include   /etc/nginx/uwsgi_params;  }}创建 nginx 镜像, 运行容器 1234docker build -t mynginx -f Dockerfile. nginx . docker run -it --rm --network polls_net -p 8000:8000 mynginx# 或者docker run -d --network polls_net -p 8000:8000 mynginx建立 带 SSL 的 Nginx 镜像和容器: 需要用多阶段的 build 先 build django, 然后 build ubuntu 做 SSL证书, 最后 build nginx 查看 Dockerfile. nginx-ssl FROM django as devWORKDIR /codeCOPY . . RUN pip install dj-database-url==0. 5. 0ARG DjangoSettings=mysite. settings_universalENV DJANGO_SETTINGS_MODULE=$DjangoSettingsRUN python manage. py collectstaticFROM ubuntu:18. 04 as sslRUN apt-get update &amp;&amp; apt-get install -y opensslWORKDIR /sslRUN openssl req -x509 -sha256 -nodes -days 365 -newkey rsa:2048 \  -keyout django-polls. key. pem -out django-polls. crt. pem \  -subj  /CN=django-polls. example. com FROM nginx:1. 17. 0WORKDIR /codeCOPY --from=dev /code/static /code/staticCOPY --from=ssl /ssl/django-polls. key. pem /ssl/django-polls. crt. pem /code/COPY mysite_nginx_ssl. conf /etc/nginx/conf. d/查看 mysite_nginx_ssl. conf 123456789101112131415161718192021222324252627282930# mysite_nginx_ssl. confupstream django {  server app1:8000;}server {  listen 443 ssl;  ssl_certificate /code/django-polls. crt. pem;  ssl_certificate_key /code/django-polls. key. pem;  server_name django-polls. example. com;  charset   utf-8;  client_max_body_size 75M;  # adjust to taste  location /media {    alias /code/media; # if Media Files are to be served  }  location /static {    alias /code/static; # if Static Files are to be served  }  # Finally, send all non-media requests to the Django server.   location / {    uwsgi_pass django;    include   /etc/nginx/uwsgi_params;  }}建立 ngins ssl 的镜像和容器 123docker build -t mynginx:ssl -f Dockerfile. nginx-ssl . docker run -it --rm --network polls_net -p 443:443 mynginx:ssl访问https https://localhost/polls/ 用 Nginx 做负载均衡: 先建立第二个 应用容器 12docker run -d -e  DATABASE_URL=postgres://pollsuser:pollspass@db/pollsdb  --network polls_net --name app2 django-polls:uwsgi4nginx然后设置 mysite_nginx_ssl. conf ,添加 app2 修改部分如下 1234upstream django { server app1:8000; server app2:8000;}重新 build nginx image , 并启动 nginx 容器 123456docker build -t mynginx:lb -f Dockerfile. nginx-ssl . docker run -d --name proxy --network polls_net -p 443:443 mynginx:lbdocker logs app1docker logs app2docker system prune打开 https://localhost/polls/ 进行操作和刷新， 然后查看两个容器的日志， 会发现该有些访问走 app1, 有些走 app2 自动化的需求: 把之前做过的内容进行自动化，让其他程序员可以快速部署和我们一样的环境 组件的依赖关系 需要建立的镜像和容器 所需要执行的命令列表  如果把所有这些放到 shell 脚本也可以执行，但是并不是很友好的方法 另一种方法是放到 k8s 中进行12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849#git clone https://github. com/pythonincontainers/base_image#cd base_image#docker build -t django -f Dockerfile. mydjango . #docker build -t uwsgi -f Dockerfile. myuwsgi . #建立 base images, django 和 uwsgigit clone https://github. com/pythonincontainers/djangoimagecd djangoimagecd base_imagedocker build -t django -f Dockerfile. mydjango . cd . . /. . git clone https://github. com/pythonincontainers/django-pollscd django-pollscd base_imagedocker build -t uwsgi -f Dockerfile. myuwsgi . cd . . #git clone https://github. com/pythonincontainers/django-polls# 建立代码镜像docker build -t django-polls:uwsgi4nginx -f Dockerfile. uwsgi4nginx --build-arg BaseImage=uwsgi . # 建立存储和网络docker network create polls_netdocker volume rm polls_voldocker volume create polls_vol# 建立数据库容器docker kill db app1 app2 proxydocker rm -f db app1 app2 proxydocker run -d --network polls_net -e  POSTGRES_USER=pollsuser  -e  POSTGRES_PASSWORD=pollspass  -e  POSTGRES_DB=pollsdb  --name db -v polls_vol:/var/lib/postgresql/data postgresdocker logs dbdocker run -it --rm --network polls_net -e  DATABASE_URL=postgres://pollsuser:pollspass@db/pollsdb  django-polls:uwsgi4nginx python manage. py migratedocker run -it --rm --network polls_net -e  DATABASE_URL=postgres://pollsuser:pollspass@db/pollsdb  django-polls:uwsgi4nginx python manage. py loaddata initial_data. jsondocker run -it --rm --network polls_net -e  DATABASE_URL=postgres://pollsuser:pollspass@db/pollsdb  django-polls:uwsgi4nginx python manage. py createsuperuserdocker run -d --name pgadmin --network polls_net -p 8080:80 -e PGADMIN_DEFAULT_EMAIL=admin@example. com -e PGADMIN_DEFAULT_PASSWORD=admin dpage/pgadmin4# 创建 app1 , app2 的容器docker run -d -e  DATABASE_URL=postgres://pollsuser:pollspass@db/pollsdb  --network polls_net --name app1 django-polls:uwsgi4nginxdocker run -d -e  DATABASE_URL=postgres://pollsuser:pollspass@db/pollsdb  --network polls_net --name app2 django-polls:uwsgi4nginx# 创建 Nginx proxy 的镜像docker build -t mynginx:ssl -f Dockerfile. nginx-ssl . # 建立 proxy 容器docker run -d --name proxy --network polls_net -p 443:443 -v ${PWD}/mysite_nginx_ssl_lb. conf:/etc/nginx/conf. d/mysite_nginx_ssl. conf mynginx:ssl访问前端页面 https://localhost/polls/ 访问管理后台 https://localhost/admin/ root admin 访问数据库管理界面 http://localhost:8080/  登录用户名密码 admin@example. com admin 创建 数据库连接 db , db , pollsuser, pollspass 查看数据， 查看用户和权限Shipping Container 容器的运输: Shipping Image 镜像运输: Python 应用运输的方法  用 docker export 将容器导入到 tar 文件 用 docker save 将镜像保存到tar 文件 用 docker load 将tar 文件导入到 镜像中 通 docker push 将本地镜像上传到 Docker Hub 上，为镜像进行注册 用 docker pull 将镜像拉到本地 镜像库 其他公有云的镜像仓库， Azure, AWS, Google, 可以建立本地镜像仓库， 自己电脑上， 或者公司界别的镜像库利用 docker export 和 docker import 在 vm 中建立镜像和容器: 12345678910111213141516171819202122232425262728293031git clone https://github. com/pythonincontainers/flask-hellocd flask-hellodocker build -t flask-hello:alpine -f Dockerfile-alpine . docker images flask-hello# 创建一个 stop 的容器docker create --name flask-hello flask-hello:alpine # 从容器导出一个 tar 包docker export -o flask-hello. tar flask-hellodocker rm flask-hello# 在 virtual box 上创建一个 叫 local 的 虚拟机docker-machine create localdocker-machine lsdocker-machine scp flask-hello. tar local:/tmpdocker-machine ssh local# 进入虚机后docker imagesdocker import --change  WORKDIR /myproject  --change  CMD python flask-hello. py  /tmp/flask-hello. tar flask-hello:alpine  docker images# 已经有新的 image 了# 运行 local 上的容器docker run -it --rm -p 5000:5000 flask-hello:alpine# 查看 虚机 local 的地址docker-machine ip local# 输出 192. 168. 99. 104访问 http://192. 168. 99. 104:5000 练习 docker save , docker load: 1234567891011121314151617181920212223# 在 vm 中删除镜像，并删除. tar 文件docker rmi flask-hello:alpinerm /tmp/flask-hello. tarexit# 回到主机docker images # REPOSITORY  TAG    IMAGE ID    CREATED    SIZE# postgres   latest  26c8bcd8b719  3 weeks ago  314MB# 将三个镜像打包到一个images. tar 文件中docker save -o images. tar postgres django-polls:uwsgi4nginx mynginx:ssldocker-machine scp images. tar local:/tmpdocker-machine ssh local# 进入 虚机docker images  # 空docker load -i /tmp/images. tardocker images # 有三个镜像了# 清理战场exitrm -f *. tardocker-machine rm localImage Registries and Repositories: 一些概念:    Repository：镜像仓库，用于存储具体的docker镜像，起到的是仓库存储作用，比如Tomcat下面有很多个版本的镜像，它们共同组成了Tomcat的Repository，我们通过tag来区分镜像版本     Registry：注册服务器，管理镜像仓库，起到的是服务器的作用，比如官方的是Docker Hub，它是开源的，我们一般通过docker pull默认是拉取官方镜像仓库的镜像，当然我们也可以自己部署一个，拉取我们私有仓库镜像只需加上私有仓库IP+端口命令格式     Registry上有很多的Repository，Redis、Tomcat、MySQL等等Repository组成了Registry。  docker pull &lt;registry&gt;:port/&lt;repository&gt;:&lt;tag&gt; 12345docker. io/pythonincontainers/simple-flask:v1. 1gcr. io/google-containers/python:3. 5. 1-slimgitlab. com/pythonincontainers/section4/lesson1/example1:latestregistry. mycompany. com/project-zeta/serviceA:0. 9. 1localhost:5001/django-polls:nginx-lbImange name Offiial images - short names:     Python Post nginx    Official image - short name + tags     Python:3. 7-slim python:2. 7. 16-alpine3. 9    Docker hub images - full name + tags:     Pythonincontainers/flask-hello:1. 0   Image Repository , 就是仓库里我们自己创建的镜像的名字下面，django-pools 就是 repository 的名字 所有镜像的名字都是一样的， 不用管 tag 的名字  Django-pools:uwsgi4nginx Django-pools:db-universal Django-pools:mariadb Django-pools:postgres Django-pools:uwsgi Django-pools:gunicornImage Registry 存储 image repositories 的地方 ， 比如 Docker Hub 用来管理和发布镜像 push image 到 registry 从 registry pull 镜像 云registry 和 本地 registry 分为公有 registry 和私有 registry查找和使用 image 时的逻辑流程  分析image 的名字，分解出 registry, repository 和 tag 检查 Docker 的 本地 image catch 是否有这个image, 如果有，跳到第四步 从 registry 下载镜像到本地的 image cache 使用本地镜像中的 imageReview of Key Cloud Registries 公有云提供的镜像仓库: 任何人或者组织都可以分享他们的 registries 一些知名的 Cloud Image Registries  docker hub Quay. io 由 RedHat 维护， 是付费的镜像仓库 Gitlab container registry 和 docker hub 很像， 知名的免费 镜像仓库， 可以集成 CICD pipeline Google Container Registry (GCR) Amazon Elastic Container Registry （ECR） 文档 国际版 Azure Container RegistryReview of Local Registry 技术 了解本地镜像仓库技术: 有一些 image registry 软件可以在本地环境进行安装，用于管理镜像仓库 本地镜像仓库的优势:  简单快速的分享镜像 自主的访问控制 高安全性 灵活的存储后台配置 灵活的整合能力，尤其是和企业内部的一些软件或者流程 有仓库缓存选项Docker Local Registry:  开源的 Docker Registry 商业版本 Docker 新任的 Registry 可以通过容器镜像提供可以在 github 中找到 https://github. com/distribution/distribution 运行一个 Docker Local Registry 123git clone https://github. com/pythonincontainers/local-registrycd local-registry/docker run -d -p 5001:5000 --restart always --name registry-local registry:2还需要一个 Docker Registry 的 UI  一个个人开发的 Docker Registry UI, https://joxit. dev/docker-registry-ui/ 可以通过刚刚下载的Dockerfile. registry-gui 来生成镜像 Dockerfile 会使用通过刚刚下载的 registry-gui. yml12345678910docker build -t myregistry:gui -f Dockerfile. registry-gui .  # 创建一个 gui 镜像docker rm -f registry-local # 删掉原来的docker run -d --name registry-local --restart always -p 5551:5551 myregistry:gui # 创建一个本地镜像仓库的容器docker logs registry-local# docker tag factors_flask:cython-multi localhost:5551/factors_flask:multidocker tag flask-hello:alpine localhost:5551/flask-hello:myversion # 为我们本地缓存中的随便一个镜像打个标签docker push localhost:5551/flask-hello:myversion # 将这个镜像 push 到 本地镜像仓库中# 创建一个 本地镜像管理界面的容器docker run -d --name registry-ui -p 8880:80 -e URL=http://localhost:5551 -e DELETE_IMAGES=true joxit/docker-registry-ui:static 访问 http://localhost:8880 就可以看到刚才 push 的镜像了 在 VM 中创建一个 Docker 镜像的本地仓库 123456789docker build -t myregistry:cache -f Dockerfile. hub-mirror . docker run -d --name registry-cache -p 5552:5552 --restart always myregistry:cachedocker logs registry-cachedocker-machine create --driver hyperv --engine-registry-mirror http://192. 168. 31. 51:5552 --engine-insecure-registry http:192. 168. 31. 51:5551 docker1# 这里报错了， 说: Error with pre-create check:  Powershell was not found in the path docker-machine env docker1# 清理掉没用的镜像和容器其他的本地镜像软件 Goharbor. io jforg. com/artifactory/ "
    }, {
    "id": 20,
    "url": "http://localhost:4000/Python-in-Container-01/",
    "title": "Python in Container Part One",
    "body": "2021/07/26 - 在本教程中，您将学习如何创建描述简单 Python 容器的 Dockerfile 文件。 使用容器创建 Web 服务器、数据库构建、运行和验证 Django、Flask 并通用 Python 应用实现程序的功能. 最后还会学习调试在容器中运行的应用程序。 Python in Container 01[TOC] 容器的生态: CNCF 可以看到整个生态图 运行时  Docker Engine containered rkt CRI-O Podman容器管理  Docker Rancher Amazon Elastic Container ServiceRegistries 镜像  Docker Hub &amp; Registry &amp; Trusted Registry GitLab Amazon Elastic Container RegistryOrchestration 编排  K8s Swarm OpenShift Mesos DC/OS Google Kubernetes EngineMonitoring 监控  Prometheus cAdvisor DataDog Elastic尝试在 Docker 中 运行 Python: 从 Git Hub 上获取代码: 1234# 获取简单的代码, 包括 Python 的 flask 文件， docker file 和 requirements. txtgit clone https://github. com/dalongli/simple-flaskcd simple-flask  用到的三个文件  DockerfileFROM python:3# 用最新的 Python3 版本WORKDIR /usr/src/app# 指定安容器的工作目录COPY requirements. txt . # 拷贝当前目录的文件到容器目录RUN pip install -r requirements. txt# 安装python依赖包COPY hello. py . # 拷贝 python 运行文件EXPOSE 5000# 暴露容器的端口号CMD python hello. py# 运行 python  requirements. txt12Flask# 依赖包安装 Flask Python 文件1234567891011121314from flask import Flaskapp = Flask(__name__)@app. route('/')def hello_world():  return 'Flask Hello world! Version 1'@app. route('/test')def test():  return 'Testing hidden functionality ;)'if __name__ == '__main__':  app. run(debug=False,host='0. 0. 0. 0', port=5000)创建Docker镜像, 并运行Docker镜像:  37016175 是我的 Docker Hub 账号 simple-flask 是 repostory 的名字 v1. 0 是tag 名称12345678910# 创建 Docker 本地镜像, 当前目录(. )中应该有一个 Dockerfile docker build -t 37016175/simple-flask:v1. 0 . # -t 镜像名字带上 tagdocker run --rm -it -p 5001:5000 --name my-simple-python 37016175/simple-flask:v1. 0# 在 Docker 镜像上运行一个临时容器# --rm 当推出容器就删除这个容器# -i, --interactive          Keep STDIN open even if not attached# -t, --tty              Allocate a pseudo-TTY# -p, --publish list          Publish a container's port(s) to the hostdocker container ps --all访问 http://localhost:5001 可以看到 Flask Hello world! Version 1 将Docker 镜像上传到 Docker Hub 上: 1docker push 37016175/simple-flask:v1. 0 push 结束后就可以在 https://hub. docker. com/repositories 上看到新的 simple-flask 仓库了 Docker Deep Dive: Play with Docker:  访问https://labs. play-with-docker. com 可以有一个沙箱环境让我们尝试 Docker 每一次只能玩 4 个小时，并且有 CPU 和存储的限制 可以创建多个 Docker Nodes Sign in 后点击 start ， 点击 add new instance 然后复制 ssh 的命令行登录到 服务器并运行如下命令12345678apk add figletfiglet pythongit clone https://github. com/dalongli/simple-flaskcd simple-flaskvim hello. py # 有的时候这里的代码会被修改掉， 注意调整正确docker build -t 37016175/simple-flask:v1. 0. 5 . # 运行 Docker 镜像docker run --rm -it -p 5000:5000 37016175/simple-flask:v1. 0. 5 点击 console 中的 5000 端口链接就可以访问 web hello. py 页面了 记住这个地址，从互联网哪里都可以访问Docker 进行虚机管理 - docker-machine:  安装 VirtualBox 之后 运行如下命令进行测试123brew install docker-machinedocker-machine create docker4krisdocker-machine rm docker4kris docker-machine命令12345678910111213141516171819202122232425262728293031323334353637docker-machine active#显示当前的活动主机docker-machine config#显示连接主机的配置docker-machine create#创建一个主机docker-machine env#设置当前的环境与哪个主机通信docker-machine inspect#查看主机的详细信息docker-machine ip#查看主机的IPdocker-machine kill#强制关闭一个主机docker-machine ls#查看所有的主机信息docker-machine provision#重新配置现在主机docker-machine regenerate-certs#为主机重新生成证书docker-machine restart#重启主机docker-machine rm#删除主机docker-machine ssh#以SSH的方式连接到主机上docker-machine scp#远程复制docker-machine status#查看主机的状态docker-machine stop#停止一个正在运行的主机docker-machine upgrade#升级主机的docker服务到最新版本docker-machine version#查看docker-machine版本Docker 命令行: 查看版本和帮助:  docker --help 可以查看主命令的用法 docker COMMAND --help 可以查看子命令用法123456docker --versiondocker-compose --versiondocker-machine --versiondocker --helpdocker run --help Docker 的子命令如下123456789101112131415161718192021222324252627282930313233343536373839404142Commands: attach   Attach local standard input, output, and error streams to a running container build    Build an image from a Dockerfile commit   Create a new image from a container's changes cp     Copy files/folders between a container and the local filesystem create   Create a new container deploy   Deploy a new stack or update an existing stack diff    Inspect changes to files or directories on a container's filesystem events   Get real time events from the server exec    Run a command in a running container export    a container's filesystem as a tar archive history   Show the history of an image images   List images import   the contents from a tarball to create a filesystem image info    Display system-wide information inspect   Return low-level information on Docker objects kill    Kill one or more running containers load    Load an image from a tar archive or STDIN login    Log in to a Docker registry logout   Log out from a Docker registry logs    Fetch the logs of a container pause    Pause all processes within one or more containers port    List port mappings or a specific mapping for the container ps     List containers pull    Pull an image or a repository from a registry push    Push an image or a repository to a registry rename   Rename a container restart   Restart one or more containers rm     Remove one or more containers rmi     Remove one or more images run     Run a command in a new container save    Save one or more images to a tar archive (streamed to STDOUT by default) search   Search the Docker Hub for images start    Start one or more stopped containers stats    Display a live stream of container(s) resource usage statistics stop    Stop one or more running containers tag     Create a tag TARGET_IMAGE that refers to SOURCE_IMAGE top     Display the running processes of a container unpause   Unpause all processes within one or more containers update   Update configuration of one or more containers version   Show the Docker version information wait    Block until one or more containers stop, then print their exit codes docker version:  查看docker 的客户端版本和服务端版本， 如果都能看到信息说明服务端已经正常启动1docker versionDocker 命令缩写: 123456789101112 docker pull = docker image pull docker create = docker container create  docker start = docker container start  docker ps = docker container ps  docker rm = docker container rm  docker run = docker create; docker start # 如果run 使用的镜像不存在，docker 会尝试自动 pull 这个镜像下来 # 比如 如下， 如果本地没有 centos 镜像， docker 会自动 pull 一个 centos 下来，然后 create + start # docker run -it --name mycentos centos docker rmi =docker image rm 一步一步，创建和运行一个能运行 Python 的容器镜像: 从 docker hub 上拉一个 python 最新版本的镜像: 1234docker image pull python# 成功后也可以再 docker desktop 中的 image tab 中可以看到docker image list # 查看所有的 本地 image创建一个新的 python 容器: 1234567docker container create --tty --interactive python# --tty              Allocate a pseudo-TTY# --interactive          Keep STDIN open even if not attached# 创建后可以在 docker desktop 中的 container tab 中看到docker container ps --all # 查看运行中的容器# CONTAINER ID  IMAGE    COMMAND         CREATED     STATUS           PORTS   NAMES# ffb2d557e2c6  python     python3         8 minutes ago  Created                fervent_lamport 容器的名字是一个随机创建的名字，可以被修改1docker container rename fervent_lamport mypython启动 python 容器: 123456789docker container start --interactive mypython# --interactive 启动后就直接进入了 Python 的命令行。不带 interactive 参数， 就是单纯启动# python 命令行中可以看到python 的主机名就是容器名，因为 Python 运行在容器中# &gt;&gt;&gt; import socket# &gt;&gt;&gt; print(socket. gethostname())# ffb2d557e2c6# 在 Docker desktop 中可以看到这个容器由红色变成了绿色docker container ps --all # 可以看到原来的STATUS 由 Created 状态变成了 Up 32 seconds 删除 Python 容器， 删除镜像: 12docker container rm mypythondocker rmi python在本地系统中和容器交互: 文件拷贝 docker cp: 我们在进行代码开发的时候经常需要把我们开发好的代码拷贝到容器中， 下面是一个例子 123456789101112131415161718git clone https://github. com/pythonincontainers/myfirstcd myfirstdocker pull pythondocker create -it --name mypython python# 在容器和本地之间互相 拷贝文件docker cp --helpdocker cp myfirst. py mypython:/tmp/myfirst. pydocker start -i mypython # 进入 docker 看看。 exec 是要在 docker 中执行一个命令（/bin/sh）# -it 是要交互方式进行docker exec -it mypython /bin/sh # 进入容器命令行cd /tmpls # 可以看到拷贝过来的文件touch abc. txtexit # 退出容器，回到主机docker cp mypython:/tmp/abc. txt . /abc. txtls # 可以看到拷贝出来的 abc. txtdocker 和主机目录共享 - Docker Bind Mount:  让容器可以访问主机的目录12345678910111213# 通过 run 命令在创建容器的时候，用-v 来指定 mount 的目录# -v, --volume list          Bind mount a volume# -d, --detach             Run container in background and print container IDdocker run -it --name mypython -v ${PWD}:/app python # or docker run -d -v ${PWD}/data:/data python docker start mypythondocker exec -it mypython /bin/sh # 进入容器cd /appls # 可以看到本地共享目录中的文件# 尝试在本地修改一个文件保存， 然后在 容器中 cat 一下 还可以在创建容器时默认执行一个python命令, 就算没有 启动容器也可以执行123456docker run -it --name myfirst -v ${PWD}:/app python python /app/myfirst. py# 参数中第一个python 是镜像的名字，第二个参数就是运行容器后要执行的命令了# 输出了 myfirst. py 的打印内容 - Python in Containers! Version 1. 2docker start -i myfirst # 以后只要启动，就会输出 Python in Containers! Version 1. 2# -i, --interactive     Attach container's STDIN  可以让容器启动时进行交互1234567docker run -it --name mysecond -v ${PWD}:/app python python /app/mysecond. py# 这里，mysecond. py 代码中有 如下代码要求用户进行交互操作# 所以当容器被创建后，会要求用户输入姓名，并打印你好+姓名# print('Python in Containers!')# name = input('What is your name? ')# print('Greetings ' + name) 创建容器后不需要 exec 直接进入容器命令行, 执行 python 命令1234docker run -it --name mypython -v ${PWD}:/app python /bin/bash # 创建容器后直接进入容器命令行cd /applspython mysecond. py 本地和容器进行端口映射:  尝试在容器中安装 flask, 并启动 flask 进行端口映射 ， 在本地修改代码然后在 浏览器中查看flask web page 的变化12345678docker run -it -p 5001:5000 --name mythird -v ${PWD}:/app python /bin/bash # 创建容器后直接进入容器命令行# or docker run -p 8080:80 python . . . 来指定 我们的主机用 8080端口映射容器的 80端口# 在 mythird 容器中执行pip install flask # 设置临时环境变量（仅限于本次登录）; export -p 列出所有当前环境变量， -n 删除指定变量export FLASK_DEBUG=1export FLASK_APP=mythird. pyflask run --host=0. 0. 0. 0  # 启动 flask 访问本机的 http://localhost:5001 可以访问到容器运行的 flask web 页面  回到本机，尝试修改 mythird. py 文件， 然后刷新浏览器，就可以看到变化。 玩转 Container Image: Container Image 包含两部分内容:  Filesystem Metadata比如 Python 的 image 里面包含了创建一个 python 容器所必要的文件系统和需要运行 python 所必要的元数据。 都是以非不要不添加的原则。 123456docker run --rm -it python bash # --rm Automatically remove the container when it exitsls -l /usr/local/bin # 可以看到容器中只有必须的一些命令env  # 可以看到环境信息， 和下面 docker image inspect python 中输入的内容相同exit查看 image 内容的命令: 12345678910111213141516docker image inspect python # 输出一个 json 文件# 其中下面部分和 python 相关。 python3 是容器启动后的默认命令 					 Env : [         PATH=/usr/local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin ,         LANG=C. UTF-8 ,         GPG_KEY=&lt;your own key&gt; ,         PYTHON_VERSION=3. 9. 3 ,         PYTHON_PIP_VERSION=21. 0. 1 ,         PYTHON_GET_PIP_URL=https://github. com/pypa/get-pip/raw/29f37dbe6b3842ccd52d61816a3044173962ebeb/public/get-pip. py ,         PYTHON_GET_PIP_SHA256=&lt;your own code&gt;       ],       Cmd : [         python3       ],image 启动的时候需要 entrypoint 和 cmd:  Entrypoint 就是 命令 + 第一个参数 cmd 就是 后面的参数123456789101112131415161718192021docker run pythonincontainers/entrypoint# pythonincontainers/entrypoint 是一个镜像的名字， 就和 python 是一个镜像的名字一样# 输出 one two threedocker run pythonincontainers/entrypoint 3 4 5 6 7# 输出 one 3 4 5 6 7docker inspect pythonincontainers/entrypoint # 可以看到 cmd 和 entrypoint 的设置内容 Config : {. . . 						 Cmd : [         two ,         three       ], . . . .        Entrypoint : [         echo ,         one       ], . . . . # 为什么上面第一个 run 命令 可以打印出 one two three # 因为组合出来的命令是 echo one two three# 为什么上面第二个 run 命令 可以打印出 one 3 4 5 6 7# 因为 echo 命令不变，one 被 3 覆盖， two 和 three 被 4，5，6，7 覆盖快速打印出 一个Image 的 entrypointy 和 cmd  下面可以看到很多 不同 image 的 entrypoint 和 cmd1234567891011121314docker inspect --format  ENTRYPOINT={{. Config. Entrypoint}} CMD={{. Config. Cmd}}  pythonincontainers/entrypoint# 输出 ENTRYPOINT=[echo one] CMD=[two three]docker inspect --format  ENTRYPOINT={{. Config. Entrypoint}} CMD={{. Config. Cmd}}  python# 输出 ENTRYPOINT=[] CMD=[python3]docker inspect --format  ENTRYPOINT={{. Config. Entrypoint}} CMD={{. Config. Cmd}}  alpine# 输出 ENTRYPOINT=[] CMD=[/bin/sh]docker inspect --format  ENTRYPOINT={{. Config. Entrypoint}} CMD={{. Config. Cmd}}  centos# 输出 ENTRYPOINT=[] CMD=[/bin/sh]docker inspect --format  ENTRYPOINT={{. Config. Entrypoint}} CMD={{. Config. Cmd}}  nginx# 输出 ENTRYPOINT=[] CMD=[nginx -g daemon 0ff;]docker inspect --format  ENTRYPOINT={{. Config. Entrypoint}} CMD={{. Config. Cmd}}  postgres# 输出 ENTRYPOINT=[docker-entrypint. sh] CMD=[postgres] 可以发现很多Image 默认是不设置 entrypoint 的 创建容器时如果想要覆盖默认的 entrypoint 可以用参数 –entrypoint 12docker run -it --entrypoint /bin/sh pythonincontainers/entrypoint# 这样，创建容器后，默认就不是 echo one ，而是 /bin/sh 作为默认进入容器的命令了Dockert image ID:  id 由 shar256 创建， 有 64 个字符 前 12 个字符作为 短 IDimage 镜像的清理: 删除无用镜像，节省空间 123456docker image lsdocker rmi pythonincontainers/entrypointdocker image prune # 删除所有没有对应容器的 image ，需要谨慎# WARNING! This will remove all images without at least one container associated to them. # Are you sure you want to continue? [y/N] 管理容器: 交互模式和非交互模式:  运行容器时进行交互 比如运行一个 python 容器，希望直接进入 python 命令行 比如运行 centos, 希望直接进入 centos 命令行12# --interactive --tty = -itdocker run --rm -it python bash  运行容器时，非交互模式。 默认会返回一个容器 ID 比如运行一个 flask web server, 就不需要卡在 命令行上，让它自己在后台运行就可以了1234# --detach = -ddocker run -d -p 5000:5000 --name simple-flask pythonincontainers/simple-flask# 输出 7754e1e53d5e534b87278576ee5650da9f01e891ebf078c6df2810ec3ab73549http://localhost:5000 查看容器日志 logs: 当容器运行不是交互模式的时候， 需要查看容器的运行情况，就需要用 logs 或者 attach 12345678910111213docker logs simple-flaskdocker logs --since 1m simple-flask # 查看最近一分钟的日志docker logs -n 10 simple-flask # 查看最后 10 行日志# 输出# 172. 17. 0. 1 - - [05/Apr/2021 02:43:50]  GET / HTTP/1. 1  200 -# 172. 17. 0. 1 - - [05/Apr/2021 02:43:51]  GET /abc HTTP/1. 1  404 -# 172. 17. 0. 1 - - [05/Apr/2021 02:43:51]  GET /favicon. ico HTTP/1. 1  404 -# 跟随模式，可以实时看到最新的日志docker logs -t -f simple-flask # -t Show timestamps . -f Follow log output贴附模式 attach:    Attach local standard input, output, and error streams to a running container   和 docker logs -f 的效果类似 不管运行的容器有什么输出，都可以看到 退出好像有点问题 ， control + c/p/q 不好使，12docker attach simple-flask # 和 docker logs -f 的效果类似在容器中执行命令 exec:  Run a command in a running container  docker exec &lt;容器名&gt; &lt;命令。。。&gt; 12345678docker exec simple-flask ps -ef#输出#UID    PID PPID C STIME TTY     TIME CMD#root     1   0 0 02:38 ?    00:00:00 /bin/sh -c python hello. py#root     8   1 0 02:38 ?    00:00:01 python hello. py#root    26   0 0 03:04 ?    00:00:00 ps -ef 进入运行中的容器命令行直接操作bash命令123docker exec -it simple-flask bash # 进入容器apt-get update # 升级安装库apt-get install vim # 安装需要的软件 vim控制容器的资源:  在创建容器的时候如果不指定资源， 默认是会使用 主机的所有资源  在创建容器的时候可以给容器指定所需要的 CPU, 内存， swap 空间  一旦容器运行时超出了规定的限制， 进程就会被 自动kill 掉 12345678docker run -it -m 100m --memory-swap 100m --cpus 0. 6 python bash# --cpus 指的是虚拟 cpu 个数pip install numpy python&gt;&gt;&gt;import numpy&gt;&gt;&gt;result = [numpy. random. bytes(1024*1024) for x in range(1024)]# 这里python 进程会因为内存使用超出限制而被 kill 掉查看 docker 状态 12docker stats # 查看docker 资源使用情况docker logs laughing_johnson # 查看 docker 最后的命令，寻找为什么 docker 被 kill 掉的原因删除所有容器 12docker rm -f $(docker ps -a -q)  # -f 强制删除# docker ps -a -q 会列出来所有停止了的容器的 id ： -a 所有， -q 只列 id运行多个容器:  让容器之间可以进行访问在容器中设置 hosts: 123456789101112131415161718# 先创建一个容器docker run -d -p 5000:5000 --name simple-flask pythonincontainers/simple-flask# 查看这个容器的 IPdocker inspect --format  {{. NetworkSettings. IPAddress}}  simple-flask# 创建另一个 centos 容器，并追加一行到 hosts 文件 docker run --rm -it --name centos --add-host simple-flask:$(docker inspect --format  {{. NetworkSettings. IPAddress}}  simple-flask) centos# docker run --rm -it --name centos --add-host simple-flask:172. 17. 0. 2 centosmore /etc/hosts # 进入centos 容器后查看 /etc/hosts 文件# 输出如下 172. 17. 0. 2	simple-flaskcurl simple-flask:5000 # 尝试访问另一个容器的 web# 输出 Flask Hello world! Version 1curl 172. 17. 0. 2:5000 # 效果一样exitdocker rm -f simple-flask创建一个容器网络 network:  好处是所有创建的容器都可以通过容器网络直接互相链接，不会因为某个容器的 IP 发生变化而需要修改配置123456789101112131415161718# 创建容器网络docker network create my-netdocker network ls# 创建flask容器并使用网络docker run -d --name simple-flask --network my-net pythonincontainers/simple-flask# 创建 centos 容器并使用网络docker run --rm -it --name centos --network my-net centoscurl simple-flask:5000 # 在centos 容器内访问 flask 网页# 输出 Flask Hello world! Version 1# 创建一个 proxy 服务器# 进入另一个 terminal 并执行如下命令创建nginx 容器docker run -d --name proxy-server --network my-net nginx# 然后返回到 centos 的 terminal 执行如下命令curl proxy-server:80# 输出 nginx 的 html 代码# 。。。&lt;title&gt;Welcome to nginx!&lt;/title&gt;。。。--link 的用处和–add-host 差不多 123456789101112# 删掉 simple-flask 并重新创建docker rm -f simple-flaskdocker run -d --name simple-flask pythonincontainers/simple-flask# 通过 --link 将 simple-flask 容器添加到 centos 的 hosts # webserver 是 simple-flask 的别名docker run --rm -it --link simple-flask:webserver centos# 进入容器curl simple-flask:5000 # 尝试访问 simple-flaskenv |grep WEBSERVER # 查看所有centos 配置的关于 simple-flask(webserver) 的信息exitdocker rm -f simple-flask创建一个 posgres 数据库: 1234567docker run -d --name postgres --network my-net --env  POSTGRES_PASSWORD=mysecret  postgresdocker logs postgres # 查看 postgres 是否正常启动# 看到输出 。。。database system is ready to accept connections# 创建一个 postgre 的管理工具容器 - pgadmin4docker run -d --name pgadmin --network my-net -e  PGADMIN_DEFAULT_EMAIL=user@example. com  -e  PGADMIN_DEFAULT_PASSWORD=supersecret  -p 8088:80 dpage/pgadmin4 访问 http://localhost:8088  用户名密码 user@example. com supersecret    创建一个 db server : mydb  指定 connection (要链接的服务器/容器名字) : postgres  username : postgres  password : mysecret  完成后左侧可以看到 mydb    创建一个新的数据库：mydatabase 下载此部分的实验代码 12345678910111213141516git clone https://github. com/pythonincontainers/sqlalchemy-psqlcd sqlalchemy-psql/# 创建一个python 容器并和当前目录共享下载下来的 python 文件docker run --rm -it -v ${PWD}:/app --network my-net python bash# 已经进入容器cd /apppip install -r requirements. txt # 安装所需的扩展包# 执行创建表和插入数据的 python 文件python alchemy-psql. py # 执行后，数据库中会创建一个 user 表并插入多条数据# mydb-&gt;mydatabase-&gt;schemas-&gt;public-&gt;tables-&gt;person 右键 -&gt;view/edit data -&gt; all rows#清理实验docker rm -f $(docker ps -a -q)docker image prune 容器网络 Networking:  当我们创建一个容器，如果不指定网络， 默认他会使用 bridge network     容器会通过连接到主机网络接口来访问外网   如果没有进行 expost 暴露端口映射， 外网不能访问容器网络，   ip 地址段是 172. 17. 0. 0/16   没有 DNS 设置   创建一个没有网络的容器 - none: 123456789101112131415docker network ls# 输出# NETWORK ID   NAME   DRIVER  SCOPE# a76c7ba20fbc  bridge  bridge  local  桥接网络# 5656ec8f0876  host   host   local  主机网络# 338713d26e03  my-net  bridge  local  我们为容器创建的网络# b3e935e8c6fe  none   null   local  docker run --rm -it --network none alpine # 创建一个没有网络的linux - alpine容器# 进入容器ifconfig # 查看本地网络信息ip add show # 查看 IP 地址ip route # 查看路由ping -c 3 8. 8. 8. 8 # ping 外网， 不通exit创建一个链接到 主机的容器 - bridge: 123456docker run --rm -it --network host alpineifconfig # 查看本地网络， 此时使用的是桥接模式， 主要看 docker0 这个网络ip add show # 查看 IP 地址ip route # 查看路由ping -c 3 8. 8. 8. 8 # ping 外网， 通exit删除网络:  当一个网络已经有容器连接他，那么需要先删除容器或者断开网络，才能删除1234567docker network create new-netdocker network ls# 找出使用了 new-net 网络的容器，并准删掉后，才能删掉对应的网络docker ps --filter network=new-net  docker network rm new-net # 删除网络创建指定网段的容器:  打开第一个 terminal123456789# docker network create --subnet 10. 10. 0. 0/16 my-addrdocker network lsdocker run --rm -it --name alpine1 --network my-addr alpine# 进入容器ifconfig# 输出 eth0 addr:10. 10. 0. 2 打开第二个 terminal123456docker run --rm -it --name alpine2 alpine# 进入容器ifconfig# 输出 eth0 addr:172. 17. 0. 2 ， 因为没有指定网络，所以使用了 bridge 打开第三个 terminal1docker network connect my-addr alpine2 回到第二个 terminal12345ifconfig# 输出 eth1 addr:10. 10. 0. 3, 因为容器 alpine2 已经连接到了指定的网段的网络ping -c 1 alpine1 # 可以ping 通 回到第三个12docker ps --filter network=my-addr # 查看所有在 my-addr 下面的容器docker network disconnect my-addr alpine2 # 断开 alpine2 的链接创建内网: 12345678910docker network create --internal int-net docker run -dit --name int1 --network int-net alpinedocker run -it --rm --name int2 --network int-net alpineping -c 1 int1 # 通ping -c 1 8. 8. 8. 8 # ping google 不通docker rm -f $(docker ps -a -q)docker image prune docker network rm my-addr int-net my-net new-net 数据永久存储 - volumes:  容器的存储是跟着容器的生命周期的， 容器没了，存储的数据就没了123456789101112131415161718docker run -it --name mypython python bashmkdir /app # 进入容器cd /appcat &lt;&lt;-EOF &gt; script. py # 创建一个 script. py 的脚本，输入 EOF 后保存退出print('Hello from a container')EOFpython script. pyexit # 退出容器docker start -i mypython  # 重新启动，并进入容器， 默认是用 bash 交互# 和 docker start mypython + docker exec -it mypython bash 效果一样exitdocker rm mypython docker run -it --name mypython python bashcd /app # 没有这个目录exitdocker rm mypython创建永久存储:  创建容器的时候是唯一的机会 mount 到存储上123456789101112131415161718192021222324docker volume create my-vol docker volume ls# 创建容器，并 mount 到 volume 上 （唯一的机会）docker run -it --name mypython --volume my-vol:/app python bashcd /app# 创建数据 （到 volume 上）cat &lt;&lt;-EOF &gt; script. pyprint( hello my volume )EOFpython script. py exit # 删除容器docker rm mypython# 查看 volume docker volume lsdocker volume inspect my-vol# 重建容器， /app 下还能看到原来的数据docker run -it --name mypython --volume my-vol:/app python bashcd /apppython script. py删除 volume:  volume 在使用的时候是不能被删除的，会报错123docker volume rm my-vol # 报错docker rm mypythondocker volume rm my-vol # 成功 创建容器时用 -v 可以把容器的目录和主机目录共享1234567891011docker run --rm -it -v ${PWD}:/app python bashcd /appcat &lt;&lt;-EOF &gt; script. pyprint( hello from a container )EOF# 在主机上用编辑器增加一行 print('second line from host')python script. py # 可以看到新增加的内容docker rm -f $(docker ps -a -q)docker image prune Dockerfile:  传统的运行 python 容器的方法12345678910git clone https://github. com/pythonincontainers/flask-hellocd flask-hellodocker run --rm -it -p 5000:5000 -v ${PWD}:/app python bashcd /apppip install Flaskexport FLASK_DEBUG=Truepython flask-hello. pyControl + Cexit了解 Dockerfile: FROM python # 这个 image 是基于 python 这个 image 做的WORKDIR /myproject # 指定要创建的容器的工作目录COPY . .  # 将当前目录（clone 下来的目录）的所有内容拷贝到容器的工作目录中RUN pip install -r requirements. txt # 安装所有必须的包EXPOSE 5000 # 设置 容器对外暴露的端口号。只是容器的，不是 host 的ENV FLASK_DEBUG=True # 打开 debug 模式CMD python flask-hello. py # 当运行容器时，默认的命令启动 flask用 dockerfile 建立一个本地image 1234567891011121314151617181920212223242526docker build -t flask-hello:1. 0 .  # . 表示当前目录，当前目录中需要有一个 dockerfile # 运行成功后就自动执行了 dockerfile 中的一系列动作，包括#设置工作目录#拷贝 clone 下来的文件到工作目录#安装 flask#暴露端口#设置环境变量#设置默认命令docker run -d -P --name flask-hello flask-hello:1. 0 # 创建容器# -p, --publish list Publish a container's port(s) to the host# -P 自动把所有容器的端口都和 hots 主机进行 mapping # flask-hello:1. 0 是本地镜像的名字docker ps # 查看端口映射情况# 输出 0. 0. 0. 0:55000-&gt;5000/tcp# 访问 http://localhost:55000docker logs flask-hello # 建立好镜像后，通过命令查看 image 的配置信息docker inspect flask-hello:1. 0docker rm -f flask-hellodocker rmi flask-hello:1. 0spDockerfile 语法: 123456789FROM 基础镜像WORKDIR 工作目录COPY 拷贝文件到工作目录RUN 安装类库和模块ENV 配置环境变量EXPOSE 设置暴露的的端口LABEL 设置标签ENTERYPOINT 容器启动后的默认命令CMD 默认命令的参数Docker Hub 介绍:  一个仓库可以由多个镜像和多个 tag. 37016175/simple-flask:latest 37016175 是账户名 simple-flask 是仓库名 latest 是 tag 12345678910111213141516171819202122docker image ls 37016175/simple-flaskdocker login # 登录 ducker hubdocker logoutcd flask-hello/docker build -t 37016175/flask-hello:1. 0 . # 为 1. 0 再打一个 latest 标签docker tag 37016175/flask-hello:1. 0 37016175/flask-hello:latest# 上传镜像到 docker hubdocker push 37016175/flask-hello:1. 0# 修改一下代码， 然后重新 buile 另一个版本的镜像docker build -t 37016175/flask-hello:2. 0 . # 把 2. 0 tag 成 latestdocker tag 37016175/flask-hello:2. 0 37016175/flask-hello:latestdocker push 37016175/flask-hello:2. 0其他人如果想要同时用我们的镜像的两个版本，就执行下面命令 12docker run -d -p 5001:5000 37016175/flask-hello:1. 0docker run -d -p 5002:5000 37016175/flask-hello:2. 0http://localhost:5001 http://localhost:5002  免费的docker hub 账号不限制 public repository 个数，只能有 1 个 private repository 付费的可以由多个私有仓库比较常用的是 python 官方库 12# 使用 python 镜像，默认是用了 latest tagdocker run -it --name mypython python bash 可以在 python 仓库的描述中找到 latest 对应的 version 3. 9. 4 docker pull python:latest 可以将最新的 Python 镜像更新到本地镜像仓库 有 slim 标签的镜像会比普通镜像小很多，可以节省空间。 正常镜像 1G, slim 的只有 200M ， 但是很多 模块需要手动安装 apt-get install alpine 也很小，但是兼容性差 windows 是要在 windows 主机上运行的容器Docker GUIs - Portainer:  大部分的 Docker 操作都是通过命令行完成的，优势是可以跨平台。但是有些不喜欢命令行的可以使用 Docker GUI Kitematic https://kitematic. com 已经不更新了，需要转到 Docker desktop Portainer https://www. portainer. io 可视化容器管理工具Portainer for Docker安装方法 12345678# 先创建一个volumedocker volume create portainer_data# 启动一个服务端docker run -d -p 8000:8000 -p 9000:9000 --name=portainer --restart=always -v /var/run/docker. sock:/var/run/docker. sock -v portainer_data:/data portainer/portainer-ce# 启动一个客户端# docker run -d -p 9001:9001 --name portainer_agent --restart=always -v /var/run/docker. sock:/var/run/docker. sock -v /var/lib/docker/volumes:/var/lib/docker/volumes portainer/agent访问 http://localhost:9000 创建密码后登陆访问  测试创建一个 image 的docker fileFROM python:3ENV TEST helloCMD echo 'print( Hello from my-test )' |pythonDocker Machine:    Docker Machine 用来创建和管理在虚拟机中的Docker 运行时环境     docker machine 可以在本地创建，也可以在云上的虚机中创建容器。     支持的本地虚机类型 完整列表https://docs. docker. com/machine/drivers/      Hyper-V   Virtual   vmware fusion   vmware workstation   Generic      支持的共有云类型      AWS   Azure   Digital Ocean   Exoscale   Google Cloud Engine   Rackspace   IBM Softlayer   VMware Vcloud Air   OpenStack   Vmware Vsphere      Docker Machine 命令   12345678910111213141516171819202122232425262728293031323334353637 docker-machine active#显示当前的活动主机docker-machine config#显示连接主机的配置docker-machine create#创建一个主机docker-machine env#设置当前的环境与哪个主机通信docker-machine inspect#查看主机的详细信息docker-machine ip#查看主机的IPdocker-machine kill#强制关闭一个主机docker-machine ls#查看所有的主机信息docker-machine provision#重新配置现在主机docker-machine regenerate-certs#为主机重新生成证书docker-machine restart#重启主机docker-machine rm#删除主机docker-machine ssh#以SSH的方式连接到主机上docker-machine scp#远程复制docker-machine status#查看主机的状态docker-machine stop#停止一个正在运行的主机docker-machine upgrade#升级主机的docker服务到最新版本docker-machine version#查看docker-machine版本   Docker Machine with VirtualBox:  可以先网上下载 virtualbox, 记得一定要允许 VirtualBox 的权限     Grant permission to VirtualBox under System Preferences &gt; Security &amp; Privacy &gt; General (this request is new to macOS High Sierra)    删除由docker-machine 创建的虚机，要用 docker-machine rm 命令， 不要从 virtualbox 手动删除 创建一个虚机后， 和虚机相关的文件会存储在 /Users/dalong/. docker/machine/machines//目录下 安装 docker-machine 官方的安装方法创建一个虚机  可以用如下参数定制虚机的容量 --virutalbox-cpu-count --virtualbox-memory --virtualbox-disk-size12345678910111213141516171819202122232425262728293031323334353637383940414243444546docker-machine create test-vm # 创建一个 docker 虚机#Running pre-create checks. . . #Creating machine. . . #(test-vm) Copying /Users/dalong/. docker/machine/cache/boot2docker. iso to /Users/dalong/. docker/machine/machines/test-vm/boot2docker. iso. . . #(test-vm) Creating VirtualBox VM. . . #(test-vm) Creating SSH key. . . #(test-vm) Starting the VM. . . #(test-vm) Check network to re-create if needed. . . #(test-vm) Waiting for an IP. . . #Waiting for machine to be running, this may take a few minutes. . . #Detecting operating system of created instance. . . #Waiting for SSH to be available. . . #Detecting the provisioner. . . #Provisioning with boot2docker. . . #Copying certs to the local machine directory. . . #Copying certs to the remote machine. . . #Setting Docker configuration on the remote daemon. . . #Checking connection to Docker. . . #Docker is up and running!#To see how to connect your Docker Client to the Docker Engine running on this virtual machine, run: docker-machine env test-vmdocker-machine env test-vm # 连接 Docker client 到刚创建的 Docker VM#export DOCKER_TLS_VERIFY= 1 #export DOCKER_HOST= tcp://192. 168. 99. 101:2376 #export DOCKER_CERT_PATH= /Users/dalong/. docker/machine/machines/test-vm #export DOCKER_MACHINE_NAME= test-vm # Run this command to configure your shell: # eval $(docker-machine env test-vm)  运行了这个命令，你当前terminal 的 docker 环境就是模拟在虚机上的命令行了docker-machine lsdocker-machine ssh test-vm  # 登录到新建的容器虚机上# 也可以 ssh docker@&lt;docker-machine-ip&gt;# 密码 tcuser# 进入后默认用户是 docker, 可以 sodu -i 成为 root 用户docker ps -a # 这时的虚机上什么容器和镜像都没有docker image ls -a docker-machine rm -f test-vm # 删掉容器虚机# 查看之前设置的环境变量docker-machine env -u# 清理掉之前设置的环境变量unset DOCKER_TLS_VERIFY DOCKER_HOST DOCKER_CERT_PATH DOCKER_MACHINE_NAME在虚机上建立容器 123456789101112docker-machine create test-vm  # 创建虚机docker-machine env test-vm # 将当前环境设置成虚机eval $(docker-machine env test-vm) # 运行了这个命令，你当前terminal 的 docker 环境就是模拟在虚机上的命令行了docker run -it -v ${PWD}:/app alpine # 创建一个小容器，并把当前目录和容器的/app 共享ls /app # 进入容器后查看共享目录touch abc. txt # 在虚机容器的共享目录中创建一个文件， 本地也可以看见# 清理掉之前设置的环境变量unset DOCKER_TLS_VERIFY DOCKER_HOST DOCKER_CERT_PATH DOCKER_MACHINE_NAME创建第二个虚机 12345678docker-machine create second-vm# 如果还用 docker-machine env second-vm 会覆盖掉原来的设置，所以可以用 docker-machine config second-vm # 查看链接 second-vm 的配置文件# 定义一个环境变量， 把 second-vm 的配置文件信息写入SECONDVM=$(docker-machine config second-vm)echo $SECONDVM docker $SECONDVM image ls # 查看 second-vm 的镜像列表升级虚机的docker服务到最新版本, 重新生成证书或者重置 1234567891011docker-machine ls# 升级虚机的docker服务到最新版本 并不会改变虚机的 IP， 已经创建的 content docker-machine upgrade test-vm # docker-machine ls#为主机重新生成TLS 证书docker-machine regenerate-certs test-vm#重新配置现在的虚机, 一般在虚机崩掉不能重启的时候，又不想毁掉数据的时候可以考虑使用docker-machine provision test-vm #查看虚机的IPdocker-machine ip test-vm Docker Machine with Hyper-V:  在 Windows 平台上可以考虑使用 Docker ,就要使用 Hyper-V. 但是不太稳定。 Windows 上不能同时使用 Hyper-V 和 Virtual BOx, 有冲突Docker Machine on AWS Cloud Hosts:    Docker Machine 可以帮助我们在 AWS 上创建容器虚机     操作就像管理本地虚机一样  准备工作：  创建 AWS 账号 ， https://aws. amazon. com     dalong_coo@163. com   @mama******    创建一个子账号，用于操作 Docker Machine , 证书的 key 构建与子账号，万一该员工离职，删除子账号就可以让 key 不生效了。     aws management console - &gt; IAM -&gt; users -&gt; add user   machine-admin with programmatic access   Attach existing policies directly with AmazonEc2FullAccess   用户创建成功, 下载含有证书的 . csv    生成证书     Access Key ID :   Secret Access Key :    保存证书到 ~/. aws/credentials12345678910mkdir ~/. awssudo vi ~/. aws/credentials # 输入如下内容# default 是 profile 的 name[default]aws_access_key_id = &lt;your own key&gt;aws_secret_access_key = &lt;your own key&gt;有三种方式传递证书给 AWS  证书文件 环境变量     aws_access_key_id   aws_secret_access_key    命令参数     –amazonec2-access-key   –amazonect-secret-key   创建虚机到 AWS 上 123456789101112131415161718192021222324252627282930313233343536docker-machine create --driver amazonec2 --amazonec2-region eu-west-1 --amazonec2-open-port 5000 aws-machine# --driver amazonec2 ： 用 aws ec2 作为虚机 driver# --amazonec2-region eu-west-1 ： 把建好的虚机放在爱尔兰# --amazonec2-open-port 5000 ： 虚机开放 5000 端口# aws-machine 虚机名# 其他可选参数# --amazonec2-instance-type t2. medium  # CPU 和内存大小， 默认是t2. micro 1 个 cpu 1g 内存# https://aws. amazon. com/ec2/instance-types/# --amazonec2-root-size 200  # 根的大小# --amazonec2-volume-type io1  # 存储类型# https://aws. amazon. com/AWSEC2/latest/Userguide/ebsvolumetypes. htmldocker-machine ls # 创建成功后， 查看虚机# 输出如下# NAME     ACTIVE  DRIVER    STATE   URL             SWARM  DOCKER   ERRORS# aws-machine  -    amazonec2  Running  tcp://34. 243. 30. 56:2376       v20. 10. 5  # second-vm   -    virtualbox  Running  tcp://192. 168. 99. 103:2376      v19. 03. 12  # test-vm    -    virtualbox  Running  tcp://192. 168. 99. 102:2376      v19. 03. 12  # 激活环境变量, 然后就可以在本地操作 aws 虚机上的容器了docker-machine env aws-machineeval $(docker-machine env aws-machine)docker-machine ip aws-machine# 输出 34. 243. 30. 56docker-machine ssh aws-machine sudo -i docker run -it --rm -p 5000:5000 pythonincontainers/flask-hello:1. 0# 访问 https://34. 243. 30. 56:5000exitexitdocker-machine rm aws-machineunset DOCKER_TLS_VERIFY DOCKER_HOST DOCKER_CERT_PATH DOCKER_MACHINE_NAME登录到 AWS web portal 看虚机实例  Services -&gt; EC2 -&gt; instances登录到 AWS 为容器创建一个安全组，设置inbound 端口为 7000  Services -&gt; EC2 -&gt; security group 创建一个新的安全组，     name machine-rule-7000   Add inbound rules         Port range : 7000     Source: anywhere           回到 instance, 选中 aws-machine, action 选中 security -&gt; change security group -&gt; 添加新建的 security groupDocker Machine on Google Cloud: 步骤  访问 cloud. google. com 为 Docker Machine 创建子账号 安装 google cloud sdk 登录，并授权应用证书Build Conrainer Images: Python 容器化项目的元素: 当创建一个基于容器的 python 项目，需要以下元素  代码 类库和依赖包 数据 网络，DNS 主机名，IP 地址 配置文件 环境变量容器化 Python 项目的生命周期: 容器化项目的阶段  Architecture &amp; Design Project Initialization Coding Creating Images Testing &amp; Debugging in Development Deployment Planning &amp; Automation Integration &amp; Deployment in Testing Environment Shipping the App Deployment in Production Updating &amp; Upgrading容器化应用的设计原则: Basic Principles  One Application or Component per container 每个应用或者组件用一个容器 Horizontal Scalling 能够水平扩展 Application Observability 应用可被观察 Security Framework 采用安全框架 Application Secrets 应用保密性 Automated Deployment 采用自动化部署 Immutable &amp; Disposable Containers 不可改变容器&amp;一次性的容器 Application Data in Volumes  应用数据放在存储中 Virtual Networks 采用虚拟网路 Small Container images 保持小镜像 Image Tagging 要给镜像打 tag Seamless Update &amp; Upgrade 无缝的自动更新和升级手动流程 Image Build Process: 我们希望所有 image build 都是自动完成， 但我们需要知道如何手动完成 image build 的流程 第一步：写代码，并在容器中进行测试:  创建一个目录12mkdir manual-buildcd manual-build 创建一个flask 文件 hello. py1234567from flask import Flask, escape, requestapp = Flask(__name__)@app. route('/')def hello():  name = request. args. get( name ,  world )  return f'Hello, {escape(name)}! Greetings from a Container' 创建一个执行脚本 start-app. sh12345cd /appexport FLASK_APP= hello export FLASK_ENV= development export FLASK_RUN_HOST= 0. 0. 0. 0 flask run 在本地创建一个python容器 名字叫 manual1234567891011121314151617181920# 创建容器docker create -it --name manual -p 5000:5000 python /bin/bash# 进入容器并创建目录 /appdocker start -i manualmkdir /appexit # 退出容器# 将新建的两个文件拷贝到容器中docker cp hello. py manual:/appdocker cp start-app. sh manual:/appdocker start -i manualcd /appls# 修改执行权限chmod +x start-app. sh #安装 flaskpip install Flask# 执行脚本, 启动 flask/app/start-app. sh# control +c 退出 flask , 退出容器# exit 打开浏览器 http://localhost:5000 就可以看到 Hello, world! Greetings from a Container 第二步： 为修改的容器创建镜像 - docker commit: commit   Create a new image from a container’s changes 12345678# 将我们修改过的 manual 创建成一个叫做 manual-image 的镜像，版本是 1. 0docker commit manual manual-image:1. 0# 运行新的容器docker run -it --rm -p 5001:5000 manual-image:1. 0# 进入容器后执行cd /appls # 新的容器已经包含了 app 目录和下面的两个文件/app/start-app. sh打开浏览器 http://localhost:5001 可以看到 新的运行的容器 第三部： 让镜像创造出来的容器可以直接运行 flask: 12345678# 通过给 docker commit 追加 --change 参数 提供dockerfile 的命令, 让这个镜像的容器创建的时候就会执行脚本docker commit --change  CMD /app/start-app. sh  manual manual-image:1. 1# 创建一个新的容器# 容器启动后就直接执行了脚本，启动了 flask ， 补充需要再进入容器后去启动这个脚本docker run -it --rm -p 5001:5000 manual-image:1. 1# control + c访问浏览器 http://localhost:5001 通过 Dockerfile 进行镜像自动创建 - docker build: build - Build an image from a docker file 镜像创建流程  Create the source code Create Dockerfile create a container with base image create a working directory copy the source code to working directory install libraries install additional softwoare commit the container to a new image test the new image要手动准备的三个文件:  hello. py Start-app. sh Dockerfile要准备的Dockerfile: # 在 manual-build 目录下创建一个 Dockerfile 文件FROM python:3  # 这是 base imageWORKDIR /app  # 创建工作目录COPY hello. py .  # copy the source code to working directoryCOPY start-app. sh .  # copy the source code to working directoryRUN pip install Flask  # install librariesCMD [ /bin/bash ,  /app/start-app. sh ]  # 设置容器的启动命令创建镜像docker build 并运行: 123456# 将当前目录下的内容 build 到新的镜像 中# -t 指定 tag docker build -t automated-image:1. 0 .  # 成功后运行容器， 可以直接启动 flask docker run -it --rm -p 5000:5000 automated-image:1. 0 # docker run -it --rm -p 5000:5000 automated-image:1. 0 /bin/bash # 这个命令用/bin/bash 代替原有的命令，不会启动 flask访问 http://localhost:5000 以后每次修改代码后，就执行docker build ， docker run 就可以把最新的代码部署到镜像和容器中了 为 dockerfile 创建环境变量:    前面手动 build 的方法中， 我们通过 start-app. sh 设置 环境变量并启动 flask     在 build image 的时候在 dockerfile 中也可以设置环境变量并启动 flask, 就不需要单独写一个启动脚本了  12345678910FROM python:3  WORKDIR /app  COPY hello. py .   # COPY start-app. sh .  # 这行就不需要了RUN pip install Flask ENV FLASK_APP  hello    # 设置三个环境变量ENV FLASK_ENV  development ENV FLASK_RUN_HOST  0. 0. 0. 0 CMD [ flask ,  run ]  # 设置容器的启动命令， 不再执行脚本，而是直接启动 flask用新的 Dockerfile 创建镜像，并运行容器 12345cp Dockerfile Dockerfile. env# 将Dockerfile. env 内容改成上面的内容# 重新 build , -f 指定 Dockerfile 的名字， 默认是 Dockerfiledocker build -f Dockerfile. env -t automated-image:1. 1 . docker run -it --rm -p 5000:5000 automated-image:1. 1访问 http://localhost:5000 Dockerfile Procedure:  Create the Source Code Create Dockerfile     Define Base Image   define a working directory   Copy source code to working directory   set environment variables   Install libraries and software   Set a start-up command    Build the image Test the image - docker runDockerfile 命令 - FROM:  FROM 必须放在第一行，否则会报错 FROM 可以有多行， 定义多个 base image 定义了 base image一些 FROM 的例子: FROM python # 默认是 latest tagFROM ubuntu:16. 04 # 可以指定版本FROM 37016175/flask-hello:1. 0 # 镜像名可以带斜杠，斜杠前面是docker hub的账户名，后面是镜像名# 通过 image ID指定进行用哪个 Python 版本的镜像FROMpython@sha256:7e6c00cc553fdce06c1bcfcbf34c73a0f3623a8fc9ce88c8fb91671f2cbc0dab FROM docker. elastic. co/elasticsearch/elasticsearch:7. 0. 0 # 从非 Docker hub 拿镜像FROM private-registry. mycompany. com:5000/my-pyt  # 从私有镜像库拿镜像创建一个Dockerfile - Dockerfile. python. 3. 7. 4: FROM python:3. 7. 4创建镜像和容器 12345# 创建一个本地的 python. 3. 7. 4 的镜像docker build -t python. 3. 7. 4 -f Dockerfile. python. 3. 7. 4 . # 创建容器, 并进入 python consoledocker run -it --rm python. 3. 7. 4多个 FROM 的例子 - Dockerfile. from. 3: FROM alpineFROM ubuntu:16. 04FROM python创建镜像和容器 12345docker build -t who-knows -f Dockerfile. from. 3 . docker run -it --rm who-knows# exit()# Dockerfile 命令 - WORKDIR, COPY, ADD: WORKDIR 定义了容器的工作目录: 比如在 Dockerfile 中设置了 WORKDIR /app, 那么，镜像在创建容器的时候就会建立这个目录。 如果目录已经存在，在建立容器的时候会报错。如果想建立两层目录，需要 # 想要设置 /app/subdir 作为工作目录WORKDIR /appWORKDIR subdir   COPY 是从当前工作电脑 copy 文件到镜像中， 未来镜像创建容器的时候会把文件 copy 到工作目录下   COPY hello. py .   # 拷贝文件到工作目录COPY hello. py /app/hello. py  # 拷贝到指定目录，定制指定名字COPY hello. py start-app. Sh /app/ # 将多个文件拷贝到指定目录， 文件用空格分隔，目录后面带斜杠；不带斜杠，会被被拷贝成文件名叫 app 的文件COPY /hello. py /very/long/path/to/a/directory/ # 拷贝当前电脑的绝对路径下文件COPY *. py sources/  # 拷贝当前工作目录下所有以. py 结尾的文件到工作目录下的 sources 目录下；目的目录前面没有斜杠表示是工作目录COPY . /app/ # 拷贝当前目录下的所有文件和目录到 /app/目录下COPY . .  # 拷贝当前目录下的所有文件和目录到 工作目录COPY [ name with spaces. py,  /app ] # 如果文件名有空格， 用这种方式   . dockerignore 文件: 用来配置需要忽略的copy 文件，防止不许忘被拷贝的文件拷贝到镜像中 下面是一个例子 Dockerfile* # 所有以Dockerfile 开头的文件被忽略*. pyc    # 所有. pyc 文件被忽略!important. pyc  # 除了 important. pyc ,这个文件可以被拷贝#comments另一个例子 # 只有 mysite 目录和 mysite_nginx. conf 可以被拷贝，其他的都不行*!mysite!mysite_nginx. confCOPY 的例子: COPY --chown=web:web html/ /usr/local/html/ # 拷贝html/ 到 /usr/local/html 目录， 并将user:group权限设置成 web:web COPY --chown=root . .  # 将当前目录的所有文件和目录拷贝到工作目录， 并user:group 都设置成 rootCOPY --chown=100:100 hello. py /app/ # 将 hello. py 拷贝到 /app/目录下，并设置 user:group ID为 100:100 的 IDADD: 和 COPY 类似，但是功能更强大    将本地的压缩包进行解压并 build     通过 Git URL , clone 一个项目，并 build  ADD https//raw. githubusercontent. Com/pythonincontainers/flask-hello/master/flask-hello. py /app一个例子：远程clone github，然后用下载文件中的 Dockerfile build image 的例子 12345# gitclone 之后， 用下载文件中的 Dockerfile build 一个 image flask-hello# 前提是下载的文件当中必须有 Dockerfiledocker build -t flask-hello https://github. com/pythonincontainers/flask-hello. gitDockerfile 命令 - RUN:  RUN 就是让镜像生成容器的时候，执行一个命令 该命令必须是镜像里面自带的命令，命令不存在会报错 RUN 只能运行非交互命令，不会有 user input , 但是会有输出，输出是在 docker build 的时候输出 如果命令执行出现错误， 那么build 会失败 如果执行一个后台命令，比如 RUN (sleep 10; echo  i am late ) &amp; 不会看到预想的结果， 因为 docker build 会另外已启动一个进程去执行这个命令使用方法 RUN command argument1 argument2RUN [ command ,  argument1 ,  argument2 ]# command 必须是在镜像生成的容器中已经存在的命令，否则报错例子- 写入，追加文件: Dockerfile. redirect: FROM pythonRUN echo  This is multiline  &gt; /tmp/fileRUN echo  message  &gt;&gt; /tmp/fileRUN more /tmp/file运行 build 1234docker build -t redirect -f Dockerfile. redirect . # build 过程中会有这两行内容# This is multiline# message例子-管道符命令: Dockerfile. pipe: FROM pythonRUN find / -name  python*  | wc -l  # 寻找所有以 python 开头的文件的个数运行 build 12docker build -t pipe -f Dockerfile. pipe . # 输出了计数 148 例子 - 把默认 bash 命令改为 python 命令行 Dockerfile. shell: python 镜像中的环境变量位置 /usr/local/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin FROM pythonSHELL [ /usr/local/bin/python ,  -c ]RUN print( Hello at Build time )12docker build -t shell -f Dockerfile. shell . # 输出了 Hello at Build time例子 - 下载，安装软件: Dockerfile. mysql FROM pythonRUN apt-get update &amp;&amp; apt-get install -y default-mysqi-client RUN mysql --versionRUN pip install Django mysqlclient12docker build -t mymysql -f Dockerfile. mysql . 镜像的层 - Image Layers:  FROM, COPY, ADD, RUN 会增加新的层, 执行上面 build mysql 的时候就可以看到有 4 层，每层分别被放在缓存中, 增加执行效率 （using cache ） WORKDIR, ENV, ENTRYPOINT, CMD, LABEL 等修改镜像的 MetadataDockerfile 命令 - ENV, LABEL, USER: ENV: 语法 ENV name valueENV name1=value1 name2=value2 . . . 例子 # 定义一个数据库链接ENV SQLALCHEMY_DATABASE_URI  postgresql://kris:secret@postgres/mydatabase ENV FLASK_APP=hello FLASK_RUN_HOST= 0. 0. 0. 0  # 定义两个变量ENV PATH $PATH:/app  # 追加 /app 到 $PATH 变量中ENV PATH  ${PATH}${PATH:-/bin}:/app  ENV PATH $PATH:/app  # 追加 /app 到 $PATH 变量中ENV PATH  ${PATH}${PATH:+:}/app   # 追加 /app 到 $PATH 变量中ENV EMPTY    # 变量不能从 Dockerfile 中删除， 但是可以被设置为空在创建容器的时候，也可以通过参数设置变量, 或者覆盖之前在 dockerfile 中设置过的变量 1docker run --env var=value . . . 一个完整的例子 - Dockerfile. var FROM pyhthonENV NEW hello12345678# 创建一个叫做 var 的镜像docker build -t var -f Dockerfile. var . # var 是镜像， 最后 env 是进入容器后执行的命令，这里 env 会打印出来所有环境变量，包含在 Dockerfile 中定义的变量docker run --rm var env # 这里执行 run 的时候用--env 参数复写了 NEW 这本变量docker run --rm --env NEW= Hi there  var env USER:    Dockerfile 中的命令默认是 root 用户权限执行的     USER 命令可以改变当前用户的 ID  Dockerfile. user # 一个例子，创建 mysql 用户和组，然后切换成 mysql 用户，然后查看该用户的 idFROM pythonRUN groupadd mysql &amp;&amp; useradd -g mysql mysqlUSER mysql# USER mysql:root # 可以同时设置用户和组RUN id创建一个叫做 user 的 image 1234docker build -t user -f Dockerfile. user . # build 过程中可以看到 myswl 的 user id 的输出docker run --rm user id # 创建容器， 并执行 id 命令， 可以看到和刚才一样的输出LABEL:    Dockerfile 可以有任意行 LABEL   没有强制的 lables label 是给镜像打标签， 为了以后好找，或者好归类Dockerfile. label FROM pythonLABEL maintainer=lidalong@cn. ibm. comLABEL com. mycompany. version= 0. 7 LABEL com. mycompany. production= true 12345docker build -t label -f Dockerfile. label . docker inspect -f  {{json . Config. Labels}}  label # 查看创建的 Labels docker image --filter  label=com. mycompany. production Dockerfile 命令 - VOLUME and EXPOSE:  当我们创建容器的时候， 就会根据 Dockerfile 中定义的 VOLUME , 给容器设置 mount 的点 用 Volume 的好处     性能更好，因为存储在容器外   数据持久， 不会因为容器毁掉而丢失数据   让数据无状态， 和容器无关   语法: VOLUME /data # 自动创建一个 volumen 叫做 data . Volume mount 到 /dataVOLUME [ /data ] # 同上VOLUME /web/ /static # Volume mount 两个点例子-Dockerfile. vol: FROM pythonVOLUME /data  # 这里是要让镜像在创建容器的时候创建一个/data 的 mount点，以后在创建容器的时候可以用这个点 mount volumeCOPY hello. py /data/ 12345678910111213141516171819202122mkdir dockerfile-vol #创建一个本地目录cd dockerfile-vol  # 进入目录docker build -t vol -f Dockerfile. vol .  #在当前目录下创建镜像 vol# 输出 VOLUME /datadocker inspect -f  {{json. Config. Volumes}}   # 查看进项的 Volume 配置# 输出 { /data :{}}docker volume create mydata # 创建一个叫做 data 的 volumedocker run -it --name myvol -v data:/data vol bash # 创建一个容器， mount 上新建的 volume# 进入容器cd /datals -lamv hello. py hello. oldecho  One more line  &gt;&gt; hello. oldexit# 退出 dockerdocker rm myvol # 删除这个容器docker volume ls # volume 还在docker run -it --rm -v mydata:/mnt vol bash # 创建另一个容器, # 进入容器， 可以看到 hello. py 还在 /data 下面，# hello. old 在 /mnt 下面不用 -v 的情况:  不指定 -v 参数， docker 会自动创建一个匿名存储，并 mount 到容器 /data 目录上 如果带上 –rm 参数, 那么当退出容器的时候， 包括容器和匿名存储都会被删除1234567891011121314docker rm myvol docker volume rm mydatadocker run -it --name myvol vol bash # 不用 -v 一样可以创建volumen 并 mount 到 /data 目录上exitdocker volume list # 输出 可以看到一个匿名的存储# DRIVER  VOLUME NAME# local   00b81be043af1dcb4e94fabaf32ba456bd401ba76bd07f3f0a758d0da3c3d00fdocker rm myvol docker volume rm 00b81be043af1dcb4e94fabaf32ba456bd401ba76bd07f3f0a758d0da3c3d00fdocker run -it --rm --name myvol vol bashexit# 这时候不需要删除容器和 volume, 他们已经在退出容器的时候被自动删除了开发本机目录 mount 到 容器:  可以把本机目录 mount 到 /data 目录上， 但是不会有 hello. py 文件的 copy1docker run -it --name myvol -v ${PWD}/temp:/data vol bash另一个例子 Dockerfile. vol2  如果要初始化文件，记得要把 RUN 命令 放在 VOLUME前面，否则可能会丢失对数据的操作FROM pythonCOPY start-app. sh /data/RUN echo  One more line  &gt;&gt; /data/start-app. shVOLUME /dataCOPY hello. py /data/RUN echo  One more line  &gt;&gt; /data/hello. pyRUN more /data/start-app. shRUN more /data/hello. py12345678docker build -t vol2 -f Dockerfile. vol2 . docker run -it --name myvol2 vol2 bashcd /datamore hello. pymore start-app. shexitdocker volume listEXPOSE:  定期imange 创建的容器的对外暴露的端口号语法 EXPOSE &lt;port&gt;EXPOSE &lt;port&gt;/&lt;protocol&gt;例子 EXPOSE 22/tcp EXPOSE 80 443EXPOSE 1234/udp例子 - Dockerfile. expose FROM pythonWORKDIR /appCOPY hello. py . COPY start-app. sh . RUN pip install FlaskCMD [ /bin/bash ,  start. app. sh ]EXPOSE 50001234567docker build -t hello-port -f Dockerfile. expose . docker inspect -f  {{json . Config. ExposedPorts}}  hello-portdocker run -d --name my-hello-port -P hello-port # -P 好像并没有给 5000 指定一个主机的端口号# -P 自动把所有容器的端口都和 hots 主机进行 mapping # docker run -d --name my-hello-port -p 5000:5000 hello-portdocker ps -a访问对应的 local 地址：端口号 Docker 命令 - ENTRYPOINT 和 CMD: ENTRYPOINT 和 CMD 是容器启动后要执行的命令:  Entrypoint 就是 命令 + 第一个参数 cmd 就是 后面的参数 entrypoint 和 cmd 经常组合使用， 很多时候 cmd 被设置为空； 也有很多时候，entrypoint 是空的，只用cmd 从 base 镜像继承 可以在 Dockerfile 中重写 可以在 run 的时候被覆盖 在每次容器启动的时候执行 多个 Entrypoint的话，最后一个生效 多个 CMD 的话，最后一个生效例子: Dockerfile. simple: FROM pythonENTRYPOINT [ python ]CMD [ --version ]# CMD [ -c ,  print('hello world') ]1234docker build -t simple -f Dockerfile. simple . docker inspect simple # 可以看到image 定义的 entrypoint 和 cmd docker run --rm simple # 可以看到版本信息例子 - Dockerfile. entry: 12345678910111213141516171819202122cd . . git clone https://github. com/pythonincontainers/entrypoint-cmdcd entrypoint-cmd# 查看 Dockerfile 文件 Dockerfile. entry#FROM python#RUN pip install psutil#WORKDIR /app/#COPY args. py /app/#ENTRYPOINT [ python ]#CMD [ args. py ]docker build -t entry -f Dockerfile. entry . docker run --rm entry# 执行的命令输出的就是 #This process PID is: 1#This process executable is: /usr/local/bin/python3. 9#This process sys. argv is: ['args. py']#This process command line is: ['python', 'args. py']#List of all processes in the current Container:#PID= 1 PPID= 0 CMDLINE= ['python', 'args. py']例子 - Dockerfile. parent:  先通过 Dockerfile. parent 创建一个parent 镜像， 然后在用 Dockerfile. child 导入 parent 镜像继承 parent 的 Dockerfile 中的内容 然后在通过 Dockerfile. child 创建一个容器，就可以使用 Dockerfile. parent 中定义的 Entrypoint 和 CMD 了12345678910111213141516171819202122232425262728# Dockerfile. parent# FROM python# RUN pip install psutil# WORKDIR /app/# COPY args. py /app/# ENTRYPOINT [ python ]# CMD [ args. py , one ]# Dockerfile. child# FROM parent# RUN datedocker build -t parent -f Dockerfile. parent .  # 创建 parent 镜像docker build -t child -f Dockerfile. child .  # 创建 child 镜像，继承自 parentdocker inspect -f  Entrypoint={{. Config. Entrypoint}} CMD={{. Config. Cmd}}  child # 查看镜像中的配置# 输出# Entrypoint=[python] CMD=[args. py one]docker run --rm child# 输出#This process PID is: 1#This process executable is: /usr/local/bin/python3. 9#This process sys. argv is: ['args. py', 'one']#This process command line is: ['python', 'args. py', 'one']#List of all processes in the current Container:#PID= 1 PPID= 0 CMDLINE= ['python', 'args. py', 'one'] Dockerfile. child 中可以直接继承或者复写 Dockerfile. parent 中定义的Entrypoint 和 CMD 的内容docker run 可以通过参数复写 entrypoint: 12docker fun -it --rm --entrypoint=   child /bin/bashDockerfile 的 ARG 参数:  ARG 用于在 Dockerfile中定义变量，定义的变量可以重复使用。 ARG 定义的值可以用于一下命令，除了 ENTRYPOINT 和 CMD     FROM   RUN   ENV   COPY   ADD   EXPOSE   LABEL   STOPSIGNAL   USER   VOLUME   WORKDIR   12345678910111213141516git clone https://github. com/pythonincontainers/dockerfile-argcd dockerfile-arg/#查看 Dockerfile. arg# ARG Python_Image_Name=python# ARG Python_Image_Tag=latest# FROM $Python_Image_Name:$Python_Image_Tag# ARG Flask_Ver=1. 0. 2# RUN pip install flask==$Flask_Ver# WORKDIR /app# COPY hello-v2. py . # CMD [ python , hello-v2. py ]# 创建image时覆盖 arg 参数docker build -t args -f Dockerfile. arg --build-arg Flask_Ver=1. 0. 0 . docker run -it --rm -p 5000:5000docker inspect argshttp://localhost:5000/test/yodas 查看定义过的 args 12docker inspect argsdocker history args用环境变量来控制 args:  在 Dockerfile 中通过 args 定义 Env , 然后在 python 文件中通过 import os 来调用这些 env 环境变量 Dockerfile 中， FROM 会导入基础镜像，有可能会覆盖掉 FROM之前定义过的 arg, 所以 FROM 之后要重新定义 argsDockerfile. env ARG Python_Image_Name=pythonARG Python_Image_Tag=latestFROM $Python_Image_Name:$Python_Image_TagARG Flask_Ver=1. 0. 2ARG Python_Image_Name=pythonARG Python_Image_Tag=latestENV PYTHON_IMAGE_NAME $Python_Image_NameENV PYTHON_IMAGE_TAG $Python_Image_TagENV FLASK_VER $Flask_VerRUN pip install flask==$Flask_VerWORKDIR /appCOPY hello-v2. py . CMD [ python , hello-v2. py ]hello-v2. py 1234567891011121314151617181920212223242526272829303132from flask import Flaskimport os # 导入 osapp = Flask(__name__)@app. route('/')def hello_world():  return 'Flask Hello world! Version 1\n'@app. route('/test/&lt;username&gt;')def test(username):  if username ==   :    return 'Testing hidden functionality ;)\n'  else:    return 'Greetings Master ' + usernameif __name__ == '__main__':  try:    image_name = os. environ['PYTHON_IMAGE_NAME'] # 通过 os 调用 Dockerfile 中定义的环境变量  except:    image_name = 'unspecified'  try:    image_tag = os. environ['PYTHON_IMAGE_TAG']  except:    image_tag = 'unspecified'  try:    flask_ver = os. environ['FLASK_VER']  except:    flask_ver = 'unspecified'  print( Base Image is   + image_name +  :  + image_tag)  print( Flask version installed is  +flask_ver)  app. run(host='0. 0. 0. 0')123docker build -t args -f Dockerfile. env --build-arg Python_Image_Name=centos/python-36-centos7 . docker run -it --rm -p 5000:5000 argshttp://localhost:5000 用 ARGS 为 LABEL 打标签:  LABEL 的用处是让开发人员可以通过标签快速查找想要的镜像信息 通过 Arg 为 Label 打标签可以更高效Dockerfile. label ARG Python_Image_Name=pythonARG Python_Image_Tag=latestFROM $Python_Image_Name:$Python_Image_TagARG Flask_Ver=1. 0. 2ARG Python_Image_Name=pythonARG Python_Image_Tag=latestENV PYTHON_IMAGE_NAME $Python_Image_NameENV PYTHON_IMAGE_TAG $Python_Image_TagENV FLASK_VER $Flask_VerRUN pip install flask==$Flask_VerWORKDIR /appCOPY hello-v2. py . CMD [ python , hello-v2. py ]LABEL com. mycompany. image-name $Python_Image_NameLABEL com. mycompany. image-tag $Python_Image_TagLABEL com. mycompany. python. flask-ver $Flask_VerLABEL com. mycompany. maintainer kris@mycompany. comLABEL com. mycompany. source-repo dockerfile-ag创建可重用的python镜像: 容器化的 python 应用需要配置的内容包括  配置文件 环境变量 命令行选项和参数1234567891011121314151617git clone https://github. com/pythonincontainers/reusablecd reusable# 查看 Dockerfile. env# FROM python# WORKDIR /app# COPY config-reader. py /app/# ENV CONFIG_COLOR white# ENV CONFIG_SHAPE triangle# ENTRYPOINT [ python ,  config-reader. py ]# CMD [ --env ]docker build -t env -f Dockerfile. env . docker run -it --rm env # 创建容器并使用定义的变量docker run -it --rm -e CONFIG_SHAPE=dot env # 覆盖一个环境变量用 default. ini 来运行 python 命令:    好处是可以将开发用的环境参数写到 . ini 文件中， 方便环境变量     命令语法 python xxx. py --file default. ini     方法是创建一个. ini 文件并配置， 然后通过 Dockerfile 把内容引入 image  # 查看 Dockerfile. fileFROM pythonWORKDIR /appCOPY config-reader. py /app/COPY configs/default. ini /app/ENTRYPOINT [ python ,  config-reader. py ]CMD [ --file ,  default. ini ]deault. ini 12345678910[DEFAULT]color = greyshape = line[db_settings]host = dbport = 3307database = mydbusername = dbuserpassword = dbpass创建镜像并运行容器 1234567891011121314151617181920docker build -t file -f Dockerfile. file . docker run -it --rm file# 输出#reading configuration from Config File default. ini# color  = grey# shape  = line#Final configuration is:  color  = grey ,  shape  = line# 开发环境中可以用本地环境的配置信息代替镜像中预定义的信息来使用，方便开发docker run -it --rm -v ${PWD}/configs/blue_circle. ini:/app/default. ini file# 输出#Reading configuration from Config File default. ini# color  = blue# shape  = circle#Final configuration is:  color  = blue ,  shape  = circle# 直接在 Dockerfile 中定义参数docker build -t args -f Dockerfile. args . docker run -t --rm argsdocker run -t --rm args --color  ye4llow  --shape  banana ### 镜像创建 Build time 与 Run time: Build 的流程  创建目录 Copy 文件和目录到镜像中 修改元数据 安装 linux 包 安装 python 类库 定义启动命令获取实验代码 123git clone https://github. com/pythonincontainers/buildtime-runtimecd buildtime-runtime创建镜像 - runtime:  这样创建的镜像，每次 run 容器的时候才会去下载必要的包 风险是也许几年后，这些包就不会被维护，无法下载了 run 容器也需要耗费一定的时间1234567891011121314151617# 查看 Dockerfile. runtime# FROM python:3. 7. 3# WORKDIR /django-mysite# COPY . . # CMD [ /bin/bash ,  run-server. sh ]# 查看 run-server. sh# #! /bin/bash# pip install -r requirements. txt# mkdir -p /data# python manage. py migrate# /bin/bash create-admin. sh# python manage. py runserver 0. 0. 0. 0:8000docker build -t runtime -f Dockerfile. runtime .  # 创建镜像docker run -it --rm -p 8000:8000 runtime # 创建容器# 在创建容器过程中会下载 Django， sqlight , 迁移数据， 配置管理员， 设置数据库，启动 django 服务http://0. 0. 0. 0:8000 http://localhost:8000/admin/login/ admin admin 创建 1 个问题和几个选项 访问 http://localhost:8000/polls/ 提交问卷 control + c 停掉容器 12# 一旦删除容器后， 数据都没有了， 等重建容器的时候，需要再做一遍所有的初始化工作，比较耗时docker rm -f runtime创建镜像 - buildtime:  通过 build time 创建镜像，可以节省创建容器的时间 好处是需要下载的内容在 build 镜像的时候就下载好，整合到镜像中了# 查看 Dockerfile. buildtime# Dockerfile 中就把之前 run-server. sh 中的工作做掉了FROM python:3. 7. 3WORKDIR /django-mysiteCOPY . . ARG DJANGO_VER=2. 2. 1RUN pip install -r requirements. txtRUN mkdir -p /data &amp;&amp; python manage. py migrateRUN bash create-admin. shVOLUME /data # 创建一个新的 volume ， mount 到 /data 目录， 防止数据丢失CMD [ python ,  manage. py ,  runserver ,  0. 0. 0. 0:8000 ]创建镜像 123docker build -t buildtime -f Dockerfile. buildtime . # build 会下载所需的内容，并进行数据库初始化等操作docker run -it --rm -v buildtime-vol:/data -p 8000:8000 buildtime # 我们没有创建过 buildtime-vol， 这里会自动帮我们创建# 容器创建速度会非常快http://0. 0. 0. 0:8000 http://localhost:8000/admin/login/ admin admin 创建 1 个问题和几个选项 访问 http://localhost:8000/polls/ 提交问卷 control + c 停掉容器 12# 同样的命令重新创建容器， 数据都还在docker run -it --rm -v buildtime-vol:/data -p 8000:8000 buildtime 一个极端的例子: 用一行 CMD 来完成所有的配置 FROM python:3. 7. 3CMD [ /bin/bash ,  -c ,  git clone https://github. com/pythonincontainers/buildtime-runtime; cd buildtime-runtime; source run-server. sh ]Build 更小的镜像:  Python 基础镜像的内容包含了大量开发工具, 对于开发阶段比较有用，但是对于单纯的运行Python环境来说没有必要     300M+ 的系统类库   600M+ 的工具在/usr/bin 下面 （C or C++）    保持镜像的小尺寸有很多好处 省钱，省时间，省空间 https://hub. docker. com/_/python 可以找到不同尺寸的 python 镜像， 包括了 slim 和 alpine获取演示代码 12git clone https://github. com/pythonincontainers/smallercd smaller基于标准Python 镜像 的 image 创建: FROM python:3. 7. 3WORKDIR /appCOPY . . RUN pip install -r requirements. txtCMD [ python , factors_flask. py ]12345678docker build -t factors_flask:standard -f Dockerfile. standard . docker run -it --rm -p 5000:5000 factors_flask:standard# 访问 http://localhost:5000/12# 查看镜像大小 - 940Mdocker images factors_flask# REPOSITORY   TAG    IMAGE ID    CREATED     SIZE# factors_flask  standard  cc61aadd153a  3 minutes ago  940MB基于 Python Slim 版本的镜像创建: FROM python:3. 7. 3-slimWORKDIR /appCOPY . . RUN pip install -r requirements. txtCMD [ python , factors_flask. py ]1234567docker build -t factors_flask:slim -f Dockerfile. slim . docker run -it --rm -p 5000:5000 factors_flask:slimdocker images factors_flask # 154M#REPOSITORY   TAG    IMAGE ID    CREATED     SIZE#factors_flask  slim    adce144d1091  2 minutes ago  154MB#factors_flask  standard  cc61aadd153a  22 minutes ago  940MB基于 python alpione 的 python 镜像: FROM python:3. 7. 3-alpineWORKDIR /appCOPY . . RUN pip install -r requirements. txtCMD [ python , factors_flask. py ]1234567docker build -t factors_flask:alpine -f Dockerfile. alpine . docker run -it --rm -p 5000:5000 factors_flask:alpinedocker images factors_flask # - 98M#REPOSITORY   TAG    IMAGE ID    CREATED     SIZE#factors_flask  alpine   482863f47adb  25 minutes ago  98. 2MB#factors_flask  slim    adce144d1091  31 minutes ago  154MB#factors_flask  standard  cc61aadd153a  52 minutes ago  940MBImage build 的多个阶段:  为了让代码运行的更快，并标尺 small size, 我们需要了解和优化镜像 build 的几个阶段 cython. org 是一个Pythobn 的扩展， 可以将 python 代码转换成 C 代码的编译工具，能提高代码执行效率用 factor. py 来计算: 12345678910111213141516git clone https://github. com/pythonincontainers/multistagecd multistage# 查看 Dockerfile. cython-standard# FROM python:3. 7. 3# WORKDIR /app# COPY . . # RUN pip install cython==0. 28. 5# RUN python compile. py build_ext --inplace# RUN pip install -r requirements. txt# CMD [ python , factors_flask. py ]docker build -t factors_flask:cython-standard -f Dockerfile. cython-standard . docker run -it --rm factors_flask:cython-standard bashexitdocker run -it --rm -p 5000:5000 factors_flask:cython-standard访问 http://localhost:5000/123456789 用了 7 秒进行计算 改成 factor. pyx 计算: 修改 compile. py 中的 factors. py 为 factors. pyx 123docker build -t factors_flask:cython-optiumized -f Dockerfile. cython-standard . docker run -it --rm -p 5000:5000 factors_flask:cython-optiumized访问 http://localhost:5000/123456789 用了 0. 4 秒进行计算 ， 提高了将近 20 倍 镜像大小 940M 用 multi stage 方法创建镜像, 镜像小，速度快:  原理是第一阶段先用全版本做一个 so 文件 第二阶段用一个 mini 版本，利用第一阶段的 so 文件做最终的 image查看 Dockerfile. cython-multi # 第一阶段FROM python:3. 7. 3 as dev # 使用标准版本作为 base image，起个别名 devWORKDIR /appCOPY . . RUN pip install cython==0. 28. 5 # 安装cython 作为更快的编译器RUN python compile. py build_ext --inplace # 创建编译类库 . so 文件# 第二阶段FROM python:3. 7. 3-slim as prod  # 使用 slim 版本作为 base image, 起个别名 prodWORKDIR /appCOPY factors_flask. py requirements. txt /app/ # 只拷贝必要的文件COPY --from=dev /app/factors. cpython-37m-x86_64-linux-gnu. so /app # 将第一阶段编译好的类库文件 . so文件靠过来RUN pip install -r requirements. txtCMD [ python , factors_flask. py ] 12345docker build -t factor_flask:cython-multi -f Dockerfile. cython-multi . docker run -it --rm -p 5000:5000 factor_flask:cython-multidocker images访问 http://localhost:5000/123456789 用了 0. 4 秒进行计算 ， 提高了将近 20 倍 镜像大小 154M, 缩小了大概 6 倍 定制自己的 python 镜像: 12345git clone https://github. com/pythonincontainers/buildcustomcd buildcustom# 查看 Dockerfile. cython-flask-slim FROM cython:3. 7. 3-full as devWORKDIR /appCOPY . . RUN python compile. py build_ext --inplace # 编译 python 类库到 . so 文件FROM flask:1. 0. 3-slim as prodWORKDIR /appCOPY factors_flask. py /app/ COPY --from=dev /app/factors. cpython-37m-x86_64-linux-gnu. so /appENV FLASK_APP=factors_flask. py # 设置环境变量123456docker build -t factors_flask:cython-flask-slim -f Dockerfile. cython-flask-slim . # 这里报错了# ERROR [internal] load metadata for docker. io/library/cython:3. 7. 3-full docker run -it --rm -p 5000:5000 factors_flask:cython-flask-slimBuild base image from scratch 从头做一个基础镜像:  创建自己的基础镜像可以提供更便于自己开发团队进行开发的定制化镜像 可以提供更安全的保证 FROM scratch 并不是导入 scratch, 而是引入一个空的镜像。 分三个步骤     创建容器并对容器进行定制化   将容器导出成一个. tar 压缩包   通过 Dockerfile 将压缩包创建成一个镜像   123git clone https://github. com/pythonincontainers/basescratchcd basescratchDockerfile. testscratch 的例子: FROM scratch # 引入空镜像作为 base ADD rootfs. tar / # 将文件系统的压缩包进行解压并 build，放到根目录，等于做了一个操作系统ENTRYPOINT [ /usr/local/bin/python ] # 设置默认启动命令12345678910111213141516171819202122232425# 运行一个 python 容器， 会自动 pull 一个 python 的 slim 版本 镜像创建容器cd devdocker run -it --name official3. 7. 3 python:3. 7. 3-slim bash # 进入容器， 做一些定制化工作# 安装 flaskpip install Flask==1. 0. 3# 删除所有没用的 linux shell ，减少没必要的文件，降低文件系统的体积rm -f /bin/bash /bin/dash /bin/sh /bin/rbashexit # 退出容器# 将容器导出成一个压缩包docker export -o rootfs. tar official3. 7. 3# export    a container's filesystem as a tar archivels -lah # 查看文件大小 rootfs. tar 145Mdocker rm official3. 7. 3 # 不需要容器了， 删掉# 通过 Dockerfile 指定的 rootfs. tar 做一个新的镜像docker build -t mypython3. 7:official -f Dockerfile. testscratch . docker run -it --rm mypython3. 7:official # 创建一个容器# 查看镜像的历史, 可以看到只有两层，都是通过 Dockerfile 创建时产生的# history   Show the history of an imagedocker history mypython3. 7:official用 debootstrap 创建一个全新的镜像:  debootstrap是debian/ubuntu下的一个工具,用来构建一套基本的系统(根文件系统)。生成的目录符合Linux文件系统标准(FHS),即包含了/boot、/etc、/bin、/usr等等目录 只能在 Debian-based system 上进行安装 可以利用 debootstrap 制作一个 mini 的文件系统，用于定制化我们的镜像 用 debootstrap 制作文件系统的步骤     获取 ubuntu 系统   安装 debootstrap linux 包   执行 debootstrap 去 创建 Ubuntu 目录   ubuntu 文件系统准备就绪   按需修改 ubuntu 目录   使用 chroot 安装或删除 Linux 包   检查确认文件系统， 打 tar 包   通过 rootfs. tar 创建镜像   下面是下载 ubuntu Xinial 版本后对操作系统的初始化脚本 Install-xenial-buildd. sh 1234567891011121314151617apt updateapt install -y debootstrapexport MY_ROOT=/data/rootmkdir -p $MY_ROOTdebootstrap --variant=buildd xenial $MY_ROOT http://archive. ubuntu. com/ubuntu/mount --bind /dev $MY_ROOT/devmount --bind /dev/pts $MY_ROOT/dev/ptsmount --bind /sys $MY_ROOT/sysmount --bind /proc $MY_ROOT/proccp /etc/hosts $MY_ROOT/etc/hostscp /proc/mounts $MY_ROOT/etc/mtabcp /etc/resolv. conf $MY_ROOT/etc/resolv. confcp /host/install-python3. 7. sh $MY_ROOT/tmpcp /host/packages_to_purge. txt $MY_ROOT/tmpchroot $MY_ROOT /bin/sh /tmp/install-python3. 7. sh # 引出下一个安装脚本mkdir -p /host/buildcp $MY_ROOT/rootfs. tar. gz /host/buildinstall-python3. 7. sh 1234567891011121314151617181920212223apt-get update# Install 'software-properties-common' to get add-apt-repositoryapt-get install -y software-properties-common build-essential# Install latest Python repository for Ubuntuadd-apt-repository ppa:deadsnakes/ppa -yapt-get update# Install Python 3. 7 and its Libraries plus 'curl' to pull PIP installation scriptapt-get install -y python3. 7 python3. 7-dev curl# Add 'python' name to 'python3. 7' binaryln /usr/bin/python3. 7 /usr/bin/python# Pull PIP installation scriptcurl  https://bootstrap. pypa. io/get-pip. py  -o  /tmp/get-pip. py # Install PIP, download specific versions of modulespython /tmp/get-pip. py pip==19. 1. 1 setuptools==41. 0. 1 wheel==0. 33. 4rm /tmp/get-pip. py# Purge and remove Ubuntu Packages not needed in Python Development Image, 'curl' for exampleapt-get remove --auto-remove --purge -y --allow-remove-essential $(cat /tmp/packages_to_purge. txt)# Clean APT cachesapt-get cleanrm -rf /tmp/*# Create a full system Tar balltar -cpzf rootfs. tar. gz --exclude=/rootfs. tar. gz --one-file-system /创建基础镜像所需的 tar 包， 然后将包导入到 docker 镜像中: 12345678910111213141516171819202122git clone https://github. com/pythonincontainers/basescratchcd basescratch/dev# 构建基础镜像所需要的 tar. gz 包# -t 是让当前目录和容器的/host 目录共享# --privileged Give extended privileges to this containerdocker run -it --rm -v ${PWD}:/host --privileged ubuntu:16. 04 /bin/sh /host/install-xenial-buildd. sh# 结果是创建一个名字叫 rootfs. tar. gz 的文件# 通过 tar. gz 创建文件系统镜像docker import --change 'ENV PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin' --change  CMD /usr/bin/python  build/rootfs. tar. gz mypython3. 7:import# import  the contents from a tarball to create a filesystem image# --change Apply Dockerfile instruction to the created image# 查看镜像docker images # 看到如下镜像， 说明创建成功# mypython3. 7             import                         87e7a85c73fc  52 seconds ago  391MBdocker inspect mypython3. 7:import还可以用 Dockerfile 把 tar. gz 做成镜像: Dockerfile. basescratch FROM scratchADD build/rootfs. tar. gz /ENV HOME=/rootCMD [ /bin/bash ]构建开发用镜像 12docker build -t mypython3. 7:dev-xenial -f Dockerfile. basescratch . 构建一个生产的镜像:  唯一区别就是 install-xenial-minbase. sh 中 debootstrap 中用的版本是一个 更小的版本， 没有开发需要的一些类库12345678910111213cd . . /proddocker run -it --rm -v ${PWD}:/host --privileged ubuntu:16. 04 /bin/sh /host/install-xenial-minbase. shdocker import --change 'ENV PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin' --change  CMD /usr/bin/python  build/rootfs. tar. gz mypython3. 7:importdocker build -t mypython3. 7:prod-xenial -f Dockerfile. basescratch . # 查看上面做的三个镜像docker images# REPOSITORY              TAG                           IMAGE ID    CREATED     SIZE# mypython3. 7             prod-xenial                       f53df66eb76b  31 seconds ago  100MB# mypython3. 7             dev-xenial                       d25da53e05f8  9 minutes ago  391MB# mypython3. 7             import                         87e7a85c73fc  11 hours ago   391MB在通过 Cython 将刚才做的镜像优化成C 编译的镜像 查看 Dockerfile. cython-multi FROM mypython3. 7:dev-xenial as dev # 先用开发版本的镜像生成一个 so 类库WORKDIR /appCOPY . . RUN pip install cython==0. 28. 5RUN python compile. py build_ext --inplace # 编译 c 版本的类库FROM mypython3. 7:prod-xenial as prod # 使用刚刚创建的生产部版本的镜像作为 base imageWORKDIR /appCOPY factors_flask. py requirements. txt /app/COPY --from=dev /app/factors. cpython-37m-x86_64-linux-gnu. so /app # 将 编译后的类库拷贝过来RUN pip install -r requirements. txtCMD [ python , factors_flask. py ]. dockerignore 防止我们拷贝不必要的文件到镜像中 12345. git. dockerignoreDockerfile*devprod构造最终的镜像 123cd . . docker build -t factors_flask:cython-custom -f Dockerfile. cython-multi . docker run -it --rm -p 5000:5000 factors_flask:cython-custom访问 http://localhost:5000/12345678 "
    }, {
    "id": 21,
    "url": "http://localhost:4000/NLP-with-Python/",
    "title": "Natural Language Processing with Python",
    "body": "2021/06/17 - Deep Learning for NLP[TOC] 环境准备:       创建一个 nlp 目录， 在目录下安装一个虚拟环境        安装 pandas numpy scipy tensorflow keras spacy nltk textblob 这些必要的python 库     12345mkdir nlpcd nlpvirtualenv venvsource . /venv/bin/activatepip install pandas numpy scipy tensorflow keras spacy nltk textblob NLP处理的类库介绍: NLTK:  NLTK是最常用的包，用于分词，分析语言结构等。 分词 12345678import nltk# Tokenizationsent_ =  I am almost dead this time tokens_ = nltk. word_tokenize(sent_)tokens_# 输出# ['I', 'am', 'almost', 'dead', 'this', 'time']获取同义词 123456789101112131415from nltk. corpus import wordnetword_ = wordnet. synsets( spectacular )print(word_)print(word_[0]. definition())print(word_[1]. definition())print(word_[2]. definition())print(word_[3]. definition())#输出#[Synset('spectacular. n. 01'), Synset('dramatic. s. 02'), Synset('spectacular. s. 02'), Synset('outstanding. s. 02')]#a lavishly produced performance#sensational in appearance or thrilling in effect#characteristic of spectacles or drama#having a quality that thrusts itself into attention资源: 图书线上链接 本书代码资源 有问题找作者 spacy 专有名词: 包括自然语言处理(NLP) 自然语言理解(NLU) 自然语言交互(NLI) 自然语言生成(NLG)， NLP 介绍: 流行的AI 应用: Machine Translation  google 翻译Speech recognition    Amazon Alexa,     Apple Siri,     Google Assistant,     Microsoft Cortana,  Question Answering - 基于speech recognition 进行问题回答 Text summarization - 读取长短信息，然后进行总结 Chatbots - 比如IBM的HRbot, 可以在线和你文字互动 Text2speech and speech2text  Google Cloud Speech-to-Text / text to speech. 把文字转换成人类说话的语音，可以支持180中声调的30中语言， 反过来也可以Voicebots - 语音机器人，比如,代替了电话销售，让人不知道对方是个机器人的情况下进行对话 Text and audio generation - 自动生成文本或者语音，比如gmail 可以根据上文自动推荐下文 Sentiment analysis - 分析情感，开心-伤心 积极-消极 高兴-生气 Information extraction - 从信息中抓取结构化数据，或者总结出关系 NLP 的拐点:    2014 年 Amazon Alexa 提升了 Speech recognition     2018年 大量预训练的语言模型使用了 Transformer 架构     2019 - 2020年 generative models 比如 OpenAI’s GPT-2, GPT-3 可以通过前文来自动生成下文  三种NLP的演变:  基于rule 的 NLP, 比如通过正则表达式来制定规则处理语言 传统机器学习来处理NLP, 通过统计分析，创建概率分布。需要标注大量的信息，是feature 工程。通过标注信息来提升模型的表现，比rule based 方法好一些 神经网络， 通过 representation learning 解决了需要人工标注feature 的问题. 神经网络需要大量的数据，自动创建规则，自动学习。以上三种最后一种最好，但是前两中也有他们生存的空间 NLP的任务的定义: Tokenization - 词语切分: 将文本切分成最小有意义的单位（token），比如单词，标点符号等。we live in paris 会被切分成四个独立的单词(token)。tokenization 是自然语言处理的第一步 Part-of-speech（POS） tagging - 词性标签: 给每个token设定词性，比如动词名词副词等，目的是让机器更好的理解token之间的关系 词类：实词 - 有6种NOUN - 名词 None PRON - 代词 pronoun , 你我他你们我们他们, 你的我的他的 PROPN - 专有名词 proper noun ， 伽利略 ADJ - 形容词 adjective ， 美丽的，干净的 NUM - 数词 Numeral , Four, Five VERB - 动词 Verb AUX - 助动词 is ,are, was ,were ADV - 副词 adverb, 副词用来修饰动词和形容词 ， in, 虚词 - 有4种DET - 冠词 determiner , a , the, this, that ADP - 介词 adposition - of, to, for, under CCONJ - 连词 conjunction ， and , or INTJ - 感叹词 Interjection, Wow, oh 符号PUNCT - 符号 punctuation 下面表格是完整地POS 词性标签       POS   Description   Example         ADJ   Adjective   Big, old, green, incomprehensible, first       ADP   Adposition   In, to, during       ADV   Adverb   Very, tomorrow, down, where, there       AUX   Auxiliary   Is, has (done), will (do), should (do)       CONJ   Conjunction   And, or, but       CCONJ   Coordinating conjunction   And, or, but       DET   Determiner   A, an, the       INTJ   Interjection   Psst, ouch, bravo, hello       NOUN   Noun   Girl, cat, tree, air, beauty       NUM   Numeral   1, 2017, one, seventy-seven, IV, MMXIV       PART   Particle   ’s, not       PRON   Pronoun   I, you, he, she, myself, themselves, somebody       PROPN   Proper noun   Mary, John, London, NATO, HBO       PUNCT   Punctuation   . , (, ), ?       SCONJ   Subordinating conjunction   If, while, that       SYM   Symbol   ×, %, §, ©, +, -, ×, ÷, =, :),       VERB   Verb   Run, runs, running, eat, ate, eating       X   Other   Sfpksdpsxmsa       SPACE   Space       dependency parsing - 依赖解析: （ Allen依赖解析demo ）标注tocken 之前的依赖关系，标注了依赖关系，整句话就是一个结构化数据了， 机器就可以阅读这种标注了依赖关系的句子。 依赖关系NSUBJ - nominal subject 名义的主语 PREP - prepositional phrase 介词短语 POBJ - Preposition of object 对象的介词 下面表格是完整地依赖解析       Label   Description         ac1   Clausal modifier of noun (adjectival clause)       advc1   Adverbial clause modifier       advmod   Adverbial modifier       amod   Adjectival modifier       appos   Appositional modifier       aux   Auxiliary       case   Case marking       cc   Coordinating conjunction       ccomp   Clausal complement       clf   Classifier       compound   Compound       conj   Conjunction       cop   Copula       csubj   Clausal subject       dep   Unspecified dependency       det   Determiner       discourse   Discourse element       dislocated   Dislocated element       expl   Expletive       fixed   Fixed multiword expression       flat   Flat multiword expression       goeswith   Goes with       iobj   Indirect object       list   List       mark   Marker       nmod   Nominal modifier       nsubj   Nominal subject       nummod   Numeric modifier       obj   Object       obl   Oblique nominal       orphan   Orphan       parataxis   Parataxis       punct   Punctuation       reparandum   Overridden disfluency       root   Root       vocative   Vocative       xcomp   Open clausal complement   Chunking - 合并tockens: 创建名词tocken 组, 动词tockens组 比如 new york city 会合并成一个chunk Lemmatization - 词形还原: 把一些词形还原成原始的样子，比如复数变单数， 过去式变现在时, 比较级变正常 horses 变 horse, slept 变 sleep， biggest 变 big Stemming - 归元: 和lemmatization 类似， 会把比较级回归原型，但是不会把过去式变成现在时 Named entity recognition(NER) - 给对象打标签: entity 这就就是objects。 给对象打标签，比如打成 人，组织，位置，日期，货币等 we live in Paris, 这里就给Paris 打成位置。 再比如我们要搜索小布什，我们希望可以在所有标注了小布什是人得文档种搜索 下表是标签类型 NER 是一个统计模型，可以帮助机器更好的理解对象，       Type   Description         PERSON   People, including fictional       NORP   Nationalities or religious or political groups       FAC   Buildings, airports, highways, bridges, etc.        ORG   Companies, agencies, institutions, etc.        GPE   Countries, cities, states       LOC   Non-GPE locations, mountain ranges, bodies of water       PRODUCT   Objects, vehicles, foods, etc. (not services)       EVENT   Named hurricanes, battles, wars, sports events, etc.        WORK_OF_ART   Titles of books, songs, etc.        LAW   Named documents made into laws       LANGUAGE   Any named language       DATE   Absolute or relative dates or periods       TIME   Times smaller than a day       PERCENT   Percentage, including %       MONEY   Monetary values, including unit       QUANTITY   Measurements, as of weight or distance       ORDINAL   “First,” “second,” etc.        CARDINAL   Numerals that do not fall under another type   Named Entity linking(NEL) - 消除歧义: 实体链接是把有可能有歧义的实体连接到一个外部知识库， 比如布什和小布什 开发环境搭建: 使用 google 的 Colaborator 作为Jupyter Notebook 环境 使用GitHub 保存源代码 三个支持NLP 的开源 类库    spaCy     fast. ai     Hugging Face  spaCy:    2015年发布，没发布之前，NLTK是最牛逼的，spaCy 有更好的性能，可伸缩性，目前最成熟的，适合生产商用   支持64种语言，并同时支持Pytorch 和 Tensorfoow 2021年之前的版本2. x 是基于recurrent neural networks (RNNs). 2021年之后给基于了更新的 transformer-based 模型， cpaCy 的母公司是 Explosion AI 还提供了注解服务 Prodigyfast. ai: fast. ai 2018年开源，基于Pytorch 开发 可以快速构建NLP 模型，但是成熟度不如spaCy 和 Hugging face Hugging Face: 2016 年成立， hugging face 是这三个中成长速度最快，被投资钱数最多的一个 他的类库用的是 transformers, 基于tensorflow 和 pytorch, 支持超过100中语言 目前来很有钱途，建议好好研究 玩转 spaCy: 安装python 最新版本, 安装spacy 扩展 , 安装方法参考官网指导 配置pip源 安装aws 命令行 安装jupyter 安装pandas 12345678910111213141516171819202122232425262728293031323334353637383940cd ~/mkdir . pipcd . pipvi pip. conf# 增加如下内容[global]timeout = 6000index-url = http://pypi. douban. com/simpletrusted-host = pypi. douban. comformat=columns# brew install python@3. 10brew unlink python@3. 9brew link --force python@3. 10mkdir nlpcd nlppython3 -m venv . envsource . env/bin/activatepip3 install -U pip setuptools wheelpip3 install -U spacypython3 -m spacy download zh_core_web_smpython3 -m spacy download en_core_web_sm#deactivate # 退出虚拟环境#pip3 freeze     # 查看 所有的Pip 环境安装的依赖包列表#pip3 freeze &gt; requirements. txt  # 将所有的Pip 环境安装的依赖包导出到文件中#pip3 install -r requirements. txt # 读取安装文件来安装 pip 包# 安装aws 命令行curl  https://awscli. amazonaws. com/AWSCLIV2. pkg  -o  AWSCLIV2. pkg sudo installer -pkg AWSCLIV2. pkg -target /# 安装pandaspip3 install pandas#使用Jupyerlabpip3 install jupyterlabexport PATH= /usr/local/opt/python@3. 10/Frameworks/Python. framework/Versions/3. 10/bin:$PATH jupyter-labToken 部分 创建一个spacy 实例，使用英文语言 把一个panda dataframe 的一个二维字典的question 列转换成token1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# Import spacy and download language modelimport spacynlp = spacy. load( en_core_web_sm )# Download data 这里要brew 安装aws 的 mac 命令行!aws s3 cp s3://applied-nlp-book/data/ data --recursive --no-sign-request!aws s3 cp s3://applied-nlp-book/models/ag_dataset/ models/ag_dataset --recursive --no-sign-request# Tokenizationsentence = nlp. tokenizer( We live in Paris.  )# 这里的 nlp( We live in Paris.  ) 和 nlp. tokenizer( We live in Paris.  ) 是不一样的。# Length of sentenceprint( The number of tokens:  , len(sentence))# Print individual words (i. e. , tokens)print( The tokens:  )for words in sentence:  print(words)import pandas as pdimport oscwd = os. getcwd()# Import Jeopardy Questionsdata = pd. read_csv(cwd+'/data/jeopardy_questions/jeopardy_questions. csv')data = pd. DataFrame(data=data)# Lowercase, strip whitespace, and view column namesdata. columns = map(lambda x: x. lower(). strip(), data. columns)# Reduce size of datadata = data[0:1000] # Tokenize Jeopardy Questionsdata[ question_tokens ] = data[ question ]. apply(lambda x: nlp(x)) # 这里没有用nlp. tokenizer, # View first questionexample_question = data. question[0]example_question_tokens = data. question_tokens[0]print( The first questions is: )print(example_question)# Print individual tokens of first questionprint( The tokens from the first question are: )for tokens in example_question_tokens:  print(tokens)Part-of-speech（POS） taggin - 给每个token 标上词性 token. text 看token 文本 token. pos_ 可以看每个单个token 的词性缩写 spacy. explain(token. pos_) 查看词性完整单词1234# Print Part-of-speech tags for tokens in the first questionprint( Here are the Part-of-speech tags for each token in the first question: )for token in example_question_tokens:  print(token. text,token. pos_, spacy. explain(token. pos_))Dependency Parsing - 依赖解析 token. dep_ 会对某个token 进行依赖解析 spacy 的 displacy. rander 库可以对一段文本中的所有token的依赖关系 进行图形化的展示1234# Print Dependency Parsing tags for tokens in the first questionfor token in example_question_tokens:  print(token. text,token. dep_, spacy. explain(token. dep_))查看图形化地依赖关系 12345# Visualize the dependency parsefrom spacy import displacydisplacy. render(example_question_tokens, style='dep',        jupyter=True, options={'distance': 120})Chunking - 合并tokens   spaCy 会把 my parents 和 New York City 合并成两个单独的token     nlp对象. noun_chunks 可以得到这个句子中的所有合并后的trunk数组  123456789# Print tokens for example sentence without chunkingfor token in nlp( My parents live in New York City.  ):  print(token. text)    # Print chunks for example sentencefor chunk in nlp( My parents live in New York City.  ). noun_chunks:   print(chunk. text)Lemmatization - 词形还原 token. lemma_ 可以得到词形的还原12345678910# Print Lemmatization for tokens in the first questionlemmatization = pd. DataFrame(data=[], \ columns=[ original , lemmatized ])i = 0for token in example_question_tokens:  lemmatization. loc[i, original ] = token. text  lemmatization. loc[i, lemmatized ] = token. lemma_  i = i+1lemmatizationNamed entity recognition （NER） - 给token 打标签也可以理解为为目标定义类型，比如人，国家，位置等  token. label_标注了token 的类型 可以用 displacy. render 来展示这句话中被标注出来的标签，用颜色表示不同类型123456789101112131415# Print NER resultsexample_sentence =  George Washington was an American political leader, \military general, statesman, and Founding Father who served as the \first president of the United States from 1789 to 1797. \n print(example_sentence)print( Text Start End Label )doc = nlp(example_sentence)for token in doc. ents:  print(token. text, token. start_char, token. end_char, token. label_)    # Visualize NER resultsdisplacy. render(doc, style='ent', jupyter=True, options={'distance': 120})Named Entity Link - 将文本中的实体解析到外部知识库的一个唯一标签上这里使用的外部知识库是 Google Knowledge Graph 要在google cloud platform 中创建API Key ,并启动 ， 然后复制key到下面代码中 key 测试knowledge base api 的链接 1234567891011121314151617import requests# Define Google Knowledge Graph API Result functiondef returnGraphResult(query, key, entityType):  if entityType== PERSON :    google = f https://kgsearch. googleapis. com/v1/entities:search\     ?query={query}&amp;key={key}     resp = requests. get(google)    url = resp. json()['itemListElement'][0]['result']\     ['detailedDescription']['url']    description = resp. json()['itemListElement'][0]['result']\     ['detailedDescription']['articleBody']    return url, description  else:    return  no_match ,  no_match    1234567# Print Wikipedia descriptions and urls for entities# You can un-comment this and run the code after you obtain your own Google Knowledge Graph API key'''for token in doc. ents:  url, description = returnGraphResult(token. text, key, token. label_)  print(token. text, token. label_, url, description)'''返回的knowledge base api 的 Json 内容链接 Transformers 和 Transfer Learning: Transfer Learning 是一种把已经训练好的模型转到另一个你需要的数据集上，并进行调整。 比如我们想要训练一个海明威类型的自动短故事生成模型，就可以通过transfer learning 用大量非海明威的小说来对模型进行训练，而不是只用少量的海明威小说。 Transformer 是一个NLP 模型 架构，如果重零开始训练他效果可能不会太理想，建议继承一些已经训练好的 transformer ，然后做一些调整进行使用。  语言模型：  就是一个通过输入一些文字，就可以返回一些通过概率分布描述出来的下面的文字，白话就是我给你几个字，你就给我把下面的故事编出来。 这就是现代的 NLP 的最基础模型 用 fastai 训练: 先用fastai 进行 transfer learning. 1pip install fastai这里会用到 ULMFiT - Universal Language Model Fine-tuning for Text Classification 技术对我们的神经网络进行训练 数据集我们会用 IMDB , 一个电影评论的数据集 fastai 不仅仅是个深度学习的类库， 还包括了帮助我们端到端的解决问题的工具。其中一个好用的工具就是内置的数据集。  TextDataLoaders. from_folder() 可以设置自己的数据集123456from fastai. text. all import *path = untar_data(URLs. IMDB) # /Users/dalong/. fastai/data/imdb# 设置自己的数据集dls = TextDataLoaders. from_folder(path, valid='test')# 显示我们的数据集 dls. show_batch() 一共显示了8个数据集，内容中包含的xxbox xxmaj 并不是原始数据集的一部分，而是通过 tokenization 来被加进去的，用来被语言模型翻译， 比如 xxmaj 表示它后面的字母要首字母大写       text   category             0   xxbos xxmaj match 1 : xxmaj tag xxmaj team xxmaj table xxmaj match xxmaj bubba xxmaj ray and xxmaj spike xxmaj dudley vs xxmaj eddie xxmaj guerrero and xxmaj chris xxmaj benoit xxmaj bubba xxmaj ray and xxmaj spike xxmaj dudley started things off with a xxmaj tag xxmaj team xxmaj table xxmaj match against xxmaj eddie xxmaj guerrero and xxmaj chris xxmaj benoit . xxmaj according to the rules of the match , both opponents have to go through tables in order to get the win . xxmaj benoit and xxmaj guerrero heated up early on by taking turns hammering first xxmaj spike and then xxmaj bubba xxmaj ray . a xxmaj german xxunk by xxmaj benoit to xxmaj bubba took the wind out of the xxmaj dudley brother . xxmaj spike tried to help his brother , but the referee restrained him while xxmaj benoit and xxmaj guerrero   pos       1   xxbos xxmaj raising xxmaj victor xxmaj vargas : a xxmaj review \n\n xxmaj you know , xxmaj raising xxmaj victor xxmaj vargas is like sticking your hands into a big , steaming bowl of oatmeal . xxmaj it ‘s warm and gooey , but you ‘re not sure if it feels right . xxmaj try as i might , no matter how warm and gooey xxmaj raising xxmaj victor xxmaj vargas became i was always aware that something did n’t quite feel right . xxmaj victor xxmaj vargas suffers from a certain overconfidence on the director ‘s part . xxmaj apparently , the director thought that the ethnic backdrop of a xxmaj latino family on the lower east side , and an idyllic storyline would make the film critic proof . xxmaj he was right , but it did n’t fool me . xxmaj raising xxmaj victor xxmaj vargas is   neg   fastai 使用 Learner对象 来处理几乎所有的事情 12learn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0. 5, metrics=accuracy) # 这里用了AWS_LIST 架构，因为比较简单，运行快速"
    }, {
    "id": 22,
    "url": "http://localhost:4000/Docker-and-Kubernetes/",
    "title": "Building Microservice Systems with Docker and Kubernetes",
    "body": "2021/05/08 - 这是一个Python Docker入门教程。在本文结束时，您将知道如何在本地机器上使用Docker。与Python一起，我们将运行Nginx和Redis容器。这些例子假设你熟悉这些技术的基本概念。会有很多shell示例，所以请继续，打开终端，用Python教程探索我们的Docker。 Building Microservice Systems with Docker and Kubernetes[TOC] 下载, 安装和试用Docker: 安装: 下载地址和安装教程 试用: 1. 查看版本, 查看帮助:  docker --help 可以查看主命令的用法 docker COMMAND --help 可以查看子命令用法123456docker --versiondocker-compose --versiondocker-machine --versiondocker --helpdocker run --help Docker 的子命令如下123456789101112131415161718192021222324252627282930313233343536373839404142Commands: attach   Attach local standard input, output, and error streams to a running container build    Build an image from a Dockerfile commit   Create a new image from a container's changes cp     Copy files/folders between a container and the local filesystem create   Create a new container deploy   Deploy a new stack or update an existing stack diff    Inspect changes to files or directories on a container's filesystem events   Get real time events from the server exec    Run a command in a running container export   Export a container's filesystem as a tar archive history   Show the history of an image images   List images import   Import the contents from a tarball to create a filesystem image info    Display system-wide information inspect   Return low-level information on Docker objects kill    Kill one or more running containers load    Load an image from a tar archive or STDIN login    Log in to a Docker registry logout   Log out from a Docker registry logs    Fetch the logs of a container pause    Pause all processes within one or more containers port    List port mappings or a specific mapping for the container ps     List containers pull    Pull an image or a repository from a registry push    Push an image or a repository to a registry rename   Rename a container restart   Restart one or more containers rm     Remove one or more containers rmi     Remove one or more images run     Run a command in a new container save    Save one or more images to a tar archive (streamed to STDOUT by default) search   Search the Docker Hub for images start    Start one or more stopped containers stats    Display a live stream of container(s) resource usage statistics stop    Stop one or more running containers tag     Create a tag TARGET_IMAGE that refers to SOURCE_IMAGE top     Display the running processes of a container unpause   Unpause all processes within one or more containers update   Update configuration of one or more containers version   Show the Docker version information wait    Block until one or more containers stop, then print their exit codes2. 登录docker 并运行hello world:  run 命令是创建一个新的容器, 并在容器中执行这条命令1234567891011121314151617181920212223242526272829docker logindocker run hello-world# 下面是输出Unable to find image 'hello-world:latest' locallylatest: Pulling from library/hello-worldd1725b59e92d: Pull complete Digest: sha256:0add3ace90ecb4adbf7777e9aacf18357296e799f81cabc9fde470971e499788Status: Downloaded newer image for hello-world:latestHello from Docker!This message shows that your installation appears to be working correctly. To generate this message, Docker took the following steps: 1. The Docker client contacted the Docker daemon. 2. The Docker daemon pulled the  hello-world  image from the Docker Hub.   (amd64) 3. The Docker daemon created a new container from that image which runs the  executable that produces the output you are currently reading. 4. The Docker daemon streamed that output to the Docker client, which sent it  to your terminal. To try something more ambitious, you can run an Ubuntu container with: $ docker run -it ubuntu bashShare images, automate workflows, and more with a free Docker ID: https://hub. docker. com/For more examples and ideas, visit: https://docs. docker. com/get-started/3. 安装一个nginx web server 并运行起来::  如果本地没有这个image, 就会从docker hub 上去拉取.  安装完成后就可以访问网页 http://localhost/ 了     -d Run container in background and print container ID   -p Publish a container’s port(s) to the host   –name string          Assign a name to the container   1234567891011docker run -d -p 80:80 --name webserver nginx#docker run --detach --publish=80:80 --name=webserver nginx# 下面是输出Unable to find image 'nginx:latest' locallylatest: Pulling from library/nginx802b00ed6f79: Pull complete e9d0e0ea682b: Pull complete d8b7092b9221: Pull complete Digest: sha256:24a0c4b4a4c0eb97a1aabb8e29f18e917d05abfe1b7a7c07857230879ce7d3d3Status: Downloaded newer image for nginx:lateste32b6d0010d34fa950f79fbca09c3939c0d3d38e24a794dcbc392fc444191ef84. 查看刚刚正在运行的容器:  docker ps 或者 docker container ls12345docker ps# 下面是输出CONTAINER ID    IMAGE        COMMAND         CREATED       STATUS       PORTS        NAMESe32b6d0010d3    nginx         nginx -g 'daemon of…   6 minutes ago    Up 6 minutes    0. 0. 0. 0:80-&gt;80/tcp  webserver5. 停止, 删除容器, 删除镜像:  用 -a, –all 参数可以查看已经停止运行的容器123456docker container ls 				# 查看运行中的容器docker container ls -a 			# 查看所用容器, 包括已经停止的docker container stop webserver 	# 按容器name停止一个运行中的容器docker container rm webserver		# 按容器name删除一个容器docker image ls						# 查看本地的image 镜像docker image rm nginx				# 按照REPOSITORY删除一个本地image重启/启动一个停止的容器:  start 命令是启动一个或者多个停止了的容器. 12docker start -i webserver  # -i 让终端显示访问请求日志Docker &amp; Docker hub: Docker 定义:  A daemon process Controls other running processes Each process thinks it’s aloneImage 定义:  Like a drive image for a virtual machine Includes everything needed to run a system Runs the same way everythereDockerfile 定义:  Script that builds imageFROM python:3 				# From line, build docker image upon another docker image. This image will based on python image with tag 3. ENV PYTHONUNBUFFERED 1		# Env line, 环境变量参数设置WORKDIR /app				# Workdir line, 指定当前的工作目录和接下来运行起来程序所在的目录, 包含image 运行时. EXPOSE 8000					# Expose line, 告诉docker, 内部的程序监听外部网络指定端口的信息COPY . /requirements. txt /app/requirements. txt	# Copy line, 拷贝文件或者目录到磁盘上, 再拷贝到image上. RUN pip3 install -r requirements. txt	# Run line, 在build容器的时候, 在image内部执行一个程序. 相当于预安装环境依赖包ENTRYPOINT [ gunicorn ,  --bind ,  0. 0. 0. 0:8000 ,  api. server:app ]COPY . /app 				# Entrypoint line, 告诉我们当我们启动容器的时候, 需要执行什么程序. 一旦这个程序停止, docker 也就停止了. 也许会有其他的程序在docker 中运行或终止, 但是docker 只监控这个程序. Images 的名字和tags  benstraub/api:latest 是一个 image 的描述, 其中benstraub/api 是名字, latest是tagRegistry 的定义:  Network storage for Docker images (类似于代码的github) Can push and pull built images     Build in one place, run somewhere else    Default is Docker Hub(https://hub. docker. com)     Library of common images   Public and private storage   Container 的定义:  Like a running virtual machine Actually just a process     Docker controls runtime environment   Process thinks it’s alone   Runtime 的 Docker  Docker 有一个Process 正在监听80端口 复制一个Docker copy, 它也在监听80端口, 这在非Docker 的网络是有问题的 Docker File 的 Expose line 可以设定网络的mapping 关系, 让内部80 maping 到外部一个自定义的端口 这样两个Docker 都有了对外的端口, 两个 Docker之间就可以通话了. Docker-Compose, Linking and Ambassadors: Docker-Compose 定义:    Docker make multi-container systems possible      But cumbersome   Only interface is command-line      Docker-Compose      Specify a single- or multi-container system declaratively.       Docker-compose 就是一个文件, 这个文件描述了一台主机上的多个容器的配置和关系以及容器和主机的通信方式     Docker-compose 不太适合在生产上使用, 因为它的管理范围只是一台主机, 没有负载均衡. Docker-compose 更适合在开发环境中使用, 可以在笔记本上运行多个容器. 可以进行 文件系统的mapping 和 aoto-reload.   这是一个Docker-compose 的例子:      有两个container 在这里被定义   第一个是 Postgres 数据库的镜像, 命名为pg   第二个是nginx的 web服务镜像, 命名为frontend, 并且定义了links 是连接到pg的   每个镜像的名字都可以被dns 解析而找到   1. 下面是一个docker-compose. yml 文件的内容: 12345678pg: image: postgres:9. 4frontend: image: nginx links:  - pg2. docker-compose up 命令 安装容器:  cd 到这个文件所在目录, 然后执行 docker-compose up 命令12345678910docker-compose up# 这个命令会根据当前目录中的 docker-compose. yml 文件的配置, 获取进行并安装容器, 最后启动容器. # 打开另一个终端窗口查看运行起来的容器docker container ls# 下面是输出CONTAINER ID    IMAGE        COMMAND         CREATED       STATUS       PORTS        NAMES81157e7d6fce    nginx         nginx -g 'daemon of…   10 minutes ago   Up 10 minutes    80/tcp       docker_frontend_177345b0a881a    postgres       docker-entrypoint. s…   10 minutes ago   Up 10 minutes    5432/tcp      docker_pg_13. 与容器进行交互:  因为已经装了两个容器(nginx 和 postgresql), 我们需要先进入第一个容器nginx的bash命令行, 然后 ping 一下 数据库.  -it 意思是让当前的命令行终端 attache 到容器运行的程序中. 相当于ssh dalong@192. 168. 10. 1 到另一个服务器上. 12345678910111213141516# 查看运行中的容器, 找到nginx 的容器名字docker_frontend_1docker container ls# 进入容器 docker_frontend_1的bash上docker exec -it docker_frontend_1 bash # 更新容器里的安装库, 并安装 ping 命令 和 ifconfig 命令apt-get updateapt install iputils-pingapt install net-tools# ping 一下 数据库pg# pg 是 docker-compose. yml中定义的, 所以dns 可以直接找到. # 也可以直接ping 容器的名字 docker_pg_1ping pg#ping docker_pg_14. 另一个更复杂的docker-compose2. yml 文件    包含四个containers 的一个 docker-compose文件, 更像是一个生产环境的真实案例     第一个是postgres 数据库     最后一个是Nginx web 服务器     另一个是 apiambassador, 它是作为另一个容器的proxy(进行类似if else 判断). 如果另一个container 正常可用, abmassador 就将信息转发给那个 container. 如果另一个container 不可用, 那么abmassador 就会拒绝连接.     中间是api 服务器, 是python 提供的服务          restart: 定义了重启条件, 比如 on-failure 就是说如果程序返回的内容不是0, 那么就重启\           build: 代替image 字段, 给docker 一个目录去寻找dockerfile, 用dockerfile 生成需要的image.           volumes: volumes的作用有点类似于VMware里面的共享目录，用于将物理主机里的目录映射到docker虚拟机里。           command: 用于告诉docker, 当源代码变更了, 就需要重新加载程序. 这个不建议在生产环境使用. 建议用在开发环境           ports: map 容器中的端口和主机(容器运行所在的机器)的端口 , 当主机接收到80端口进来的网络请求, 就会转给frontend 这个容器的80端口, 然后frontend 通过 docker dns 映射的容器名字和其他容器进行通信.            1234567891011121314151617181920212223242526272829pg: image: postgresapi: restart: always build: . /api volumes:  - . /api:/app command:  --reload  links:  - pg ports:  -  8000:8000 apiambassador: image: cpuguy83/docker-grand-ambassador volumes:  - /var/run/docker. sock:/var/run/docker. sock command: -name=chapter2_api_1frontend: image: nginx volumes:  - . /frontend/nginx. conf:/etc/nginx/nginx. conf links:  - apiambassador:api ports:  -  80:80 Kubernetes: kubernetes 定义:  A system that run docker containers for you Labeling system for control Spans many physical machines (Runs Google Container Engine) 谷歌在用Kubernetes 的架构图  有多个机器组成, 包括master 和 minion master 作为 kubernetes 的API, 就像是老板, 可以向minion发号施令 minion 是运行dockers的机器, 和master进行沟通, 做master 让做的事情 PODS 的概念:  是kubernetes 的基础 Atomic unit of runtime in kubernetes Set of running Docker containers     Often only one container    Kubernetes 让每个PODS 有一个独立的IP ,所以 Containers “think” they’re running on the same machine Always on the same host. Replication Controller(RC) 概念  Manage pods lifetime 给定一个目标数字, tries to maintain a number of matching pods. 如果不够就启动一些新的, 如果超过了, 就停掉一些.  每个Pod 都有很多label. Matches using labeling system下图介绍RC 和 Pods的关系  RC 通过配置count 的数值来决定维护几个pods. count 的增减决定了自动增加pods还是减少pods RC 通过配置自己的label name 和 label version 名字来控制 pods的labels 这个RC”A” 有两个 Pods, 每个pods 都有一模一样的name 和 version label RC”B” 有一个 Pod, 版本和前两个不一样 Services 的概念    负责pods之间的通信     类似于DNS, 可以通过名字找到其他的pods     我们的容器通过Services从互联网上获得数据     我们的容器通过Services找到另一个容器            类似于Load Balancer, 通过 labels 找到pods ,并把流量带过来.          Set of pods comes from label matching           下图是Master, Minions, RC, Service, Pods 之间的关系              Master 管理多个Minions     一个Minions 有多个RC     每个RC有多个pods     Service 可以定义一群跨RC的pods, 让他们之间相互通信.           AWS: 这里讨论Docker 和 Kubernetes会用到的 AWS服务 EC2, ASG, VPC, ELB, Route53, RDS, IAM, ECS EC2 - Elastic compute cloud  Rent computers by the hour Actually virtual machines Kubernetes     One instance for Master   Several instance as Minions   ASG - Auto scaling group  Start or stop EC2 instance en masse Includes machine configuration Maintains a desired count of instances     Like a Kubernetes RC    Can adjust count at any timeVPC - Virtual private cloud  AWS intranet Can span datacenters and availability IP address range FirewallELB - Elastic Load Balancer  Distribute TCP or HTTP(s) traffic to EC2 instances External port Internal port SSL certificate for HTTPS terminationRoute53  Amazon-hosted NDS Can alias to ELB names. 可以给域名起一个别名到ELB的名字, 将流量直接引到负载均衡. RDS - Relational database service  Amazon-managed database instance Replication Backups UpgradesIAM - Identity/access management  Lightweight user system Fine-grainted access controlECS - EC2 Container service  System for running Docker containers No facilities for:     Deployment and updates   Service discovery and DNS   Kubernetes Cluster: 在 AWS 上搭建 Kubernetes 集群 安装教程 如下步骤需要执行    AWS 上创建账号     AWS上找到IAM服务, 并创建用户, 授予全部的API访问权限.      add一个新用户 dalong,   并且为这个用户add permissions, Attach existing policies directly, AdministratorAccess的权限      安装AWS命令行工具 网址 并进行配置   1234pip install awscliaws configure # 需要到aws console 中找到这个用户, 创建一个新的access key 和 security key 填到交互命令行中. # 安装成功后运行下面命令aws iam list-users # 查看iam 服务中已经授权过的用户      设置AWS Kubernetes cluster 生产环境. 注意, 这里仅适用于1. 5及之前的版本          github地址 下载kubernetes zip包, 并解压,           cd 到 kubernetes目录下, 开始创建 kubernetes cluster 集群          这里需要翻墙, 因为需要从storage. googleapis. com 上下载一个包     安装时间比较长, 会配置一系列环境        1KUBERNETES_PROVIDER=aws cluster/kube-up. sh              kube-up. sh 干了什么的? 它创建了如下内容          New VPC:127. 20. 0. *     创建了 5 EC2 instances (master + 4 minions), with public IPs     ASG for minions     SSH keys for direct access             ~/. ssh/kube_aws_rsa                kubectl is configured             ~/. kube/config                        Kops 来设置AWS 的 kubernets 生产环境 文档     安装 kops , kubernetes-cli   12345678910111213141516171819202122brew update &amp;&amp; brew install kopsbrew install kubernetes-cliexport AWS_DEFAULT_PROFILE=default # 这里的default 是 ~/. aws/credentials 中的访问用户名aws iam create-group --group-name kops # 创建一个组,名字是 kopsaws iam create-user --user-name kops  # 再创建一个用户,名字kopsaws iam add-user-to-group --user-name kops --group-name kops # 将用户加入组中aws iam create-access-key --user-name kops # 为新的用户创建 keys   # configure the aws client to use your new IAM useraws configure      # Use your new access and secret key hereaws iam list-users   # you should see a list of all your IAM users here, 这里如果说没有权限, 就到aws console中的 iam -&gt; user -&gt; kops -&gt; add permissions -&gt; attach existing policies directly 中加上 AdministratorAccess 权限.    # 生产环境的话, 就加上如下权限#AmazonEC2FullAccess#AmazonRoute53FullAccess#AmazonS3FullAccess#IAMFullAccess#AmazonVPCFullAccess   # Because  aws configure  doesn't export these vars for kops to use, we export them nowexport AWS_ACCESS_KEY_ID=$(aws configure get aws_access_key_id)export AWS_SECRET_ACCESS_KEY=$(aws configure get aws_secret_access_key)      创建一个 AWS S3 bucket   1234aws s3 mb s3://dlongcluster. dev. example. comexport KOPS_STATE_STORE=s3://dlongcluster. dev. example. comexport NAME=dlong. k8s. localkops create cluster --zones=us-east-1c ${NAME} # 这里要翻墙   "
    }, {
    "id": 23,
    "url": "http://localhost:4000/Design-Microservices-with-Python/",
    "title": "Design Microservices with Python",
    "body": "2021/03/14 - 微服务架构是一种软件设计方法，将大型应用程序分解为独立服务，每项服务都旨在满足特定的业务需求。这些服务在专用资源上运行，包括单独的数据库实例和计算能力。与整体系统不同，微服务应用程序是松散耦合的，允许更大的灵活性。本文我将带领大家深入浅出的了解微服务框架以及通过 Python 来实现一些案例. [TOC] 微服务介绍有人把微服务比作一把双刃剑, 一方面它把单个问题域的复杂度降低了, 服务可以独立更新,快速交付; 但另一方面, 面对一个有不同技术栈支撑的整体系统, 运维和交付的难度增大了. 概念::  可以协同工作的, 小而自助的服务. 是我们能够更快的响应不可避免的变化 服务很小, 专注于做好一件事 自治性, 可以独立部署在PaaS上, 也可以作为一个操作系统存在.      尽量避免把多个服务部署在同一个机器上.    服务之间通过网络调用进行通信.    服务之间独立修改, 避免耦合.    服务只暴露出API.    优缺点:: 好处::  技术异构性: 比如, 一个应用的不同部分用不同的技术和存储实现的最佳实践方式. 如下图, 帖子用ruby代码写, 用文档数据库, 社交关系用go编写代码,用图数据库存储数据. 图片用java编写用blob数据库保存数据. 可以快速使用新技术实现功能, 并降低了使用新的技术的风险.  提高系统弹性 扩展性: 庞大的服务只能作为一个整体扩展. 即使只有一小部分的功能有性能瓶颈也需要整体扩展. 微服务只需要对特定有瓶颈的服务进行扩展. 如下图, 图片是瓶颈, 那么就对图片的实例进行扩展.  简化部署: 发布时只对需要升级的服务进行部署升级, 不需要对整个系统进行重新部署 可组合性, 可代替性: 对于已有系统, 可以轻松进行重新组合创建出新的系统. 老旧的系统可以轻松换掉某些不需要的服务. 缺点::  缺乏经验导致差劲的切分; 尽量避免过早的对产品进行微服务切分 更多的网络交互. 因为切分后, 一个进程变成了多个进程, 本来的内部请求都变成了外部网络调用请求. 而且还要考虑到网络延迟, 网络请求失败, 同步请求还是异步, 请求响应时间, 网络错误码的设计.  数据的存储和分享. 很难保持数据的重复和微服务的数据隔离(独立); 进行多表联合查询也是一件挺费事的事. 微服务之间的数据分享变得困难 版本兼容问题. 如果一个新的需求产生了向后兼容问题, 并且需要修改多个微服务, 且服务之间还需要进行数据交换, 那么就会产生问题. 可以通过版本控制解决问题.  测试和部署. 切分微服务后, 整个产品端到端的测试就会演变成对每个微服务的端到端测试, 增加了许多工作量. 架构师的战略目标,原则和实践:    战略目标关心的是公司走向以及如何才能让客户满意, 比如开拓亚洲市场, 或者让客户尽量使用自助服务     原则遵从战略目标, 最好不要超过十个     通过实践来保证目标得到实施, 实践更偏重技术层面, 比如 HTTP/REST 接口标准. 下面是一个例子  如何建模微服务: 传统的架构 微服务架构 好的服务, 松耦合, 高内聚:    如果做到了松耦合, 修改一个服务就不需要修改另一个服务;   一个松耦合的服务应该尽量少的知道与之协作的其他服务的信息. 就是说限制两个服务之间不同调用形式的数量, 因为过度通信会导致紧耦合 把相关的行为聚合一起, 无关的行为放在别处. 因为当需要改变一个行为的时候, 最好在一个地方就可以完成修改, 然后尽快的发布. 如果在不同的地方进行修改, 那么就可能会有多个微服务发布. 在多个地方进行修改会很慢, 并且会提高发布的风险. 限界上下文:  就是对提供的微服务限制使用边界和使用场景, 哪些需要对外提供服务, 哪些不需要; 边界和场景就是上下文 每个上下文都有明确的接口, 该接口决定了他会暴露哪些模型给其他的上下文.  比如财务部门和仓库是两个独立的限界上下文. 他们都有明确的对外接口. 财务部门不需要知道仓库的内部细节, 只需要知道一些必要的信息. 反之亦然. 如下图. 财务部门和仓库之间的共享模型. 两个上下文的衔接部分就是库存项.  同一个名字在不同的上下文中也有不同的含义. 比如退货, 在客户的上下文中, 退货意味着打印运送标签, 寄送包裹, 等待退款. 在仓库的上下文中, 表示即将到来一个包括, 并且等待重新入库.  当规划上下文的时候, 不应该从上下文提供的共享数据考虑, 而是从上下文提供的服务来考虑, 比如仓库上下文要考虑提供的服务是共享库存清单, 财务的上下文是共享月末账目或者为新员工创建工资单.  逐步划分上下文, 就是说不需要一次性的把微服务分的太细, 可以先大块的进行切分上下文, 然后等业务需要在进行更细粒度的划分. 比如可以先将仓库划分为一个上下文. 如果之后有必要, 再将库存管理划分为一个单独的上下文.  按地理位置和组织结构划分上下文. 有的时候不同团队负责不同的模块开发, 那么尽量将不同团队做的模块划分成不同的上下文. 如果两个地区的两个团队再做同一个模块的开发, 也要考虑是否将他们的工作内容分开到不同的上下文中, 这样有利于对基于上下问的微服务进行快速响应和部署发布. 集成: 集成是微服务相关技术中最重要的一个 理想的集成技术:  向后兼容, 避免破坏性修改. 如果一个微服务在响应中增加了一个字段, 已有的消费方式不应该受到影响 保证API的技术无关性. 就是说不管是用. net, java, php 还是 python, 都应该保证微服务的通信方式. 技术无关性. 意味着不应该选择对微服务的具体实现技术有限制的集成方式.  易于消费方使用. 如果消费方使用我们的微服务比登天还难, 那么再好的微服务也没有任何意义. 消费方应该可以使用任何技术都可以获得微服务提供的服务. 可以考虑使用客户端(SDK), 但是容易造成紧耦合(也就是说修改一个微服务, 同时也要修改SDK 才能给消费端使用).  隐藏内部实现细节. 我们不希望消费方和微服务的内部细节绑定起来, 否则会增加耦合度. 增加不影响消费端的修改难度. 为了可以更容易的进行向后兼容, 就应该尽量少的暴露细节. 为微服务创建接口: 举个例子, 创建一个 新建用户 的接口 新建用户不仅仅是CRUD, 还需要调用其他的微服务(业务流程), 比如进行付账设置, 发送欢迎邮件等. 修改或者删除用户也会出发其他相关的业务流程. 共享数据库: 不要让消费方直接访问数据库. 因为数据库本身可能会被替换, 表结构也可能经常变化. 因此就造成了高耦合. 微服务的同步与异步:    同步通信是指调用方发起远程服务调用后,需要阻塞自己直到整个操作完成.     异步通信, 调用方不需要等待操作完成就可以返回. 不需要关心操作是否可以完成.   同步方式比较简单, 很容易知道调用的成功和失败, 但是增加了耦合度. 异步降低了耦合度, 但是却无法知道成功失败.  建议使用异步+回调的方式进行. REST:  REST 是 RPC 的一种代替方式, 是一种数据通信的风格和标准 通过 HTTP的 POST, GET, PUT, DELETE 等动词请求, 通过JSON格式进行数据的发送和响应.  需要为每一个API请求提供足够丰富的Error Code/Message 来帮助消费方了解发生错误时具体遇到了什么问题.  建议所有对微服务的请求需要证书来进行认证JSON , XML 还是其他:    相比于XML, JSON 的内容更加紧凑.     XML 的优势是可以进行超媒体控制  版本管理:  使用语义化的版本管理, 既给每个版本一个可以看懂,并有一些含义的版本名称 比如 Major. Minor. Patch. 这样一个版本名字     Major 意味着包含了向后不兼容的修改.    Minor 意味着有新功能增加, 但是向后兼容.    Patch 的改变意味着对已有功能的缺陷修改   比如: 1. 2. 0 到1. 3. 0 就不需要消费方进行任何修改, 而1. 3. 0 到2. 0. 0 就需要消费方进行代码的升级更新了.     同一个接口的不同版本: 为了降低消费方修改代码的几率, 我们可以将不同版本共存, 比如同时提供 1. 3. 0 和2. 0. 0 的服务, 保存新老两个接口. 好处是我们可以尽快发布新版本, 不需要等待客户方的修改, 同时也给了客户足够的时间进行版本的迁移. 一旦观察到老的版本不再需要(没有任何访问了), 就可以删除掉老的接口和代码. 但是绝对要避免维护太多的版本, 建议不要超过2个. 可以将不同版本号加到URI 中, 比如/v1/storePatientData, /v2/storePatientData为前端提供服务的后端:  前端web/app 界面可以在一个界面中请求一个或者多个API 有时, 为了提高后端的响应效率, 后端可以将信息集成后发送给前端. 就是说前端之访问一个 gateway 接口, gateway 接口负责调用其他API 获得数据后返回给前端.  也可以为每一种前端单独定制自己的独立API入口分解单块系统: 如何将一个已有的系统进行分解, 变成可用的微服务 分解上下文: 识别上下文: 以在线音乐服务 MusicCorp 为例, 首先识别有哪些上下文, 分解如下  产品目录: 正在销售的音乐CD产品相关的元数据 财务: 账户, 支付, 退款等信息 仓库: 分发客户的订单, 处理退货, 库存等 推荐: 推荐算法和专利. 分解的原则::    按上下文分解     按照团队分解: 不同地区的团队应该具有独立自主的对某些上下文全权负责.     按技术分解: 对于某些专门的技术, 比如推荐系统, 或某种开发语言比如Java, 将这种内容才分成独立的     安全: 将有特殊安全要求的服务单独分离出来作为一个微服务  打破外键:  财务报表直接使用总账表, 总账表中只有产品ID, 没有产品名称, 名称在产品表中.  原来的代码是财务报表是通过外键直接访问产品表获取产品名称,  经过修改, 让财务服务直接访问产品服务来获取产品名称  缺点是增加了一次数据库访问, 但是这个是可以进行优化的.  如果原来数据库中有事物的需要, 那么就需要将数据库的事物改到代码中. 共享静态数据:  比如多个微服务都需要使用一个静态的国家列表数据, 解决方法是将这些数据拷贝成多份, 每个微服务读取期中一份. 重构数据库:  在拆分上下文的时候, 也要考虑重构表结构(会有一些必要的拆分动作). 拆分表结构之后会造成数据库的访问次数增加.  表结构拆分成功后, 如果每个微服务都可以顺利跑桶, 就可以考虑进行数据库拆分了.  最后将整个应用程序拆分成若干个微服务. 事物边界:  数据库中实现事物很简单, 但是一旦拆分到不同的服务中, 想要保持事物就比较困难了, 比如微服务1提交成功后, 需要微服务2继续提交,但是失败了. 那么就需要两种方法      继续提交, 直到成功   撤销微服务1之前的提交.    生成报告:  未被拆分的数据库可以很容易生成跨表进行联查的报告, 但是一旦将原有数据库拆分, 进行查询就困难了 解决方法有两种:      每个微服务就提供获取有限报告的接口   每个微服务将数据推送到独立的报告系统中, 然后再在报告系统中生成视图, 组装报告    部署: 持续集成 CI: 测试别人是否真正理解CI的三个问题  你是否每天签入代码到主线? 要尽可能频繁的把代码提交到主干或分支中 你是否有一组测试来验证修改? 没有对代码进行检验的CI不是真正的CI 当构建失败, 团队是否把修改CI单过第一优先级的事情来做? 构建失败后需要立即修复, 否则拖的时间越长, 新代码越多, 也越难修复. 把持续集成映射到微服务:    简单的做法: 只有一个构建, 任何修改都会出发构建. 每个构架你会产生三个微服务. 缺点是每次都是所有微服务同时发布. 好处是简单. 建议项目初期使用.     中间做法:将一个代码库分成多个子目录, 应扫到不同的构建中     最好的方法: 每个微服务有自己的源代码库, 和CI 构建  构建流水线和持续交付:    将构建分成多个阶段是有必要的 , 下面是标准的构建流水线的发布流程   微服务的世界中, 每个微服务都有一套像上面一样自己的构建流程.  唯一例外是在项目刚开始阶段, 很难划清上下文边界和构建边界, 那个阶段必须把所有服务打包发布. 当API稳定之后, 就可以把他们移到自己的构建当中了.  随着微服务的增多, 部署的整个流程需要自动化完成特定构建物: 不同语言会有不同的构建物  Java - JAR Ruby - gem Python - egg Node. js - NPM可以使用 puppet 或者 chef , Ansible 这样的自动化配置管理工具. 通过虚拟机来安装软件和部署环境, 可以加快部署速度 微服务和主机之间的映射: 一个微服务到底需要几个主机(物理或者虚拟或者容器) , 下面是几种选择    单主机多服务: 好处是成本低, 易管理. 缺点是难以监控(比如CUP高了, 不知道是哪个服务造成的), 也会造成部署复杂. 因为有可能一个服务的部署会依赖另一个服务. 或者两个服务之间的部署环境冲突.     单主机多容器,多服务:一台主机部署多个容器, 每个同期部署一个微服务.      单主机单服务: 但是成本会更高.     使用PaaS 云服务的单主机但服务: 基于3#, 可以降低成本.  虚拟化,容器化: 虚拟化可以将单台物理机更有效的利用起来 不同的虚拟技术有  AWS, WMWare, VSphere, Xen, KVM. Docker CentOS 有一个专门为 Docker 设计的操作系统 Docker 是一个简单的Paas Kubernets 和 CoreOS , Deis 这些工具可以帮助我们扩平台管理容器. 用Python 开发 微服务微服务用到的Python扩展: 网络请求  twisted 异步处理框架  Tornado 异步处理框架  Greenlet 高并发的线程框架  Gevent 在Greenlet 基础上做的基于协程的高并发看框架  Asyncio 官方推荐的异步网络操作, 并发, 协程 Python 编译器  Pypy 比Cpython 更快的编译器, 可以用于微服务. 日志    Signal : Flask 整合了 Blinker (https://pythonhosted. org/blinker/), 支持信号. 当一个事件被触发, 就会发出一个信号, 并调用执行我们自定义的方法. 比如template_rendered 就是当前程序在获取到模板渲染完毕后就会触发这个信号. 可以用于发送日志和统计数据. 我们可以自定义信号, 比如下载完一个PDF的时候, 可以定义一个 pdf_ready. 缺点是产生的信号是同步处理的, 如果不是很快完成信号触发的任务, 就会造成程序阻塞.   RabbitMQ 可以解决 Blinker 的同步问题.  Flask 配置参数管理       Konfig 用Flask 默认取配置信息的方法是将素有配置文件放在一个单独的python 文件中(比如prod_settings. py), 然后在再在主程序中调用app. config. from_object('prod_settings. Config') , 这种方法有两个缺点, 1 配置文件可能会出现负责的python代码逻辑, 2 需要将这个代码放在代码库中, 但是这是不合适的.  Konfig 支持读取. ini文件, 并可以读取Yaml格式的文件.    123456789101112$ more settings. ini  [flask]  DEBUG = 0  SQLURI = postgres://tarek:xxx@localhost/db  $ python  &gt;&gt;&gt; from konfig import Config  &gt;&gt;&gt; from flask import Flask  &gt;&gt;&gt; c = Config('settings. ini')  &gt;&gt;&gt; app = Flask(__name__)  &gt;&gt;&gt; app. config. update(c. get_map('flask'))  &gt;&gt;&gt; app. config['SQLURI']  'postgres://tarek:xxx@localhost/db      微服务Flask代码框架       setup. py: Distutils’ setup file, which is used to install and release the project     Makefile: A Makefile that contains a few useful targets to make, build, and run the project  settings. ini: The application default settings in the INI file  requirements. txt: The project dependencies following the pip format  myservices/: The actual package       __init__. py    app. py: The app module, which contains the app itself    views/: A directory containing the views organized in blueprints init. py    home. py: The home blueprint, which serves the root endpoint    tests: The directory containing all the tests           __init__. py      test_home. py: Tests for the home blueprint views               测试  requests-mock 用于单元测试的模拟请求  Boom 是一个基于AB的压力测试框架  12pip install boom boom http://127. 0. 0. 1:5000/api -c 100 -d 10 -q  flask-profiler 是一个带报表的压力测试框架 网络请求用到的相关技术: 异步网络框架 Twisted and Tornado: 简单介绍 多任务协程请求: Greenlet and Gevent: Greenlet 和 Gevent 都是python 的协程类库, 帮助Python 实现多携程, 不用等待一个请求的结束就可以执行另一个请求. 简单介绍 高并发模块 asyncio (python3): 简单介绍 asyncio 是干什么的？  异步网络操作 并发 协程Flask 的使用: 使用python3 或者 python2都可以, 因为微服务可以让每个服务有自己的版本, 但Flask官方还是建议开始使用python3 建议 因为 2020年之后Python官方也不再支持python2反方消息, 而且很多新的技术和框架都是基于3的. Flask处理网络请求: 最简单的网络请求响应 123456789101112from flask import Flask, jsonifyapp = Flask(__name__)@app. route('/api')def my_microservice():  return jsonify({'Hello': 'World!'})if __name__ == '__main__':  app. run()用curl 来测试请求 12curl -v http://127. 0. 0. 1:5000/api服务之间的调用: 同步请求:    需要创建一个 request, 并用session保持连接     需要通过异常捕获处理超时的问题     可以使用连接池     可以http cache 头 缓存内容, 改变数据时更新 Etag     request请求时用gzip压缩内容     将内容变成二进制进行传输, 用 Protocol Buffer 和 MessagePack 库  异步请求:  如果用同一个进程的多线程/协程处理多请求, 那么主进程挂了或者卡出了就会出问题 更好的方法是发一个消息出去 Celery, Redis 都可以作为消息代理 Task Queue是一个服务向一个queue中丢任务,另一端的workers从queue中取任务, Celery 实现的是Task Queue Topic Queue是Task Queue的一个变种, queue两端是根据topic来发送和接收消息的. 是通过订阅的方式来进行的. Rabbit MQ 就是这种实现, 使用的是 AMQP 协议.  Rabit MQ + Pika 组合可以在python中实现topic queue.  Pick-pool 是另一个类库, 可以建立一个消息池, 这样就不用每次都打开和关闭rabitMQ了. 但是需要一个独立的flask 执行一个pick-client 进程来接收信息. 发布和订阅:  和queue相比，消息订阅和发布可以让一个消息传送到多个消费者手中， 相当于把一个message 广播给多个微服务。测试请求 – 模拟调用:  requests-mock 是一个模拟调用的类库 可以通过 unittest 单元测试结合 requests-mock 进行模拟网络调用。 既可以模拟同步调用， 也可以模拟异步调用 测试微服务的请求原则是隔绝测试， 既只测试自己发送和接收消息的能力， 不要依赖其他微服务。多个微服务的整体的测试需要日志系统协助来trouble shooting。监控微服务:    好处      当多个微服务相互调用时出现了问题， 很难追踪到底哪里出了问题。   解决方法时将log 中心化， 放在一个地方统一的进行观察和分析。   同时需要监控每个微服务的内存用量，CPU用量， 每分钟请求数等。   从成本考虑， 也可以根据监控的出的结论来的计算结果推导出来目前的平台网络成本和建议   对于新上线的修改和变化，可以通过监控来了解是否一切正常。   日志管理:  Python 的 Logging 类库可以用来进行 log 的收集 Sentry 是一个进行错误日志集中收集的Python 类库， 他还有一个UI界面可以查看日志， 并能够设计处理错误信息的工作流程。 GrayLog 可以收集的不仅仅是错误日志，还可以收集普通log信息, 有一个强大的搜索引擎， 基于Elasticsearch 。 日志数据存储在MongoDB中。     设置Graylog比较麻烦， 简单的方法是使用它的docker image http://docs. graylog. org/en/latest/pages/installation/docker. html   Graylog也有后台界面可以访问。   Graypy 是 python 的一个访问 GrayLog的类库， 可以通过UDP 或者 AMQP协议，upd可能会丢失log, 如果要求严格， 可以用基于AMQP协议的RabitMQ 传输log   监控管理: 监控分为    系统级别监控， 磁盘， 内存，cpu等     代码级别监控     web server 级别监控  系统级别监控:  psutil 是一个python 类库， 相似与 linux 的ps命令。结合 graypy 就可以将信息通过日志发送到日志中心。可以运行一个独立的python专门获取cpu信息并发送日志 system-metrics 更强大一些， 可以获得更多的系统信息， 并且可以和Graylog结合， 显示在GrayLog的UI上， 并发送接收报警。代码级别监控:    代码级别可以查看jinjia2 的读取速度和数据库的响应速度等。   一种方法是用benchmark 的方式在读取模板或者数据库的代码前后用代码包住， 计算时间。 但是这种方法有风险， 因为会污染代码 方法是写一个装饰器 @timeitweb server 级别监控: 这个级别的监控日志也要发到日志中心， 但是需要nginx直接发送，而不是通过 flask  Graylog 有一个nginx content pack 包， 可以将error logs, access log 发送到 graylog 可以获取的信息如下：     The average response time   The number of requests per minute   The remote address   The endpoint and verb of the request   The status code and size of the response   微服务安全: 认证 OAuth2 用于第三方访问我们微服务的认证 CCG用于微服务间的认证。 防火墙 为了防止DDoS攻击， 我们需要做一个basic firewall 来进行防御 代码安全 我们还要进行一些代码扫描， 确保代码没有安全问题。 OAuth2 认证: OAuth2 用于第三方访问我们微服务的认证 对于微服务我们要知道两点  we need to know who is calling theservice (authentication) and we need to make sure that the caller is allowed to perform thecall (authorization). OAuth2是一个灵活的解决方案来解决微服务的安全认证问题。  OAuth2 是一个中心化的认证系统，让访问者先用code 或者 token 进行身份验证。这个验证也叫 three-legged OAuth (三条腿认证) ， 图示如下(右上角可以暂时理解为微信的认证服务)：   当用户访问app时（1），会被重新跳转到微信认证服务，让app可以得到被允许访问微信的授权（2）。一旦授权通过，app就会通过一个http回调请求拿到授权， 并代表用户使用微信的API OAuth2 用于第三方访问我们微服务的认证 CCG - 基于Token的认证:  OAuth 2. 0 定义了四种授权方式：authorizationcode、implicit、resource owner password credentials、client credentials。 CCG(Client Credentials Grant) 是其中同一种https://tools. ietf. org/html/rfc6749#section-4. 4 CCG认证用于微服务之间的认证。 微服务之间的认证不需要牵扯用户， 所以只需要使用 Client Credentials Grant (CCG) 就可以了。 Token 就相当于密码， 它允许你访问一个指定的资源， 不管你是用户还是微服务。 Token可以是     用户名或者ID   scope, 请求者可以访问的资源的范围，或者权限   时间戳， 表示token 颁发的时间点   时间戳， 表示token过期时间    一个Token 可以被设置成访问多个微服务。JWT 标准:    Jason Web Token的缩写   OAuth2 为它的Token 使用JWT标准   JWT tokens用base64的加密方式   JWT Token 是用两个点分割的三个部分组成         Header: 提供token的信息， 比如加密方式           Payload：真实的数据           Signature： hash签字， 用来确认合法性           一个例子 : 其实token 是一行，下面例子为了容易看， 分成三行     123eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9. eyJ1c2VyIjoidGFyZWsifQ. OeMWz6ahNsf-TKg8LQNdNMnFHNtReb0x3NMs0eY64WA          PyJWT 可以轻松的创建JWT token 并读取JWT token. : pyJWT既可以创建对称加密解密的token, 也可以创建公钥私钥的tlken 基于 X. 509 证书的认证: SSL 就是基于X. 509的协议。 这是基于公钥和私钥的token方式 这要先通过 openssl 命令生成公钥私钥。 然后再利用PyJWT , 用私钥加密， 用公钥解密 TokenDealer 的微服务代码案例:  是系统中的唯一的认证微服务 使用CCG 流程 当前微服务会收到其他微服务获取token的请求, 并授予一个一天期限的token 使用私钥处理token, 然后发布公钥给其他服务验证token.  这个微服务会保存所有client ids 和 安全码 TokenDealer 暴露出来的三个接口     Get /. well-known/jwks. json 是Json Web Key, 用于其他微服务自己我验证token的   POST /oauth/token 会返回一个token.    POST /verify_token 返回token 的 payload, 是验证token. 如果验证失败, 返回400   Web防火墙: 了解一些基本的网络攻击形式  SQL 注入 Cross Site Scripting(XSS) 跨站攻击 Cross-Site REquest Forgery(XSRF/CSRF) 跨站请求伪造 DDOS 攻击OpenResty - Lua + Nginx 实现 WAF: OpenResty® 是一个基于 Nginx 与 Lua 的高性能 Web 平台，其内部集成了大量精良的 Lua 库、第三方模块以及大多数的依赖项。用于方便地搭建能够处理超高并发、扩展性极高的动态 Web 应用、Web 服务和动态网关 使用NGINX+Openresty实现WAF功能 英文：Web Application Firewall，简称： WAF 代码安全: 两个基本原则：  外部的任何请求都需要谨慎对待后再进入内部系统处理 内部系统做任何事情都需要在一个定义良好的范围内判断进来的数据:  防止SQL注入 防止模板注入限制应用的执行范围:  用JWT Token 可以限制微服务之间的读写权（POST, Get, Put, Delete） 可以在阿里云上设置防火墙的黑白名单 不要用root用户在Linux上执行操作。     web 服务需要用非root用户执行   尽量不要在代码中执行系统命令   使用Bandit linter 进行代码扫描: 打包， 运行微服务: 开发过程中我们需要将多个微服务安装到自己的本地环境中进行测试， 如何打包和部署测试是一个重要任务。 关于打包的一些名词的定义：    Python package: 是包含python modules的目录树， 是命名空间   Python project: 是包含多个package和资源的项目， 可以被单独release. 我们的每个微服务是一个独立的python project Python application: 是一个可以和用户交互的python project. UI 或者命令行都可以是和用户交互的方式。 Python library: 是一类特别的python project, 可以被其他的Python project 进行内部调用。 没有直接的用户交互接口。打包工具链: 打包用的配置文件: 打包项目之前需要有三个文件  setup. py: 可以驱动所有事情的模块 requirement. txt: 项目的依赖列表 MANIFEST. in: 一个模板文件列出了release 时需要被包含的文件列表。setup. py说明 pip 是在线安装一个包的命令， 而如果想将一个自己开发的包发布出去， 那么就需要有一个setup. py文件来管理你开发的这个项目需要依赖的内容了。 假如我在本机开发一个程序，需要用到python的redis、mysql模块以及自己编写的redis_run. py模块。我怎么实现在服务器上去发布该系统，如何实现依赖模块和自己编写的模块redis_run. py一起打包，实现一键安装呢？ 在这种应用场景下，pip工具似乎派不上了用场，只能使用python的构建工具setup. py了，使用此构建工具可以实现上述应用场景需求，只需在 setup. py 文件中写明依赖的库和版本，然后到目标机器上使用python setup. py install安装。 setup. py的一些参数说明 123456789101112131415161718192021222324252627282930--name 包名称--version (-V) 包版本--author 程序的作者--author_email 程序的作者的邮箱地址--maintainer 维护者--maintainer_email 维护者的邮箱地址--url 程序的官网地址--license 程序的授权信息--description 程序的简单描述--long_description 程序的详细描述--platforms 程序适用的软件平台列表--classifiers 程序的所属分类列表--keywords 程序的关键字列表--packages 需要处理的包目录（包含__init__. py的文件夹） --py_modules 需要打包的python文件列表--download_url 程序的下载地址--cmdclass --data_files 打包时需要打包的数据文件，如图片，配置文件等--scripts 安装时需要执行的脚步列表--package_dir 告诉setuptools哪些目录下的文件被映射到哪个源码包。一个例子：package_dir = {'': 'lib'}，表示“root package”中的模块都在lib 目录中。--requires 定义依赖哪些模块 --provides定义可以为哪些模块提供依赖 --find_packages() 对于简单工程来说，手动增加packages参数很容易，刚刚我们用到了这个函数，它默认在和setup. py同一目录下搜索各个含有 __init__. py的包。             其实我们可以将包统一放在一个src目录中，另外，这个包内可能还有aaa. txt文件和data数据文件夹。另外，也可以排除一些特定的包             find_packages(exclude=[ *. tests ,  *. tests. * ,  tests. * ,  tests ])--install_requires = [ requests ] 需要安装的依赖包--entry_points 动态发现服务和插件，下面详细讲setup. py的一个例子 12345678910111213141516171819202122232425from setuptools import setup, find_packages    with open('README. rst') as f:      LONG_DESC = f. read()    setup(name='MyProject',       version='1. 0. 0',       url='http://example. com',       description='This is a cool microservice based on strava. ',       long_description=LONG_DESC,       author='Tarek', author_email='tarek@ziade. org',       license='MIT',       classifiers=[        'Development Status :: 3 - Alpha',        'License :: OSI Approved :: MIT License',        'Programming Language :: Python :: 2',        'Programming Language :: Python :: 3'],       keywords=['flask', 'microservice', 'strava'],       packages=find_packages(),       include_package_data=True,       zip_safe=False,       entry_points=          [console_scripts]       mycli = mypackage. mymodule:myfunc          ,       install_requires=['stravalib']))这里的install_requires 部分可以留空，用requirements. txt 来管理依赖包。 requirements. txt 文件   依赖包列表文件， 可以使用 pip install -r thefile. txt 来调用并安装依赖列表。 本地环境下一般需要先进入虚拟环境下之后再执行这个命令。   开发环境和生产环境可以有不同的依赖列表。dev-requirements. txt, prod-requirements. txt pip-tools 是一个扩展包 pip install pip-tools 之后可以查看当前环境下安装了的所有依赖包和相应版本。pip-compile pip freeze 也是一个获取当前环境已经安装过的依赖包的命令MANIFEST. in 文件这个文件包含了你想要打包的文件目录和列表 示例文件 123456include requirements. txtinclude README. rstinclude LICENSErecursive-include myservice *. inirecursive-include docs *. rst *. png *. svg *. css *. html conf. pyprune docs/build/* prune 是不包含自动生成的文档目录。 check-manifest 命令可以检查MANIFEST. in 文件是否语法正确和包含后的效果。一个典型的微服务项目包含的文件目录如下  setup. py: The setup file README. rst: The content of the long_description option MANIFEST. in: The MANIFEST template requirements. txt: PIP requirement files generated from install_requires docs/: The Sphinx documentation package/: The package containing the microservice code版本管理: 版本号写在setup. py 中 Python 打包工具并不强制需要一个版本号， 且版本号的命名规范也没有要求， 这就需要我们自己制定一个命名规范。 我们可以借鉴pip 管理依赖包中的做法， 用两个点分割三个数字。 Major. Minor. Patch    Major: 自增数字，当新的一个向后不兼容的大功能产生， 就会增加1     Minor: 自增数字， 对当前版本没有大影响（向后兼容）的小功能     Patch: 自增数字， bug修复  还没有第一个release 之前的版本的名字往往是0. 1. 0， 第一个release 的版本往往是1. 0. 0 打包命令:  sdist 是打包发布命令     python setup. py sdist 命令需要在项目根目录下执行。执行后会产生一个包含了项目文件的归档包。   sdist 读取了setup. py 和 MANIFEST. in， 然后获取到所有相应的文件并放到存档包中。生成的包放在 dist/目录下。文件名是由MANIFEST. in中的项目名加上版本号组成。   生成的存档包可以直接被pip命令进行安装 pip install dist/runnerly-tokendaler-0. 1. 0. tar. gz    wheel 是另一种打包发布命令, 安装包的速度会更快些。     需要先 pip install wheel 安装 wheel, 然后 python setup. py bidst_wheel universal 将项目目录下的内容进行打包   Distributing 部署: 如果我们想开源我们的项目， 就可以考虑将我们的项目发布到 pypi上， 让大家下载。    项目的名字要和已有的pypi项目不重名     需要去pypi注册一个账号， 然后在项目的根目录下创建一个 . pypirc的文件并包含如下内容  1234[pypi]username = &lt;username&gt;password = &lt;password&gt; 然后用twine （pip install twine）命令将包发布到pypi上面12twine register dist/runnerly-tokendealer-0. 1. 0. tar. gztwine upload dist/* 现在就可以在pypi 索引中找到我们的项目了运行所有的微服务: 如果是在本地运行一个微服务， 那么用flask自带的命令 python xxx. py runserver 就可以了 另一种方法是创建自己的启动服务的命令行脚本 - 使用 argparse 模块。 argparse 模块创建自己的启动服务的命令行: 官方说明  创建启动脚本 在setup. py中声名脚本 命令行运行自定义的命令的名字（比如runnerly-dataservice） 就可以了多进程管理: 安装 uWSGI: 说明: uWSGI 是实现了WSGI协议的应用程序， 好处是可以同时运行多个flask 进程， 方便在开发测试阶段对多个微服务统一管理。 安装运行后就不需要在用 python3. 6 test. py -r -d -h0. 0. 0. 0 来启动服务了, 而是 uwsgi --http 10. 11. 56. 210:5000 --wsgi-file 具体安装和使用方法请看 Flask 的笔记 uWSGI缺点是只能用来启动web服务。 安装Circus: Circus的好处是可以运行任何类型的进程， 比如多个微服务的WEB进程和Redis, Mongo进程。 Circus配置文件中要配置如下内容， 要跑5个web进程和1个redis进程 myconfig. ini [watcher:web]cmd = chaussette --fd $(circus. sockets. web) server. applicationuse_sockets = Truenumprocesses = 5[watcher:redis]cmd = /usr/local/bin/redis-server use_sockets = Falsenumprocesses = 1[socket:web]host = 0. 0. 0. 0port = 8000运行命令 1circusd myconfig. ini容器 - 将微服务放在容器中: 如果想快速的部署环境，可以考虑使用容器。 VMWare, Virtual-Box 还可以， 但是虚拟机更占内存， 而且往往不是开源的，所以作为demo环境还可以，但是用于生产就不是很好了。 Docker是一个理想的方案， 他是一个开源的虚拟工具， 他最大的好处是可以在生产环境运行，速度就像本地一样快。 什么是Docker:  Docker是一个容器平台， 让我们可以在一个隔绝的环境运行我们的应用。 我们要做的就是将镜像告诉Docker, Docker就会帮我们做所有的脏活了。 image 就是一个能够让Docker运行起来的，基于linux kernal 的所有进城资源。比如Ubuntu 的 image去官网下载Docker for mac, 并安装    Docker 有一个服务端和客户端， 之间用 socket 进行通信。     docker 客户端可以连接其他服务器上的docker 服务端     1docker version   Docker 101: Docker Hub (https://hub. docker. com) 有很多已有的Docker 镜像可以提供下载。 一些Python环境的镜像可以直接使用， Alpine Linux (refer to http://gliderlabs. viewdocs. io/docker-alpine/) 是最小的python运行环境 为Docker安装全套环境 - OpenResty, Circus and Flask: 部署Docker: 把微服务部署在Docker中后， 我们还需要让Docker之间相互通信。  假设两个Docker, 一个叫 host a , 一个叫host b.  只要两个docker 都有公共地址， 并且都暴露出了本地的sockets， 就可以桥接。 还可以创建 docker 的专属虚拟网络 Virtual Network. 12docker network create -driver=bridge runnerlydocker run --rm --net=runnerly --name=tokendealer -v /tmp/logs:/logs -p5555:8080 -it runnerly/tokendealerDocker Compose: Docker Compose (https://docs. docker. com/compose/) 可以在一个配置文件中定义多个docker的配置。 Docker 的clustering and Provisioning（服务发现）:  可以将同一个 image 启动多个实例， 我们管这个叫做 clustering.  多个同样的docker 实例可以用负载均衡来实现请求的分配策略。Nginx可以做这个事情。 如果想要让微服务有高可用， 并可以自动的上线或者下线， 需要有服务发现功能，它自动的让负载均衡识别到新的容器， 或者自动下线出故障不可用的容器。 Consul (https://www. consul. io/) or Etcd (https://coreos. com/etcd/) and Docker’s swarm mode 可以用来进行服务的配置工作 部署微服务需要好多步骤:     Read a configuration file that describes the instances needed via a few Docker Compose files.    Start a few VMs on the cloud provider.    Wait for all VMs to be up and running.    Make sure everything needed to run services on the VM is set.    Interact with the Docker daemon on each VM to start some containers.    Ping whatever service needs to be pinged to make sure the new instances are all interlinked.     Ansible (https://www. ansible. com/) or Salt (https://docs. saltstack. com) 也可以用来进行服务的自动化部署和配置工作 Kubernetes (https://kubernetes. io/) 是另一个部署分布式容器的工具. 和Ansible 和 Salt不同的是, Kubernets 只针对容器提供全面的解决方案. 让容器可以部署在任何地方, 包括主流的云服务AWS, Digital Ocean, Openstack等. 在 AWS 上部署微服务: 在AWS上部署微服务的好处:  便宜 稳定 安全 有很多现成的服务可以使用AWS有很多很多服务, 但是主要用于微服务的服务可分为4类  路由型: 这类服务会将请求重定向(跳转)到正确的地方, 比如DNS服务和负载均衡服务一样.  执行型: 执行你的代码的服务, 比如EC2 和 Lambda 存储型: 存储数据的服务, 比如 缓存, 传统数据库, 长期数据存储, CDN等 消息型: 处理消息的服务, 比如发送推送, 邮件等下图是一个分类示意图: Routing - Route53, ELB, AutoScaling: 路由: Route53 (https://aws. amazon. com/route53/) 是亚马逊的DNS服务.  我们可以在Route53中定义DNS的入口, 自动将请求路由到指定的AWS上我们的应用或者文件上. 负载均衡: Route53可以无缝的和ELB进行协作. Elastic Load Balancing (ELB)(https://aws. amazon. com/elasticloadbalancing/) 是负载均衡服务.  如果一个微服务有多个vms, 那么就可以用ELB. ELB 还可以进行健康检查, 发现问题就可以自动发邮件 服务自动启停: AutoScaling (https://aws. amazon. com/autoscaling/) 可以通过一些事件触发, 自动添加实例. 当ELB 的健康检查发现了一个实例没有响应了, 就可以通知AutoScaling 将他下线, 并启动一个新的备用实例代替原来的那个. Execution - EC2 and Lambda: EC2 (https://aws. amazon. com/ec2/) 可以让我们创建虚拟机, 是AWS的核心计算单元. EC2 Container Service (ECS) (https://aws. amazon. com/ecs) 是一个专门管理Docker 部署的EC2, 和 Kubernets 的功能类似. Lambda 是一个分布式定时任务管理器 Storage - EBS, S3, RDS, ElasticCache, and CloudFront: Simple Storage Service (S3) (https://aws. amazon. com/s3/) 是存储服务, 将数据存储到 bucket中. Bucket 是key-value 存储, value 就是我们要存的文件数据. Python中使用S3很方便, 所以使用 S3作为微服务的数据存储是很理想的. ElasticCache (https://aws. amazon. com/elasticache/) 是一个缓存服务. 它是利用 Redis 和 Memcached 作为后端的支撑. Relational Database Service (RDS) (https://aws. amazon. com/rds/) 是关系型数据库服务, 背后是Mysql, PostgreSQL 支撑. CloudFront (https://aws. amazon. com/cloudfront/) 是 Amazon’s Content Delivery Network (CDN). 被频繁访问的静态文件可以放在CDN中, 但是少量的,就直接放在微服务中即可. Messaging - SES, SQS, and SNS: AWS 提供的三种主要消息服务  Simple Email Service (SES): 邮件服务 Simple Queue Service (SQS): 消息队列服务, 类似于 RabbitMQ Simple Notification Service (SNS): 消息推送系统Simple Email Service (SES): 邮件服务 Simple Email Service (SES) (https://aws. amazon. com/ses/ ) 发送邮件 SQS (https://aws. amazon. com/sqs/) 可以理解成是RabbitMQ的一个子集. 可以创建两种队列, 一种是先进先出型, 执行效果就等同于使用 Celery和Redis, 并发消息是2万. 另一种是标准型, 但是消息的顺序是不保障的, 并发消息限制是12万. SNS (https://aws. amazon. com/sns/) 消息推送提供两种API, 第一种是发布/订阅型 pub/sub API, 发布者可以是某种AWS服务或者是我们的应用, 订阅者可以是SQS消息队列, Lambda, 或者是我们的某个微服务.  另一种是Push 推送API, 比如给手机发送短信. 微服务的部署- 实践: 步骤:    设置一个AWS账号,创建账号, 并设定最低消费提醒.     创建一个EC2实例, 使用CoreOS的PV版本作为镜像, 过程中要设置一个keypair, 保存好. 并远程登录   123ssh -i  AmazonKeyPairDalong. pem  core@ec2-52-206-71-113. compute-1. amazonaws. comupdate_engine_client -updatesudo reboot      并在CentOS中运行一个Docker命令(这时Docker已经在CoreOS上默认安装了)   1234ssh -i  AmazonKeyPairDalong. pem  core@ec2-52-206-71-113. compute-1. amazonaws. comdocker -v Docker version 17. 05. 0-ce, build 89658bedocker run busybox /bin/echo hello        在Docker中配置Flask 的镜像 docker-flask   1docker run -d -p 80:80 p0bailey/docker-flask    AWS默认值开启22端口给SSH, 我们还要去EC2中的安全组中开启inbound 的 http的服务(80端口)     再配置ECS的自动分布式. ECS负责自动部署Docker images.          ECS 部署需要包含一些因素          一个ELB (Elastic Load Balancer)     一个 Task Definition (任务定义), 它被用来决定那个Docker image 需要被部署, 那个端口可以用来给host 和 容器使用     一个 Service, 使用任务定义去驱动EC2 的创建, 并运行EC2中的Docker     一个 Cluster, 将 所有的服务, 任务定义, ELB 分组.                   在AWS界面上点击 Deploy a sample application onto Amazon ECS Cluster , 按照指引完成上面内容的配置.               最后用Route53来在同一个域名下发布我们的众多微服务      第5步完成后, 已经创建了load blancing.    给LB创建一个别名: https://console. aws. amazon. com/route53, 点击 hosted zones   加入我们已经有一个域名了, 那么就可以在 AWS 的 DNS服务中点击 Create Hosted Zone 把自己的域名加进来.    我们可以在域名这里看到之前创建好的LB , 这里要把域名连接到我们不熟的ECS cluster上, 并可以设定二级域名到不同的为服务上.    在阿里云上部署微服务:    创建账号     创建ECS实例, 按量付费, 选择华东2的D区, 通用型. CentOS 镜像   EC   12345678910111213141516# 设置成功后连接远程ECS服务器ssh root@101. 132. 164. 124  # 安装Python3. 6yum -y groupinstall  Development tools yum -y install zlib-devel bzip2-devel openssl-devel ncurses-devel sqlite-devel readline-devel tk-devel gdbm-devel db4-devel libpcap-devel xz-devel  curl -O https://www. python. org/ftp/python/3. 6. 6/Python-3. 6. 6. tar. xz  tar -xvJf Python-3. 6. 6. tar. xz  cd Python-3. 6. 6  . /configure --prefix=/usr/local/python3  make &amp;&amp; make install   对微服务API服务网关的理解1. 简介: 通过以下几个问题去理解微服务中的一个重要组件：API网关。  什么是API网关 为什么需要API网关 API网关在微服务架构体系中处于什么位置 网关技术实现有哪些 zuul网关工作原理是什么样的 技术上如何选型2. 什么是API网关:  API网是一个反向路由：屏蔽内部细节，为调用者提供统一入口，接收所有调用者请求，通过路由机制转发到服务实例。 API网关是一组“过滤器”集合：可以实现一系列与核心业务无关的横切面功能，如安全认证、限流熔断、日志监控。3. 为什么需要API网关: 　　什么是API网关中已给出理由，简单总结：  屏蔽内部细节 反向路由 安全认证 限流熔断 日志监控4. API网关在微服务架构体系中处于什么位置: 4. 1 调用者眼中的API网关:  统一入口 内部服务对于调用者是透明的4. 2 所处的位置:  处于负载均衡与业务服务之间 API网关也可实现负载均衡功能5. 网关技术实现有哪些: 简单列举，不做比较，有兴趣可查询资料，后续为基于zuul网关来理解工作流程原理  zuul：是netflix开源的一个API Gateway 服务器, 本质上是一个web servlet应用。Spring Cloud已集成 kong：是一款基于Nginx_Lua模块写的高可用，易扩展由Mashape公司开源的API Gateway项目6. zuul网关工作原理是什么样的: 因为后续API网关组件会聚焦于zuul，这里对zuul工作原理提前学习。 6. 1 整体处理流程图:  zuul本身是一个Servlet HttpServlet Request请求到达Zuul Servlet 通过ZuulFilter Runner ，并且根据routing filter 类型执行filter 链 根据FilterType类型不同，执行的顺序不同且可完成不同的功能     pre：在请求被路由之前调用，可实现日志监控、身份认证、黑名单等功能   route：在路由请求时候被调用   post：在route和error过滤器之后被调用，可实现审计、统计等功能   error：处理请求时发生错误时被调用、可实现统一异常处理等功能    Request Context：在请求生命周期中共享变量 Filter的实现是可插拔的     脚本实现Filter   发布到filter目录中   由filter manager与filter loader自动加载   6. 2 请求生命周期:  拦截请求 根据filter类型，执行已加载的filter chain 返回请求7. 技术上如何选型:  若使用的微服务框架是Spring Cloud，可选择Netflix的zuul，已经很好的集成到SC中 kong ，当然，能力强的可以自定义实现满足自己需求的API网关对微服务路由发现体系的理解简介:  路由发现是微服务体系中一块重要组成，从以下几个方面理解微服务路由：  什么是路由 为什么需要路由 从路由的角度看微服务的体系架构是什么样的 路由、服务发现、负载均衡有什么关系 补充：负载均衡算法有哪些1. 什么是微服务路由:  直白理解：“路由”是指根据请求URL，将请求分配到对应的处理程序。 　　如SpringMVC的DispatchServlet，统一接收所有需要SpringMVC处理的请求，再根据指定饿匹配规则，将请求映射到最终的Controller中的某个方法。这里不再扩展，有兴趣的可以直接查看相关资料。 2. 微服务为什么需要路由:  微服务需要一套完善的请求分发机制来保证一个请求到来能正确的找到对应的服务实例 　　微服务一般是由几十、上百个服务组成，无论是外部调用GW，或者内部服务之间的调用，对于一个URL请求，最终会确认一个服务实例进行处理。对每个服务实例手动指定一个唯一访问地址，然后根据URL去手动实现请求匹配是不可取的，尤其是基于云环境的微服务体系。 　　反向路由是微服务的网关的功能之一，可以为我们处理每个请求到服务实例的绑定，结合网关特点，可以做许多额外的横切面的工作，让更多的精力聚焦于业务。对于为什么需要API网关，请查看《五：对微服务API服务网关的理解》 3. 从路由的角度看微服务的体系架构是什么样的:  理解阶段，只能先了解整体流程，后续实际代码阶段以及原理、源码分析阶段会深入了解。 3. 1 内部服务如何调用:  基础服务之间的调用：结合服务注册中心以及专属的具有负载均衡功能的客户端，如Eureka+（restTemplate+Ribbon）或者Eureka+Feign 聚合服务调用：结合服务注册中心以及专属的具有负载均衡功能的客户端，如Eureka+（restTemplate+Ribbon）或者Eureka+Feign3. 2 外部路由如何实现: 　　基于Netflix的zuul，做了简单了解，SpringCloud与zuul集成的方式。这里先对核心流程做个简单了解，后续会有深入的应用、分析。 　　  Spring Cloud很好的集成了zuul，并且可以通过注解的形式来进行请求的反向路由以及API网关功能 Spring Cloud集成zuul，对与url映射的处理方式与SpringMVC对url的请求方式类似，都是通过RequestMapping来进行请求绑定的。核心类：ZuulHandlerMapping zuul的核心是ZuulServlet，一个请求核心流程：HttpServletRequest –&gt;ZuulHandlerMapping –&gt;ZuulController –&gt; ZuulServlet –&gt; ZuulFilter –&gt; HttpServletResponse，详细流程请查看《五：对微服务API服务网关的理解》4. 路由、API网关、服务发现、负载均衡有什么关系: 4. 1 关系  都是组成微服务的不可或缺的一部分  路由是API网关的一个功能点 API网关需要定期或实时同步服务注册中心的服务列表，服务发现是将服务实例与服务注册中心进行实时关联 微服务架构中的负载均衡一般采用进程内负载均衡或者独立主机负载均衡形式，要求客户端同样需要定时或实时同步服务注册中心维护的服务列表4. 2 从服务发现工作流程理解一次请求过程  请求到来之前肯定需要服务提供者正常工作  服务提供者启动时，将提供的服务名称、服务器地址注册到服务配置中心 服务消费者通过服务配置中心来获取需要调度的服务机器列表 一次请求到来，经过路由分发定位到一组服务，再通过负载均衡算法之后，选取一台服务器调用 当服务器宕机或下线，相应的机器动态从服务配置中心移除，并通知相应的服务消费者 服务消费者一次访问服务配置中心之后，会将查询到的信息缓存到本地，后面调用先查缓存，从而降低服务配置中心压力5. 补充：负载均衡算法有哪些:  轮询法：将请求按顺序轮流地分配到后端服务器上，它均衡地对待后端的每一台服务器，而不关心服务器实际的连接数和当前的系统负载。 随机法：通过系统的随机算法，根据后端服务器的列表大小值来随机选取其中的一台服务器进行访问。由概率统计理论可以得知，随着客户端调用服务端的次数增多，其实际效果越来越接近于平均分配调用量到后端的每一台服务器，也就是轮询的结果。 加权轮询法：不同的后端服务器可能机器的配置和当前系统的负载并不相同，因此它们的抗压能力也不相同。给配置高、负载低的机器配置更高的权重，让其处理更多的请；而配置低、负载高的机器，给其分配较低的权重，降低其系统负载，加权轮询能很好地处理这一问题，并将请求顺序且按照权重分配到后端。 最小连接法：最小连接数算法比较灵活和智能，由于后端服务器的配置不尽相同，对于请求的处理有快有慢，它是根据后端服务器当前的连接情况，动态地选取其中当前积压连接数最少的一台服务器来处理当前的请求，尽可能地提高后端服务的利用效率，将负责合理地分流到每一台服务器。 源地址哈希法：源地址哈希的思想是根据获取客户端的IP地址，通过哈希函数计算得到的一个数值，用该数值对服务器列表的大小进行取模运算，得到的结果便是客服端要访问服务器的序号。采用源地址哈希法进行负载均衡，同一IP地址的客户端，当后端服务器列表不变时，它每次都会映射到同一台后端服务器进行访问。"
    }, {
    "id": 24,
    "url": "http://localhost:4000/Tensorflow-with-Python/",
    "title": "Tensorflow with Python",
    "body": "2021/02/21 - TensorFlow是由Google创建和发布的用于快速数值计算的Python库。 它是一个基础库，可用于直接创建深度学习模型，也可以使用包装库来简化在 TensorFlow 之上构建的过程。 在这篇文章中，我将带您一起探索深度学习的TensorFlow。 TensorFlow[TOC] 安装: 方法一: 通过 pip 命令安装: 12pip3. 6 install --upgrade pippip3. 6 install --upgrade tensorflow测试 Tensorflow 安装是否成功  进入Python 命令行 python3. 6 用 Tensorflow 写一段 向量相加的代码12345678import tensorflow as tfa = tf. constant([1. 0,2. 0],name='a')b = tf. constant([2. 0,3. 0],name='b')result = a+bsess = tf. Session()sess. run(result)# 输出:# array([3. , 5. ], dtype=float32)如果出现以上结果, 那么就算安装成功了 方法二, 通过 Anaconda 安装: 下载Anaconda 最新版本for Mac-OS 更新Anaconda 的源 网上文档 1234567/anaconda3/bin/conda config --add channels https://mirrors. tuna. tsinghua. edu. cn/anaconda/pkgs/free/ /anaconda3/bin/conda config --add channels https://mirrors. tuna. tsinghua. edu. cn/anaconda/cloud/conda-forge//anaconda3/bin/conda config --add channels https://mirrors. tuna. tsinghua. edu. cn/anaconda/cloud/msys2//anaconda3/bin/conda config --set show_channel_urls yesvi ~/. condarc# 删除 -defaults 这一行/anaconda3/bin/conda upgrade --all # 更新 anaconda 然后启动anaconda 应用, 并创建一个新的环境 TensorFlowEnv1, 并为环境安装上 Tensorflow 的扩展. ] 资源:  Tensorflow 的示例代码 playgroundTensorflow 入门: 概念: TensorFlow 是一个编程系统, 使用图来表示计算任务。图中的节点被称之为 op (operation 的缩写)。 一个 op 获得 0 个或多个Tensor, 执行计算, 产生 0 个或多个 Tensor. 每个 Tensor 是一个类型化的多维数组. 例如, 你可以将一小组图像集表示为一个四维浮点数数组, 这四个维度分别是 [batch, height, width, channels]. 一个 TensorFlow 图描述了计算的过程. 为了进行计算, 图必须在 会话 里被启动. 会话 将图的 op 分发到诸如 CPU 或 GPU 之类的 设备 上, 同时提供执行 op 的方法. 这些方法执行后, 将产生的 tensor 返回. 在 Python 语言中, 返回的 tensor 是 numpy ndarray 对象；  TensorFlow是谷歌基于DistBelief进行研发的第二代人工智能学习系统 用途: TensorFlow可被用于语音识别或图像识别等多项机器学习和深度学习领域 跨平台: TensorFlow 可在小到一部智能手机、大到数千台数据中心服务器的各种设备上运行 Tensor 翻译为张量, 可以理解为多维数组, 表示数据结构 Flow 翻译为流, 表达了张量之间通过计算相互转化的过程 TensorFlow 是通过计算图的形式表达计算的编程系统 节点 是计算图上的计算单元, 节点由边连接 边 连接计算节点, 描述了计算之间的依赖关系 TensorBoard 是 Tensorflow 的可视化工具  上图中 a 和 b 是常量, 不依赖其他计算节点 add 计算依赖两个常量, 所以有两条边从 a,b 两个常量到addTensorFlow 的特点：  使用图 (graph) 来表示计算任务.  在被称之为 会话 (Session) 的上下文 (context) 中执行图.  使用 tensor 表示数据.  通过 变量 (Variable) 维护状态.  使用 feed 和 fetch 可以为任意的操作(arbitrary operation) 赋值或者从其中获取数据. Hello World: 123456789101112131415161718192021222324#导入TensorFlowimport tensorflow as tf# 定义常量 op,# 加到默认图中. hello = tf. constant('Hello Tensorflow')python = tf. constant('Python')hello, python# 在一个会话中启动图# 构造阶段完成后, 才能启动图. 启动图的第一步是创建一个 Session 对象, 如果无任何创建参数, 会话构造器将启动默认图. # 启动默认图. # 创建一个 session 对象sess = tf. Session()# 用session 的 run 方法来执行常量sess. run(hello)sess. run(python)# session 使用完之后必须关闭, 因为比较消耗内存sess. close()Session 对象在使用完后需要关闭以释放资源. 除了显式调用 close 外, 也可以使用 “with” 代码块 来自动完成关闭动作 比较下面两段代码 – 常量和变量的使用::  使用变量前, 需要先创建一个初始化对象,然后在session中运行它12345678910111213141516171819import tensorflow as tfhello = tf. constant('hello')world = tf. constant('world')sess = tf. Session()sess. run(hello+' '+world)# 输出 b'hello world'hello = tf. Variable('hello')world = tf. Variable('world')# 使用变量前, 需要先创建一个初始化对象,然后在session中运行它sess = tf. Session()init = tf. initialize_all_variables()sess. run(init)sess. run(hello+' '+world)# 输出 b'hello world'sess. close()Tensorflow 的 placeholder 的使用: placeholder 的一个例子 123456789101112import tensorflowtf#定义数据对象和计算对象a = tf. placeholder(tf. string)b = tf. placeholder(tf. string)add = tf. string_join([a,b],separator=' ')#定义会话, 并在会话中运行计算对象,同时给计算对象传递数据对象的值sess = tf. Session()sess = tf. Session()sess. run(add, feed_dict={a:'hello', b:'world'})简单加减法计算 1234567891011121314151617181920212223242526272829303132333435363738import tensorflow as tf# 第一种方式，先声明常量, 然后进行算数运算# 定义两个数字常量a = tf. constant(2)b = tf. constant(3)# 启动默认图. with tf. Session() as sess:  print( a = %i  %sess. run(a),  b = %i  %sess. run(b))  print( 加法运算结果: %i  %sess. run(a+b))  print( 乘法运算结果: %i  %sess. run(a*b))# 第二种方式，先创建placeholder占位符对象, 然后定义计算对象, 最后将占位符对象和对应值作为字典参数传到计算对象中a = tf. placeholder(tf. int32)b = tf. placeholder(tf. int32)c = tf. placeholder(tf. int32)# 定义加法add = tf. add(a,b)# 定义乘法mul = tf. multiply(a,b)# 定义减法sub = tf. subtract(a,b)# 定义除法div = tf. divide(a,b)with tf. Session() as sess:  print( 加法 %i  %sess. run(add,feed_dict={a:2,b:4}))  print( 减法 %i  %sess. run(sub,feed_dict={a:2,b:4}))  print( 乘法 %i  %sess. run(mul,feed_dict={a:2,b:4}))  print( 除法 %. 2f  %sess. run(div,feed_dict={a:2,b:4}))简单矩阵计算 12345678910111213141516171819# 矩阵乘法import tensorflow as tfimport numpy as np# 定义计算对象和数据对象a = np. array([1,2,3,4,5,6], dtype=float). reshape((1,6))# aa = tf. constant(a)b = np. array([6,5,4,3,2,1], dtype=float). reshape((6,1))# bb = tf. constant(b)# np. dot(a,b) # 输出 array([[56]])# matmul 是矩阵乘, 参数可以是tf常量, 也可以是ndarray数组matmul = tf. mat(a,b)# matmul = tf. matmul(aa,bb) # 这种输出的结果和上面一行的结果一致. # 整个执行过程是自动化的, 会话负责传递 op 所需的全部输入. op 通常是并发执行的. with tf. Session() as sess:  print(sess. run(matmul)) # 输出 [[56. ]]这段很短的 Python 程序生成了一些三维数据，然后用一个平面拟合它 1234567891011121314151617181920212223242526272829303132import tensorflow as tfimport numpy as np# 使用 NumPy 生成假数据(phony data), 总共 100 个点. x_data = np. float32(np. random. rand(2, 100)) # 随机输入y_data = np. dot([0. 100, 0. 200], x_data) + 0. 300# 构造一个线性模型# b = tf. Variable(tf. zeros([1]))W = tf. Variable(tf. random_uniform([1, 2], -1. 0, 1. 0))y = tf. matmul(W, x_data) + b# 最小化方差loss = tf. reduce_mean(tf. square(y - y_data))optimizer = tf. train. GradientDescentOptimizer(0. 5)train = optimizer. minimize(loss)# 初始化变量init = tf. initialize_all_variables()# 启动图 (graph)sess = tf. Session()sess. run(init)# 拟合平面for step in xrange(0, 201):  sess. run(train)  if step % 20 == 0:    print step, sess. run(W), sess. run(b)# 得到最佳拟合结果 W: [[0. 100 0. 200]], b: [0. 300]下面的例子演示了如何使用变量实现一个简单的计数器。 123456789101112131415161718192021222324252627282930# 创建一个变量, 初始化为标量 0. state = tf. Variable(0, name= counter )# 创建一个 op, 其作用是使 state 增加 1one = tf. constant(1)new_value = tf. add(state, one)update = tf. assign(state, new_value)# 启动图后, 变量必须先经过`初始化` (init) op 初始化,# 首先必须增加一个`初始化` op 到图中. init_op = tf. initialize_all_variables()# 启动图, 运行 opwith tf. Session() as sess: # 运行 'init' op sess. run(init_op) # 打印 'state' 的初始值 print(sess. run(state)) # 运行 op, 更新 'state', 并打印 'state' for _ in range(3):  sess. run(update)  print(sess. run(state))# 输出:# 0# 1# 2# 3使用TensorFlow实现线性回归: 案例一:: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788import tensorflow as tfimport numpy as npimport matplotlib. pyplot as plt%matplotlib inline# 定义训练次数learning_epochs，# 卷曲神经的学习率learning_rate, # 显示打印数据的步幅display_steplearning_epochs = 1000learning_rate = 0. 01 display_step = 50# 生成训练数据, 并加噪# tensorflow 是无监督学习train_X = np. linspace(0,10,num=20) + np. random. randn(20)train_Y = np. linspace(1,4,num=20) + np. random. randn(20)# train_X. shape, train_Y. shapen_samples = train_X. shape[0]# 画训练数据的散点图plt. scatter(train_X,train_Y)# 定义TensorFlow参数：X，Y，W，b # 对应 f(x) = wx + b # 用 placeholder 定义两个变量X = tf. placeholder('float')Y = tf. placeholder('float')# 随机生成斜率和截距# variable 变量定义了斜率W 和截距b, 都是随机数W = tf. Variable(np. random. randn(), name='weight')b = tf. Variable(np. random. randn(), name='bias')# 创建线性模型 f(x) = W*X + b# 预测值 y_pred, 真实值 train_Y# 此时只是一个模型, X值是个占位符而已y_pred = tf. add(tf. multiply(W,X),b)# 启动会话(启动默认图. )# sess = tf. Session()# 初始化所有变量# init = tf. global_variables_initializer()# sess. run(init)# 打印预测值# print(sess. run(W), sess. run(b))# 求均方误差 - 是反应估计量和被估计量之间的差异的一种度量# 均方差公式就是 ((训练样本值-预测值)平方)/训练样本个数cost = tf. reduce_sum(tf. pow(Y - y_pred, 2))/20# 用均方差作为参数 使用tf 自带的梯度下降法# minimize 是对梯度下降的一个最小限制条件optimizer = tf. train. GradientDescentOptimizer(learning_rate). minimize(cost)init = tf. global_variables_initializer()# 开始训练with tf. Session() as sess:  # 初始化  sess. run(init)  # 训练所有数据, 总共训练1000次  for epoch in range(learning_epochs):    # 每次训练有20个数据, 执行20次梯度下降法    # x,y 训练数据的每个点的坐标    for(x,y) in zip(train_X, train_Y):      sess. run(optimizer, feed_dict={X:x, Y:y}) # 执行梯度下降法    # 没执行50次, 显示一次运算结果    if (epoch+1) % display_step == 0:#       c = sess. run(cost, feed_dict={X:train_X, Y:train_Y})      print(sess. run(W), sess. run(b))  # 算法优化结束  print( 梯度算法优化结束 )  # 再利用平均方差, 求得最终的结果  training_cost = sess. run(cost, feed_dict={X:train_X, Y:train_Y})      # 画图    # 先看训练的数据  plt. plot(train_X, train_Y, 'ro', label='o data')  # 再看预测数据  plt. plot(train_X, sess. run(W)*train_X + sess. run(b), label = 'f line')  plt. legend()输出如下: 案例二:: 123456789101112131415161718192021222324252627282930313233343536373839#导入依赖库import numpy as np #这是Python的一种开源的数值计算扩展，非常强大import tensorflow as tf #导入tensorflow import matplotlib. pyplot as plt%matplotlib inline##构造数据##x_data=np. random. rand(100). astype(np. float32) #随机生成100个类型为float32的值y_data=x_data*0. 1+0. 3 #定义方程式y=x_data*A+B##-------### 为训练数据画散点图plt. scatter(x_data, y_data)##建立TensorFlow神经计算结构##weight=tf. Variable(tf. random_uniform([1],-1. 0,1. 0)) biases=tf. Variable(tf. zeros([1]))   # 获取预测值y=weight*x_data+biases##-------##loss=tf. reduce_mean(tf. square(y-y_data)) #判断预测值与正确值的差距optimizer=tf. train. GradientDescentOptimizer(0. 5) #根据差距进行反向传播修正参数train=optimizer. minimize(loss) #建立训练器init=tf. initialize_all_variables() #初始化TensorFlow训练结构sess=tf. Session() #建立TensorFlow训练会话sess. run(init)   #将训练结构装载到会话中for step in range(400): #循环训练400次   sess. run(train) #使用训练器根据训练结构进行训练   if step%20==0: #每20次打印一次训练结果    print(step,sess. run(weight),sess. run(biases)) #训练次数，A值，B值    sess. close()案例三:: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374import tensorflow as tfimport numpyimport matplotlib. pyplot as pltrng = numpy. random# Parameterslearning_rate = 0. 01training_epochs = 2000display_step = 50# Training Datatrain_X = numpy. asarray([3. 3,4. 4,5. 5,6. 71,6. 93,4. 168,9. 779,6. 182,7. 59,2. 167,7. 042,10. 791,5. 313,7. 997,5. 654,9. 27,3. 1])train_Y = numpy. asarray([1. 7,2. 76,2. 09,3. 19,1. 694,1. 573,3. 366,2. 596,2. 53,1. 221,2. 827,3. 465,1. 65,2. 904,2. 42,2. 94,1. 3])n_samples = train_X. shape[0]# tf Graph InputX = tf. placeholder( float )Y = tf. placeholder( float )# Create Model# Set model weightsW = tf. Variable(rng. randn(), name= weight )b = tf. Variable(rng. randn(), name= bias )# Construct a linear modelactivation = tf. add(tf. multiply(X, W), b)#拟合 X * W + b# Minimize the squared errors# reduce_sum就是求和# cost是真实值y与拟合值h&lt;hypothesis&gt;之间的距离cost = tf. reduce_sum(tf. pow(activation-Y, 2))/(2*n_samples) #L2 lossoptimizer = tf. train. GradientDescentOptimizer(learning_rate). minimize(cost) #Gradient descent# Initializing the variablesinit = tf. initialize_all_variables()# Launch the graphwith tf. Session() as sess:  sess. run(init)  # Fit all training data  # training_epochs是迭代次数  for epoch in range(training_epochs):    for (x, y) in zip(train_X, train_Y):      sess. run(optimizer, feed_dict={X: x, Y: y})    #Display logs per epoch step    if epoch % display_step == 0:      print( Epoch: , '%04d' % (epoch+1),  cost= ,  {:. 9f} . format(sess. run(cost, feed_dict={X: train_X, Y:train_Y})), \         W= , sess. run(W),  b= , sess. run(b))  print( Optimization Finished! )  training_cost = sess. run(cost, feed_dict={X: train_X, Y: train_Y})  print( Training cost= , training_cost,  W= , sess. run(W),  b= , sess. run(b), '\n')  # Testing example, as requested (Issue #2)  test_X = numpy. asarray([6. 83,4. 668,8. 9,7. 91,5. 7,8. 7,3. 1,2. 1])  test_Y = numpy. asarray([1. 84,2. 273,3. 2,2. 831,2. 92,3. 24,1. 35,1. 03])  print( Testing. . . (L2 loss Comparison) )  testing_cost = sess. run(tf. reduce_sum(tf. pow(activation-Y, 2))/(2*test_X. shape[0]),              feed_dict={X: test_X, Y: test_Y}) #same function as cost above  print( Testing cost= , testing_cost)  print( Absolute l2 loss difference: , abs(training_cost - testing_cost))  #Graphic display  plt. plot(train_X, train_Y, 'ro', label='Original data')  plt. plot(test_X, test_Y, 'bo', label='Testing data')  plt. plot(train_X, sess. run(W) * train_X + sess. run(b), label='Fitted line')  plt. legend()  plt. show() "
    }, {
    "id": 25,
    "url": "http://localhost:4000/Use-Numpy-Panda-And-Sklearn-for-Data-analytics-and-AI/",
    "title": "Use Numpy, Panda and Sklearn for Data Analytics and AI",
    "body": "2020/09/30 - 到目前为止，我已经编写了所有没有库的教程，现在我正在将我们的旅程提升到一个新的水平，我们将使用 python 库进行分类、可视化和聚类。在本文中，我们将简要介绍NumPy，SciPy，matplotlib，scikit-learn，pandas, 让我们开始吧! IPython, Numpy, Panda, sklearn 基础[TOC] 人工智能职业生涯三个层次:  了解大部分的数据分析工具, 掌握基本的机器学习模型和使用及原理 掌握更多的机器学习模型, 对机器学习模型有较完整的深入的知识 深入研究数学理论, 对算法进行改进和优化IPython 与 Jupyter 介绍: ipython 就是 jupyter, 只不过是改了名字而已.  Jupyter 是一个集文本, 代码, 图像, 公式, 的展现于一体的超级 python web 界面 Jupyther 是基于浏览器的Notebook 还是 Shell: ipython 提供两种方式的交互命令, 一种是基于命令行的 shell, 另一种是基于浏览器的 notebook. 他们各有优势 装和启动 Shell 方法:: 12pip install jupyteripython安装和启动 notebook 方法:: 12pip install jupyterjupyter notebookCell 概念和快捷键:  就是一个输入框, 我们可以将输入框的格式修改成 code 或者 markdown 格式. code 就是写代码, markdown 就是写笔记 control + enter: 运行当前 cell 的快捷键 shift + enter: 运行当前 cell 并跳到下一个cell, 如果没有下一个, 新建新cell a: 在当前cell 上面新建一个 cell b : 新建一个 cell 双击 d: 删掉一个选中的 cell 单击 x: 删掉一个选中的 cell 单击 m: 将当前 cell 转成 markdown 格式 单击 y: 将当前 cell 转成 code 格式 shift+tab 查看函数使用说明更多的快捷键: ⌘: Command ⌃: Control ⌥: Option ⇧: Shift ↩: Return ␣: Space ⇥: Tab Command Mode (press Esc to enable): F: find and replace ↩: enter edit mode ⌘⇧P: open the command palette ⇧↩: run cell, select below ⌃↩: run selected cells ⌥↩: run cell, insert below Y: to code M: to markdown R: to raw 1: to heading 1 2: to heading 2 3: to heading 3 4: to heading 4 5: to heading 5 6: to heading 6 K: select cell above ↑: select cell above ↓: select cell below J: select cell below ⇧K: extend selected cells above ⇧↑: extend selected cells above ⇧↓: extend selected cells below ⇧J: extend selected cells below A: insert cell above B: insert cell below X: cut selected cells C: copy selected cells ⇧V: paste cells above V: paste cells below Z: undo cell deletion D,D: delete selected cells ⇧M: merge selected cells, or current cell with cell below if only one cell selected ⌘S: Save and Checkpoint S: Save and Checkpoint L: toggle line numbers O: toggle output of selected cells ⇧O: toggle output scrolling of selected cells H: show keyboard shortcuts I,I: interrupt kernel 0,0: restart the kernel (with dialog) Esc: close the pager Q: close the pager ⇧␣: scroll notebook up ␣: scroll notebook down Edit Mode (press Enter to enable): ⇥: code completion or indent ⇧⇥: tooltip ⌘]: indent ⌘[: dedent ⌘A: select all ⌘Z: undo ⌘⇧Z: redo ⌘Y: redo ⌘↑: go to cell start ⌘↓: go to cell end ⌥←: go one word left ⌥→: go one word right ⌥⌫: delete word before ⌥⌦: delete word after ⌃M: command mode Esc: command mode ⌘⇧P: open the command palette ⇧↩: run cell, select below ⌃↩: run selected cells ⌥↩: run cell, insert below ⌃⇧Minus: split cell ⌘S: Save and Checkpoint ↓: move cursor down ↑: move cursor up jupyter 帮助 ? 和 查看源代码中的解释 ??: 12len? # 查看python 函数 len 的帮助文档len?? # 查看源代码中的解释tab 自动补全: tab: 自动补全 notebook 中的代码 1234prin&lt;tab&gt;a = 'abc'a. &lt;tab&gt;import &lt;tab&gt;用通配符查找相关命令 *: 12*Warning?str. *find*?魔法命令 %:  jupyter 中可以使用 % 开头来运行一些魔法命令 查看jupyter 的魔法命令的帮助 %magic %lsmagic: 12%magic # 列出魔法命令的使用说明%lsmagic # 列出所有可用的魔法命令运行外部文件 %run:  %run first. py 运行一个 python 文件, 并获取到这个文件中定义的变量及函数. 查看代码运行时间 %time, %timeit: 12345678910def cal(num):  result = 0  for i in range(1,num+1):    result += i    for j in range(1, num+1):      result -= 1%time cal(1000) # 计算这个函数的运行时间 58. 5 ms%timeit cal(1000) # 计算这个函数运行多次的平均运行时间 54. 5 ms ± 1. 59 ms %who, %whos 查看当前会话所有的变量和函数名称及详细信息 123%who # 快速查看当前会话的所有变量与函数名称%whos # 查看当前会话的所有变量与函数名称的详细信息%who_ls # 返回一个字符串列表，里面元素是当前会话的所有变量与函数名称 其他代码性能查看命令 %prun %lprun %memit %mprun:  %prun  Run code with the profiler  %lprun  Run code with the line-by-line profiler  %memit  Measure the memory use of a single statement  %mprun  Run code with the line-by-line memory profiler 查看当前会话中的所有变量和函数 %who, %whos: 12%who # 查看所有变量和函数%whos # 查看所有变量,函数和响应的type 以及数据信息查看命令的历史 %history: 12%history # 查看所有历史%history -n 1-4 # 查看最近四条历史执行 linux 指令 !: 12!pwd # 查看当前目录 directory = !pwd #将命令行的输出作为python 变量的值. OpenCV 视觉库: 人脸识别案例: 首先安装 opencv 计算机视觉库 1pip install opencv-pythonGithub上用到的人脸识别训练库 案例1: 金正恩脸换小狗 1234567891011121314151617181920212223242526272829import numpy as npimport matplotlib. pyplot as plt#%matplotlib inline# 人脸识别# opencv: 计算机视觉库import cv2# RGB 但是cv 库读取的时候, 是按照BGR 读取的. sanpang = cv2. imread( . /cv2_change_head/j. jpg )# 展示图片plt. imshow(sanpang[::,::,::-1])# 接下来开始识别图片中的人脸cascade = cv2. CascadeClassifier()# 加载现有算法cascade. load( . /cv2_change_head/haarcascade_frontalface_default. xml )# 使用人脸识别的类进行识别face = cascade. detectMultiScale(sanpang)# 此时 输出的对象是一个二维数组, 包含四个坐标表示脸在图片中的位置(横坐标, 纵坐标, 横坐标拉长度, 纵坐标拉伸长度伸,)face# 获取小狗的图片dog = cv2. imread( . /cv2_change_head/dog. jpg )# 从新将小狗的图片做成人脸尺寸相同的尺寸small_dog = cv2. resize(dog,(72,72))small_dog# 替换三胖的脸为狗照片for (h, w, p, p) in face:  sanpang[w:w+p, h:h+p] = small_dogplt. imshow(sanpang[::,::,::-1])案例2: 合影中的人脸获取 12345678910111213141516171819202122# 识别多张人脸import numpy as npimport matplotlib. pyplot as pltimport cv2img = cv2. imread( . /cv2_change_head/WechatIMG259. jpeg )plt. imshow(img)img. shapecc = cv2. CascadeClassifier()cc. load( . /cv2_change_head/haarcascade_frontalface_default. xml )faces = cc. detectMultiScale(img)facesplt. figure(figsize=(10,10))plt. imshow(img[258:258+99,393:393+99])i = 0for face in faces:  (h,w,p,p) = face  if p&gt;90:    plt. subplot(6,3,i+1)    i+=1    plt. imshow(img[w:w+p,h:h+p,::-1])K-近邻算法（KNN）: k-近邻算法原理: 简单地说，K-近邻算法采用测量不同特征值之间的距离方法进行分类。  优点：精度高、对异常值不敏感、无数据输入假定。 缺点：时间复杂度高、空间复杂度高。 适用数据范围：数值型和标称型。工作原理:  存在一个样本数据集合，也称作训练样本集，并且样本集中每个数据都存在标签，即我们知道样本集中每一数据 与所属分类的对应关系。输人没有标签的新数据后，将新数据的每个特征与样本集中数据对应的 特征进行比较，然后算法提取样本集中特征最相似数据（最近邻）的分类标签。一般来说，我们 只选择样本数据集中前K个最相似的数据，这就是K-近邻算法中K的出处,通常K是不大于20的整数。 最后 ，选择K个最相似数据中出现次数最多的分类，作为新数据的分类。  回到前面电影分类的例子，使用K-近邻算法分类爱情片和动作片。有人曾经统计过很多电影的打斗镜头和接吻镜头，下图显示了6部电影的打斗和接吻次数。假如有一部未看过的电影，如何确定它是爱情片还是动作片呢？我们可以使用K-近邻算法来解决这个问题。 使用K-近邻算法分类爱情片和动作片。有人曾经统计过很多电影的打斗镜头和接吻镜头，下图显示了6部电影的打斗和接吻次数。假如有一部未看过的电影，如何确定它是爱情片还是动作片呢？我们可以使用K-近邻算法来解决这个问题。 首先我们需要知道这个未知电影存在多少个打斗镜头和接吻镜头，上图中问号位置是该未知电影出现的镜头数图形化展示，具体数字参见下表。 即使不知道未知电影属于哪种类型，我们也可以通过某种方法计算出来。首先计算未知电影与样本集中其他电影的距离，如图所示。 现在我们得到了样本集中所有电影与未知电影的距离，按照距离递增排序，可以找到K个距 离最近的电影。假定k=3，则三个最靠近的电影依次是California Man、He’s Not Really into Dudes、Beautiful Woman。K-近邻算法按照距离最近的三部电影的类型，决定未知电影的类型，而这三部电影全是爱情片，因此我们判定未知电影是爱情片。 欧几里得距离(Euclidean Distance): 欧氏距离是最常见的距离度量，衡量的是多维空间中各个点之间的绝对距离。公式如下 12345678# 一维空间距离# x1=1, x2=10, 距离=10-1=9# 二维空间距离# 两个点 x(1,3), y(-1,-2)# *((1+1)^2 + (3+2)^2)^0. 5# 三维空间距离# x(1,2,3) , y(-1,-2,-3)# ((1+1)^2 + (2+2)^2 + (3+3)^2)^0. 5在scikit-learn库中使用k-近邻算法: 12# 安装 scikit-learn 库pip install sklearn   分类问题：from sklearn. neighbors import KNeighborsClassifier     回归问题：from sklearn. neighbors import KNeighborsRegressor  例一: 近邻分类算法 - 用身高、体重、鞋子尺码的特征数据判断性别: 1234567891011121314151617181920212223242526272829303132333435363738import numpy as npimport pandas as pdimport matplotlib. pyplot as pltfrom pandas import Series, DataFrame%matplotlib inline# 导入近邻分类库 KNeighborsClassifier from sklearn. neighbors import KNeighborsClassifier# n_neighbors 参数表示近邻的个数, 一般是一个奇数, 因为奇数可以方便的确定如何给这个点分类# weight 参数选择默认的 uniform ，意味着所有最近邻样本权重都一样，在做预测时一视同仁。如果是 distance ，则权重和距离成反比例，即距离预测目标更近的近邻具有更高的权重，这样在预测类别或者做回归时，更近的近邻所占的影响因子会更加大。# distance 是一个比较好的选择。如果用 distance 发现预测的效果的还是不好，可以考虑自定义距离权重来调优这个参数knn = KNeighborsClassifier(n_neighbors=3)# 创建训练样本数据# 为了提高准确率, 更多的一些属性可以提高准确率, 比如, 三围, 心率, 荷尔蒙等X_train = np. array([[175, 60, 42],           [160, 48, 36],           [226, 135, 49],           [155, 50, 35],           [159, 70, 39]])# 每一个样本集的数据都有一个对应的标签y_train = np. array(['男','女','男','女','男'])# 训练数据knn. fit(X_train, y_train)# 创建预测数据 男, 男, 女x_test = np. array([[171,65,40],          [178,65,42],          [157,45,36]])y_ = knn. predict(x_test)y_#输出结果如下:# array(['男', '男', '女'], #   dtype='&lt;U1')例二: 利用knn紧邻分类算法预测 鸢尾(花) 的种类: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104import numpy as npimport pandas as pdimport matplotlib. pyplot as pltfrom pandas import Series, DataFrame%matplotlib inlineimport sklearn. datasets as datasets# 获取 鸢尾(花) 数据iris_data = datasets. load_iris()data = iris_data. datafeature_names = iris_data. feature_namestarget_names = iris_data. target_namestarget = iris_data. target# 绘制数据样本的特征趋势图# 可以看出来左中右是三种花的样本数据DataFrame(data). plot()# 拆分训练数据from sklearn. model_selection import train_test_split# test_size 表示 测试数据占总数据的比例# 一般训练数据占大头, 测试数据占小头# 拆分后的训练数据是被打乱的, 但是 X_train, y_train 的样本数据和标签还是一一对应的X_train, x_test, y_train, y_test = train_test_split(data,target, test_size=0. 1)data. shape, X_train. shape, x_test. shape, y_train# 声明算法knn = KNeighborsClassifier()# 训练数据knn. fit(X_train, y_train)# 进行预测y_ = knn. predict(x_test)# 比较预测结果和真实结果y_, y_test# 查看预测的准确率# 因为训练时使用的是 训练数据 knn. fit(X_train, y_train), 所以# 将测试数据 x_test, y_test 放入训练好的模型中, 就可以获得结果knn. score(x_test, y_test)# 输出如下: (可能每次的预测会有所不同)# 0. 93333333333333335# 返过来给训练数据打分,也不一定是百分百的正确. # 因为用于训练的真实数据中可能有两种花的数据完全一样的情况, 所以训练出来的模型并不能100%正确预测一个数据是属于哪一种花knn. score(X_train,y_train)#输出#0. 97777777777777775# 绘制散点图# 因为散点图形只需要x, y 两个坐标, 所以切分原始数据的前两列获得花萼长度和宽度即可x = data[:,:2]y = targetx,y# 使用散点图展示数据# x 是花萼的长度, y 是花萼的宽度# c 是用标签zhi作为分类颜色的方法plt. scatter(x=x[:,0], y=x[:,1], c=y_train, cmap='rainbow')# 将分类和预测结果在图形上展示出来# 方法是将画布上所有的点都找出来, 然后对这些点全部进行预测# 所有的点就是一个无限细化的网格 # 先确定范围 获得花萼长度的最大值最小值, 获得花萼宽度的最大值最小值xmin,xmax = X_train[:,0]. min()-0. 5, X_train[:,0]. max()+0. 5ymin,ymax = X_train[:,1]. min()-0. 5, X_train[:,1]. max()+0. 5# 切割数据x = np. arange(xmin,xmax,0. 05)y = np. arange(ymin,ymax,0. 05)# 创建网格数据# xx 相当于1234,1234 . . ; yy相当于 4444,3333 . . . xx,yy = np. meshgrid(x,y)# 然后再拼接成最后的网格数组x_test = np. c_[xx. ravel(),yy. ravel()]# 重新训练 -- 因为之前是用四列(四个特征值)进行训练的, 这次是用两个特征值进行训练. # 否则会会因为训练时用四个特征值, 预测时只输入两个特征值二knn. fit(X_train, y_train)# 开始预测y_ = knn. predict(x_test)# 导入颜色映射包from matplotlib. colors import ListedColormap# 定义映射颜色colormap=ListedColormap(['red','blue','green'])# 展示预测数据散点图plt. scatter(x_test[:,0],x_test[:,1], c=y_)# 叠加上测试用的数据散点图plt. scatter(X_train[:,0],X_train[:,1], c=target, cmap=colormap)# 另一种方法画图 linspacex = np. linspace(xmin,xmax,500)y = np. linspace(ymin,ymax,300)xx,yy = np. meshgrid(x,y)x_test = np. c_[xx. ravel(),yy. ravel()]knn. fit(X_train, y_train)y_ = knn. predict(x_test)# 用pcolormesh画图plt. pcolormesh(xx,yy,y_. reshape(xx. shape),cmap='rainbow')# contourf() 也可以画图# plt. contourf(xx,yy,y_. reshape(xx. shape),cmap='rainbow')plt. scatter(X_train[:,0],X_train[:,1], c=target)  例三: 近邻回归算法 - 自己制造数据做正弦回归预测: 12345678910111213141516171819202122232425262728293031323334353637383940# 用近邻算法进行回归预测import numpy as npimport pandas as pdimport matplotlib. pyplot as pltfrom pandas import Series, DataFrame%matplotlib inlinefrom sklearn. neighbors import KNeighborsRegressor# 生成训练数据# 创建80个随机数, 一共两维, 第一维80个,第二维1个, 所有生成的随机数是0~1# *10 表示生成1~10之间的随机数X_train = np. random. rand(80,1)*10 X_train = np. sort(X_train, axis=0)y_train = np. sin(X_train)X_train, y_train# 为数据增加噪声# 创建40个正太分布的数据作为加噪的数据# 因为randn(40,1) 表示返回第一维是40个, 第二维是1个, 返回所有数据是0~1# *0. 3 表示对返回数据 是 0~0. 3y_train[::2] += np. random. randn(40,1)*0. 3 # 画出训练数据的散点图plt. scatter(X_train,y_train)# 生成模型，并训练数据knn = KNeighborsRegressor()knn. fit(X_train,y_train)# 使用模型，预测数据# 在x轴上从0~10 找1000个点# 并 reshape 成一维是1000个,二维是1个的二维数组x_test = np. linspace(0,10,1000). reshape(1000,1) y_ = knn. predict(x_test)y_# 绘图显示数据plt. scatter(X_train,y_train, color='red')plt. plot(x_test,y_, color='green', alpha=0. 8) 例四: 近邻分类算法预测人类动作识别: 步行，上楼，下楼，坐着，站立和躺着 数据采集每个人在腰部穿着智能手机，进行了六个活动（步行，上楼，下楼，坐着，站立和躺着）。采用嵌入式加速度计和陀螺仪，以50Hz的恒定速度捕获3轴线性加速度和3轴角速度，来获取数据 12345678910111213141516171819202122232425262728293031323334353637383940import numpy as npimport pandas as pdimport matplotlib. pyplot as pltfrom pandas import Series, DataFrame%matplotlib inlineX_train = np. load('. /x_train. npy')y_train = np. load('. /y_train. npy')x_test = np. load('. /x_test. npy')y_test = np. load('. /y_test. npy')X_train. shape, y_train. shape, x_test. shape,y_test. shape#输出: ((7352, 561), (7352,), (2947, 561), (2947,))# 7353 条训练数据(个样本数据) # 561列(个特征值)# 2947 条测试数据Series(y_train). unique()#输出: array([5, 4, 6, 1, 3, 2])label = {1:'walking',     2:'upstairs',     3:'downstairs',     4:'sitting',     5:'standing',     6:'laying'}# 预测并打分knn = KNeighborsClassifier()knn. fit(X_train, y_train). score(x_test,y_test)# 输出0. 9015948422124193# 绘制20个小图在一个大图中# 设置整个画布的宽高# i*100 的意思是每个100个, 获取一个数据plt. figure(figsize=(4*4,5*5)) for i in range(20):  axes = plt. subplot(5,4,(i+1)) # 设置5行4列, 目前是第i+1行  axes. plot(x_test[i*100])  axes. set_title( true: %s\n predict: %s  %(label[y_test[i*100]], label[y_[i*100]]))   例五: 近邻分类算法识别手写数字: 123456789101112131415161718192021222324252627282930313233343536373839404142434445# 导包import numpy as npimport pandas as pdfrom pandas import Series, DataFrameimport matplotlib. pyplot as plt%matplotlib inlinefrom sklearn. neighbors import KNeighborsClassifierfrom sklearn. model_selection import train_test_split# 获得图片数据集data_list = []target_list = []for i in range(10):  for j in range(1,501):    # 读取图片数据    img_data = plt. imread('. . /exercise/data/%d/%d_%d. bmp' %(i,i,j))    data_list. append(img_data)    target_list. append(i)        # 对数据进行整理data = np. array(data_list)target = np. array(target_list)# 对图片数据降一维data = data. reshape(5000,-1)# 将data 拆分成训练数据和测试数据X_train, x_test, y_train, y_test = train_test_split(data, target, test_size=0. 02)# 建立模型并进行训练knn = KNeighborsClassifier()knn. fit(X_train,y_train). score(x_test,y_test)# 预测y_ = knn. predict(x_test)# 绘制前100个数字plt. figure(figsize=(2*10,3*10))for i in range(100):  axes = plt. subplot(10,10,(i+1))  # 这里需要 reshape 是因为读片读出来后是 一个28*28 的一维数组, 需要转成28*28的二维数组  axes. imshow(x_test[i]. reshape((28,28)))  axes. set_title('true: %d\npredict: %d' %(y_test[i],y_[i])) 例六: 近邻分类算法预测年收入是否大于50k: 12345678910111213141516171819202122232425262728293031323334import numpy as npimport pandas as pdfrom pandas import Series, DataFramefrom sklearn. neighbors import KNeighborsClassifierfrom sklearn. model_selection import train_test_splitdf = pd. read_csv('. . /data/adults. txt')#获取年龄、教育程度、职位、每周工作时间作为机器学习数据#获取薪水作为对应结果data = df[['age','workclass','education','marital_status','occupation','race','sex','hours_per_week','native_country']]. copy()target = df[['salary']]. copy()# 将非数字类型的元素转成数字类型cols = ['workclass','education', 'marital_status', 'occupation', 'race', 'sex', 'native_country']for column in cols:  # 对每一列去重  uni = data[column]. unique()  def convert(item):    index = np. argwhere(uni == item)[0,0]    return index  data[column] = data[column]. map(convert)  # 训练数据和预测数据X_train, x_test, y_train, y_test = train_test_split(data, target, test_size=0. 01)knn = KNeighborsClassifier()knn. fit(X_train,y_train). score(x_test,y_test)# 输出: 0. 803680981595092# 不叫预测结果和真实结果y_ = knn. predict(x_test[::10])y_y_test[::10]. values. reshape(-1)例七: 近邻分类算法预测乳腺癌: 12345678910111213141516171819202122232425import numpy as npimport pandas as pdfrom pandas import Series, DataFrameimport matplotlib. pyplot as plt%matplotlib inlinefrom sklearn. neighbors import KNeighborsClassifierfrom sklearn. model_selection import train_test_splitimport sklearn. datasets as datasets# 获取数据cancer = datasets. load_breast_cancer()data = cancer. datatarget = cancer. targetdata. shape, target. shape# 分割数据X_train, x_test, y_train, y_test = train_test_split(data, target, test_size=0. 2)# 预测,打分knn = KNeighborsClassifier()knn. fit(X_train,y_train)y_ = knn. predict(x_test)y_knn. score(x_test,y_test)普通线性回归 - LinearRegression: 【关键词】最小二乘法，线性 例一: 用线性回归预测糖尿病的得病可能性 12345678910111213141516171819202122232425262728293031323334353637import numpy as npimport pandas as pdfrom pandas import Series, DataFrameimport matplotlib. pyplot as plt%matplotlib inlinefrom sklearn. linear_model import LinearRegressionimport sklearn. datasets as datasetsdiabetes = datasets. load_diabetes()data = diabetes. datatarget = diabetes. target# 抽取训练数据和预测数据linear = LinearRegression()from sklearn. model_selection import train_test_splitX_train, x_test, y_train, y_test = train_test_split(data, target, test_size=0. 2)# 训练和预测linear. fit(X_train, y_train)y_ = linear. predict(x_test)y_# 画图, 比较预测结果和真实结果plt. plot(y_)plt. plot(y_test)# 画图, 查看散点图plt. scatter(x_test[:,2],y_test)plt. scatter(x_test[:,2],y_)# 画图, 查看所有测试数据属性之间的关系from pandas. plotting import scatter_matrixdf_test = pd. DataFrame(x_test, columns=diabetes. feature_names)scatter_matrix(df_test, alpha=0. 2, figsize=(10, 10), diagonal='kde') 岭回归 Ridge: 如果数据的特征比样本点还多应该怎么办？是否还可以使用线性回归和之前的方法来做预测？答案是否定的，即不能再使用前面介绍的方法。这是因为输入数据的矩阵X不是满秩矩阵。非满秩矩阵在求逆时会出现问题。为了解决这个问题，统计学家引入了岭回归（ridge regression)的概念 1. 岭回归可以解决特征数量比样本量多的问题 2. 岭回归作为一种缩减算法可以判断哪些特征重要或者不重要，有点类似于降维的效果 3. 缩减算法可以看作是对一个模型增加偏差的同时减少方差 岭回归用于处理下面两类问题： 1. 数据点少于变量个数 2. 变量间存在共线性（最小二乘回归得到的系数不稳定，方差很大） 一个简单的岭回归的例子: 比较岭回归和普通线性回归的预测差异: 12345678910111213141516171819202122232425262728293031import numpy as npimport pandas as pdfrom pandas import Series, DataFrameimport matplotlib. pyplot as plt%matplotlib inlinefrom sklearn. linear_model import LinearRegressionfrom sklearn. linear_model import Ridge# 训练样本集, 3个样本, 5个属性X_train = np. array([[1,1,1,1,1],[2,0,1,0,-1],[0,3,0,4,-2]])# 训练targety_train = np. array([1,2,3])x_train = np. array([[2,2,2,2,2]])# 使用普通线性回归进行预测linear = LinearRegression()linear. fit(X_train, y_train)y_lin = linear. predict(x_train)y_lin# 输出: array([0. 57142857])# 使用岭回归进行预测ridge = Ridge(alpha=1)ridge. fit(X_train, y_train)y_rid = ridge. predict(x_train)y_rid# 输出: array([0. 86695279])# 可以判断出来岭回归更接近1, 更准确扩展知识 - 寻找岭回归的最佳alpha值 1234567891011121314151617181920212223242526272829# 扩展知识 -- 寻找最佳的alpha# 参数 alpha 就是 lambda, 当alpha 等于0 时, 岭回归就是线性回归# 创建测试数据# 创建一个10 * 10 的矩阵 X_train = 1 / (np. arange(1,11) + np. arange(0,10). reshape(10,1))y_train = np. ones(10)# 生成 200个alpha 值alpha_num = 200# 以log形式进行切割, 和 linspace 类似, 以10的-10次方 到 10的-2次方切分200份alphas = np. logspace(-10, -1, alpha_num)# 将生成的200个 alpha 值带入到 岭回归对象中w = []ridge = Ridge(fit_intercept=False) # fit_intercept 是截距, 这里需要忽略for a in alphas:  ridge. set_params(alpha=a)  ridge. fit(X_train, y_train)  # 训练后, 岭回归对象可以得到一个系数, 这个系数记录在 ridge. coef_ 中   # 将所有系数放到一个列表中, 会个得两百个系数  w. append(ridge. coef_)  # 用 w 和 alpha 画图plt. figure(figsize=(12,9))axes = plt. subplot(111)axes. plot(alphas,w)axes. set_xscale('log') "
    }, {
    "id": 26,
    "url": "http://localhost:4000/Django-Rest-From-Zero-To-Hero/",
    "title": "Django Rest From Zero to Hero",
    "body": "2020/07/07 - 让我们看看如何使用Django Rest Framework（DRF）为我们的应用创建RESTFul API，DRF是一个用于基于Django模型快速构建RESTful API的应用程序。 我将带领大家一步一步的了解 DRF 并使用 DRF 实现一个电商项目。 Django Rest 框架[TOC] 新的开发理念:  Django Rest 不等于 Django Django Rest是不同的开发模式, 完全采用接口方式开发,完全不用管前端的内容,包括模板等.  传统开发方式: 前后端绑在一起, MVC, MTV) 新的开发模式 :Django Rest + ionic. js /react js/ vue. js 好处是, 如果有一天公司不需要用 python, 而改用java 或者php, 那么前端部分不需要做任何修改. HTTP 动词: rest 接口是 客户端通过 http 的 动词 和 rest 接口进行交互 具体的动词如下: 常用的HTTP动词有下面五个（括号里是对应的SQL命令）。  GET（查询）：从服务器取出资源（一项或多项）。 POST（创建）：在服务器新建一个资源。 PUT（更新）：在服务器更新资源（客户端提供改变后的完整资源） PATCH（部分更新）：在服务器更新资源（客户端只提供改变的属性）。 DELETE（删除）：从服务器删除资源。HTTP 动词 举例:  GET /zoos：列出所有动物园 POST /zoos：新建一个动物园 GET /zoos/ID：获取某个指定动物园的信息 PUT /zoos/ID：更新某个指定动物园的信息（提供该动物园的全部信息 PATCH /zoos/ID：更新某个指定动物园的信息（提供该动物园的部分信息） DELETE /zoos/ID：删除某个动物园 GET /zoos/ID/animals：列出某个指定动物园的所有动物 DELETE /zoos/ID/animals/ID：删除某个指定动物园的指定动物其他:    HEAD：获取资源的元数据。   1234HTTP 200 OKAllow: GET, PUT, PATCH, DELETE, HEAD, OPTIONSContent-Type: application/jsonVary: Accept      OPTIONS：获取信息，关于资源的哪些属性是客户端可以改变的。   123456789101112131415161718192021222324252627282930313233343536373839404142434445{   name :  User Instance ,   description :  API endpoint that allows users to be viewed or edited.  ,   renders : [     application/json ,     text/html   ],   parses : [     application/json ,     application/x-www-form-urlencoded ,     multipart/form-data   ],   actions : {     PUT : {       url : {         type :  field ,         required : false,         read_only : true,         label :  Url       },       username : {         type :  string ,         required : true,         read_only : false,         label :  Username ,         help_text :  Required. 150 characters or fewer. Letters, digits and @/. /+/-/_ only.  ,         max_length : 150      },       email : {         type :  email ,         required : false,         read_only : false,         label :  Email address ,         max_length : 254      },       groups : {         type :  field ,         required : false,         read_only : false,         label :  Groups ,         help_text :  The groups this user belongs to. A user will get all permissions granted to each of their groups.        }    }  }}   服务端响应状态码:: 客户端和服务端进行rest 交互, 服务端会给一个反馈状态码. 下面是状态码汇总 状态码汇总 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647  +------+-------------------------------+--------------------------+  | Code | Reason-Phrase         | Defined in. . .       |  +------+-------------------------------+--------------------------+  | 100 | Continue           | Section 6. 2. 1      |  | 101 | Switching Protocols      | Section 6. 2. 2      |  | 200 | OK              | Section 6. 3. 1      |  | 201 | Created            | Section 6. 3. 2      |  | 202 | Accepted           | Section 6. 3. 3      |  | 203 | Non-Authoritative Information | Section 6. 3. 4      |  | 204 | No Content          | Section 6. 3. 5      |  | 205 | Reset Content         | Section 6. 3. 6      |  | 206 | Partial Content        | Section 4. 1 of [RFC7233] |  | 300 | Multiple Choices       | Section 6. 4. 1      |  | 301 | Moved Permanently       | Section 6. 4. 2      |  | 302 | Found             | Section 6. 4. 3      |  | 303 | See Other           | Section 6. 4. 4      |  | 304 | Not Modified         | Section 4. 1 of [RFC7232] |  | 305 | Use Proxy           | Section 6. 4. 5      |  | 307 | Temporary Redirect      | Section 6. 4. 7      |  | 400 | Bad Request          | Section 6. 5. 1      |  | 401 | Unauthorized         | Section 3. 1 of [RFC7235] |  | 402 | Payment Required       | Section 6. 5. 2      |  | 403 | Forbidden           | Section 6. 5. 3      |  | 404 | Not Found           | Section 6. 5. 4      |  | 405 | Method Not Allowed      | Section 6. 5. 5      |  | 406 | Not Acceptable        | Section 6. 5. 6      |  | 407 | Proxy Authentication Required | Section 3. 2 of [RFC7235] |  | 408 | Request Timeout        | Section 6. 5. 7      |  | 409 | Conflict           | Section 6. 5. 8      |  | 410 | Gone             | Section 6. 5. 9      |  | 411 | Length Required        | Section 6. 5. 10      |  | 412 | Precondition Failed      | Section 4. 2 of [RFC7232] |  | 413 | Payload Too Large       | Section 6. 5. 11      |  | 414 | URI Too Long         | Section 6. 5. 12      |  | 415 | Unsupported Media Type    | Section 6. 5. 13      |  | 416 | Range Not Satisfiable     | Section 4. 4 of [RFC7233] |  | 417 | Expectation Failed      | Section 6. 5. 14      |  | 426 | Upgrade Required       | Section 6. 5. 15      |  | 500 | Internal Server Error     | Section 6. 6. 1      |  | 501 | Not Implemented        | Section 6. 6. 2      |  | 502 | Bad Gateway          | Section 6. 6. 3      |  | 503 | Service Unavailable      | Section 6. 6. 4      |  | 504 | Gateway Timeout        | Section 6. 6. 5      |  | 505 | HTTP Version Not Supported  | Section 6. 6. 6      |  +------+-------------------------------+--------------------------+Json 和 python 字典的区别:: 从形式来讲，这两者的确很像，都为 key : value 的形式    简单来说，python 字典的数据格式就json的数据格式。  但本质上来讲，字典是一种数据结构，json是一种格式；字典有很多内置函数，有多种调用方法，而json是数据打包的一种格式，并不像字典具备操作性，并且是格式就会有一些形式上的限制，比如json的格式要求必须且只能使用双引号作为key或者值的边界符号，不能使用单引号，而且“key”必须使用边界符（双引号），但字典就无所谓了。 　　形式上的相近也让python提供了json. loads()转换函数，方便json数据的调用。 　　使用方法如下： 一段Python 字典 12345ab = { 'liu' :'liu@python. info',     'Larry' : 'larry@wall. org',     'Matsumoto' : 'matz@ruby-lang. org',     'Spammer' : 'spammer@hotmail. com' }一段JSON 123456789101112{  clusterInfo : {   id :1324053971963,   startedOn :1324053971963,   state : STARTED ,   resourceManagerVersion : 0. 23. 1-SNAPSHOT ,   resourceManagerVersionBuiltOn : Tue Dec 13 22:12:48 CST 2011 ,   hadoopVersion : 0. 23. 1-SNAPSHOT ,   hadoopVersionBuiltOn : Tue Dec 13 22:12:26 CST 2011  }}Quick start: (1)环境准备: 新建一个项目目录 tutorial, 并进入该目录  django-admin startproject tutorial . 这里后面加了一个点, 作用是把当前目录作为项目目录. 而如果不加点, 那么就会在当前目录下再创建一个项目目录.  django-admin. py startapp quickstart 是在项目目录的主目录下创建app. 123456789101112131415161718192021222324252627282930313233343536# Create the project directorymkdir tutorialcd tutorial# Create a virtualenv to isolate our package dependencies locallyvirtualenv envsource env/bin/activate# Install Django and Django REST framework into the virtualenvpip3. 7 install djangorestframeworkpip3. 7 install markdownpip3. 7 install django==2. 0pip3. 7 install django-cors-headers==2. 1. 0pip3. 7 install django-crispy-forms==1. 7. 0pip3. 7 install django-filter==1. 1. 0pip3. 7 install django-haystack==2. 6. 1pip3. 7 install pillow==4. 3. 0pip3. 7 install httpie==0. 9. 9pip3. 7 install django-guardian# 创建新的项目和应用django-admin startproject tutorial .  # Note the trailing '. ' charactercd tutorialdjango-admin. py startapp quickstartcd . . # 第一次进行数据库同步python3. 7 manage. py migrate# 创建超级用户python3. 7 manage. py createsuperuser(2)编写 Serializers: 序列器的两大作用:  作为数据验证 做数据转换翻译在应用目录下新建 序列化文件 tutorial/quickstart/serializers. py  Serializers 相当于在模型上面的一个封装 定义要序列化的数据模型以及要展示的字段12345678910111213141516171819202122# 导入两个models User 和 Groupfrom django. contrib. auth. models import User, Group# 引入序列化器from rest_framework import serializers# 定义一个序列化器, 继承自 超链接模型序列器基类. 这个序列器是基于模型的,而且会产生url连接# rest 一共提供三种序列器, 这是其中一种. class UserSerializer(serializers. HyperlinkedModelSerializer):  class Meta:   	# 定义模型和要序列化的字段    model = User    # url 是固定名称, 是序列器模型固定用法.     # 如果需要在用户中显示用户所属组, 那么就加一个 'groups'    fields = ('id', 'url', 'username', 'email', 'groups')class GroupSerializer(serializers. HyperlinkedModelSerializer):  class Meta:    model = Group    # 如果想要在组中显示所有的用户, 就加一个 'user_set'    fields = ('id', 'url', 'name', 'user_set')(3)创建视图类: 视图类, 视图集, 用于将 quickstart/views. py 123456789101112131415161718192021222324252627282930313233from django. shortcuts import render# 再次导入数据模型from django. contrib. auth. models import User, Group# 导入 rest 的 viewsets 基类from rest_framework import viewsets# 导入刚刚建立的两个序列化类from tutorial. quickstart. serializers import UserSerializer, GroupSerializer# Create your views here. # 创建一个视图类, 继承rest ModelViewSet 基类class UserViewSet(viewsets. ModelViewSet):       查看, 编辑用户数据的 API 接口 &lt;br&gt;  A viewset that provides default `create()`, `retrieve()`, `update()`,  `partial_update()`, `destroy()` and `list()` actions.        # queryset 是固定用法, 定义获取的数据过滤和排序  queryset = User. objects. all(). order_by('-date_joined')  # 定义使用哪个序列化器  serializer_class = UserSerializerclass GroupViewSet(viewsets. ModelViewSet):       查看, 编辑组数据的 API 接口 &lt;br&gt;  A viewset that provides default `create()`, `retrieve()`, `update()`,  `partial_update()`, `destroy()` and `list()` actions.        queryset = Group. objects. all()  serializer_class = GroupSerializer(4)对 url 和 setting 进行设置: tutorial/urls. py 12345678910111213141516171819202122232425from django. contrib import adminfrom django. urls import pathfrom django. conf. urls import url, include# 导入 rest 的 路由模块from rest_framework import routers# 导入 刚刚建立的视图模块from tutorial. quickstart import views# 初始化一个 rest 路由. 记得最后的括号别忘了router = routers. DefaultRouter()# 将视图中的用户和组注册到 rest 路由中# 第一个参数是放在url 地址中的 path 部分中的路径router. register(r'users', views. UserViewSet)router. register(r'groups', views. GroupViewSet)urlpatterns = [  path('admin/', admin. site. urls),  # 这里将首页根路径的请求转发到 rest 路由的urls进行处理  url(r'^', include(router. urls)),  # 将登陆/登出 的请求转发到 rest_framework 的 urls进行处理  url(r'^api-auth/', include('rest_framework. urls', namespace='rest_framework')),]tutorial/settings. py 将 下面代码放到 INSTALLED_APPS 中 12345678910111213141516INSTALLED_APPS = [	. . .   'rest_framework',]REST_FRAMEWORK = {  # 设定默认显示分页  'DEFAULT_PAGINATION_CLASS': 'rest_framework. pagination. PageNumberPagination',  'PAGE_SIZE': 2,  # 设定权限, 只有管理员才能访问API 页面, 否则 403 forbidden 	# 注意这里的CLASSES 是复数  'DEFAULT_PERMISSION_CLASSES': [    'rest_framework. permissions. IsAdminUser'  ]}(5)测试API: 运行服务 1python3. 7 manage. py runserver测试API 方法一:  要安装 httpie pip3. 7 install httpie -a 参数可以提供用户名和密码的验证支持 -p 输出内容.      ‘H’ request headers   ‘B’ request body   ‘h’ response headers   ‘b’ response body   123http -a root:@****** http://127. 0. 0. 1:8000/users/http -a root:@****** http://127. 0. 0. 1:8000/users/?format=api # 通过内容协商来定义返回的格式. 默认是jsonhttp -a root:@****** --print hbHB http://127. 0. 0. 1:8000/helloworld/users/方法二: 1dalong$ curl -H 'Accept: application/json; indent=4' -u root:@****** http://127. 0. 0. 1:8000/users/方法三:  浏览器直接访问: http://127. 0. 0. 1:8000/ 如果需要让外网访问, 需要做两件事情: (1): 启动服务时, 设置IP 地址为 0. 0. 0. 0 1python3. 7 manage. py runserver 0. 0. 0. 0:8001(2): 在 settings. py 中 设置 1ALLOWED_HOSTS = ['*']snippet代码片段分享项目: 创建 snippet app: 在原有项目中创建一个新的app 也可以创建一个新的项目. 1python3. 7 manage. py startapp snippetsapp将新的app 放到 settings. py中 12345INSTALLED_APPS = [	. . .   'rest_framework',  'snippetsapp. apps. SnippetsappConfig',]创建一个模型: models. py 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556from django. db import modelsfrom pygments. lexers import get_all_lexersfrom pygments. styles import get_all_stylesfrom pygments. lexers import get_lexer_by_namefrom pygments. formatters. html import HtmlFormatterfrom pygments import highlightLEXERS = [item for item in get_all_lexers() if item[1]]LANGUAGE_CHOICES = sorted([(item[1][0], item[0]) for item in LEXERS])STYLE_CHOICES = sorted((item, item) for item in get_all_styles())class Snippet(models. Model):  created = models. DateTimeField(auto_now_add=True)  title = models. CharField(max_length=100, blank=True, default='')  code = models. TextField()  # 行号  linenos = models. BooleanField(default=False)  # choices 相当于数据库中的 enum 类型字段, 多个选项中选一个  language = models. CharField(choices=LANGUAGE_CHOICES, default='python', max_length=100)  style = models. CharField(choices=STYLE_CHOICES, default='friendly', max_length=100)  # auth. User 是特殊用法, 一般情况都是需要先 import 需要的模块, 在把模块放在第一个参数  # related_name 就是用来定义在 serializers. py 中指定  owner = models. ForeignKey('auth. User', related_name='snippets', on_delete=models. CASCADE, null=True, blank=True)  # 新加一个字段, 让用户提交代码高亮显示. 默认是空  # 后面的save() 方法用于将 code 的代码进行渲染高亮, 然后保存到这个字段中  highlighted = models. TextField(null=True, blank=True)  class Meta:    ordering = ('created',)  # save方法继承自基类, 当有数据要保存时,会自动执行该方法  def save(self, *args, **kwargs):           Use the `pygments` libaray to create a highlighted HTML representation of the code snippet           # get_lexer_by_name() 方法通过传入的参数(比如: python) 获取词法对象    # self. language 是字段名 , 默认值是 python    lexer = get_lexer_by_name(self. language)    # 如果字段 linenos 是 True, 那么将'table' 保存到linenos 变量中, 否则将False 保存到变量中.     # 结果, 变量中就是 'table' 或者是 False    linenos = self. linenos and 'table' or False    # 如果title 字段有值, 那么就将 {. . . } 放到options字段中, 否则将空字典放到 options变量中    options = self. title and {'title': self. title} or {}    # HtmlFormatter() 方法将源代码转化成HTML 的格式    # 第一个参数是一个风格. 数据来自字段style(比如: friendly), 第二个字段确定是否显示行号    # full=True 表示显示所有代码.     formatter = HtmlFormatter(style=self. style, linenos=linenos, full=True, **options)    # 用highlight() 方法把 code 代码转换成高亮显示的代码, 并保存到highlighted 字段中    # 第一个参数是原代码, 第二个参数是词法分析器对象, 第三个参数是格式化的风格    self. highlighted = highlight(self. code, lexer, formatter)    # 执行父类的保存    super(Snippet, self). save(*args, **kwargs)进行数据迁移 123python3. 7 manage. py makemigrationspython3. 7 manage. py migrate创建 serializers. py 文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778from rest_framework import serializersfrom . models import Snippet, LANGUAGE_CHOICES, STYLE_CHOICESfrom django. contrib. auth. models import Userclass UserSerializer(serializers. ModelSerializer):  # view_name 是反向关联, 值需要在 urls. py 中同时设置  # HyperlinkedRelatedField 是生成外键关联的链接  snippets = serializers. HyperlinkedRelatedField(many=True, view_name='snippet-detail', read_only=True)  class Meta:    model = User    # 这里的 snippets 是在 models. py 中 owner = models. ForeignKey('auth. User', related_name='snippets' . . . . 定义的    fields = ('id', 'username', 'snippets', )# 重构序列化代码# 这是第三种序列器 ModelSerializer, 其他两种是 Serializer, 和 HyperLinkedModelSerializer# HyperLinkedModelSerializer 比 ModelSerializer 多了一个 url 功能class SnippetSerializer(serializers. ModelSerializer):  # owner 原本对应 owner_id. 下面这行代码让 owner_name 可以指定 是 user 表中的任意字段.   # ReadonlyField 设置了只能显示, 不能创建和修改.   owner_name = serializers. ReadOnlyField(source='owner. username')  # highlight 并不是一个模型中的字段, 而是一个超链接, 链接到路由 snippet-highlight 中  # HyperlinkedIdentityField 是超链接字段.  Identity 说明要带一个ID  # HyperlinkedIdentityField 是生成由id生成的链接  # format='html' 表示要渲染成html  highlight = serializers. HyperlinkedIdentityField(view_name='snippet-highlight', format='html')  class Meta:    model = Snippet    fields = ('id', 'title', 'code', 'highlighted', 'linenos', 'language', 'style', 'owner', 'owner_name', 'highlight',)# # 这里的序列器继承自 Serializer , 这里的序列器基类和 HyperlinkedModelSerializer 类似, 只是更加基本# # 序列器中定义的每个字段都是和 models. py 中的模型中的字段是对应的. 定义字段的用处是为验证数据和数据转换# class SnippetSerializer(serializers. Serializer):#   id = serializers. IntegerField(read_only=True)#   title = serializers. CharField(required=False, allow_blank=True, max_length=100)#   # code 是字符串类型, 但是在模板中需要用 textarea 来展示, 并用 base_template#   # serializers 中没有Text 类型字段, 要用CharField#   code = serializers. CharField(style={'base_template': 'textarea. html'})#   # 行号#   linenos = serializers. BooleanField(required=False)#   language = serializers. ChoiceField(choices=LANGUAGE_CHOICES, default='python')#   style = serializers. ChoiceField(choices=STYLE_CHOICES, default='friendly')##   # create 方法名是固定的, 继承自基类#   # self 和 validated_date 这两个参数也是固定的#   # validated_date 是一个字典, 内容是经过验证的合法数据, 数据内容就是上面定义的字段数据. #   def create(self, validated_date):#        #     创建#        #     # 将验证过的数据创建到 Snippet 表中, 并返回#     return Snippet. objects. create(**validated_date)##   # update 名字是固定的. #   # instance 代表一个要被更新的 snippet 对象#   # validated_date 是对这个实例进行更新的数据#   def update(self, instance, validated_date):#        #     更新#        #     # 第一个参数'title' 是validated_date 中的 key 的名字#     # 第二个参数 instance. title 是默认值. 可以省略, 也可以设置成其他默认值, 比如 'abc'#     instance. title = validated_date. get('title', instance. title)#     instance. code = validated_date. get('code', instance. code)#     instance. linenos = validated_date. get('linenos', instance. linenos)#     instance. language = validated_date. get('language', instance. language)#     instance. style = validated_date. get('style', instance. style)#     instance. save()#     # 返回 instance 是固定用法#     return instance使用命令行模拟服务端的响应和客户端的请求的过程 12345678910111213141516171819202122232425262728293031323334353637383940414243&gt;&gt;&gt; from snippetsapp. models import Snippet&gt;&gt;&gt; from snippetsapp. serializers import SnippetSerializer&gt;&gt;&gt; from rest_framework. renderers import JSONRenderer # 引入一个JSON 渲染器, 用于将response 的JSON代码进行渲染. &gt;&gt;&gt; from rest_framework. parsers import JSONParser # 引入一个JSON 解析器, 用于解析前端request 过来的JSON代码&gt;&gt;&gt; snippet = Snippet(code='hello world')&gt;&gt;&gt; snippet. save()&gt;&gt;&gt; snippet = Snippet(code='good bye')&gt;&gt;&gt; snippet. save()# 模拟服务端response输出&gt;&gt;&gt; serializer = SnippetSerializer(snippet) # 将数据对象传给序列器. &gt;&gt;&gt; serializer. data # 输出序列器的数据 -- 就是将数据对象翻译成了一个字典{'id': 2, 'title': '', 'code': 'good bye', 'linenos': False, 'language': 'python', 'style': 'friendly'} # 单引号, 表示输出的是字典&gt;&gt;&gt; content = JSONRenderer(). render(serializer. data) # 将字典传给渲染器进行渲染&gt;&gt;&gt; content # 是二进制JSONb'{ id :2, title :  , code : good bye , linenos :false, language : python , style : friendly }' # 双引号, 表示输出是二进制 JSON 数据 # 模拟客户端获取到数据, 然后转换成字典&gt;&gt;&gt; from django. utils. six import BytesIO # six 用于模拟前端数据传输时的二进制数据&gt;&gt;&gt; stream = BytesIO(content)  # 前端获取到response 二进制数据&gt;&gt;&gt; stream &lt;_io. BytesIO object at 0x10f8942b0&gt;&gt;&gt;&gt; data = JSONParser(). parse(stream) # 用 JSON解析器解析二进制代码,转换成字典&gt;&gt;&gt; data{'id': 2, 'title': '', 'code': 'good bye', 'linenos': False, 'language': 'python', 'style': 'friendly'}# 保存数据到数据库&gt;&gt;&gt; serializer = SnippetSerializer(data=data) # 将前端获取的到数据作为参数传给序列器&gt;&gt;&gt; serializer. is_valid() # 验证数据是否合法, 因为需要保存数据, 所以要先进行过验证. 才能获得 validated_data 并 save() True&gt;&gt;&gt; serializer. validated_data # 获取经过验证的数据OrderedDict([('title', ''), ('code', 'good bye'), ('linenos', False), ('language', 'python'), ('style', 'friendly')])&gt;&gt;&gt; serializer. save() # 保存数据&lt;Snippet: Snippet object (3)&gt;# 对比通过数据模型获取数据和 通过 虚话器获取数据&gt;&gt;&gt; Snippet. objects. all() # 获取数据模型中的所有数据&lt;QuerySet [&lt;Snippet: Snippet object (1)&gt;, &lt;Snippet: Snippet object (2)&gt;, &lt;Snippet: Snippet object (3)&gt;, &lt;Snippet: Snippet object (4)&gt;]&gt;&gt;&gt;&gt; serializer = SnippetSerializer(Snippet. objects. all(), many=True) # 获取序列器中的所有数据, many=True 是必须的参数&gt;&gt;&gt; serializer. data # 获取序列化的数据验证重构后的序列器是否还能输出之前的内容 1234567891011&gt;&gt;&gt; from snippetsapp. serializers import SnippetSerializer # 重新加载重构后的序列器&gt;&gt;&gt; serializer = SnippetSerializer() # 重新初始化序列器对象&gt;&gt;&gt; print(repr(serializer)) # 打印序列器. repr 是呈现对象的函数SnippetSerializer():  id = IntegerField(read_only=True)  title = CharField(allow_blank=True, max_length=100, required=False)  code = CharField(style={'base_template': 'textarea. html'})  linenos = BooleanField(required=False)  language = ChoiceField(choices=[('abap', 'ABAP'), . . . . . . , ('zephir', 'Zephir')], default='python')  style = ChoiceField(choices=[('abap', 'abap'), . . . . . . . , ('xcode', 'xcode')], default='friendly')编写views. py 函数 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403# 第五级from rest_framework. decorators import detail_route, api_viewfrom snippetsapp. models import Snippetfrom snippetsapp. serializers import SnippetSerializerfrom rest_framework import genericsfrom django. contrib. auth. models import Userfrom snippetsapp. serializers import UserSerializerfrom rest_framework import permissions, renderers, viewsetsfrom rest_framework. response import Responsefrom rest_framework. reverse import reverseclass SnippetViewSet(viewsets. ModelViewSet):       这个 viewset 自动提供了 list, create, retrive, delete, update 方法  这里需要和 urls. py 进行配合       queryset = Snippet. objects. all()  serializer_class = SnippetSerializer  permission_classes = (permissions. IsAuthenticatedOrReadOnly,)  # detail_route 是详情路由装饰器, 定义 detail 页面的时候可以被装饰  # 该装饰器指定该详情页需要用html 静态方式进行渲染.   # 该装饰器也可以在 urls. py 中进行定义, 效果相同  @detail_route(renderer_classes=[renderers. StaticHTMLRenderer])  def highlight(self, request, *args, **kwargs):    snippet = self. get_object()    # snippet. highlighted 是命名空间, 对应 urls. py 中的定义    return Response(snippet. highlighted)  # perform_create 方法继承自基类  # 该方法只有在新一条记录时才会被调用  def perform_create(self, serializer):    # 强制将代码片段作者设置为当前登录用户    serializer. save(owner=self. request. user)class UserViewSet(viewsets. ModelViewSet):       This viewset automatically provides 'list' and 'detail' actions       queryset = User. objects. all()  serializer_class = UserSerializer  permission_classes = (permissions. IsAuthenticatedOrReadOnly,)@api_view(['GET'])def api_root(request, format=None):  return Response({    # 生成一个跳到users 的链接    # user-list 是views中的命名空间,    'users': reverse('user-list', request=request, format=format),    'snippets': reverse('snippet-list', request=request, format=format)  })#### # 第四级: 继承自generics 对应的几个类指定请求方法的基类# # 不再需要get, post 等方法. ## from snippetsapp. models import Snippet# from snippetsapp. serializers import SnippetSerializer# from rest_framework import generics# from django. contrib. auth. models import User# from snippetsapp. serializers import UserSerializer# from rest_framework import permissions# from rest_framework. decorators import api_view# from rest_framework. response import Response# from rest_framework. reverse import reverse# from rest_framework import renderers####### class SnippetList(generics. ListCreateAPIView):#   queryset = Snippet. objects. all()#   serializer_class = SnippetSerializer#   # 这是权限控制, 如果没有登录, 就只可能查看, 不能做修改删除增加操作的显示界面. #   # permission_classes = (permissions. IsAuthenticated,)#   # IsAuthenticatedOrReadOnly 是没登录的话, 可以看, 但不能创建,修改,删除#   permission_classes = (permissions. IsAuthenticatedOrReadOnly, )##   # 确保创建代码的用户就是作者本人. 不能伪造用户#   # 作用: 每次新创建一条记录的时候, 就会自动执行该方法, 创建作者就是登陆用户. #   # 这个方法名是固定用法, 继承自基类. #   def perform_create(self, serializer):#     serializer. save(owner=self. request. user)### class SnippetDetail(generics. RetrieveUpdateDestroyAPIView):#   queryset = Snippet. objects. all()#   serializer_class = SnippetSerializer#   permission_classes = (permissions. IsAuthenticatedOrReadOnly,)### class UserList(generics. ListCreateAPIView):#   queryset = User. objects. all()#   serializer_class = UserSerializer#   permission_classes = (permissions. IsAuthenticatedOrReadOnly,)### class UserDetail(generics. RetrieveUpdateDestroyAPIView):#   queryset = User. objects. all()#   serializer_class = UserSerializer#   permission_classes = (permissions. IsAuthenticatedOrReadOnly,)#### @api_view(['GET'])# def api_root(request, format=None):#   return Response({#     # 生成一个跳到users 的链接#     # user-list 是views中的命名空间,#     'users': reverse('user-list', request=request, format=format),#     'snippets': reverse('snippet-list', request=request, format=format)#   })### # 一个自定义的类, 继承自通用视图类# class SnippetHighlight(generics. GenericAPIView):#   queryset = Snippet. objects. all()#   # 指定渲染器类为静态HTML 渲染器, 也可以指定其他渲染器. #   renderer_classes = (renderers. StaticHTMLRenderer,)##   # 定义接收的是GET 方法, 继承自基类#   # 后面三个参数没用, 但是必须有, 否则报错#   def get(self, request, *args, **kwargs):#     # 默认返回当前对象 ( 通过pk )#     snippet = self. get_object()#     # 渲染的内容就是#     return Response(snippet. highlighted)## # 第三点五级别: 使用Mixin类作为虚拟基类, 一般Mixin 不会直接用, 而是专门用来被继承的基类# # Mixin 分为list, create, retrieve, update, delete. # # 这一级别, UI 上的表单更加细化, 可以分项输入不同信息. ### from snippetsapp. models import Snippet# from snippetsapp. serializers import SnippetSerializer# from rest_framework import mixins# from rest_framework import generics## class SnippetList(mixins. ListModelMixin,#          mixins. CreateModelMixin,#          generics. GenericAPIView):#   queryset = Snippet. objects. all()#   serializer_class = SnippetSerializer##   def get(self, request, *args, **kwargs):#     # self. list 是ListModelMixin 的方法, 同时实线了 Response 方法. #     return self. list(request, *args, **kwargs)##   def post(self, request, *args, **kwargs):#     # self. create 是 CreateModelMixin 的方法, 同时实线了 Response 方法. #     return self. create(request, *args, **kwargs)#### class SnippetDetail(mixins. RetrieveModelMixin,#           mixins. UpdateModelMixin,#           mixins. DestroyModelMixin,#           generics. GenericAPIView):#   queryset = Snippet. objects. all()#   serializer_class = SnippetSerializer##   def get(self, request, *args, **kwargs):#     # retrieve 是 继承自 RetrieveModelMixin#     return self. retrieve(request, *args, **kwargs)##   def put(self, request, *args, **kwargs):#     # update 继承自 UpdateModelMixin#     return self. update(request, *args, **kwargs)##   def delete(self, request, *args, **kwargs):#     # destroy 继承自 DestroyModelMixin#     return self. destroy(request, *args, **kwargs)### # 第三级 面向对象: 继承自django 的 APIView# # 不再需要对不同method 进行判断. # # 对应不同类型请求, 创建相对名字的方法. ## from snippetsapp. models import Snippet# from snippetsapp. serializers import SnippetSerializer# from django. http import Http404# from rest_framework. views import APIView# from rest_framework. response import Response# from rest_framework import status### # APIView 是 rest 框架中的最基本的 view 基类# class SnippetList(APIView):#      #   LC#      #   # 如果是GET 方法, 就会执行get 方法. 如果是POST 方法, 就执行 post 方法#   # format=None 的作用是可以在url 后面加个点(比如:http://127. 0. 0. 1:8000/snippets. json . api ) . 这个需要和 urls. py 中的 urlpatterns = format_suffix_patterns(urlpatterns) 配合使用#   def get(self, request, format=None):#     snippets = Snippet. objects. all()#     serializer = SnippetSerializer(snippets, many=True)#     return Response(serializer. data)##   def post(self, request, format=None):#     serializer = SnippetSerializer(data=request. data)#     if serializer. is_valid():#       serializer. save()#       return Response(serializer. data, status=status. HTTP_201_CREATED)#     return Response(serializer. errors, status=status. HTTP_400_BAD_REQUEST)### class SnippetDetail(APIView):#      #   RUD#      #   # get_object() 方法是必须的, 需要先获得需要进行操作的那条数据对象. #   # get_object() 方法会被后面三个方法调用#   def get_object(self, pk):#     try:#       return Snippet. objects. get(pk=pk)#     except Snippet. DoesNotExist:#       raise Http404##   def get(self, request, pk, format=None):#     snippet = self. get_object(pk)#     serializer = SnippetSerializer(snippet)#     return Response(serializer. data)##   def put(self, request, pk, format=None):#     snippet = self. get_object(pk)#     serializer = SnippetSerializer(snippet, data=request. data)#     if serializer. is_valid():#       serializer. save()#       return Response(serializer. data)#     return Response(serializer. errors, status=status. HTTP_400_BAD_REQUEST)##   def delete(self, request, pk, format=None):#     snippet = self. get_object(pk)#     snippet. delete()#     return Response(status=status. HTTP_204_NO_CONTENT)## 第二级 仍然是函数: 用 django 自带的 Response 处理 json的响应# 第二级的代码好处是可以看到 漂亮的API 呈现UI界面, 可以在界面直接操作交互. ## from rest_framework import status# from rest_framework. decorators import api_view# from rest_framework. response import Response# from snippetsapp. models import Snippet# from snippetsapp. serializers import SnippetSerializer## # api_view装饰器, 可以帮我们生成友好的 API 交互界面# # 参数 GET, POST 相当于开关, 如果删掉某一个, 就不能用该方法访问该函数. # @api_view(['GET', 'POST'])# def snippet_list(request, format=None):#      #   LC#      #   if request. method == 'GET':#     snippets = Snippet. objects. all()#     # 这里的get 是获取列表, 所以要传递snippets, 并且many=True是必须的#     serializer = SnippetSerializer(snippets, many=True)#     return Response(serializer. data)##   elif request. method == 'POST':#     # 这里不再需要 JSONParser(). parse(request) 方法做转化, 而是直接使用 SnippetSerializer 将发送过来的数据进行转化. #     # request. data 比 request. POST 更强大, 获取能多的信息, 可以直接获取各种Http 请求方法(GET, POST, PUT, DELETE). #     # 有了 request. data 就不需要 JSONParser()#     serializer = SnippetSerializer(data=request. data)#     if serializer. is_valid():#       serializer. save()#       # status=status. HTTP_201_CREATED 的好处是代码更清晰了, 更多的状态码可以在官网上查到. #       return Response(serializer. data, status=status. HTTP_201_CREATED)#		 # serializer. errors 将保存报错信息, 比如: 提交的字段信息超过规定长度, 会报错: 确保字段不超过100个字符#     return Response(serializer. errors, status=status. HTTP_400_BAD_REQUEST)#### @api_view(['GET', 'PUT', 'DELETE'])# def snippet_detail(request, pk, format=None):#      #   RUD#      #   try:#     snippet = Snippet. objects. get(pk=pk)#   except Snippet. DoesNotExist:#     return Response(status=status. HTTP_404_NOT_FOUND)#   if request. method == 'GET':#     # 这里的get 是获取一条信息, 所以要传递snippet对象#     serializer = SnippetSerializer(snippet)#     return Response(serializer. data)##   elif request. method == 'PUT':#     # 因为要更新数据, 所有需要传递两个参数, 第一个参数是被更新的数据, 第二个参数是新的数据. #     serializer = SnippetSerializer(snippet, data=request. data)#     if serializer. is_valid():#       serializer. save()#       return Response(serializer. data)#     return Response(serializer. errors, status=status. HTTP_400_BAD_REQUEST)##   elif request. method == 'DELETE':#     snippet. delete()#     return Response(status=status. HTTP_204_NO_CONTENT)## 第一级: 最原始, 使用函数# 第一级只能看到 json 的 row data , 看不到漂亮的API 界面# 第一级只能用 http 命令行模拟处理交互动作# from django. http import HttpResponse, JsonResponse# # 获取视图模块装饰器模块的 跨域攻击免除模块, 就是不进行跨域保护检验# from django. views. decorators. csrf import csrf_exempt# from rest_framework. renderers import JSONRenderer# from rest_framework. parsers import JSONParser# from . models import Snippet# from . serializers import SnippetSerializer### # 跨域保护解除装饰器# # 实现列表和创建的功能# @csrf_exempt# def snippet_list(request):#      #   LC: List all code snippets, or Create a new snippet. #      #   # List#   if request. method == 'GET':#     snippets = Snippet. objects. all()#     # 现将数据对象进行序列化#     serializer = SnippetSerializer(snippets, many=True)#     # serializer. data 是字典格式数据, 经过 JsonResponse() 转换后,成为JSON格式#     return JsonResponse(serializer. data, safe=False)#   # Create#   elif request. method ==  POST :#     # 将传递过来的对象转换成字典. data 的内容就是一个字典#     data = JSONParser(). parse(request)#     serializer = SnippetSerializer(data=data)#     if serializer. is_valid():#       serializer. save()#       return JsonResponse(serializer. data, status=201)#     return JsonResponse(serializer. errors, status=400)### # 实现获取详情, 更新详情, 删除详情的功能# @csrf_exempt# def snippet_detail(request, pk):#      #   RUD: Retrieve, update or delete a code snippet. #      #   try:#     snippet = Snippet. objects. get(pk=pk)#   except Snippet. DoesNotExist:#     return HttpResponse(status=404)##   if request. method == 'GET':#     serializer = SnippetSerializer(snippet)#     # serializer. data 是字典格式数据, 经过 JsonResponse() 转换后,成为JSON格式#     return JsonResponse(serializer. data)##   elif request. method == 'PUT':#     # 将传递过来的对象转换成字典. data 的内容就是一个字典#     data = JSONParser(). parse(request)#     serializer = SnippetSerializer(snippet, data=data)#     if serializer. is_valid():#       serializer. save()#       return JsonResponse(serializer. data)#		 # serializer. errors 将保存报错信息, 比如: 提交的字段信息超过规定长度, 会报错: 确保字段不超过100个字符#     return JsonResponse(serializer. errors, status=400)##   elif request. method == 'DELETE':#     snippet. delete()#     return HttpResponse(status=204)###修改snippets/urls. py 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798# 第五级# 好处是代码少, 缺点是不够灵活, 所以一般不用. 也不用第五级预科# 所以最好的方法时用第四级from django. conf. urls import url, includefrom snippetsapp import viewsfrom rest_framework. routers import DefaultRouter# 定义一个默认路由的对象router = DefaultRouter()# 将视图类注册到路由中# 第一个参数snippets 是定义的 url 路径名,# 第二个参数是 对应的视图类名router. register(r'snippets', views. SnippetViewSet)router. register(r'users', views. UserViewSet)urlpatterns = [  url(r'^', include(router. urls)),  # url(r'^api-auth/', include('rest_framework. urls', namespace='rest_framework')),]# 第五级预科# from . views import SnippetViewSet, UserViewSet, api_root# from rest_framework import renderers# from django. conf. urls import url# from rest_framework. urlpatterns import format_suffix_patterns### # 将一个字典参数传递给as_view() 方法# # 这种方法可以直接在 url 中定义了 ViewSet 可以支持的请求方法# snippet_list = SnippetViewSet. as_view({#   # 这里字典中的值是固定用法, 需要照着用就可以了. #   'get': 'list',#   'post': 'create',# })# # }, renderer_class=[renderers. StaticHTMLRenderer])### snippet_detail = SnippetViewSet. as_view({#   'get': 'retrieve',#   'put': 'update',#   'patch': 'partial_update',#   'delete': 'destroy',# })## snippet_highlight = SnippetViewSet. as_view({#   'get': 'highlight',# })## user_list = UserViewSet. as_view({#   'get': 'list',#   'post': 'create',# })## user_detail = UserViewSet. as_view({#   'get': 'retrieve',#   'put': 'update',#   'patch': 'partial_update',#   'delete': 'destroy',# })## urlpatterns = format_suffix_patterns([#   url(r'^$', api_root),#   url(r'^snippets/$', snippet_list, name='snippet-list'),#   url(r'^snippets/(?P&lt;pk&gt;[0-9]+)/$', snippet_detail, name='snippet-detail'),#   url(r'^snippets/(?P&lt;pk&gt;[0-9]+)/highlight/$', snippet_highlight, name='snippet-highlight'),#   url(r'^users/$', user_list, name='user-list'),#   url(r'^users/(?P&lt;pk&gt;[0-9]+)/$', user_detail, name='user-detail'),# ])#### 第四级 urls. py# from django. conf. urls import url# from snippetsapp import views# from rest_framework. urlpatterns import format_suffix_patterns### urlpatterns = [#   # url(r'^snippets/$', views. snippet_list),#   url(r'^snippets/$', views. SnippetList. as_view(), name='snippet-list'),#   # url(r'^snippets/(?P&lt;pk&gt;[0-9]+)/$', views. snippet_detail),#   url(r'^snippets/(?P&lt;pk&gt;[0-9]+)/$', views. SnippetDetail. as_view(), name='snippet-detail'),#   url(r'^users/$', views. UserList. as_view(), name='user-list'),#   url(r'^users/(?P&lt;pk&gt;[0-9]+)/$', views. UserDetail. as_view(), name='user-detail'),#   url(r'^snippets/(?P&lt;pk&gt;[0-9]+)/highlight/$', views. SnippetHighlight. as_view(), name='snippet-highlight'),#   url(r'^$', views. api_root),# ]## urlpatterns = format_suffix_patterns(urlpatterns)修改主项目的urls. py 123456789101112131415161718192021from django. contrib import adminfrom django. urls import pathfrom django. conf. urls import url, include# from rest_framework import routers# from quickstartapp import views## router = routers. DefaultRouter()# router. register(r'users', views. UserViewSet)# router. register(r'groups', views. GroupViewSet)urlpatterns = [  path('admin/', admin. site. urls),  # url(r'^', include(router. urls)),  # url(r'^', include('snippetsapp. urls')),  url(r'^helloworld/', include('quickstartapp. urls')),  url(r'^', include('snippetsapp. urls')),  # 将登陆/登出 的请求转发到 rest_framework 的 urls进行处理  url(r'^api-auth/', include('rest_framework. urls', namespace='rest_framework')),]获取完整列表  浏览器访问 http://127. 0. 0. 1:8000/snippets/ 或者通过 http 命令1http http://127. 0. 0. 1:8000/snippets/ 输出如下:1[{ id : 1,  title :   ,  code :  hello world ,  linenos : false,  language :  python ,  style :  friendly }, { id : 2,  title :   ,  code :  good bye ,  linenos : false,  language :  python ,  style :  friendly }, { id : 3,  title :   ,  code :  good bye ,  linenos : false,  language :  python ,  style :  friendly }, { id : 4,  title :   ,  code :  good bye ,  linenos : false,  language :  python ,  style :  friendly }]创建一条数据  准备JSON 数据文件 src. json 如下123{   code :  http 创建的数据 } 使用http 命令创建数据123http POST http://127. 0. 0. 1:8000/snippets/ &lt; src. json &gt; output. html# 如果参数比较少, 可以直接在命令行中加上要传递的参数http POST http://127. 0. 0. 1:8000/snippets/ code=himan获取某一条数据 1http http://127. 0. 0. 1:8000/snippets/2/ OUTPUT:TTP/1. 1 200 OKContent-Length: 115Content-Type: application/jsonDate: Tue, 30 Jan 2018 10:23:54 GMTServer: WSGIServer/0. 2 CPython/3. 7. 0a2X-Frame-Options: SAMEORIGIN{   code :  good bye ,   id : 2,   language :  python ,   linenos : false,   style :  friendly ,   title :   }更新一条数据  先准备一个json 文本 update. json 如下:1234567{   code :  good morning ,   language :  python ,   linenos : false,   style :  friendly ,   title :  my title } 用 http 执行更新命令如下:123http PUT http://127. 0. 0. 1:8000/snippets/2/ &lt; update. json# 如果数据简单, 那么可以直接在命令行中添加要更改的数据http PUT http://127. 0. 0. 1:8000/snippets/2/ code=hi222 title=222 输出:123456789101112131415HTTP/1. 1 200 OKContent-Length: 115Content-Type: application/jsonDate: Tue, 30 Jan 2018 10:18:41 GMTServer: WSGIServer/0. 2 CPython/3. 7. 0a2X-Frame-Options: SAMEORIGIN{   code :  good morning ,   id : 2,   language :  python ,   linenos : false,   style :  friendly ,   title :  my title }删除一条数据  用http 执行如下命令1http DELETE http://127. 0. 0. 1:8000/snippets/3/ 输出:1234567HTTP/1. 1 204 No ContentContent-Length: 0Content-Type: text/html; charset=utf-8Date: Tue, 30 Jan 2018 10:22:28 GMTServer: WSGIServer/0. 2 CPython/3. 7. 0a2X-Frame-Options: SAMEORIGIN前端代码: (1). settings. py 增加如下代码 12345678910111213141516INSTALLED_APPS = [	. . .   'corsheaders',	. . . ]MIDDLEWARE = [ . . .  'django. contrib. sessions. middleware. SessionMiddleware', 'corsheaders. middleware. CorsMiddleware', . . .  ]CORS_ORIGIN_ALLOW_ALL = True(2). 将front 代码包放到任意目录下 (3). 设置 index. js 中的url地址和端口号, 对应到snippets 服务的url 和 端口 (4). 浏览器中打开 index. html 就可以看到snippet应用了. 卖电脑的电商项目: 创建 eshop 项目 和 computerapp 1234django-admin startproject eshop . python3. 7 manage. py startapp computerapppython3. 7 manage. py migratepython3. 7 manage. py createsuperuser设置 settings. py 1234567891011121314151617INSTALLED_APPS = [ 	. . .   'rest_framework', 	'corsheaders', # 跨域保护  'computerapp. apps. ComputerappConfig',]MIDDLEWARE = [ . . . 	'corsheaders. middleware. CorsMiddleware', # 跨域保护  . . . ]# 任何人都可以访问我们的网站CORS_ORIGIN_ALLOW_ALL = TrueMEDIA_URL = '/media/'MEDIA_ROOT = os. path. join(os. path. dirname(BASE_DIR), 'estore', 'eshop', 'media')创建静态文件 目录 estore/eshop/media 建模 cumputerapp/models. py  分类 厂商 产品 收货地址 用户扩展 订单123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114from django. db import modelsfrom django. conf import settings# Create your models here. class Category(models. Model):       商品类别: 笔记本, 平板电脑, 一体机, 台式机, 服务器等       name = models. CharField(max_length=200, verbose_name='类别名称')  created = models. DateTimeField(auto_now_add=True, verbose_name='创建时间')  updated = models. DateTimeField(auto_now=True, verbose_name='更新时间')  def __str__(self):    return self. nameclass Manufacturer(models. Model):       生产厂商       name = models. CharField(max_length=200, verbose_name= 厂商名称 )  description= models. TextField(verbose_name='品牌描述')  # ImageField 图片类型字段  # upload_to 表示要上传到的位置.   logo = models. ImageField(blank=True, null=True, max_length=200, upload_to='manufacturer/uploads/%Y/%m/%d/', verbose_name='品牌图标')  created = models. DateTimeField(auto_now_add=True, verbose_name='创建时间')  updated = models. DateTimeField(auto_now=True, verbose_name='更新时间')  def __str__(self):    return self. nameclass Product(models. Model):       产品       model = models. CharField(max_length=200, verbose_name='产品名称')  description = models. TextField(verbose_name='产品描述')  image = models. ImageField(max_length=200, upload_to='product/uploads/%Y/%m/%d/', verbose_name='产品图片')  # DecimalField 带小数类型数字字段,  # max_digits = 12 表示小数点前后共12位.   # decimal_places = 2 小输掉保留两位  price = models. DecimalField(max_digits=12, decimal_places=2, verbose_name='产品价格')  # PositiveIntegerField 正整数类型字段  # sold 是销量  sold = models. PositiveIntegerField(default=0, verbose_name='产品销量')  # product_in 表示产品属于 . . . 类目  category = models. ForeignKey(Category, related_name='product_in', on_delete=models. DO_NOTHING, verbose_name='产品分类')  # product_of 表示产品属于 . . . 品牌  manufacturer = models. ForeignKey(Manufacturer, related_name='product_of', on_delete=models. DO_NOTHING, verbose_name='产品生产厂商')  created = models. DateTimeField(auto_now_add=True, verbose_name='创建时间')  updated = models. DateTimeField(auto_now=True, verbose_name='更新时间')  def __str__(self):    return self. modelclass DeliveryAddress(models. Model):       收货地址       # settings. AUTH_USER_MODEL 是认证用户模型, 推荐使用  user = models. ForeignKey(settings. AUTH_USER_MODEL, on_delete=models. DO_NOTHING, related_name='delivery_address_of')  contact_person = models. CharField(max_length=200)  contact_mobile_phone = models. CharField(max_length=200)  delivery_address = models. TextField()  created = models. DateTimeField(auto_now_add=True)  updated = models. DateTimeField(auto_now=True)  def __str__(self):    return self. delivery_addressclass UserProfile(models. Model):       用户扩展信息       user = models. OneToOneField(settings. AUTH_USER_MODEL, on_delete=models. DO_NOTHING, related_name='profile_of')  mobile_phone = models. CharField(blank=True, null=True, max_length=200)  nickname = models. CharField(blank=True, null=True, max_length=200)  description = models. TextField(blank=True, null=True)  icon = models. ImageField(blank=True, null=True, max_length=200, upload_to='user/uploads/%Y/%m/%d/')  created = models. DateTimeField(auto_now_add=True)  updated = models. DateTimeField(auto_now=True)  delivery_address = models. ForeignKey(DeliveryAddress, related_name='user_delivery_address', on_delete=models. DO_NOTHING, blank=True, null=True)class Order(models. Model):       Order 订单       # 订单状态  STATUS_CHOICES = (    ('0', 'new'),    ('1', 'not paid'),    ('2', 'paid'),    ('3', 'transport'),    ('4', 'closed'),  )  status = models. CharField(choices=STATUS_CHOICES, default='0', max_length=2)  user = models. ForeignKey(settings. AUTH_USER_MODEL, on_delete=models. DO_NOTHING, related_name='order_of',)  # 产品详情描述  remark = models. TextField(blank=True, null=True)  product = models. ForeignKey(Product, related_name='order_product', on_delete=models. DO_NOTHING)  price = models. DecimalField(max_digits=12, decimal_places=2)  quantity = models. PositiveIntegerField(default=1)  address = models. ForeignKey(DeliveryAddress, related_name='order_address', on_delete=models. DO_NOTHING)  created = models. DateTimeField(auto_now_add=True)  updated = models. DateTimeField(auto_now=True)  def __str__(self):    return 'order of %d' % (self. user. id)重新迁移 12python3. 7 manage. py makemigrationspython3. 7 manage. py migrate设置 serializers. py 文件 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364from rest_framework import serializersfrom computerapp. models import Product, Category, Manufacturer, UserProfile, DeliveryAddressfrom django. contrib. auth. models import User# 自定义的用户信息的序列器class UserProfileSerializer(serializers. ModelSerializer):  class Meta:    model = UserProfile    fields = ('id', 'user', 'mobile_phone', 'nickname', 'description', 'icon', 'created', 'updated',)    # 这里定义了user 字段是只读. 因为user 的属性是 OneToOneField    read_only_fields = ('user', )# django 框架自带的用户信息的序列器class UserInfoSerializer(serializers. ModelSerializer):  # 从自定义的用户信息序列器中获取自定义的用户信息, 然后合并到框架的用户信息中.   profile_of = UserProfileSerializer()  class Meta:    model = User    # 这里的 profile_of 是在 models. py 中定义了 user 字段的 related_name='profile_of'    fields = ('id', 'username', 'email', 'first_name', 'last_name', 'date_joined', 'profile_of',)class ProductListSerializer(serializers. ModelSerializer):  class Meta:    model = Product    fields = ('id', 'model', 'description', 'price', 'category', 'manufacturer', 'created', 'updated', 'image', 'sold',)class CategorySerializer(serializers. ModelSerializer):  class Meta:    model = Category    fields = ('id', 'name',)class ManufacturerSerializer(serializers. ModelSerializer):  class Meta:    model = Manufacturer    fields = ('id', 'name',)class ProductRetrieveSerializer(serializers. ModelSerializer):  # 下面两行可以注释掉, 但是产品详情就不会显示和产品相关的类目详细信息和厂商详细信息  # 下面梁行代码也可以通过 HyperLinkedIdentifiedField, HpyerLinkedRelatedField, 但是下面这种方法更好.   category = CategorySerializer()  manufacturer = ManufacturerSerializer()  class Meta:    model = Product    fields = ('id', 'model', 'description', 'price', 'category', 'manufacturer', 'created', 'updated', 'image', 'sold',)class DeliveryAddressSerializer(serializers. ModelSerializer):  class Meta:    model = DeliveryAddress    fields = ('id', 'user', 'contact_person', 'contact_mobile_phone', 'delivery_address', 'created', 'updated',)    read_only_fields = ('user',)设置 views. py 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150from django. shortcuts import renderfrom rest_framework import genericsfrom rest_framework import permissionsfrom rest_framework. filters import OrderingFilter, SearchFilterfrom rest_framework. pagination import LimitOffsetPaginationfrom computerapp. models import Product, UserProfile, DeliveryAddressfrom computerapp. serializers import ProductListSerializer, ProductRetrieveSerializer, UserInfoSerializer, UserProfileSerializer, DeliveryAddressSerializerfrom rest_framework. views import APIViewfrom rest_framework. response import Responseclass ProductListView(generics. ListAPIView):       产品列表       queryset = Product. objects. all()  serializer_class = ProductListSerializer  permission_classes = (permissions. AllowAny,)  # 可以过滤的后台, 第一个参数表示搜索功能, 第二个表示排序功能. 这两个参数是固定用法.   # 增加该字段后, API UI 出现一个 filters 功能  # 前端通过 GET /computer/product_list/?ordering=category&amp;search=%E5%B9%B3%E6%9D%BF 调用  filter_backends = (SearchFilter, OrderingFilter,)  # 这里是设置具体的可搜索字段.   # 如果不设置, 默认返回所有列表数据  search_fields = ('description', 'model')  # 这里是设置具体的可排序字段  # 如果不设置, 那么默认所有字段都可以排序  ordering_fields = ('category', 'manufacturer', 'created', 'sold',)  # 设置默认排序是id  ordering = ('id',)  # 指定分页方式为 LimitOffsetPagination, 这种方式的  # 之前的另一种 方式 PageNumberPagination 也可以用  # PageNumberPagination 方式 是显示页码为数字的方式(每页显示的个数是配置文件中 PAGE_SIZE 设置死的),  # LimitOffsetPagination 可以灵活的, 客户端可以通过携带 ?limit=2&amp;offset=6 参数设置每页显示2条数据, 显示第4页  pagination_class = LimitOffsetPaginationclass ProductListByCategoryView(generics. ListAPIView):       产品按类别展示列表       serializer_class = ProductListSerializer  permission_classes = (permissions. AllowAny,)  filter_backends = (SearchFilter, OrderingFilter,)  search_fields = ('description',)  ordering_fields = ('category', 'manufacturer', 'created', 'sold', 'stock', 'price',)  ordering = ('id',)  # 和 上面 ProductListView 类的区别就是用 get_queryset() 方法 代替了 queryset 属性, 重写了列表获取的条件  def get_queryset(self):    # 获取列表的条件是先获取 request 传递过来的 分类的的id    # query_params 表示查询参数. 前端可以通过GET 或者 POST 方式发送参数. 比如GET ?category=1 , 或者POST 过来的 category 的值    # 如果没有拿到值, 默认值是 None    category = self. request. query_params. get('category', None)    # 如果传递过来的信息非空 就进行过滤, 否则全部显示    if category is not None:      queryset = Product. objects. filter(category=category)    else:      queryset = Product. objects. all()    return querysetclass ProductListByCategoryManufacturerView(generics. ListAPIView):       产品按类别并按品牌展示列表       serializer_class = ProductListSerializer  permission_classes = (permissions. AllowAny,)  filter_backends = (SearchFilter, OrderingFilter,)  search_fields = ('description',)  ordering_fields = ('category', 'manufacturer', 'created', 'sold', 'stock', 'price',)  ordering = ('id',)  def get_queryset(self):    # 先获取分类id 和 厂商id    # 调用方法例子GET: ?category=1&amp;manufacturer=1 或者 POST 传递这两个参数    category = self. request. query_params. get('category', None)    manufacturer = self. request. query_params. get('manufacturer', None)    if category is not None:      queryset = Product. objects. filter(category=category, manufacturer=manufacturer,)    else:      queryset = Product. objects. all()    return querysetclass ProductRetrieveView(generics. RetrieveAPIView):       产品详情       queryset = Product. objects. all()  serializer_class = ProductRetrieveSerializer  permission_classes = (permissions. AllowAny,)class UserInfoView(APIView):       用户基本信息       permissions_classes = (permissions. IsAuthenticated,)  # 定义 GET 方法发送过来的数据的处理方法  def get(self, request, format=None):    # 获取当前用户对象, 无法获取其他用户信息.     user = self. request. user    serializer = UserInfoSerializer(user)    return Response(serializer. data)class UserProfileRUView(generics. RetrieveUpdateAPIView):       用户其他信息更新获取       # 不能用下一行代码, 因为会让API 直接访问到其他用户的信息, 而不是当前用户的信息. 比如: http://127. 0. 0. 1:8001/computer/user_profile_ru/2/  # queryset = UserProfile. objects. all()  serializer_class = UserProfileSerializer  permission_classes = (permissions. IsAuthenticated,)  # 下面方法是用来限制用户只能访问自己的信息.   # get_object 用来只返回一个对象  def get_object(self):    # 获取当前用户.     user = self. request. user    # 通过当前用户,获取当前用户档案    obj = UserProfile. objects. get(user=user)    return objclass DeliveryAddressLCView(generics. ListCreateAPIView):       收货地址LC       serializer_class = DeliveryAddressSerializer  permission_classes = (permissions. IsAuthenticated,)  def get_queryset(self):    user = self. request. user    queryset = DeliveryAddress. objects. filter(user=user)    return queryset  def perform_create(self, serializer):    user = self. request. user    s = serializer. save(user=user)    profile = user. profile_of    profile. delivery_address = s    profile. save()设置两个urls. py 文件 eshop/urls. py 123456789101112131415161718192021from django. contrib import adminfrom django. urls import pathfrom django. conf. urls import url, includefrom django. conf import settingsfrom django. conf. urls. static import staticfrom rest_framework. authtoken import viewsurlpatterns = [  url(r'^admin/', admin. site. urls),  url(r'^computer/', include('computerapp. urls')),  # 让用户可以在API UI 上进行登录,登出  url(r'^api-auth', include('rest_framework. urls', namespace='rest_framework')),  # 让客户端可以通过API 的方式进行登录注册, 只能通过POST 方式提交  # 如果获取到token 信息, 就跳转  url(r'^api-token-auth', views. obtain_auth_token),]if settings. DEBUG:  urlpatterns += static(settings. MEDIA_URL, document_root=settings. MEDIA_ROOT)computerapp/urls. py 123456789101112131415161718192021from django. conf. urls import urlfrom rest_framework. urlpatterns import format_suffix_patternsfrom computerapp import viewsurlpatterns = [  url(r'^product_list/$', views. ProductListView. as_view(), name='product_list'),  url(r'^product_retrieve/(?P&lt;pk&gt;[0-9]+)/$', views. ProductRetrieveView. as_view(), name='product_retrieve'),  url(r'^product_list_by_category/$', views. ProductListByCategoryView. as_view(), name='product_list_by_category'),  url(r'^product_list_by_category_manufacturer/$', views. ProductListByCategoryManufacturerView. as_view(), name='product_list_by_category_manufacturer'),  url(r'^user_info/$', views. UserInfoView. as_view(), name='user_info'),  url(r'^user_profile_ru/(?P&lt;pk&gt;[0-9]+)/$', views. UserProfileRUView. as_view(), name='user_profile_ru'),  url(r'^delivery_address_lc/$', views. DeliveryAddressLCView. as_view(), name='delivery_address_lc'),]urlpatterns = format_suffix_patterns(urlpatterns, allowed=['api', 'json', ])再修改settings. py  增加翻页 列表页的搜索过滤 增加默认访问权限管理 增加认证1234567891011121314151617181920212223INSTALLED_APPS = [	. . .  	'django_filters', # 列表中用到的搜索排序功能  'rest_framework. authtoken', # 登录认证用的token 	. . . ]REST_FRAMEWORK = {  'DEFAULT_PAGINATION_CLASS': 'rest_framework. pagination. PageNumberPagination',  'PAGE_SIZE': 6,  'DEFAULT_PERMISSION_CLASSES': (    # 不写下面这一行, 默认也是 AllowAny    'rest_framework. permissions. AllowAny',  ),  'DEFAULT_AUTHENTICATION_CLASSES': (    'rest_framework. authentication. BasicAuthentication', # 最基本的验证方式    'rest_framework. authentication. SessionAuthentication', # Session 验证方式, 用于前后端一体的情况下使用    'rest_framework. authentication. TokenAuthentication', # Token验证方式, 用于前后端分离.   ),}为token 功能重新迁移 12python3. 7 manage. py makemigrationspython3. 7 manage. py migrate 数据库中会新建一个表 authtoken_token 表中三个字段, key(token), created, user_id测试 token 功能 12345http POST http://127. 0. 0. 1:8001/api-token-auth/ username=xiaolong password=password123# 返回内容如下# {#    token :  44a75a6fd2c35f5f4a62c031af1a790c1641b3ea # }启动服务, 前端代码要求是 8001 端口 1python3. 7 manage. py runserver将客户端用的js, html, css 代码包 front 拷贝到项目目录中 如果后台服务正常, 那么, 打开front/index. html 就应该可以直接连接到服务端了. 最终代码: eshop/settings. py: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146import os# Build paths inside the project like this: os. path. join(BASE_DIR, . . . )BASE_DIR = os. path. dirname(os. path. dirname(os. path. abspath(__file__)))# Quick-start development settings - unsuitable for production# See https://docs. djangoproject. com/en/2. 0/howto/deployment/checklist/# SECURITY WARNING: keep the secret key used in production secret!SECRET_KEY = 'f7n$1*n!+v-fxhtibh=in6$%&amp;73=1$ugg#r1&amp;vvo&amp;b&amp;9=q+7cs'# SECURITY WARNING: don't run with debug turned on in production!DEBUG = TrueALLOWED_HOSTS = []# Application definitionINSTALLED_APPS = [  'django. contrib. admin',  'django. contrib. auth',  'django. contrib. contenttypes',  'django. contrib. sessions',  'django. contrib. messages',  'django. contrib. staticfiles',  'rest_framework',  'rest_framework. authtoken', # 登录认证用的token  'django_filters',  'corsheaders', # 跨域保护  'computerapp. apps. ComputerappConfig',]MIDDLEWARE = [  'django. middleware. security. SecurityMiddleware',  'django. contrib. sessions. middleware. SessionMiddleware',  'corsheaders. middleware. CorsMiddleware', # 跨域保护  'django. middleware. common. CommonMiddleware',  'django. middleware. csrf. CsrfViewMiddleware',  'django. contrib. auth. middleware. AuthenticationMiddleware',  'django. contrib. messages. middleware. MessageMiddleware',  'django. middleware. clickjacking. XFrameOptionsMiddleware',]ROOT_URLCONF = 'eshop. urls'TEMPLATES = [  {    'BACKEND': 'django. template. backends. django. DjangoTemplates',    'DIRS': [],    'APP_DIRS': True,    'OPTIONS': {      'context_processors': [        'django. template. context_processors. debug',        'django. template. context_processors. request',        'django. contrib. auth. context_processors. auth',        'django. contrib. messages. context_processors. messages',      ],    },  },]WSGI_APPLICATION = 'eshop. wsgi. application'# Database# https://docs. djangoproject. com/en/2. 0/ref/settings/#databasesDATABASES = {  'default': {    'ENGINE': 'django. db. backends. sqlite3',    'NAME': os. path. join(BASE_DIR, 'db. sqlite3'),  }}# Password validation# https://docs. djangoproject. com/en/2. 0/ref/settings/#auth-password-validatorsAUTH_PASSWORD_VALIDATORS = [  {    'NAME': 'django. contrib. auth. password_validation. UserAttributeSimilarityValidator',  },  {    'NAME': 'django. contrib. auth. password_validation. MinimumLengthValidator',  },  {    'NAME': 'django. contrib. auth. password_validation. CommonPasswordValidator',  },  {    'NAME': 'django. contrib. auth. password_validation. NumericPasswordValidator',  },]# Internationalization# https://docs. djangoproject. com/en/2. 0/topics/i18n/LANGUAGE_CODE = 'en-us'TIME_ZONE = 'Asia/Shanghai'USE_I18N = TrueUSE_L10N = TrueUSE_TZ = False# Static files (CSS, JavaScript, Images)# https://docs. djangoproject. com/en/2. 0/howto/static-files/STATIC_URL = '/static/'CORS_ORIGIN_ALLOW_ALL = TrueMEDIA_URL = '/media/'MEDIA_ROOT = os. path. join(os. path. dirname(BASE_DIR), 'estore', 'eshop', 'media')REST_FRAMEWORK = {  'DEFAULT_PAGINATION_CLASS': 'rest_framework. pagination. PageNumberPagination',  'PAGE_SIZE': 6,  'DEFAULT_PERMISSION_CLASSES': (    # 不写下面这一行, 默认也是 AllowAny    'rest_framework. permissions. AllowAny',  ),  'DEFAULT_AUTHENTICATION_CLASSES': (    'rest_framework. authentication. BasicAuthentication', # 最基本的验证方式    'rest_framework. authentication. SessionAuthentication', # Session 验证方式, 用于前后端一体的情况下使用    'rest_framework. authentication. TokenAuthentication', # Token验证方式, 用于前后端分离.   ),  # 节流类 - 限制用户访问次数  'DEFAULT_THROTTLE_CLASSES': (    'rest_framework. throttling. AnonRateThrottle', # 匿名用户节流类    'rest_framework. throttling. UserRateThrottle'  # 登录用户节流类  ),  'DEFAULT_THROTTLE_RATES': {    'anon': '100/day', # 限制匿名用户每天访问次数不超过100次    'user': '1000/day' # 登录用户同一个用户限制每天不超过1000次(所有页面)  }}SEARCH_PARAM = 'se222'eshop/urls. py: 1234567891011121314151617181920212223242526272829303132333435363738   eshop URL ConfigurationThe `urlpatterns` list routes URLs to views. For more information please see:  https://docs. djangoproject. com/en/2. 0/topics/http/urls/Examples:Function views  1. Add an import: from my_app import views  2. Add a URL to urlpatterns: path('', views. home, name='home')Class-based views  1. Add an import: from other_app. views import Home  2. Add a URL to urlpatterns: path('', Home. as_view(), name='home')Including another URLconf  1. Import the include() function: from django. urls import include, path  2. Add a URL to urlpatterns: path('blog/', include('blog. urls'))   from django. contrib import adminfrom django. conf. urls import url, includefrom django. conf import settingsfrom django. conf. urls. static import staticfrom rest_framework. authtoken import viewsurlpatterns = [  url(r'^admin/', admin. site. urls),  url(r'^computer/', include('computerapp. urls')),  # 让用户可以在API UI 上进行登录,登出  url(r'^api-auth', include('rest_framework. urls', namespace='rest_framework')),  # 让客户端可以通过API 的方式进行登录注册, 只能通过POST 方式提交  # 如果获取到token 信息, 就跳转  url(r'^api-token-auth', views. obtain_auth_token),]if settings. DEBUG:  urlpatterns += static(settings. MEDIA_URL, document_root=settings. MEDIA_ROOT)computerapp/models. py: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113from django. db import modelsfrom django. conf import settings# Create your models here. class Category(models. Model):       商品类别: 笔记本, 平板电脑, 一体机, 台式机, 服务器等       name = models. CharField(max_length=200, verbose_name='类别名称')  created = models. DateTimeField(auto_now_add=True, verbose_name='创建时间')  updated = models. DateTimeField(auto_now=True, verbose_name='更新时间')  def __str__(self):    return self. nameclass Manufacturer(models. Model):       生产厂商       name = models. CharField(max_length=200, verbose_name= 厂商名称 )  description= models. TextField(verbose_name='品牌描述')  # ImageField 图片类型字段  # upload_to 表示要上传到的位置.   logo = models. ImageField(blank=True, null=True, max_length=200, upload_to='manufacturer/uploads/%Y/%m/%d/', verbose_name='品牌图标')  created = models. DateTimeField(auto_now_add=True, verbose_name='创建时间')  updated = models. DateTimeField(auto_now=True, verbose_name='更新时间')  def __str__(self):    return self. nameclass Product(models. Model):       产品       model = models. CharField(max_length=200, verbose_name='产品名称')  description = models. TextField(verbose_name='产品描述')  image = models. ImageField(max_length=200, upload_to='product/uploads/%Y/%m/%d/', verbose_name='产品图片')  # DecimalField 带小数类型数字字段,  # max_digits = 12 表示小数点前后共12位.   # decimal_places = 2 小输掉保留两位  price = models. DecimalField(max_digits=12, decimal_places=2, verbose_name='产品价格')  # PositiveIntegerField 正整数类型字段  # sold 是销量  sold = models. PositiveIntegerField(default=0, verbose_name='产品销量')  # product_in 表示产品属于 . . . 类目  category = models. ForeignKey(Category, related_name='product_in', on_delete=models. DO_NOTHING, verbose_name='产品分类')  # product_of 表示产品属于 . . . 品牌  manufacturer = models. ForeignKey(Manufacturer, related_name='product_of', on_delete=models. DO_NOTHING, verbose_name='产品生产厂商')  created = models. DateTimeField(auto_now_add=True, verbose_name='创建时间')  updated = models. DateTimeField(auto_now=True, verbose_name='更新时间')  def __str__(self):    return self. modelclass DeliveryAddress(models. Model):       收货地址       # settings. AUTH_USER_MODEL 是认证用户模型, 推荐使用  user = models. ForeignKey(settings. AUTH_USER_MODEL, on_delete=models. DO_NOTHING, related_name='delivery_address_of', )  contact_person = models. CharField(max_length=200)  contact_mobile_phone = models. CharField(max_length=200)  delivery_address = models. TextField()  created = models. DateTimeField(auto_now_add=True)  updated = models. DateTimeField(auto_now=True)  def __str__(self):    return self. delivery_addressclass UserProfile(models. Model):       用户扩展信息       user = models. OneToOneField(settings. AUTH_USER_MODEL, related_name='profile_of', on_delete=models. DO_NOTHING, )  mobile_phone = models. CharField(blank=True, null=True, max_length=200)  nickname = models. CharField(blank=True, null=True, max_length=200)  description = models. TextField(blank=True, null=True)  icon = models. ImageField(blank=True, null=True, max_length=200, upload_to='user/uploads/%Y/%m/%d/')  created = models. DateTimeField(auto_now_add=True)  updated = models. DateTimeField(auto_now=True)  delivery_address = models. ForeignKey(DeliveryAddress, related_name='user_delivery_address', on_delete=models. DO_NOTHING, blank=True, null=True, )class Order(models. Model):       Order 订单       # 订单状态  STATUS_CHOICES = (    ('0', 'new'),    ('1', 'not paid'),    ('2', 'paid'),    ('3', 'transport'),    ('4', 'closed'),  )  status = models. CharField(choices=STATUS_CHOICES, default='0', max_length=2)  user = models. ForeignKey(settings. AUTH_USER_MODEL, on_delete=models. DO_NOTHING, related_name='order_of',)  # 产品详情描述  remark = models. TextField(blank=True, null=True)  product = models. ForeignKey(Product, related_name='order_product', on_delete=models. DO_NOTHING)  price = models. DecimalField(max_digits=12, decimal_places=2)  quantity = models. PositiveIntegerField(default=1)  address = models. ForeignKey(DeliveryAddress, related_name='order_address', on_delete=models. DO_NOTHING)  created = models. DateTimeField(auto_now_add=True)  updated = models. DateTimeField(auto_now=True)  def __str__(self):    return 'order of %d' % (self. user. id,)computerapp/admin. py: 12345678910111213141516171819202122232425262728293031323334353637from django. contrib import adminfrom . models import Category, Manufacturer, Product, UserProfile, DeliveryAddress, Order# Register your models here. class CategoryAdmin(admin. ModelAdmin):  list_display = ['id', 'name',]class ManuFacturerAdmin(admin. ModelAdmin):  list_display = ['id', 'name',]class ProductAdmin(admin. ModelAdmin):  list_display = ['id', 'model', 'price', 'category', 'manufacturer', 'sold',]  list_editable = ['price', 'sold', 'category',]class DeliveryAddressAdmin(admin. ModelAdmin):  list_display = ['id', 'user', 'contact_person', 'contact_mobile_phone', 'delivery_address',]class UserProfileAdmin(admin. ModelAdmin):  list_display = ['id', 'mobile_phone', 'nickname', 'user',]class OrderAdmin(admin. ModelAdmin):  list_display = ['id', 'status', 'user',]admin. site. register(Category, CategoryAdmin)admin. site. register(Manufacturer, ManuFacturerAdmin)admin. site. register(Product, ProductAdmin)admin. site. register(DeliveryAddress, DeliveryAddressAdmin)admin. site. register(UserProfile, UserProfileAdmin)admin. site. register(Order, OrderAdmin)computerapp/serializers. py: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109from django. contrib. auth. models import Userfrom computerapp. models import Product, Category, Manufacturer, UserProfile, DeliveryAddress, Orderfrom rest_framework import serializers# 自定义的用户信息的序列器class UserProfileSerializer(serializers. ModelSerializer):  class Meta:    model = UserProfile    fields = ('id', 'user', 'mobile_phone', 'nickname', 'description', 'icon', 'created', 'updated', 'delivery_address',)    # 这里定义了user 字段是只读. 因为user 的属性是 OneToOneField    read_only_fields = ('user', )# django 框架自带的用户信息的序列器class UserInfoSerializer(serializers. ModelSerializer):  # 从自定义的用户信息序列器中获取自定义的用户信息, 然后合并到框架的用户信息中.   profile_of = UserProfileSerializer()  class Meta:    model = User    # 这里的 profile_of 是在 models. py 中定义了 user 字段的 related_name='profile_of'    fields = ('id', 'username', 'email', 'first_name', 'last_name', 'date_joined', 'profile_of',)class UserSerializer(serializers. ModelSerializer):  class Meta:    model = User    fields = ('id', 'username', 'password', 'last_name', 'first_name', 'email', )    # extra_kwargs 定义 额外的关键词参数    # 目的是让 password 字段只能写, 但不是显示, 是专门为密码使用.     extra_kwargs = {'password': {'write_only': True}}  def create(self, validated_data):    user = User(**validated_data)    # 这里的set_password 是将传过来的密码进行加密, 然后再存入数据库    user. set_password(validated_data['password'])    user. save()    user_profile = UserProfile(user=user)    user_profile. save()    return userclass ProductListSerializer(serializers. ModelSerializer):  class Meta:    model = Product    fields = ('id', 'model', 'description', 'price', 'category', 'manufacturer', 'created', 'updated', 'image', 'sold',)class CategorySerializer(serializers. ModelSerializer):  class Meta:    model = Category    fields = ('id', 'name',)class ManufacturerSerializer(serializers. ModelSerializer):  class Meta:    model = Manufacturer    fields = ('id', 'name',)class ProductRetrieveSerializer(serializers. ModelSerializer):  # 下面两行可以注释掉, 但是产品详情就不会显示和产品相关的类目详细信息和厂商详细信息  # 下面梁行代码也可以通过 HyperLinkedIdentifiedField, HpyerLinkedRelatedField, 但是下面这种方法更好.   category = CategorySerializer()  manufacturer = ManufacturerSerializer()  class Meta:    model = Product    fields = ('id', 'model', 'description', 'price', 'category', 'manufacturer', 'created', 'updated', 'image', 'sold',)class DeliveryAddressSerializer(serializers. ModelSerializer):  class Meta:    model = DeliveryAddress    fields = ('id', 'user', 'contact_person', 'contact_mobile_phone', 'delivery_address', 'created', 'updated',)    read_only_fields = ('user',)class OrderListSerializer(serializers. ModelSerializer):  product = ProductListSerializer()  address = DeliveryAddressSerializer()  class Meta:    model = Order    fields = ('id', 'status', 'user', 'product', 'price', 'quantity', 'remark', 'address', 'created', 'updated', )class OrderCreateSerializer(serializers. ModelSerializer):  class Meta:    model = Order    fields = ('id', 'user', 'product', 'price', 'quantity', 'remark', 'address', 'created', 'updated',)    read_only_fields = ('user', 'price', 'address', )class OrderRUDSerializer(serializers. ModelSerializer):  class Meta:    model = Order    fields = ('id', )computerapp/views. py: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266from django. shortcuts import renderfrom rest_framework import genericsfrom rest_framework import permissionsfrom rest_framework. filters import OrderingFilter, SearchFilterfrom rest_framework. pagination import LimitOffsetPaginationfrom django. contrib. auth. models import Userfrom computerapp. models import Product, UserProfile, DeliveryAddress, Orderfrom computerapp. serializers import ProductListSerializer, ProductRetrieveSerializer, UserInfoSerializer, UserProfileSerializer, UserSerializer, DeliveryAddressSerializer, OrderListSerializer, OrderCreateSerializer, OrderRUDSerializerfrom rest_framework. views import APIViewfrom rest_framework. response import Responsefrom rest_framework. exceptions import NotFoundfrom rest_framework. throttling import AnonRateThrottleclass ProductListView(generics. ListAPIView):       产品列表       queryset = Product. objects. all()  serializer_class = ProductListSerializer  permission_classes = (permissions. AllowAny,)  # 可以过滤的后台, 第一个参数表示搜索功能, 第二个表示排序功能. 这两个参数是固定用法.   # 增加该字段后, API UI 出现一个 filters 功能  # 前端通过 GET /computer/product_list/?ordering=category&amp;search=%E5%B9%B3%E6%9D%BF 调用  filter_backends = (SearchFilter, OrderingFilter,)  # 这里是设置具体的可搜索字段.   # 如果不设置, 默认返回所有列表数据  search_fields = ('description', 'model')  # 这里是设置具体的可排序字段  # 如果不设置, 那么默认所有字段都可以排序  ordering_fields = ('category', 'manufacturer', 'created', 'sold',)  # 设置默认排序是id  ordering = ('id',)  # 指定分页方式为 LimitOffsetPagination, 这种方式的  # 之前的另一种 方式 PageNumberPagination 也可以用  # PageNumberPagination 方式 是显示页码为数字的方式(每页显示的个数是配置文件中 PAGE_SIZE 设置死的),  # LimitOffsetPagination 可以灵活的, 客户端可以通过携带 ?limit=2&amp;offset=6 参数设置每页显示2条数据, 显示第4页  pagination_class = LimitOffsetPaginationclass ProductListByCategoryView(generics. ListAPIView):       产品按类别展示列表       serializer_class = ProductListSerializer  permission_classes = (permissions. AllowAny,)  filter_backends = (SearchFilter, OrderingFilter,)  search_fields = ('description',)  ordering_fields = ('category', 'manufacturer', 'created', 'sold', 'stock', 'price',)  ordering = ('id',)  # 和 上面 ProductListView 类的区别就是用 get_queryset() 方法 代替了 queryset 属性, 重写了列表获取的条件  def get_queryset(self):    # 获取列表的条件是先获取 request 传递过来的 分类的的id    # query_params 表示查询参数. 前端可以通过GET 或者 POST 方式发送参数. 比如GET ?category=1 , 或者POST 过来的 category 的值    # 如果没有拿到值, 默认值是 None    category = self. request. query_params. get('category', None)    # 如果传递过来的信息非空 就进行过滤, 否则全部显示    if category is not None:      queryset = Product. objects. filter(category=category)    else:      queryset = Product. objects. all()    return querysetclass ProductListByCategoryManufacturerView(generics. ListAPIView):       产品按类别并按品牌展示列表       serializer_class = ProductListSerializer  permission_classes = (permissions. AllowAny,)  filter_backends = (SearchFilter, OrderingFilter,)  search_fields = ('description',)  ordering_fields = ('category', 'manufacturer', 'created', 'sold', 'stock', 'price',)  ordering = ('id',)  def get_queryset(self):    # 先获取分类id 和 厂商id    # 调用方法例子GET: ?category=1&amp;manufacturer=1 或者 POST 传递这两个参数    category = self. request. query_params. get('category', None)    manufacturer = self. request. query_params. get('manufacturer', None)    if category is not None:      queryset = Product. objects. filter(category=category, manufacturer=manufacturer,)    else:      queryset = Product. objects. all()    return querysetclass ProductRetrieveView(generics. RetrieveAPIView):       产品详情       queryset = Product. objects. all()  serializer_class = ProductRetrieveSerializer  permission_classes = (permissions. AllowAny,)class UserInfoView(APIView):       用户基本信息       permissions_classes = (permissions. IsAuthenticated,)  # 定义 GET 方法发送过来的数据的处理方法  # 在第三级的时候需要用 format=None 配合 urls. py中的format_suffix_patterns 进行content negotiation  # 第四级, 第五级就不用了  def get(self, request, format=None):    # 获取当前用户对象, 无法获取其他用户信息.     user = self. request. user    serializer = UserInfoSerializer(user)    return Response(serializer. data)class UserProfileRUView(generics. RetrieveUpdateAPIView):       用户其他信息更新获取       # 不能用下一行代码, 因为会让API 直接访问到其他用户的信息, 而不是当前用户的信息. 比如: http://127. 0. 0. 1:8001/computer/user_profile_ru/2/  # queryset = UserProfile. objects. all()  serializer_class = UserProfileSerializer  permission_classes = (permissions. IsAuthenticated,)  # 下面方法是用来限制用户只能访问自己的信息.   # get_object 用来只返回一个对象  def get_object(self):    # 获取当前用户.     user = self. request. user    # 通过当前用户,获取当前用户档案    obj = UserProfile. objects. get(user=user)    return objclass UserCreateView(generics. CreateAPIView):  serializer_class = UserSerializer# class DeliveryAddressLCView(generics. ListCreateAPIView):#      #   收货地址LC#      #   serializer_class = DeliveryAddressSerializer#   permission_classes = (permissions. IsAuthenticated,)##   def get_queryset(self):#     user = self. request. user#     # 获取当前用户的地址列表#     queryset = DeliveryAddress. objects. filter(user=user)#     return queryset##   def perform_create(self, serializer):#     user = self. request. user#     # serializer 是上面 serializer_class = DeliveryAddressSerializer 定义的序列器#     # serializer. save 保存的是 models. py 中 定义的DeliveryAddress 表#     s = serializer. save(user=user)#     # 获取当前用户的档案#     # 更新当前用户的默认收货地址, 如果不想这么做, 就把下面三行注释掉#     profile = user. profile_of#     profile. delivery_address = s#     profile. save()class DeliveryAddressLCView(generics. ListCreateAPIView):       收货地址LC       serializer_class = DeliveryAddressSerializer  permission_classes = (permissions. IsAuthenticated,)  def get_queryset(self):    user = self. request. user    queryset = DeliveryAddress. objects. filter(user=user)    return queryset  def perform_create(self, serializer):    user = self. request. user    s = serializer. save(user = user)    profile = user. profile_of    profile. delivery_address = s    profile. save()class DeliveryAddressRUDView(generics. RetrieveUpdateDestroyAPIView):       收货地址RUD       serializer_class = DeliveryAddressSerializer  permission_classes = (permissions. IsAuthenticated,)  def get_object(self):    user = self. request. user    try:      # 这里的 id=self. kwargs['pk'] 是获取前端url中传过来的关键字      # 为了安全, 不让当前用户看到其他用户的地址, 这里要传递两个参数绝对满足才能限制当前用户只能看到自己的地址.       obj = DeliveryAddress. objects. get(id=self. kwargs['pk'], user=user)    except Exception as e:      raise NotFound('not found')    return objclass CartListView(generics. ListAPIView):       购物车列表       serializer_class = OrderListSerializer  permission_classes = (permissions. IsAuthenticated,)  def get_queryset(self):    user = self. request. user    queryset = Order. objects. filter(user=user, status='0')    return querysetclass OrderListView(generics. ListAPIView):       订单列表       serializer_class = OrderListSerializer  permission_classes = (permissions. IsAuthenticated,)  # 设置节流限制  # throttle_classes = (AnonRateThrottle,)  def get_queryset(self):    user = self. request. user    queryset = Order. objects. filter(user=user, status__in=['1', '2', '3', '4'])    return querysetclass OrderCreateView(generics. CreateAPIView):       Order Create       queryset = Order. objects. all()  serializer_class = OrderCreateSerializer  permission_classes = (permissions. IsAuthenticated,)  def perform_create(self, serializer):    user = self. request. user    product = serializer. validated_data. get('product')    serializer. save(user=user, price=product. price, address=self. request. user. profile_of. delivery_address)class OrderRUDView(generics. RetrieveUpdateDestroyAPIView):       订单查改删       serializer_class = OrderRUDSerializer  permission_classes = (permissions. IsAuthenticated,)  def get_object(self):    user = self. request. user    obj = Order. objects. get(user=user, id=self. kwargs['pk'])    return obj  def perform_update(self, serializer):    user = self. request. user    serializer. save(user=user, status='1')VUE + django rest 框架实现前后端分离的电商项目: 预安装环境: mysql, navcat, python 虚拟环境, Vue. js mysql 客户端连接方式 1/Applications/XAMPP/xamppfiles/bin/mysql -uroot -pnavicat 连接方式  下载 navicat 客户端  启动 XAMPP 的 mysql 服务  创建MariaDB 连接, 并连接端口 3308, 用户名 root, 虚拟环境配置 创建一个 ecom 目录, 并进入该目录创建虚拟环境 123456mkdir ecomcd ecomwhich python3. 7virtualenv -p /Library/Frameworks/Python. framework/Versions/3. 7/bin/python3. 7 envsource env/bin/activatedeactivateVUE. js 环境配置 Node. js + cnpm 因为 vue 依赖 node. js 所以需要安装配置 node. 官网下载安装包 安装后提示如下: 1234This package has installed:	•	Node. js v9. 5. 0 to /usr/local/bin/node	•	npm v5. 6. 0 to /usr/local/bin/npmMake sure that /usr/local/bin is in your $PATH. 查看node 版本 12node --versionv9. 5. 0使用淘宝镜像 cnpm 代替 npm cnpm 淘宝官网 1234sudo npm i -g npmsudo npm install -g cnpm --registry=https://registry. npm. taobao. org# 安装包的使用方法cnpm install [name]准备前端代码和依赖包  将 online-store-master 前端代码目录拷贝到 ecom 目录中  然后cd 到 online-store-master 安装cnpm 依赖包 12cd online-store-mastercnpm install 安装完成后会产生一个node_modules 目录"
    }, {
    "id": 27,
    "url": "http://localhost:4000/Django-In-Action/",
    "title": "Django In Action",
    "body": "2020/05/10 - Django 是一个免费的开源 Python 框架，旨在消除后端编程中的麻烦和麻烦。有了它，开发人员可以快速创建复杂、安全和可扩展的应用程序。每个欣赏效率和干净代码的后端或全栈编码人员都可以从他们的工具箱中拥有 Django 中受益。本文我将带领大家从头开始了解 Django 的环境安装以及实现一些令人兴奋的基本功能. DjangoDjango 介绍: Django 1. 11 中文文档Django 官网 Pycharm 针对 Django 技巧:       快捷键   说明         option + fn + f12   打开/关闭 终端       cmd + 1   开启 project 侧边栏       control(+option) + r   运行项目               Django 的安装: 1234pip3. 7 install Django==2. 0# pip3. 7 install Django==1. 10. 6qpython3. 7 -m django --version  # 查看版本print(django. get_version())		# 查看版本查看安装的 django 有哪些 package 12345678910111213141516171819202122import djangohelp(django)PACKAGE CONTENTS  __main__  apps (package)  conf (package)  contrib (package)  core (package)  db (package)  dispatch (package)  forms (package)  http (package)  middleware (package)  shortcuts  template (package)  templatetags (package)  test (package)  urls (package)  utils (package)  views (package)django 基本命令: 查看 django 所有命令. 1python3. 7 manage创建项目 django-admin startproject 项目名: 新建一个 django 项目: 1django-admin startproject 项目名生成目录如下: 12345678/mysite	manage. py	/mysite		__init__. py		settings. py # 配置文件		urls. py		 # 设置路由		wsgi. py界面改成中文mysite/mysite/settings. py 12LANGUAGE_CODE = 'zh-Hans'# LANGUAGE_CODE = 'en-us'时间改成中国时间 123#TIME_ZONE = 'UTC'TIME_ZONE = 'Asia/Shanghai'USE_TZ = False创建项目中的应用 django-admin startapp 应用名: 一个应用  比如一个电商网站项目的用户管理算是一个应用 一个项目中可以有多个应用, 当然, 通用的 app 也可以在多个项目中出现新建一个应用 123django-admin startapp 应用名# 或者python manage. py startapp 应用名目录结构 12345678/polls	/migrations	__init__. py	admin. py	apps. py	models. py	tests. py	views. py启动 web 服务 python3. 7 manage. py runserver [端口]: 启动开发服务器 1python manage. py runserver [端口]当服务器端口被其他端口占用时, 端口号可以自定义 1python3. 7 manage. py runserver 8001让外网,局域网可访问的服务 1python3. 7 manage. py runserver 0. 0. 0. 0:8002迁移(创建)数据库 python3. 7 manage. py makemigrations python3. 7 manage. py migrate: 同步数据库 12python3. 7 manage. py makemigrationspython3. 7 manage. py migrate   命令成功后, 在项目目录下就可以看到一个 db. sqlite3   python manage. py makemigrations 当你在 models. py 中创建了数据类, 改动数据模型时, 运行这个命令就可以自动在数据库中创建表和修改表了. 创建超级用户 : python3. 7 manage. py createsuperuser 并进入管理界面: 创建用户名, 邮件, 密码 用户名 root 邮箱 37016175@qq. com 密码 @moyu**NI 输入完成后, 启动服务, 然后再进入下面URL 进入后台管理界面 http://127. 0. 0. 1:8000/admin/login/ 在 pycharm 中的数据表视图中可以看到 auth_user 表中多了一个记录. 清空数据 python3. 7 manage. py flush:  命令会提示 yes/no.  如果选择 yes, 将清空表中的所有数据, 只留下空表. 将数据库数据导出到文件中 python3. 7 manage. py dumpdata 应用名 &gt; 应用名. json:  导出数据到 json 文件将 json 数据导入到应用中 python3. 7 manage. py loaddata 应用名. json:  将 json 数据导入到应用中进入项目的 Terminal shell终端 python3. 7 manage. py shell:  进入项目的 Terminal shell终端 我们使用上述命令而不是简单地键入“python”进入python环境，是因为manage. py 设置了DJANGO_SETTINGS_MODULE 环境变量，该环境变量告诉Django导入mysite/settings. py文件的路径。进入项目的数据库 Terminal shell 终端 python3. 7 manage. py dbshell:  进入项目的数据库 Terminal shell 终端django项目中的常见文件: 网址入口 urls. py:  网址入口, 对应到 views. py中的一个函数. 视图文件 views. py:  响应用户发出的请求. 请求的数据是来自 urls. py 的.  通过渲染 templates中的网页可以将内容显示. 数据管理文件 models. py:  与数据库相操作相关的文件. 存取数据时用到. 如果不用数据库就可以不用这个文件. 表单处理文件 forms. py:  表单. 输入框生成, 用户的输入数据提交, 验证数据等工作. 模板目录 templates/:  views. py 中的函数用来渲染 templates 中的 html 模板, 得到最终的网页. 后台入口 admin. py:  后台, 可以用少量代码得到强大的后台. 配置文件 settings. py:  Django 的配置文件. 比如 debug 开关, 静态文件位置等. 创建一个 django 项目 django-admin startproject mysite: 概念: 一个项目会有多个应用. 一个应用也可以被多个项目使用. 将会在你的当前目录下生成一个 mysite 目录 12cd /Users/dalong/code/python1702/djdjango-admin startproject mysite生成目录如下 1234567mysite 	  # 根目录仅仅是项目的一个容器。 它的名字与Django无关；可以将其重命名为你喜欢的任何内容。	manage. py   # 一个命令行工具，可以使你用多种方式对Django项目进行交互。	mysite/  	 # 你的项目的真正的Python包。 它是你导入任何东西时将需要使用的Python包的名字		__init__. py  # 一个空文件，它告诉Python这个目录应该被看做一个Python包。		settings. py  # 该Django 项目的设置/配置		urls. py    # 此Django项目的URL声明；Django驱动的网站的路由。		wsgi. py    # 用于你的项目的与WSGI兼容的Web服务器入口。在项目中创建应用  python3. 7 manage. py startapp polls: 应用是一个Web应用程序，它完成具体的事项 —— 比如一个博客系统、一个存储公共档案的数据库或者一个简单的投票应用。 一个项目是特定网站的配置和应用程序的集合。 一个项目可以包含多个应用。 一个应用可以运用到多个项目中去。 创建一个叫 polls 的应用 12cd /Users/dalong/code/python1702/dj/mysite/python3. 7 manage. py startapp polls生成目录如下: 12345678polls/ #   __init__. py  admin. py  apps. py  migrations/  models. py  tests. py  views. py应用创建成功后分三个步骤修改  修改项目路径(路由): 应用名 创建应用路径(路由) 修改应用 views 文件(1) 修改项目路由 mysite/mysite/urls. py: 1234567891011from django. conf. urls import include, url # 1 from django. contrib import adminurlpatterns = [  url(r'^polls/', include('polls. urls')), # 2   url(r'^admin/', admin. site. urls) # 1]# 1.  include 用于处理路由的模块. 当你要包含其他URL模式时，应始终使用include()。 admin. site. urls是唯一的例外。# 2. 一旦匹配到以  polls/ 开头的路径,就对应到 polls文件夹中的 urls. py 文件(2) 创建应用路由.  mysite/polls/urls. py: 在生成的应用目录下创建一个 urls. py 的文件  此处的 name=’index’ 是定义了一个命名空间1234567from django. conf. urls import urlfrom . import viewsurlpatterns = [  url(r'^$', views. index, name='index'), # 如果本层的路径为空,那么就进入 views. py 中的 index 方法](3) 修改应用 views 文件: polls/views. py: 123456from django. shortcuts import renderfrom django. http import HttpResponsedef index(request): # index 这里指的就是路由定义的 index 方法  return HttpResponse( Hello, world. You're at the polls index.  ) # 定义相应信息 引入 HttpResponse 模块, 他是用来向网页返回内容的. 就像 print 一样,只不过是返回到网页上.  index 函数的第一个参数必须是 request. 包含了网页发过来的请求数据. 可以包含 GET 或 POST 的数据内容. 运行项目, 访问应用 python3. 7 manage. py runserver: 12345678cd /Users/dalong/code/python1702/dj/mysite/python3. 7 manage. py runserver#会有下面的信息展示出来. 其中 http://127. 0. 0. 1:8000/ 就是服务地址December 23, 2017 - 15:27:45Django version 2. 0, using settings 'mysite. settings'Starting development server at http://127. 0. 0. 1:8000/如果看到有 url 地址出来,就说明启动成功了, 用你的浏览器访问 http://127. 0. 0. 1:8000/。 请注意：    不要在任何生产环境使用这个服务器。 它仅仅是用于在开发中使用。 （我们的重点是编写Web框架，非Web服务器。）     更改端口: 可以自定义端口号          1python3. 7 manage. py runserver 8080             自动重载      开发服务器会根据需要自动重新载入Python代码。 你不必为了使更改的代码生效而重启服务器。 然而，一些行为比如添加文件，不会触发服务器的重启，所以在这种情况下你需要手动重启服务器。   访问应用:  http://127. 0. 0. 1:8000/polls/ http://127. 0. 0. 1:8000/learn/处理URL 中带的参数: 获取 GET 参数 ?a=1&amp;b=2: 在 views. py 中定义方法 12345# 测试获取url 中的 参数, 比如 http://127. 0. 0. 1:8000/blog/add/?a=1&amp;b=2def add(request):  a = request. GET. get('a')  b = request. GET. get('b')  return HttpResponse(str(int(a)+int(b)))设置urls. py 1url(r'^add/$', views. add, name='add'),获取 路径中的参数 add/3/4: 在 views. py中定义方法 123  # 获取url 中的数字路径参数进行加法运算 http://127. 0. 0. 1:8000/blog/add2/10/10/def add2(request, a, b):  return HttpResponse(str(int(a)+int(b)))// 设置urls. py 123url(r'^add2/(?P&lt;a&gt;\d+)/(?P&lt;b&gt;\d+)/$', views. add2, name='add2'),# 也可以不为传递的变量定义名称, 这样就是按照顺序在 view中赋值#url(r'^add2/(\d+)/(\d+)/$', views. add2, name='add2'),生成URL {% url %} 和 reverse: 通过 {% url %} 我们可以按照 urls. py 中定义的规则来反向生成 url: 1&lt;a href= {% url 'blog:add2' 4 5 %} &gt;{% url 'blog:add2' 4 5 %}&lt;/a&gt;会生成如下代码 1&lt;a href= /blog/add2/4/5/ &gt;/blog/add2/4/5/&lt;/a&gt;通过 reverse 反向生成 路径: 1234&gt;&gt;&gt; from django. urls import reverse&gt;&gt;&gt; reverse(viewname='blog:add2', args=(4,5))'/blog/add2/4/5/'&gt;&gt;&gt; 为 django 连接 mysql 并初始化数据库: 需要几个步骤如下:  启动 mysql 服务, 并在 mysql 中创建一个django 用的数据库 项目名/项目名/setting. py 文件中设置数据库配置信息, 同时设置 TIME_ZONE 项目名/项目名/__init__. py 导入 pymysql 模块 为 django 做数据库初始化 python3. 7 manage. py migrate(1)启动 mysql 服务, 并在 mysql 中创建一个django 用的数据库: create database py1702dj charset utf8;(2)mysite/mysite/setting. py 文件中设置数据库配置信息: 1234567891011121314DATABASES = {  'default': {    'ENGINE': 'django. db. backends. mysql',    'NAME': 'py1702dj',    'USER': 'root',    'PASSWORD': 'xxxxxx',    'HOST': '/Applications/XAMPP/xamppfiles/var/mysql/mysql. sock',     'CHARSET': 'utf8',    'PORT': '3308'  }}TIME_ZONE = 'Asia/Shanghai'(3)mysite/mysite/__init__. py 导入 pymysql 模块:  用 sqlite的的跳过这一步12import pymysqlpymysql. install_as_MySQLdb()(4)为 django 做数据库初始化 python3. 7 manage. py migrate: 初始化命令是根据以下两点决定如何进行初始化的. .  在setting 中读取数据库设置(DATABASES={}), 以及 setting 中的应用列表(INSTALLED_APPS=[])中了解到要对哪些 app 做数据库初始化. 进入到 项目第一层目录中运行命令 python3. 7 manage. py migrate    命令行会输出一堆信息   12345678Operations to perform: Apply all migrations: admin, auth, contenttypes, sessionsRunning migrations: Applying contenttypes. 0001_initial. . . OK . . . . .       进入 mysql 的 py2017dj 会看到一堆新的表, 这些表就是INSTALLED_APPS=[] 中的 app 需要用到的表.      auth_group   auth_group_permissions   auth_permission   auth_user   auth_user_groups   auth_user_user_permissions   django_admin_log   django_content_type   django_migrations   django_session   建模 models. py 数据模型. : QuerySet 查询数据库相关的操作手册 创建数据模型需要如下步骤  在应用名/models. py 文件中创建数据模型 将应用添加到 项目名/项目名/setting. py 文件中 将应用的数据模型转化为迁移文件 python manage. py makemigrations polls 再次初始化数据库 python manage. py migrate(1)在应用名/models. py 文件中创建数据模型:  每个类名对应一个表名 类中的每个属性对应一个字段 某些字段类型需要必选参数, 比如 max_length 等.  一个字段可以同时有多个参数 任何对models. py 文件的更改最好要做一次迁移.  字段默认是必填项(Not Null), 设置 blank=True, null=True 可以让字段变成非必填项     blank=True 是前端输入限制, blank=True 就是管理后台插入数据时,该字段可以什么都不输入. 插入后该字段的数据就是 ''   null=True 是数据库操作限制, null=True 就是在数据库中操作插入数据时,可以不给该字段指定数据. 插入后该字段的数据是 Null   一般都是 blank 和 null 一起用 blank=True, null=True   123456789101112131415161718192021from django. db import modelsfrom django. utils import timezoneimport datetime# Create your models here. class Question(models. Model):  # 字段默认是不能为空, 设置 blank=True, null=True 可以让字段变成非必填项  # question_text = models. CharField(max_length=200, null=True, blank=True)  question_text = models. CharField(max_length=200)   pub_date = models. DateTimeField('date published')	  # 定义一个类, 让生成的对象可以看到内容  def __str__(self):    return str(self. id) + self. question_text + str(self. pub_date)	  # 定义一个方法, 让返回的对象可以调用该方法查看是否是这个问题是在24小时内创建的  def was_registered_in24hours(self):    return self. pub_date &gt;= (timezone. now() - datetime. timedelta(days=1))   	# return self. pub_date &gt;= (timezone. now() - datetime. timedelta(days=1)) and (self. pub_date &lt;= timezone. now())定义字段的可选参数如下: 1234567def __init__(self, verbose_name=None, name=None, primary_key=False,       max_length=None, unique=False, blank=False, null=False,       db_index=False, rel=None, default=NOT_PROVIDED, editable=True,       serialize=True, unique_for_date=None, unique_for_month=None,       unique_for_year=None, choices=None, help_text='', db_column=None,       db_tablespace=None, auto_created=False, validators=(),       error_messages=None):(2)将应用添加到 mysite/mysite/setting. py 文件中: 将新建的站点追加到 INSTALLED_APPS 数组中 注意:  如果不添加, 那么就无法看到最终的模板1234567891011INSTALLED_APPS = [  'django. contrib. admin',     # 管理站点。 你会很快使用它。  'django. contrib. auth',     # 认证系统。  'django. contrib. contenttypes', # 用于内容类型的框架。  'django. contrib. sessions',   # 会话框架。  'django. contrib. messages',   # 消息框架。  'django. contrib. staticfiles',  # 管理静态文件的框架。	'polls. apps. PollsConfig',		# polls 是应用名; apps 表示 apps. py; PollsConfig 是 apps. py 中的一个类名# polls 文件夹下 apps. py 这个文件类 PollsConfig# apps. py 这个文件一般不需要修改]可以帮助新建的 app 找到模板文件(应用名/templates/)和静态文件(应用名/static/) (3)将应用的数据模型转化为迁移文件 python3. 7 manage. py makemigrations: 运行命令, 生成迁移文件 1234567python3. 7 manage. py makemigrations polls#会得到如下输出#Migrations for 'polls':# polls/migrations/0001_initial. py#  - Create model Choice#  - Create model Question#  - Add field question to choice 这时, 会生成一个新的迁移文件 应用名/migrations/0001_initial. py 该文将数据模型改换成了迁移文件的代码格式. 运行命令, 生成 SQL语句 python3. 7 manage. py sqlmigrate polls 0001:  这个步骤可以省略, 只要 makeimgrations + migrate 就可以了12345678python3. 7 manage. py sqlmigrate polls 0001# 输出 sql 建表语句#--#-- Create model Question#--#CREATE TABLE  polls_question  ( id  integer NOT NULL PRIMARY KEY AUTOINCREMENT, # question_text  varchar(200) NOT NULL,  pub_date  datetime NOT NULL);#COMMIT; sqlmigrate命令并不会在你的数据库上真正运行迁移文件 —— 它只是把Django 认为需要的SQL打印在屏幕上以让你能够看到。 这对于检查Django将要进行的数据库操作或者你的数据库管理员需要这些SQL脚本是非常有用的。(4)再次初始化数据库 python3. 7 manage. py migrate: 命令执行后,会在sqlite 库中生成新的表 123python3. 7 manage. py migrate# 命令行中可以看到 如下输出, 表示表已经在 DB 中成功初始化. # Applying polls. 0001_initial. . . OK migrate 命令会在应用列表中找到还没有同步的应用,然后将应用的迁移文件迁移到数据库中   任何对 models. py 的修改都会让 migrate 命令进行增量迁移.   修改你的模型（在models. py文件中）。之后的任何对数据模型的修改就简化为如下三步  修改你的模型（在models. py文件中）。 运行python3. 7 manage. py makemigrations ，为这些修改创建迁移文件 运行python3. 7 manage. py migrate ，将这些改变更新到数据库中。在django shell 中尝试查询数据: 更多数据查询方法  如果对数据库模型或者代码做了修改, 那么就需要退出命令行再进入 因为命令行驱动的时候只是读取当时的程序代码到内存中, 所以没有办法识别新的更改.  技巧: 要多用 tab 命令.      当创建一个对象 u 后, 输入u. + tab 可以看到这个对象的所有可能的方法和属性   当使用一个类的时候,也可以同上面一样操作.     pk 是 primary key 的意思, 和 id 一样1python3. 7 manage. py shell12345678910111213141516171819202122232425262728293031323334353637383940414243444546# 以下步骤是shell中逐条交互进行的from polls. models import Questionqset = Question. objects. all()print(queryset[5])[q for q in qset]from django. utils import timezonetimezone. now()str(timezone. now())q = Question(question_text='hello', pub_date=timezone. now())qq. question_textq. save()q = Question. objects. create(question_text='hahaha', pub_date=timezone. now()) # 另一种创建的方法. 不需要save()q. question_text =  byebye q = Question. objects. get(id=1) # 精确匹配, 返回一个对象, qq. was_registered_in24hours() # 返回True 因为是24小时创建的问题. q = Question. objects. filter(id=1) # 筛选, 返回一组对象, qq[0]# 这时去admin 后台的Question 那里增加一堆新的Questions. 然后再回来Question. objects. all() # 查看虽有数据Question. objects. filter(id__gt = 1) # 查看id 大于1 的数据Question. objects. filter(id__gte = 1) # 大于等于1Question. objects. filter(id__lt = 1) # 小于1Question. objects. filter(id__lte = 1) # 小于等于1Question. objects. filter(question_text__startswith = 'g') # question_text字段以g开头的所有数据. 不区分大小写Question. objects. filter(question_text__endswith = 'g') # 查以 g 结尾Py1702. objects. filter(reg_date__lte='2018-01-01 00:00:00') # 日期范围Py1702. objects. get(stu_name = 'xiaolong') # 大小写敏感Py1702. objects. get(stu_name__contains = 'iaolon') # 模糊匹配, 相当于sql语句的like '%iaolon%'Py1702. objects. filter(stu_name__contains = 'wang'). filter(stu_score__lte = 60) # 查名字带wang, 并且成绩小于等于60分的同学Py1702. objects. filter(stu_name__contains = 'wang'). filter(reg_date__year = '2018') # 查姓wang, 并且在2018年注册的同学Question. objects. filter(pub_date__year = timezone. now(). year) # 查询所有今年的数据Py1702. objects. exclude(stu_name__contains = 'wang'). filter(reg_date__year = '2018') # 查除了在2018年注册的姓王之外的其他同学Py1702. objects. filter(id__in = [1,2,3]) # 按指定id 获取数据p = Py1702. objects. filter(id=1)# pk 是 primary key 的意思, 和 id 一样Question. objects. get(pk=1)p[0]. delete() # 删除对象Py1702. objects. all(). delete() # 清空数据表Py1702. objects. filter(id__in = [1,2,3]). update(stu_score = 100) # 同时更新多条记录Py1702. objects. all(). update(stu_score = 100) # 更新所有记录Py1702. objects. count() # 统计条数在管理后台管理新建的表对象 polls/admin. py:  为了可以在管理后台管理表, 需要创建一个管理类 这个管理类需要列出想要管理的字段名称, 然后再admin. site上进行注册. 1234567891011from django. contrib import adminfrom . models import Question# Register your models here. # 用列表形式显示class QuestionAdmin(admin. ModelAdmin):  list_display = ['id', 'pub_date', 'question_text']admin. site. register(Question, QuestionAdmin)修改完成后,进入后台, 就可以看到可以管理的数据表了  在后台可以操作对表CRUD 操作. 外键关联, 反向关联, models. ForeignKey(Question, on_delete=models. CASCADE): polls/models. py 12345678910class Choice(models. Model): 	# 建立外键, 第一个参数是目标的表对象, 第二个参数是在目标对象删除某数据时, 级联当前表中的数据也被删掉. 不如删掉了吃饭的问题,那么关于吃饭的选项也会被删掉  question = models. ForeignKey(Question, on_delete=models. CASCADE)#  question = models. ForeignKey(Question, on_delete=models. DO_NOTHING) # 不使用级联操作  choice_text = models. CharField(max_length=200)  vote = models. IntegerField(default=0)  		# 整数  vote = models. PositiveIntegerField(default=0) 	# 正整数  def __str__(self):    return self. choice_textForeignKey:    A many-to-one relationship     建立外键, 第一个参数是目标的表对象, 第二个参数是在目标对象删除某数据时, 级联当前表中的数据也被删掉. 不如删掉了吃饭的问题,那么关于吃饭的选项也会被删掉      on_delete 选项:         CASCADE: 级联删除外键对应的数据对象     PROTECT: 当有外键关联到这个对象的时候, 将不进行删除操作, 并报错. 如果没有外键关联到这个对象, 则执行删除操作.      DO_NOTHING: 不对外键关联做任何保护, 可以随意删除.              外键的另一个用处就是能够在目标表中产生反向引用. 可以在直接查询某个 question 对象的所有对应 choice   12q = Question. objects. get(id=1)q. choice_set. all()    ​  字段整型的定义: IntegerField:  Values from -2147483648 to 2147483647PositiveIntegerField :  Values from 0 to 2147483647下面是对应的交互命令行的操作 12345678910111213141516from polls. models import Questionfrom polls. models import Choicefrom django. utils import timezoneq = Question. objects. create(question_text='are u hangury now?', pub_date=timezone. now())c = Choice. objects. create(choice_text='yes, very', vote=0, question=q)c. idc. question_idc. question. question_textq. choice_set. all()q. choice_set. create(choice_text='No, not really. ', vote = 9) q. choice_set. count() # 统计条数len(q. choice_set. all()) # 也可以统计c = q. choice_set. all()[0]c. choice_text = '555'c. save()q. delete() # 级联删除q 和与它关联的所有choice 表中的数据创建模板文件: 在 polls/views. py 中增加一些新的视图函数  order_by() 是排序功能, 参数需要是参数, 倒序的话,参数前面加一个 - 号     order_by() 默认是按照id 正序排列    [:5] 表示从头显示到第5条记录 ​1234567891011121314151617181920212223242526272829303132333435363738394041from django. http import HttpResponsefrom . models import Question# loader 是 django 的模板加载器# from django. template import loaderfrom django. shortcuts import renderfrom django. shortcuts import get_object_or_404# 定义一个视图函数, 名字叫做index# def index(request):#   return HttpResponse( Hello, world, I'ma in Shanghai )# def index(request):#   latest_question_list = Question. objects. order_by('-pub_date')[:5]#   output = ', '. join([q. question_text for q in latest_question_list])#   return HttpResponse(output)def index(request):  latest_question_list = Question. objects. order_by('-pub_date')[:5]  # 这里会从 应用目录下的templates 目录中寻找参数中指定的模板.   # template 获得一个 list  # template = loader. get_template('polls/index. html')  context = {    'latest_question_list': latest_question_list,  }	# return HttpResponse(template. render(context, request))  return render(request, 'polls/index. html', context)# 第二个参数的变量名是在 urls. py 中正则表达式中定义的变量名. def detail(request, question_id): 	# # 获取多个参数  # return HttpResponse( you are viewing question %s. second id is %s  % (question_id, second_id))  return HttpResponse( you are viewing question %s.   % question_id)def result(request, question_id):  response =  You are looking at the result of questoin %s.    return HttpResponse(response % question_id)def vote(request, question_id):  response =  You are voting at the question %s.    return HttpResponse(response % question_id)   另一种模板渲染方式: , 上面代码中把这部分注释打开   123# from django. template import loader# template = loader. get_template('polls/index. html')# return HttpResponse(template. render(context, request))   在 polls/urls. py 增加正则路由 123456789# 第一个参数是正则表达式, 可以将内容赋给一个变量, 变量要在回调函数中作为参数引入. # 第二个参数是回调函数, 第三个参数是定义的命名空间, 在后面的 reverse() 或者 url() 中会用到# # 多个参数的传递方法# url(r'^(?P&lt;question_id&gt;[0-9]+)/(?P&lt;second_id&gt;[\w]+)/$', views. detail, name='detail'),url(r'^(?P&lt;question_id&gt;[0-9]+)/$', views. detail, name='detail'),url(r'^(?P&lt;question_id&gt;[0-9]+)/results/$', views. result, name='result'),url(r'^(?P&lt;question_id&gt;[0-9]+)/vote/$', views. vote, name='vote'),在 polls目录下创建 templates/polls/index. html 文件 templates/polls/index. html  模板中的链接url 都默认从应用目录下的templates 目录下开始寻找12345678910{% if latest_question_list %}  &lt;ul&gt;    {% for question in latest_question_list %}   		{# 此处的路径/polls/ 指的是 polls 目录下 templates 下面的路径 #}      &lt;li&gt;&lt;a href= /polls/{{ question. id }}/ &gt;{{ question. question_text }}&lt;/a&gt;&lt;/li&gt;    {% endfor %}  &lt;/ul&gt;{% else %}  &lt;p&gt;No polls are available. &lt;/p&gt;{% endif %}增加详情页的视图函数 polls/views. py  get_object_or_404 需要 from django. shortcuts import get_object_or_4041234def detail(request, question_id):  question = Question. objects. get(id=question_id)  # question = get_object_or_404(Question, id=question_id)  return render(request, 'polls/detail. html', {'question': question})增加详情页模板 1234567&lt;h1&gt;{{ question. question_text }}&lt;/h1&gt;&lt;ul&gt;  {# django 模板要求函数不能输入后面的括号, 所以如果写 all() 会报错. #}  {% for choice in question. choice_set. all %}    &lt;li&gt;{{ choice. choice_text }}&lt;/li&gt;  {% endfor %}&lt;/ul&gt;修改debug 模式: 修改 django 的 debug 模式 mysites/settings. py 1234DEBUG = FalseALLOWED_HOSTS = ['localhost', '127. 0. 0. 1',]自定义的报错信息页面: polls/views. py 12345678910111213from django. shortcuts import Http404def detail(request, question_id):  # question = Question. objects. get(id=question_id)  try:    # question = get_object_or_404(Question, id=question_id)    question = Question. objects. get(id=question_id)  except Question. DoesNotExist:    print('hahaha')    raise Http404('Question does not exit')  return render(request, 'polls/detail. html', {'question': question})命名空间: 命名空间的意义:    让模板文件中的超链接在多个app间跳转的时候,不会因为app中有同名的函数而找错对应的函数   ​  使用方法: (1) 在 polls/urls. py 中为 app 定义一个命名空间变量 app_name='polls' 123# 下面这个变量是polls 的命名空间上定义的app名# 如果省略下面这个变量, 那么在模板中也要响应的去掉app的命名指向app_name = 'polls'(2) 在模板文件 templates/polls/index. html 中将原来的超链接href属性进行修改 1234 {# 此处的路径/polls/ 指的是 polls 目录下 templates 下面的路径 #}{# &lt;li&gt;&lt;a href= /polls/{{ question. id }}/ &gt;{{ question. question_text }}&lt;/a&gt;&lt;/li&gt; #}{# &lt;li&gt;&lt;a href= {% url 'detail' question. id %} &gt;{{ question. question_text }}&lt;/a&gt;&lt;/li&gt; #}&lt;li&gt;&lt;a href= {% url 'polls:detail' question. id %} &gt;{{ question. question_text }}&lt;/a&gt;&lt;/li&gt; url 函数用来生成URL创建form 表单 和 处理表单 templates/polls/detail. html: csrf - Cross-site request forgery跨站请求伪造    是用来防止跨域攻击的, 下面代码会生成如下 html     &lt;input type='hidden' name='csrfmiddlewaretoken' value='viyT4IFnnKWBfjzbgjQFei7D0YCiHe3kd5peZUpCtDVQ5azUAGmvuQxkk6JpAdZG' /&gt;  forloop  是用来控制当前循环的序号, 可以展示循环计数, 可以反向展示等等. 具体用法可以输入forloop. 然后看提示 forloop. counter 是用来展示当前循环的序号123456789101112131415161718192021222324{#&lt;h1&gt;{{ question. question_text }}&lt;/h1&gt;#}{#&lt;ul&gt;#}  {# django 模板要求函数不能输入后面的括号, 所以all() 会报错. #}{#	{% for choice in question. choice_set. all %}#}{#	  &lt;li&gt;{{ choice. choice_text }}&lt;/li&gt;#}{#	{% endfor %}#}{#&lt;/ul&gt;#}&lt;h1&gt;{{ question. question_text }}&lt;/h1&gt;{% if error_message %}  &lt;p&gt;&lt;strong&gt;{{ error_message }}&lt;/strong&gt;&lt;/p&gt;{% endif %}&lt;form action= {% url 'polls:vote' question. id %}  method= post &gt;  {# csrf 是用来防止跨域攻击的, 下面代码会生成如下 html  #}  {# &lt;input type='hidden' name='csrfmiddlewaretoken' value='viyT4IFnnKWBfjzbgjQFei7D0YCiHe3kd5peZUpCtDVQ5azUAGmvuQxkk6JpAdZG' /&gt; #}  {% csrf_token %}  {% for choice in question. choice_set. all %}    {# forloop 是用来控制当前循环的序号, 可以展示循环计数, 可以反向展示等等. 具体用法可以输入forloop. 然后看提示 #}    {# forloop. counter 是用来展示当前循环的序号 #}    &lt;input type= radio  name= choice  id= choice{{ forloop. counter }}  value= {{ choice. id }} /&gt;    &lt;label for= choice{{ forloop. counter }} &gt;{{ choice. choice_text }}&lt;/label&gt;&lt;br/&gt;  {% endfor %}  &lt;input type= submit  value= 提交 /&gt;&lt;/form&gt; form 提交信息到 polls/views. py 的 vote函数中修改 polls/views. py 的 vote 方法 和 result 方法  request. POST[]可以获取所有post 过来的变量 HttpResponseRedirect 用于重镜像 reverse 的作用和模板中的 url 函数一样, 都是反向建立URL 路径.  区别是 url 用在模板中, reverse 用在python 文件中 reverse 通过urls. py 的规则反向的创造出url. “reverse(viewname=’polls:result’, args=(question. id,)))” 可以创建的url 是 “/polls/3/result/” args 参数是一个tuple 元组, 要注意, 如果只有一个参数, 后面一定加一个逗号12345678910111213141516171819202122232425262728293031323334353637from django. http import HttpResponseRedirectfrom django. urls import reversefrom django. shortcuts import get_object_or_404def vote(request, question_id):  question = get_object_or_404(Question, pk=question_id)  try:    # request. POST[]可以获取所有post 过来的变量    # print(str(request. POST))    # 打印获得变量如下:    # &lt;QueryDict: {'csrfmiddlewaretoken': ['RnjlKSnzipfrHvFzDFvGreSbQeKCCQGzzaaGF47OoieGxmFiX21wHMiSamRJvPCV'], 'choice': ['3']}&gt;    selected_choice = question. choice_set. get(pk=request. POST['choice'])  # 如果是没有选择内容选项就提交, 那么就会走到 except中  # KeyError 的定义: Raised when a mapping (dictionary) key is not found in the set of existing keys.   # DoesNotExist 官方定义: 当一个给定参数的查询未能发现对象时抛出该异常。  except (KeyError, Choice. DoesNotExist): #    return render(request, 'polls/detail. html', context={      'question': question,      'error_message': '还没选中选项',    })  else:    selected_choice. vote += 1    selected_choice. save()    # HttpResponseRedirect 用于重镜像    # reverse 的作用类似于模板中的 url 函数, 只不过这个reverse 用在view. py 中    # reverse 通过urls. py 的规则反向的创造出url.  reverse(viewname='polls:result', args=(question. id,)))  可以创建的url 是  /polls/3/result/     # reverse 的第一个参数是要跳转的目标地址的命名空间    # reverse 的第二个参数是跳转需要带的参数, 用 args 变量来携带    # 这里的 args 参数是一个tuple 元组, 要注意, 如果只有一个参数, 后面一定加一个逗号		# reverse 第一个参数一定注意, polls:result 中间是冒号,不是 /    return HttpResponseRedirect(reverse(viewname='polls:result', args=(question. id,)))      def result(request, question_id):  question = get_object_or_404(Question, id=question_id)  return render(request, 'polls/result. html', context={'question': question})最后在增加一个 result 模板  犯的错误: polls:detail, 这里中间是冒号, 别写成 /12345678&lt;h1&gt;{{ question. question_text }}&lt;/h1&gt;&lt;ul&gt;	{% for choice in question. choice_set. all %}		{#  这里 |pluralize 可以判断管道符前面的内容是1还是大于1, 然后来决定后面是否加 s 复数  #}	  &lt;li&gt;{{ choice. choice_text }} -- {{ choice. vote }} vote{{ choice. vote|pluralize }}&lt;/li&gt;	{% endfor %}&lt;/ul&gt;&lt;a href= {% url 'polls:detail' question. id %} &gt;再投一票&lt;/a&gt;代码重构 - 让投票app 面向对象: 注意:  model = Question 和 def get_queryset(self): 可以二选一, 只要用一个就可以.      get_queryset() 方法用来对默认的排序方式进行调整和过滤    context_object_name 可以省略, 但前提是template模板中循环的对象的变量名必须是和models 中定义的模型的类名一致(头字母变成小写)     比如, models. py中定义的类是 class Question(models. Model): 那么 模板中就需要使用 {{ question. question_text }}   polls/views. py 123456789101112131415161718192021222324252627282930313233343536373839404142434445from django. views import genericfrom django. utils import timezone# 命名规范: view. py 中所有的类名都要以View 结尾, 比如 XxxxView# generic. ListView 是普通类型. 列表视图class IndexView(generic. ListView):  # template_name 是基类中的属性, 所以名字不能拼错  template_name = 'polls/index. html'  # context_object_name 也是基类的属性, 不能拼错  # latest_question_list 是一个自定义的列表的名字.   context_object_name = 'latest_question_list'  # self 表示当前对象.   # get_queryset , 定义获取列表数据的方法. 方法将获取的列表数据赋值到latest_question_list 变量,并返回一个列表就可以了.   # 这个get_quertset 方法不是必须的  def get_queryset(self):    return Question. objects. order_by('-pub_date')[:5]      # DetailView 基层自class DetailView(generic. DetailView):  # model 是一个基类定义的变量名,不能随便改.   # model 变量定义了这个detail view 需要使用的申诉局是 Question中的数据  model = Question  template_name = 'polls/detail. html'    def get_queryset(self):    #    return Question. objects. filter(pub_date__lte=timezone. now())      # ResultView 也是继承自DetailView 基类class ResultView(generic. DetailView):  # model  model = Question  template_name = 'polls/result. html'  # def get_queryset(self):  #   return Question. objects. filter(pub_date__lte=timezone. now())polls/urls. py 12345678# 重构后index, url 的回调函数使用方法IndexView. as_view() , 理解为作为view来使用. url(r'^$', views. IndexView. as_view(), name='index'),# 重构后的 detail. 参数必须改成pk, pk是基类中定义的. url(r'^(?P&lt;pk&gt;[0-9]+)/$', views. DetailView. as_view(), name='detail'),url(r'^(?P&lt;pk&gt;[0-9]+)/result/$', views. ResultView. as_view(), name='result'),模板中的展示 mysite/template/detail. html {% for choice in question. choice_set. all %} 12345  {# forloop 是用来控制当前循环的序号, 可以展示循环计数, 可以反向展示等等. 具体用法可以输入forloop. 然后看提示 #}  {# forloop. counter 是用来展示当前循环的序号 #}  &lt;input type= radio  name= choice  id= choice{{ forloop. counter }}  value= {{ choice. id }} /&gt;  &lt;label for= choice{{ forloop. counter }} &gt;{{ choice. choice_text }}&lt;/label&gt;&lt;br/&gt;{% endfor %}优化样式: 创建静态内容的目录和文件: polls/static/polls/images/background2. jpg polls/static/polls/style. css css 文件如下: 12345678910li a {  color: red;}body{  /*这里的图片路径是针对于css 文件自己的相对路径*/  background: white url( images/background2. jpg ) no-repeat center center;  text-align: center;}templates/index. html 作如下修改 123456{# load static 是django 模板自带的方法, 注意拼写 #}{% load static %}&lt;link rel= stylesheet  type= text/css  href= {% static 'polls/style. css' %} /&gt;后台管理界面的定制化: 可以控制列表显示的内容和编辑页面显示的可编辑字段 polls/admin. py  list_display: 定义显示的字段 fieldsets: 对可编辑的字段集合进行分组显示.  inlines: 增加 Question对象外键关联的Choice 内容, 定义列表中的内容是上面类的名字 list_filter: 定义管理界面右侧的过滤项 search_fields: 定义可搜索的字段12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849from django. contrib import adminfrom . models import Question, Choice# Register your models here. # 这个ChoiceInline 需要继承 一个显示以后台的显示样式class ChoiceInline(admin. StackedInline):  # 指定 model 的内容是 Choice  model = Choice  # 除了显示原有的内容外, 再增加三个空白选项,让管理员可以增加新的内容.   extra = 3# 用列表形式显示class QuestionAdmin(admin. ModelAdmin):  # 控制列表页面显示哪些字段  list_display = ['id', 'question_text', 'pub_date', ]  # 可以控制详情页面可以显示哪些内容  # fields = ['pub_date', 'question_text']  # 对可编辑的字段集合进行分组显示.   fieldsets = [    # 第一栏的None 表示没有标题. 包含的字段是 question_text    # 包含的字段中不能加 id 字段    ('问题名称', {'fields': ['question_text']}),    # 第二栏的标题是 Date information, 包含的字段是 pub_date    ('发布日期', {'fields': ['pub_date']}),  ]  # 定义在Question对象外键关联的Choice 内容, 定义列表中的内容是上面类的名字  inlines = [ChoiceInline]  # 在Question 管理界面添加一个筛选项, 可以对应增加很多筛选的  # 过滤项的标题是在 models. py文件中定义的  list_filter = ['pub_date', 'question_text']  # 增加搜索功能  search_fields = ['question_text']class ChoiceAdmin(admin. ModelAdmin):  list_display = ['id', 'choice_text', 'vote']  fieldsets = [    ('选择项名称', {'fields': ['choice_text']}),    ('票数', {'fields': ['vote']}),  ]  list_filter = ['choice_text', 'vote']  search_fields = ['choice_text']admin. site. register(Question, QuestionAdmin)admin. site. register(Choice, ChoiceAdmin)测试驱动开发 - 自动化测试. :  下面的测试是针对 sqlite 进行的.  如果换成 mysql, 需要连接的账号有 mysql 的建表建库权限(比如 root). 测试目标: 针对 models. py 进行测试:  was_published_recently() 方法的目的是验证一个问卷问题是否在过去的24小时之内发表的.  测试目标: 用三个case 测试这个方法,     用一个未来时间测试该方法, 预期结果: False   用一个过去24小时之前的时间测试该方法, 预期结果: False   用一个过去24小时之内的时间测试该方法, 预期结果: True   创建测试步骤:: (1) polls/models. py 文件中的一段代码改成有bug( 问题可以发布自未来 ) 1234# 查看最近24小时内发布的问题, 且不是发布自未来  def was_published_recently(self):    # return self. pub_date &gt;= (timezone. now() - datetime. timedelta(days=1)) and (self. pub_date &lt;= timezone. now())    return self. pub_date &gt;= (timezone. now() - datetime. timedelta(days=1))(2)在 polls/test. py 文件中增加如下代码  test. py 文件的执行会自动调用测试数据, 不会影响现有数据库. 执行完毕后, 就会把测试数据库删掉.  测试模型的类名随便起, 但需要继承 TestCase 的测试用例基类 类中的所有测试方法执行顺序不是顺序执行, 而是因及其不同而异 如果一个方法中有数据在表中创造出来, 那么在这个方法结束的时候, 数据也会从表中删除, 不会影响下一个方法的测试.  每个测试方法中都要定义测试对象以及用断言校验测试对象的执行结果 技巧: 每个断言都是测试失败的可能, 那么排除了所有的失败, 最后的结果就是成功. 1234567891011121314151617181920212223242526272829303132# 引入django 测试用例模块from django. test import TestCase# Create your tests here. import datetimefrom django. utils import timezonefrom . models import Question# 测试模型的名字随便起, 但需要继承 TestCase 的测试用例基类class QuestionModelTests(TestCase):  # 测试一个问卷问题是否可以从未来发布的可能性.   def test_was_published_recently_with_future_question(self):    # 创建一个未来30天的时间点    time = timezone. now() + datetime. timedelta(days=30)    # 创建一条测试数据对象    # 测试中, 创建对象可以不用填必选字段. 只要用需要测试的字段创建对象即可    future_question = Question(pub_date=time)    # 执行 is 断言测试, 验证 数据对象was_published_recently() 是 False    self. assertIs(future_question. was_published_recently(), False)  # 测试一个问卷问题是否可以从过去1天之前发布.   def test_was_published_recently_with_old_question(self):    time = timezone. now() - datetime. timedelta(days=1, seconds=10)    old_question = Question(pub_date=time)    self. assertIs(old_question. was_published_recently(), False)  # 测试一个正确的时间段的问题可以被正确验证.   def test_was_published_recently_with_rencent_question(self):    time = timezone. now() - datetime. timedelta(hours=23, minutes=59, seconds=59)    recent_question = Question(pub_date=time)    self. assertIs(recent_question. was_published_recently(), True)(3) 执行测试命令 python3. 7 manage. py test polls  命令中不需要指定测试文件, 只要指定测试的app 名字 django 框架会帮助TDD 创建一个临时空数据库和对应测试需要的空表1python3. 7 manage. py test polls有错误的输出结果如下: 1234567891011121314151617Creating test database for alias 'default'. . . System check identified no issues (0 silenced). F======================================================================FAIL: test_was_published_recently_with_future_question (polls. tests. QuestionModelTests)----------------------------------------------------------------------Traceback (most recent call last): File  /Users/dalong/code/python1702/django/day2/project/mysite/polls/tests. py , line 22, in test_was_published_recently_with_future_question  self. assertIs(future_question. was_published_recently(), False)AssertionError: True is not False----------------------------------------------------------------------Ran 1 test in 0. 001sFAILED (failures=1)Destroying test database for alias 'default'. . . 改掉bug, 无错误的数据结果如下: 123456789Creating test database for alias 'default'. . . System check identified no issues (0 silenced). . ----------------------------------------------------------------------Ran 1 test in 0. 000sOKDestroying test database for alias 'default'. . . 一些常用断言:       断言   说明         assertEqual   断言相等, 相当于 ==, 相等返回True       assertNotEqual   断言不等, 相当于 !=, 不等返回True       assertFalse   断言结果是 False , 是False 返回True       assertTrue   断言结果是 True , 是True 返回True       assertGreater   断言大于(assertTrue(a &gt; b)), 大于返回True       assertGreaterEqual           assertLess   断言小于(assertTrue(a &lt; b)), 小于返回True       assertLessEqual           assertIsNone   断言对象是空(assertTrue(obj is None)) 空返回True       assertIsNotNone           assertIn   断言包含(assertTrue(a in b)) 包含返回True       assertNotIn           assertIs   断言是(assertTrue(a is b)) 是返回True       assertIsNot           assertIsInstance   断言是实例(assertTrue(isinstance(obj, cls))) 是返回True       assertNotIsInstance           assertListEqual   断言是列表 , 是返回True       assertDictEqual   断言是字典, 是返回True       assertTupleEqual   断言是元组, 是返回True       assertSetEqual   断言是集合, 是返回True       assertRaises   断言异常, with self. assertRaises(SomeException) as cm:       assertContains   断言包含(assertContains(response, “NoN polls are available. ”)) 包含返回True       assertQuerysetEqual   断言query集包含assertQuerysetEqual(response. context[‘latest_question_list’], []), 一般是在view 函数中定义的context_object_name = 'latest_question_list' 包含返回True   django 命令行 进行测试: 123456789101112131415161718192021222324252627# 导入测试用的环境变量from django. test. utils import setup_test_environment# 执行设置测试环境方法setup_test_environment()# 从测试模块引入客户端类from django. test import Client# 实例化模拟浏览器客户端client = Client()# 用 GET 方法访问根目录, 将结果存到response 中. # 如果这时web 服务已经启动, 这里默认请求的就是 http://127. 0. 0. 1:8000/response = client. get('/') # 结果是 Not Found: /rp. + 双击 tab # 查看响应对应的所有属性和方法. # 查看响应的状态码response. status_code # 输出 404response = client. get('/polls/') response # 输出一个 响应对象 &lt;TemplateResponse status_code=200,  text/html; charset=utf-8 &gt;response. status_code # 输出 200# 获得响应的内容. 就是HTML 中的内容. response. content # 获得响应的上下文, 这里是django 设置的环境变量. 上下文中已经带上了view中设置的context_object_name 设置的对象response. contextresponse. context['latest_question_list']# 进入详情页response = client. get('/polls/3/')# 获得详情页当前对象的信息. 这里的object 就是 view. py 中的 context_object_name 对应的对象response. context['object']. idresponse. context['object']. question_text 更多的 状态码测试目标: 针对 views. py 进行测试: 首页测试用例 polls/test. py  client. get(reverse('polls:index')) 这里的 get 是 http的请求方法GET 的意思.  类中的所有测试方法不会依次执行, 而是乱的 如果一个方法中有数据在表中创造出来, 那么在这个方法结束的时候, 数据也会从表中删除, 不会影响下一个方法的测试.  每个测试方法中都要定义测试对象以及用断言校验测试对象的执行结果 测试数据就是要找各种边界条件的数据, 比如测试过去五天的数据,那就要测试过去1秒和过去5天零1秒的数据. 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758from django. urls import reverse# 一个创建问题的函数, 接收问题和创建日期两个参数def create_question(question_text, days):  time = timezone. now() + datetime. timedelta(days=days)  # crete() 方法的好处是不用 save()  return Question. objects. create(question_text=question_text, pub_date=time)class QuestionIndexViewTests(TestCase):  def test_no_questions(self):    # print( ### reverse URL ###  + reverse('polls:index')) # /polls/    response = self. client. get(reverse('polls:index'))    self. assertEqual(response. status_code, 200)    # print('### response ###:' + str(response)) # &lt;TemplateResponse status_code=200,  text/html; charset=utf-8 &gt;    # 包含断言    self. assertContains(response,  No polls are available.  ) # 参数中的文字是在    # print('### latest ###:' + str(response. context['latest_question_list'])) # &lt;QuerySet []&gt;    self. assertQuerysetEqual(response. context['latest_question_list'], [])  # 测试目标: 测试过去发表的问题, 预期是能从浏览器看到数据.   def test_past_question(self):    create_question(question_text= Past question.  , days=-30)    response = self. client. get(reverse('polls:index'))    self. assertQuerysetEqual(response. context['latest_question_list'], ['&lt;Question: Past question. &gt;'],)  # 测试目标: 测试未来问题.   # 测试方法: 创建一个未来问题,并从网页获取, 预期获取不到未来数据  def test_future_question(self):    create_question(question_text='Future question. ', days=1)    response = self. client. get(reverse('polls:index'))    self. assertNotContains(response, 'Future question. ')	# 插入两条数据,一个是过去的,一个是未来的,那么查询的数据应该只有一条.   def test_future_question_and_past_question(self):    create_question(question_text= Past question.  , days=-30)    create_question(question_text='Future question. ', days=1)    response = self. client. get(reverse('polls:index'))    # self. assertNotContains(response, 'Future question. ')    self. assertQuerysetEqual(response. context['latest_question_list'], ['&lt;Question: Past question. &gt;'], )    class QuestionDetailView(TestCase):  def test_future_question(self):    future_question = create_question(question_text='Future', days=5)    url = reverse('polls:detail', args=(future_question. id,))    rp = self. client. get(url)    self. assertEqual(rp. status_code, 404)  def test_past_question(self):    past_question = create_question(question_text='Past', days=-5)    url = reverse('polls:detail', args=(past_question. id,))    rp = self. client. get(url)    self. assertEqual(rp. status_code, 200)注意:  如果多条记录排序的字段值都相同, 那么测试框架返回的顺序是随机的.  可以去为网站创建首页: 方法一: 为网站创建一个首页 mysite/urls. py 1url(r'^$', views. index, name='index'),polls/views. py 12def index(request):  return HttpResponse( Hello, world, I'ma in Shanghai )方法二: 将首页转到 polls 的首页  注意, 正则中不能有 $ , 因为URL 后续的部分还要在 polls/urls 中继续截取, 那里才可以用 $ 结束URL截取工作. mysite/urls. py 1url(r'^', include('polls. urls')),方法三: 为首页创建模板的范式 (1)创建项目根目录下的mysite/templates/home. html 文件夹和文件. 12345678910&lt;!DOCTYPE html&gt;&lt;html lang= en &gt;&lt;head&gt;  &lt;meta charset= UTF-8 &gt;  &lt;title&gt;Title&lt;/title&gt;&lt;/head&gt;&lt;body&gt;Hi, this is home page!&lt;/body&gt;&lt;/html&gt;(2)mysites/urls 1url(r'^$', views. home, name='home'),(3)polls/view. py 123456def home(request):  home = Question. objects. all()  context = {    'home': home,  }  return render(request, 'home. html', context)(4)mysite/settings. py 12# 'DIRS': [],'DIRS': [os. path. join(BASE_DIR, 'templates')],简单登录和登出: mysite/urls. py 123from django. contrib. auth import views as auth_viewsurl(r'^login/$', auth_views. login, name='login'),url(r'^logout/$', auth_views. logout, name='logout'),创建 project/mysite/template/registration/login. html 目录和文件 ```html # project/mysite/template/registration/login. html{% block title %} 登录页面 {% endblock %}{% block content %}  &lt;h2&gt;登录页面&lt;/h2&gt;  &lt;form action=   method= post &gt;    {% csrf_token %}    {{ form. as_p }}    登录  &lt;/form&gt; {% endblock %} 12345678mysite/settings. py 文件底部 增加两行```pythonLOGIN_URL='/login/'LOGIN_REDIRECT_URL='/'为投票页 detail 增加登录限制 polls/urls. py ```python # polls/urls. pyfrom django. contrib. auth. decorators import login_requiredurl(r’^(?P[0-9]+)/$', login_required(views. DetailView. as_view()), name='detail'), 1234567891011121314151617181920212223之后进入 http://127. 0. 0. 1:8000/polls/ 查看进入# Django 博客项目## 环境配置*requirements. txt*```bashDjango==2. 0django-haystack==2. 6. 1jieba==0. 39Markdown==2. 6. 9Pygments==2. 2. 0pytz==2017. 2Whoosh==2. 7. 4先为博客创建虚拟环境以及目录 创建项目目录/Users/dalong/code/python1702/django/day6/blog 1234567891011cd /Users/dalong/code/python1702/django/day6/blogdjango-admin startproject mysitecd mysite/virtualenv venvsource . /venv/bin/activatepip3. 7 install -r /Users/dalong/Downloads/requirements. txtpython3. 7 manage. py initpython3. 7 manage. py makemigrationspython3. 7 manage. py migratepython3. 7 manage. py createsuperuserpython3. 7 manage. py startapp blog修模模板默认路径, 让模板在项目根目录下  修改 mysite/settings. py 在 INSTALLEDAPP中增加123'blog. apps. BlogConfig', 'DIRS': [os. path. join(BASE_DIR, 'templates')], 新建一个临时的模板文件 mysite/templates/blog/index. html1234567{% block title %}  {{ title }}{% endblock %}{% block content %}  {{ welcome }}{% endblock %}hello需求分析: 博客分类 -&gt; 博客文章 (一对多) 博客文章 -&gt; 博客评论 (一对多) 博客文章 -&gt; 标签 (多对多) 博客: 列表页, 详情页, 评论页 分类: 增删改查 标签: 管理 用户: 管理 建模:  一对多关系 models. ForeignKey() 多对多关系 models. ManyToManyField(),     null=True 对多对多关系字段无效, 所以不需要    auto_now_add=True 是创建时间的时候自动增加创建日期. 要注意, 如果添加了这个, 那么后台就没有办法对时间进行了编辑123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657from django. db import models# 导入django 默认自带的 User 用户模型from django. contrib. auth. models import Userfrom django. urls import reverse# Create your models here. # 分类模型, 需要放在Post 类的上面class Category(models. Model):  name = models. CharField(max_length=100)  def __str__(self):    return self. name# 标签class Tag(models. Model):  name = models. CharField(max_length=100)  def __str__(self):    return self. nameclass Post(models. Model):  title = models. CharField(max_length=70, verbose_name='标题')  body = models. TextField(verbose_name='正文')  # auto_now_add 是  create_time = models. DateTimeField(auto_now_add=True, verbose_name='发布日期')  # auto_now 意思是自动修改时会更新该字段为now  modified_time = models. DateTimeField(auto_now=True, verbose_name='修改日期')  # 摘要  excerpt = models. CharField(max_length=200, blank=True, null=True, verbose_name= 摘要 )  # 浏览次数, 正整数字段  views = models. PositiveIntegerField(default=0, verbose_name='浏览次数')  # 分类的外键. 默认  category = models. ForeignKey(Category, on_delete=models. CASCADE, verbose_name= 分类 )  # tag 多对多关联  tags = models. ManyToManyField(Tag, blank=True, verbose_name= 标签 )  # 作者, User 类时从from django. contrib. auth. models import User 导入的  author = models. ForeignKey(User, on_delete=models. CASCADE, verbose_name='用户')  def __str__(self):    return self. title	# redirect 会用到 Post 中定义的 get_absolute_url 方法找到对应的路径,然后进行跳转  def get_absolute_url(self):    return reverse('blog:detail', kwargs={'id': self. pk})class Review(models. Model):  post = models. ForeignKey(Post, on_delete=models. CASCADE)  body = models. TextField(verbose_name='评论')  create_time = models. DateTimeField(auto_now_add=True, verbose_name='评论时间')  def __str__(self):    return self. body做数据迁移 123python3. 7 manage. py makemigrationspython3. 7 manage. py migrate后台创建一些数据, 然后进入shell python3. 7 manage. py shell 123456789from blog. models import Category, Tag, PostPost. objects. all()p = Post. objects. get(id=1)# 查看p 中对应的所有 tags 的标签p. tags. all()Tag. objects. all()# 查看tag 中是 post id 是 p 的所有 tagTag. objects. filter(post=p)视图 views. py: 创建首页视图, 详情, 分类, 作者, 标签, 评论, 搜索结果, 边栏 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596from django. shortcuts import renderfrom django. http import HttpResponse, HttpResponseRedirect# Create your views here. from django. views. generic import ListView, DetailViewfrom . models import Post, Category, Tag, Reviewfrom django. contrib. auth. models import Userfrom django. urls import reversefrom django. core. paginator import Paginatorfrom django. db. models import Qfrom django. shortcuts import get_object_or_404import markdowndef index(request):  posts = Post. objects. all(). order_by('-create_time')  paginator = Paginator(posts, 5)  page = request. GET. get('page')  post_list = paginator. get_page(page)  return render(request, 'blog/index. html', context={'post_list': post_list, })#def archive(request, year, month):  # post_list = Post. objects. filter(create_time__year=year). filter(create_time__month=month). order_by('-create_time') # 这个方法和下一个方法效果相同  post_list = Post. objects. filter(create_time__year=year, create_time__month=month). order_by('-create_time')  return render(request, 'blog/index. html', context={'post_list': post_list, })def detail(request, id):  p = get_object_or_404(Post, id=id)  p. views += 1  #p. save()  # 可以指定 更新的字段名. 好处是防止有其他字段被程序更新掉  p. save(update_fields=['views'])  p. body = markdown. markdown(p. body, extensions=[    'markdown. extensions. extra',    #    'markdown. extensions. codehilite', # 代码高亮    'markdown. extensions. toc',  ])  return render(request, 'blog/detail. html', context={'p': p, })def category(request, id):  c = Category. objects. get(id=id)  return render(request, 'blog/category. html', context={'c': c, }) # 用类的方法也可以实现分类页# class CategoryView(DetailView):#  model = Category#  template_name = 'blog/category. html' def author(request, id):  a = User. objects. get(id=id)  return render(request, 'blog/author. html', context={'a': a, })def review(request, id):  r = Post. objects. get(id=id). review_set. create(body=request. POST['review'])  r. save()  return HttpResponseRedirect(reverse(viewname='blog:detail', args=(id,)))def tag(request, id):  t = Tag. objects. get(id=id)  return render(request, 'blog/tag. html', context={'t': t, })def search(request):  kwd = request. GET. get('keyword')  error_msg = ''  if not kwd:    return HttpResponse( 请输入关键词 )  post_list = Post. objects. filter(Q(title__contains=kwd) | Q(body__contains=kwd))  # s = sidebar()  return render(request, 'blog/index. html', context={'post_list': post_list, })# 新博客的输入页面def new(request):  return render(request, 'blog/new. html')# 处理新创建的博客, 写入数据库def create(request):  category_ = Category. objects. get(id=request. POST['category'])  author_ = User. objects. get(id=request. user. id)  print(author_)  print(str(request. POST. getlist('tags')))  p = Post(title=request. POST['title'], body=request. POST['body'], excerpt=request. POST['excerpt'], category=category_, author=author_, )  p. save()  for tag in request. POST. getlist('tags'):    p. tags. add(Tag. objects. get(id=tag))  p. save()  return HttpResponseRedirect(reverse(viewname='blog:detail', args=(p. id,)))URLs. py 创建: 创建 blog/urls. py 123456789101112131415161718from django. conf. urls import urlfrom django. urls import pathfrom . import viewsapp_name = 'blog'urlpatterns = [  url(r'^$', views. index, name='index'),  url(r'^(?P&lt;id&gt;[0-9]+)/detail/$', views. detail, name='detail'),  url(r'^(?P&lt;year&gt;[0-9]4)/(?P&lt;month&gt;[0-9]{1,2})/archive/$', views. archive, name='archive'),  url(r'^(?P&lt;id&gt;[0-9]+)/category/$', views. category, name='category'),  url(r'^(?P&lt;id&gt;[0-9]+)/author/$', views. author, name='author'),  url(r'^(?P&lt;id&gt;[0-9]+)/review/$', views. review, name='review'),  url(r'^(?P&lt;id&gt;[0-9]+)/tag/$', views. tag, name='tag'),  url(r'^search/$', views. search, name='search'),]模板: 创建一个基础模板 base. html, 用于被其他继承: 技巧:  {% load staticfiles %} 读取静态文件 {% static ‘blog/js/jquery-2. 1. 3. min. js’ %} 引用静态文件 在css 和 js 代码下面增加如下 块, 这样就可以让子模板继承并添加自己的css , js 代码 {% block css %}{% endblock %} {% block js %}{% endblock %} 将框架在base. html 中定义, 把每个页面需要变动的地方用 {% block main_content %}主要内容{% endblock %} 留出来, 让子类模板继承. 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170{# 加载静态文件. #}{% load staticfiles %}{% load blog_tags %}&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;  &lt;title&gt;{% block title %}Python1702 论坛{% endblock %}&lt;/title&gt;  &lt;!-- meta --&gt;  &lt;meta charset= UTF-8 &gt;  &lt;meta name= viewport  content= width=device-width, initial-scale=1 &gt;  &lt;!-- css --&gt;  &lt;link rel= stylesheet  href= {% static 'blog/css/ionicons. min. css' %} &gt;  &lt;link rel= stylesheet  href= {% static 'blog/css/bootstrap. min. css' %} &gt;  &lt;link rel= stylesheet  href= {% static 'blog/css/pace. css' %} &gt;  &lt;link rel= stylesheet  href= {% static 'blog/css/custom. css' %} &gt;  &lt;link rel= stylesheet  href= {% static 'blog/css/highlights/zenburn. css' %} &gt;  {% block css %}{% endblock %}  &lt;!-- js --&gt;  &lt;script src= {% static 'blog/js/jquery-2. 1. 3. min. js' %} &gt;&lt;/script&gt;  &lt;script src= {% static 'blog/js/bootstrap. min. js' %} &gt;&lt;/script&gt;  &lt;script src= {% static 'blog/js/pace. min. js' %} &gt;&lt;/script&gt;  &lt;script src= {% static 'blog/js/modernizr. custom. js' %} &gt;&lt;/script&gt;  {% block js %}{% endblock %}&lt;/head&gt;&lt;body&gt;&lt;div class= container &gt;  &lt;header id= site-header &gt;    &lt;div class= row &gt;      &lt;div class= col-md-4 col-sm-5 col-xs-8 &gt;        &lt;div class= logo &gt;          &lt;h1&gt;&lt;a href= {% url 'blog:index' %} &gt;&lt;b&gt;Py1702&lt;/b&gt; 论坛&lt;/a&gt;&lt;/h1&gt;        &lt;/div&gt;      &lt;/div&gt;&lt;!-- col-md-4 --&gt;      &lt;div class= col-md-8 col-sm-7 col-xs-4 &gt;        &lt;nav class= main-nav  role= navigation &gt;          &lt;div class= navbar-header &gt;              &lt;span class= ion-navicon &gt;&lt;a href= {% url 'blog:index' %}  data-hover= 首页 &gt;首页&lt;/a&gt;&lt;/span&gt;              &lt;span class= ion-navicon &gt;&lt;a href= {% url 'blog:new' %}  data-hover= 博客 &gt;发表博客&lt;/a&gt;&lt;/span&gt;              &lt;span class= ion-navicon &gt;&lt;a href= about. html  data-hover= 注册 &gt;注册&lt;/a&gt;&lt;/span&gt;              &lt;span class= ion-navicon &gt;&lt;a href= contact. html  data-hover= 登录 &gt;登录&lt;/a&gt;&lt;/span&gt;          &lt;/div&gt;        &lt;/nav&gt;        &lt;div id= header-search-box &gt;          &lt;a id= search-menu  href= # &gt;&lt;span id= search-icon  class= ion-ios-search-strong &gt;&lt;/span&gt;&lt;/a&gt;          &lt;div id= search-form  class= search-form &gt;            &lt;form role= search  method= get  id= searchform  action= {% url 'blog:search' %} &gt;              {% csrf_token %}              &lt;input type= search  name= keyword  placeholder= 搜索  required&gt;              &lt;button type= submit &gt;&lt;span class= ion-ios-search-strong &gt;&lt;/span&gt;&lt;/button&gt;            &lt;/form&gt;          &lt;/div&gt;        &lt;/div&gt;      &lt;/div&gt;&lt;!-- col-md-8 --&gt;    &lt;/div&gt;  &lt;/header&gt;&lt;/div&gt;&lt;div class= content-body &gt;  &lt;div class= container &gt;    &lt;div class= row &gt;      &lt;main class= col-md-8 &gt;      {% block main_content %}        主要内容      {% endblock %}      &lt;/main&gt;      &lt;aside class= col-md-4 &gt;        {% block toc %}        {% endblock %}        &lt;div class= widget widget-recent-posts &gt;          &lt;h3 class= widget-title &gt;最新文章&lt;/h3&gt;          {% get_recent_posts as lastes_posts %}          &lt;ul&gt;            {% for latest_p in lastes_posts %}              &lt;li&gt;{#                &lt;a href= {% url 'blog:detail' latest_p. id %} &gt;{{ latest_p. title }}&lt;/a&gt;#}                &lt;a href= {{ latest_p. get_absolute_url }} &gt;{{ latest_p. title }}&lt;/a&gt;              &lt;/li&gt;            {% empty %}              暂无文章            {% endfor %}          &lt;/ul&gt;        &lt;/div&gt;        &lt;div class= widget widget-archives &gt;          &lt;h3 class= widget-title &gt;归档&lt;/h3&gt;          {% archives as dates %}          &lt;ul&gt;            {% for date in dates %}              &lt;li&gt;                {# url 反向生成 URL, 如果要传递多个参数, 参数间用空格分割. #}                &lt;a href= {% url 'blog:archive' date. year date. month %} &gt;{{ date. year }}                  年 {{ date. month }} 月&lt;/a&gt;              &lt;/li&gt;            {% empty %}              暂无文章            {% endfor %}          &lt;/ul&gt;        &lt;/div&gt;        &lt;div class= widget widget-category &gt;          &lt;h3 class= widget-title &gt;分类&lt;/h3&gt;          {% get_categories as categories %}          &lt;ul&gt;            {% for cate in categories %}              &lt;li&gt;                &lt;a href= {% url 'blog:category' cate. id %}  &gt;{{ cate. name }}&lt;span                    class= post-count &gt;({{ cate. post_set. count }})&lt;/span&gt;&lt;/a&gt;              &lt;/li&gt;            {% endfor %}          &lt;/ul&gt;        &lt;/div&gt;        &lt;div class= widget widget-tag-cloud &gt;          &lt;h3 class= widget-title &gt;标签云&lt;/h3&gt;          {% get_tags as tags %}          &lt;ul&gt;            {% for tag in tags %}              &lt;li&gt;                &lt;a href= {% url 'blog:tag' tag. id %} &gt;{{ tag. name }}&lt;/a&gt;              &lt;/li&gt;            {% endfor %}          &lt;/ul&gt;        &lt;/div&gt;        &lt;div class= rss &gt;          &lt;a href=  &gt;&lt;span class= ion-social-rss-outline &gt;&lt;/span&gt; RSS 订阅&lt;/a&gt;        &lt;/div&gt;      &lt;/aside&gt;    &lt;/div&gt;  &lt;/div&gt;&lt;/div&gt;&lt;footer id= site-footer &gt;  &lt;div class= container &gt;    &lt;div class= row &gt;      &lt;div class= col-md-12 &gt;        &lt;p class= copyright &gt;&amp;copy 2017        &lt;/p&gt;      &lt;/div&gt;    &lt;/div&gt;  &lt;/div&gt;&lt;/footer&gt;&lt;!-- Mobile Menu --&gt;&lt;div class= overlay overlay-hugeinc &gt;  &lt;button type= button  class= overlay-close &gt;&lt;span class= ion-ios-close-empty &gt;&lt;/span&gt;&lt;/button&gt;  &lt;nav&gt;    &lt;ul&gt;      &lt;li&gt;&lt;a href= index. html &gt;首页&lt;/a&gt;&lt;/li&gt;      &lt;li&gt;&lt;a href= full-width. html &gt;博客&lt;/a&gt;&lt;/li&gt;      &lt;li&gt;&lt;a href= about. html &gt;关于&lt;/a&gt;&lt;/li&gt;      &lt;li&gt;&lt;a href= contact. html &gt;联系&lt;/a&gt;&lt;/li&gt;    &lt;/ul&gt;  &lt;/nav&gt;&lt;/div&gt;&lt;script src= {% static 'blog/js/script. js' %} &gt;&lt;/script&gt;&lt;/body&gt;&lt;/html&gt;创建首页模板, 继承 templates/blog/base. html:  {% extends “blog/base. html” %} 继承父类模板 {% for post in post_list %} {% empty %} {% endfor %} 其中的 empty 是 当for 为空的情况时需要加载的模板样式.                {{ post. create_time     date:”SHORT_DATE_FORMAT” }} 日期格式化. 也可以改成 date:”Y-m-D H:i:s”          1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768{% extends  blog/base. html  %}{% block title %}首页{% endblock %}{% block css %}  &lt;style type= text/css &gt;    . entry-title {      text-align: left;      margin-left: 0;      padding-left: 0;    }    . entry-meta {      text-align: right;      margin-left: 0;      padding-left: 0;    }  &lt;/style&gt;{% endblock %}{% block main_content %}{% for post in post_list %}          &lt;article class= post post-{{ post. pk }} &gt;            &lt;header class= entry-header &gt;              &lt;h1 class= entry-title &gt;                &lt;a href= {% url 'blog:detail' post. id %} &gt;{{ post. title }}&lt;/a&gt;              &lt;/h1&gt;              &lt;h4&gt;{{ post. excerpt }}&lt;/h4&gt;                  &lt;a href= {% url 'blog:detail' post. id %}  class= more-link &gt;继续阅读 &lt;span                      class= meta-nav &gt;→&lt;/span&gt;&lt;/a&gt;              &lt;div class= entry-meta &gt;                &lt;span class= post-category &gt;&lt;a                    href= {% url 'blog:category' post. category. id %} &gt;{{ post. category. name }}&lt;/a&gt;&lt;/span&gt;                &lt;span class= post-date &gt;&lt;time class= entry-date                                datetime= {{ post. create_time }} &gt;{{ post. create_time|date: SHORT_DATE_FORMAT  }}&lt;/time&gt;&lt;/span&gt;                &lt;span class= post-author &gt;&lt;a                    href= {% url 'blog:author' post. author_id %}  &gt;{{ post. author }}&lt;/a&gt;&lt;/span&gt;                &lt;span class= comments-link &gt;&lt;a                    href= {% url 'blog:detail' post. id %} &gt;{{ post. review_set. count }} 评论&lt;/a&gt;&lt;/span&gt;                &lt;span class= views-count &gt;{{ post. views }} 阅读&lt;/span&gt;              &lt;/div&gt;            &lt;/header&gt;          &lt;/article&gt;        {% empty %}          &lt;div class= no-post &gt;暂时还没有发布的文章！&lt;/div&gt;        {% endfor %}        {# 分页 #}        &lt;div class= pagination &gt;          &lt;span class= step-links &gt;            {% if post_list. has_previous %}              &lt;a href= {% url 'blog:index' %}?page=1 &gt;&amp;laquo; 第一页&lt;/a&gt;&amp;nbsp; &amp;nbsp;              &lt;a href= {% url 'blog:index' %}?page={{ post_list. previous_page_number }} &gt;上一页&lt;/a&gt;            {% endif %}            &lt;span class= current &gt;               &amp;nbsp; &amp;nbsp; {{ post_list. number }} / {{ post_list. paginator. num_pages }} &amp;nbsp; &amp;nbsp;            &lt;/span&gt;            {% if post_list. has_next %}              &lt;a href= {% url 'blog:index' %}?page={{ post_list. next_page_number }} &gt;下一页&lt;/a&gt;&amp;nbsp; &amp;nbsp;              &lt;a href= {% url 'blog:index' %}?page={{ post_list. paginator. num_pages }} &gt;最后一页 &amp;raquo;&lt;/a&gt;            {% endif %}          &lt;/span&gt;        &lt;/div&gt;{% endblock %}在 detail 页 为Blog 添加 Markdown 功能: 网上的详细介绍 首先安装 markdow 和 语法高亮的 Pygments 扩展 12pip3. 7 install markdownpip3. 7 install Pygments视图函数导入markdown 和 Pygments  blog/views. py12345678910111213import markdowndef detail(request, id):  p = get_object_or_404(Post, id=id)  p. views += 1  p. save()  p. body = markdown. markdown(p. body, extensions=[    'markdown. extensions. extra',    'markdown. extensions. codehilite', # 语法高亮    'markdown. extensions. toc',  ])  s = sidebar()  return render(request, 'blog/detail. html', context={'p': p, 's': s, })在 模板中引入高亮样式, 并将需要显示 markdown语法的内容用 Safe 进行过滤                {{ p. body     safe }} 用于 告诉 Django 输出内容是安全的, 所以可以将源代码进行转义成Markdown 语法 并显示在网页上          123&lt;link rel= stylesheet  href= {% static 'blog/css/highlights/zenburn. css' %} &gt;. . . {{ p. body|safe }}创建自定义标签: 目的: 自定义标签可以在模板中被使用 方法:  在blog 目录下创建一个 templatetags 目录, 这个目录名是固定的 创建空文件 blog/templatetags/__init__. py # 空文件 创建标签文件 blog/templatetags/blog_tags. py 如下123456789101112131415161718192021222324252627from django import templatefrom . . models import Post, Category, Tag# Library() 是 A class 用于注册模板标签和过滤器register = template. Library()# @register. simple_tag 装饰器 将get_recent_posts() 方法注册成一个模板的标签@register. simple_tagdef get_recent_posts(num=5):  return Post. objects. all(). order_by('-create_time')[:num]# @register. simple_tag 装饰器 将get_recent_posts() 方法注册成一个模板的标签@register. simple_tagdef archives():  return Post. objects. dates('create_time', 'month', order='DESC')@register. simple_tagdef get_categories():  return Category. objects. all()@register. simple_tagdef get_tags():  return Tag. objects. all() 在 base. html 中对标签进行引用, 并循环输出内容. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253{# load blog_tags 会默认去 应用/templatetags 目录下去找 blog_tags. py 这个文件并获取它里面的所有注册了tags的函数 #}{% load blog_tags %}&lt;div class= widget widget-recent-posts &gt;  &lt;h3 class= widget-title &gt;最新文章&lt;/h3&gt;  {% get_recent_posts as lastes_posts %}  &lt;ul&gt;    {% for latest_p in lastes_posts %}      &lt;li&gt;        &lt;a href= {% url 'blog:detail' latest_p. id %} &gt;{{ latest_p. title }}&lt;/a&gt;      &lt;/li&gt;    {% empty %}      暂无文章    {% endfor %}  &lt;/ul&gt;&lt;/div&gt;&lt;div class= widget widget-archives &gt;  &lt;h3 class= widget-title &gt;归档&lt;/h3&gt;  {% archives as dates %}  &lt;ul&gt;    {% for date in dates %}      &lt;li&gt;        &lt;a href= {% url 'blog:archive' date. year date. month %} &gt;{{ date. year }}          年 {{ date. month }} 月&lt;/a&gt;      &lt;/li&gt;    {% endfor %}  &lt;/ul&gt;&lt;/div&gt;&lt;div class= widget widget-category &gt;  &lt;h3 class= widget-title &gt;分类&lt;/h3&gt;  {% get_categories as categories %}  &lt;ul&gt;    {% for cate in categories %}      &lt;li&gt;        &lt;a href= {% url 'blog:category' cate. id %}  &gt;{{ cate. name }}&lt;span            class= post-count &gt;({{ cate. post_set. count }})&lt;/span&gt;&lt;/a&gt;      &lt;/li&gt;    {% endfor %}  &lt;/ul&gt;&lt;/div&gt;&lt;div class= widget widget-tag-cloud &gt;  &lt;h3 class= widget-title &gt;标签云&lt;/h3&gt;  {% get_tags as tags %}  &lt;ul&gt;    {% for tag in tags %}      &lt;li&gt;        &lt;a href= {% url 'blog:tag' tag. id %} &gt;{{ tag. name }}&lt;/a&gt;      &lt;/li&gt;    {% endfor %}  &lt;/ul&gt;&lt;/div&gt;Django实现登录、注册: 本章将注册登录作为一个独立的app 在原有的项目中进行创建. 参考 步骤:  创建app logReg, 并进行设置 设计数据模型 User 写视图函数 views. py 设置路由 urls. py 设置模板创建app logReg 1django-admin startapp logReg添加应用 mysite/settings. py 1'logReg. apps. LogregConfig',修改 mysite/urls. py 12# path('logReg', include('logReg. url')),url(r'^', include('logReg. urls')),设计数据模型 logReg/models. py 123456789101112131415from django. db import models# Create your models here. class User(models. Model):  username = models. CharField(max_length=50, verbose_name='用户名')  password = models. CharField(max_length=50, verbose_name='密码')  email = models. CharField(max_length=50, blank=True, null=True, verbose_name='邮件地址')  status = models. IntegerField(default=0, blank=True, null=True, verbose_name='激活状态')  def __str__(self):    return self. username编辑视图文件 logReg/views. py todo: 待续 Django 表单类 的使用 – 博客评论功能重构: 修改 评论数据模型 blog/models. py 12345678910111213class Review(models. Model):  name = models. CharField(max_length=100, verbose_name= 作者 )  email = models. EmailField(max_length=100, verbose_name= 邮箱 )  url = models. URLField(blank=True, verbose_name= 链接 )  body = models. TextField(verbose_name= 内容 )  create_time = models. DateTimeField(auto_now_add=True)  modified_time = models. DateTimeField(auto_now=True)  # 如果是跨app进行foreignkey, 那么需要在目标模型前面加上app 的名字, 或者是从另一个app中导入这个模型  # post = models. ForeignKey('blog. Post', on_delete=models. CASCADE)  post = models. ForeignKey(Post, on_delete=models. CASCADE)  # 只返回文本内容的钱20个文字.   def __str__(self):    return self. body[:20]创建 blog/forms/form. py 1234567891011121314from django import formsfrom blog. models import Review# 创建一个表单类, 该类继承自django 的 ModelForm 基类class ReviewForm(forms. ModelForm):  # 嵌套类, 这个类是一个元数据类, 名字固定  # 这是一个 Inner Class,  class Meta:    # 指定元数据的数据模型是 Review    model = Review    # 指定模型中需要哪些字段展现在form 表单中    fields = ['name', 'email', 'url', 'body']修改原有的 review 方法 blog/view. py 12345678910111213141516171819202122232425262728293031323334353637383940from . forms. form import ReviewFormdef detail(request, id):  form = ReviewForm()  p = get_object_or_404(Post, id=id)  p. views += 1  p. save()  p. body = markdown. markdown(p. body, extensions=[    'markdown. extensions. extra',    #    'markdown. extensions. codehilite', # 代码高亮    'markdown. extensions. toc',  ])  return render(request, 'blog/detail. html', context={'p': p, 'form': form, })def review(request, id):  p = get_object_or_404(Post, pk=id)  # 先建一个空表单  form = ReviewForm()  if request. method == 'POST':    # 如果有提交过来的POST 数据, 那么将数据作为参数放到表单对象中    form = ReviewForm(request. POST)    # 验证表单是否符合数据格式要求(格式的要求就是 Models 中字段定义类型)    if form. is_valid():      # form. save 是将表单对象(包括表单数据)保存在r对象中.       # commit=False 是暂时不存在数据库中.       r = form. save(commit=False)      # 把缺的post对象加到r 对象中.       r. post = p      r. save()      # redirect 的定义如下:      # Return an HttpResponseRedirect to the appropriate URL for the arguments passed.       return redirect(p)    else:      return HttpResponse( 信息输入错误 )  else:    # return render(request, 'blog/detail. html', context={'id': id, 'form': form, })    return redirect(p)修改原有模板 templates/blog/detail. html 1234567891011121314151617181920212223242526272829303132&lt;section class= comment-area  id= comment-area &gt;    &lt;hr&gt;    &lt;h3&gt;发表评论&lt;/h3&gt;    &lt;form action= {% url 'blog:review' p. id %}  method= post  class= comment-form &gt;      {% csrf_token %}      &lt;div class= row &gt;          {{ form. as_table }}          &lt;button type= submit  class= comment-btn &gt;发表&lt;/button&gt;        &lt;/div&gt;      &lt;/div&gt;  &lt;!-- row --&gt;    &lt;/form&gt;    &lt;div class= comment-list-panel &gt;      &lt;h3&gt;评论列表，共 &lt;span&gt;{{ p. review_set. count }}&lt;/span&gt; 条评论&lt;/h3&gt;      &lt;ul class= comment-list list-unstyled &gt;        {% for r in p. review_set. all %}        &lt;li class= comment-item &gt;          &lt;span class= nickname &gt;&lt;a href= mailto:{{ r. email }} &gt;{{ r. name }}&lt;/a&gt; &lt;/span&gt;          &lt;time class= submit-date  datetime= {{ r. create_time }} &gt;{{ r. create_time|date:'Y-m-d H:i:s' }}&lt;/time&gt;          &lt;a href= {{ r. url }} &gt;链接&lt;/a&gt;          &lt;div class= text &gt;            {{ r. body }}          &lt;/div&gt;        &lt;/li&gt;        {% empty %}          还没有任何评论啊. . .         {% endfor %}      &lt;/ul&gt;    &lt;/div&gt;  &lt;/section&gt;分页功能 blog/views. py: 方法一 :  效果: « 第一页  上一页   2 / 2blog/views. py 1234567891011121314# 导入分页模块from django. core. paginator import Paginatordef index(request):  username = request. COOKIES. get('username')  posts = Post. objects. all(). order_by('-create_time')  # 实例化一个分页对象, 需要传递数据对象和每页的页数  paginator = Paginator(posts, 5)  # 获取传递过来的分页参数, 然后将参数  page = request. GET. get('page')  # 将当前页码传递到分页对象中, 获得一个列表对象.   post_list = paginator. get_page(page)   return render(request, 'blog/index. html', context={'post_list': post_list, })templates/blog/index. html 123456789101112131415161718{# 分页 #}&lt;div class= pagination &gt;  &lt;span class= step-links &gt;    {% if post_list. has_previous %}      &lt;a href= {% url 'blog:index' %}?page=1 &gt;&amp;laquo; 第一页&lt;/a&gt;&amp;nbsp; &amp;nbsp;      &lt;a href= {% url 'blog:index' %}?page={{ post_list. previous_page_number }} &gt;上一页&lt;/a&gt;    {% endif %}    &lt;span class= current &gt;       &amp;nbsp; &amp;nbsp; {{ post_list. number }} / {{ post_list. paginator. num_pages }} &amp;nbsp; &amp;nbsp;    &lt;/span&gt;    {% if post_list. has_next %}      &lt;a href= {% url 'blog:index' %}?page={{ post_list. next_page_number }} &gt;下一页&lt;/a&gt;&amp;nbsp; &amp;nbsp;      &lt;a href= {% url 'blog:index' %}?page={{ post_list. paginator. num_pages }} &gt;最后一页 &amp;raquo;&lt;/a&gt;    {% endif %}  &lt;/span&gt;&lt;/div&gt;方法二  效果和方法一一致: « 第一页  上一页   2 / 2blog/views. py 12345class IndexView(ListView):  model = Post  template_name = 'blog/index. html'  context_object_name = 'post_list'  paginate_by = 5templates/blog/index. html 1234567891011121314151617181920{#分页方法二#}{% if is_paginated %}  &lt;div class= pagination &gt;    &lt;span class= step-links &gt;      {% if page_obj. has_previous %}        &lt;a href= {% url 'blog:index' %}?page=1 &gt;&amp;laquo; 第一页&lt;/a&gt;&amp;nbsp; &amp;nbsp;        &lt;a href= {% url 'blog:index' %}?page={{ page_obj. previous_page_number }} &gt;上一页&lt;/a&gt;      {% endif %}      &lt;span class= current &gt;         &amp;nbsp; &amp;nbsp; {{ page_obj. number }} / {{ page_obj. paginator. num_pages }} &amp;nbsp; &amp;nbsp;      &lt;/span&gt;      {% if page_obj. has_next %}        &lt;a href= {% url 'blog:index' %}?page={{ page_obj. next_page_number }} &gt;下一页&lt;/a&gt;&amp;nbsp; &amp;nbsp;        &lt;a href= {% url 'blog:index' %}?page={{ page_obj. paginator. num_pages }} &gt;最后一页 &amp;raquo;&lt;/a&gt;      {% endif %}    &lt;/span&gt;  &lt;/div&gt;{% endif %}方法三  视图函数不需要做任何修改, 只是改一下模板文件即可 当要显示的数据条数大于paginate_by 时 is_paginated 为 True, 否则为False12345678910111213141516171819202122232425{% if is_paginated %}  &lt;div class= pagination &gt;    &lt;span class= step-links &gt;      {% if page_obj. has_previous %}        &lt;a href= {% url 'blog:index' %}?page=1 &gt;&amp;laquo; 第一页&lt;/a&gt;&amp;nbsp; &amp;nbsp;        &lt;a href= {% url 'blog:index' %}?page={{ page_obj. previous_page_number }} &gt;上一页&lt;/a&gt;      {% endif %}      {% for x in page_obj. paginator. page_range %}        {% if x == page_obj. number %}          &lt;span class= current &gt;        {% else %}          &lt;span&gt;        {% endif %}            &lt;a href= {% url 'blog:index' %}?page={{ x }} &gt;{{ x }}&lt;/a&gt;          &lt;/span&gt;      {% endfor %}      {% if page_obj. has_next %}        &lt;a href= {% url 'blog:index' %}?page={{ page_obj. next_page_number }} &gt;下一页&lt;/a&gt;&amp;nbsp; &amp;nbsp;        &lt;a href= {% url 'blog:index' %}?page={{ page_obj. paginator. num_pages }} &gt;最后一页 &amp;raquo;&lt;/a&gt;      {% endif %}    &lt;/span&gt;  &lt;/div&gt;{% endif %}方法四: 用 telplatetags 方法实现. views. py 代码不变 blog/templatetags/blog_tags. py 1234567891011121314151617from django import templatefrom django. utils. html import format_htmlregister = template. Library()@register. simple_tag()def circle_page(curr_page, loop_page):  offset = abs(curr_page - loop_page)  # 当前页左右留几个页码, 如果设置offset 为3, 那么就会显示5个页码  if offset &lt; 3:    if curr_page == loop_page:      pe = '&lt;li class= active &gt;&lt;a href= ?page=%s &gt;%s&lt;/a&gt;&lt;/li&gt;' % (loop_page, loop_page)    else:      pe = '&lt;li&gt;&lt;a href= ?page=%s &gt;%s&lt;/a&gt;&lt;/li&gt;' % (loop_page, loop_page)    return format_html(pe)  else:    return ''templates/blog/index. html 123456789101112131415161718192021{% load blog_tags %}{% if is_paginated %}    &lt;nav aria-label= Page Navigation &gt;      &lt;ul class= pagination &gt;        {% if page_obj. has_previous %}          &lt;li&gt;&lt;a href= {% url 'blog:index' %}?page=1 &gt;&amp;laquo; 第一页&lt;/a&gt;&amp;nbsp; &amp;nbsp;&lt;/li&gt;          &lt;li&gt;&lt;a href= {% url 'blog:index' %}?page={{ page_obj. previous_page_number }} &gt;上一页&lt;/a&gt;&lt;/li&gt;        {% endif %}        {% for x in page_obj. paginator. page_range %}          {% circle_page page_obj. number x %}        {% endfor %}        {% if page_obj. has_next %}          &lt;li&gt;&lt;a href= {% url 'blog:index' %}?page={{ page_obj. next_page_number }} &gt;下一页&lt;/a&gt;&amp;nbsp; &amp;nbsp;&lt;/li&gt;          &lt;li&gt;&lt;a href= {% url 'blog:index' %}?page={{ page_obj. paginator. num_pages }} &gt;最后一页 &amp;raquo;&lt;/a&gt;&lt;/li&gt;        {% endif %}      &lt;/ul&gt;    &lt;/nav&gt;  {% endif %}简单搜索功能: 创建 form 表单 templates/base. html 12345678910&lt;div id= header-search-box &gt;  &lt;a id= search-menu  href= # &gt;&lt;span id= search-icon  class= ion-ios-search-strong &gt;&lt;/span&gt;&lt;/a&gt;  &lt;div id= search-form  class= search-form &gt;    &lt;form role= search  method= get  id= searchform  action= {% url 'blog:search' %} &gt;      {% csrf_token %}      &lt;input type= search  name= q  placeholder= 搜索  required&gt;      &lt;button type= submit &gt;&lt;span class= ion-ios-search-strong &gt;&lt;/span&gt;&lt;/button&gt;    &lt;/form&gt;  &lt;/div&gt;&lt;/div&gt;定义视图函数 blog/views. py 1234567891011def search(request):  q = request. GET. get('q')  error_msg = ''  if not q:    return HttpResponse( 请输入关键词 )  # Q 类, 可以更简单的使用 &amp; 和 | 进行 类似 sql 的过滤功能  # Q 的官方定义: Encapsulate filters as objects that can then be combined logically (using `&amp;` and `|`).   post_list = Post. objects. filter(Q(title__contains=q) | Q(body__contains=q))  # s = sidebar()  return render(request, 'blog/index. html', context={'post_list': post_list, })定义 路由 blog/urls. py 1url(r'^search/$', views. search, name='search'),实现高级搜索 whoosh django-haystack: 网上的资料 安装 搜索引擎需要的包 123pip3. 7 install jiebapip3. 7 install whooshpip3. 7 install django-haystack配置 mysite/settings. py app中注册一个新的应用heystack, 并配置haystack参数 123456789101112'haystack',. . . HAYSTATIC_CONNECTIONS = {  'default': {    'ENGINE': 'blog. whoosh_cn_backend. WhooshEngine',    'PATH': os. path. join(BASE_DIR, 'whoosh_index'), # 创建索引后, 会在app 目录下生成一个 whoosh_index 文件夹  }}HAYSTACK_SEARCH_RESULTS_PER_PAGE = 10HAYSTACK_SIGNAL_PROCESSOR = 'haystack. signals. RealtimeSignalProcessor'创建搜索文件 新建一个文件 blog/search_indexes. py 123456789101112from haystack import indexesfrom . models import Postclass PostIndex(indexes. SearchIndex, indexes. Indexable):  text = indexes. CharField(document=True, use_template=True)  def get_model(self):    return Post  def index_queryset(self, using=None):    return self. get_model(). objects. all()创建文件post_text. txt 和对应目录 templates/search/indexes/blog/post_text. txt 指定要索引的内容 12{{ object. title }}{{ object. body }}修改 mysite/urls. py haystack 是 pip 安装好的 1url(r'^search', include('haystack. urls')),将 blog/urls. py 中的 url 注释掉, 防止冲突 1# url(r'^search/$', views. search, name='search'),将文件 /Users/dalong/code/python1702/django/day8/mysite/venv/lib/python3. 7/site-packages/haystack/backends/whoosh_backend. py 拷贝到 /mysite/blog 目录下改名 whoosh_cn_backend. py 并对文件进行修改 1234from jieba. analyse import ChineseAnalyzer        # schema_fields[field_class. index_fieldname] = TEXT(stored=True, analyzer=StemmingAnalyzer(), field_boost=field_class. boost, sortable=True)        schema_fields[field_class. index_fieldname] = TEXT(stored=True, analyzer=ChineseAnalyzer(), field_boost=field_class. boost, sortable=True)建立索引 12# python3. 7 manage. py rebuild_index # 这个不行, 要用下一个python3. 7 manage. py update_index数据模型 models. py 高级用法 class Meta; def save():  save() 方法 可以帮助我们把一些没有填写的数据的字段按照我们到的需要进行保存 class Meta 内嵌类可以帮助我们对数据进行默认排序. blog/models. py 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748class Post(models. Model):  title = models. CharField(max_length=70, verbose_name='标题')  body = models. TextField(verbose_name='正文')  # auto_now_add 是  create_time = models. DateTimeField(auto_now_add=True, verbose_name='发布日期')  # auto_now 意思是自动修改时会更新该字段为now  modified_time = models. DateTimeField(auto_now=True, verbose_name='修改日期')  # 摘要  excerpt = models. CharField(max_length=200, blank=True, null=True, verbose_name= 摘要 )  # 浏览次数, 正整数字段  views = models. PositiveIntegerField(default=0, verbose_name='浏览次数')  # 分类的外键. 默认  category = models. ForeignKey(Category, on_delete=models. CASCADE, verbose_name= 分类 )  # tag 多对多关联  tags = models. ManyToManyField(Tag, blank=True, verbose_name= 标签 )  # 作者, User 类时从from django. contrib. auth. models import User 导入的  author = models. ForeignKey(User, on_delete=models. CASCADE, verbose_name='用户')  def __str__(self):    return self. title  def get_absolute_url(self):    return reverse('blog:detail', kwargs={'id': self. pk})  def increase_views(self):    self. views += 1    # # 可以指定 更新的字段名. 好处是防止有其他字段被程序更新掉    self. save(update_fields=['views'])  def save(self, *args, **kwargs):    if not self. excerpt:      # 首先实例化一个 markdown 类, 用于渲染 body 的文本      md = Markdown(extensions=[        'markdown. extensions. extra',        'markdown. extensions. codehilite',      ])      # 先将Markdown 语法转成 HTML 语法      # 再用 strip_tags 去掉 html 中的全部 html 标签      # 目的是将所有的 markdown 和 html 标签全部干掉.       # 最后从文本中获取前 54个字符, 赋值给 excerpt 字段      self. excerpt = strip_tags(md. convert(self. body))[:54]    # 将过滤结果保存到数据库    super(Post, self). save(*args, **kwargs)  # 按时间倒序排序  # 这样一来, 就不需要在views. py中 用 get_queryset() 方法进行排序了  class Meta:    ordering = ['-create_time']面向对象 views. py 传递多个上下文参数给模板: 该方法可以替代老实讲的方法, 实际分页代码只有1行 blog/views. py 123456789101112131415161718192021class TestView(ListView):  model = Post  template_name = 'blog/test2. html'  # context_object_name 专门指定模板中用来迭代 Post 数据模型的变量名  # 如果制定了 paginate_by , 那么就不需要再指定 context_object_name 了. 是二选一的  # context_object_name = 'pl'  # 一旦指定了 paginate_by 变量, 那么就不再需要 context_object_name 了.   # 而且在模板中可以直接使用 page_obj. object_list 对数据进行循环  # 并且使用 is_paginated, page_obj 的 has_previous, previous_page_number, has_next, next_page_number, number  # paginator. num_pages, paginator. page_range 等 预定义的分页模板变量  paginate_by = 2  # get_context_data 用来定义附加的需要传递给模板的参数 -- context  def get_context_data(self, *, object_list=None, **kwargs):    context = super(). get_context_data(**kwargs)    context['aaa'] = 'aaa'    context['bbb'] = True    context['ccc'] = 123    context['ddd'] = '&lt;h2&gt;haha&lt;/h2&gt;'    return contextblog/urls. py 1url(r'^test2/$', views. TestView. as_view(), name='test2'),templates/blog/test2. html 1234567891011121314151617181920212223242526272829303132333435+++++{{ aaa }}+++++{{ bbb }}+++++{{ ccc }}+++++{{ ddd }}{#用 page_obj 对象 代替 context_object_name 定义的 post_list 对象#}{#好处是可以少传一个参数#}{% for p in page_obj. object_list %}  &lt;br&gt;&lt;hr&gt; {{ p. title }}{% endfor %}分页方法二{% if is_paginated %}  &lt;div class= pagination &gt;    &lt;span class= step-links &gt;      {% if page_obj. has_previous %}        &lt;a href= {% url 'blog:test2' %}?page=1 &gt;&amp;laquo; 第一页&lt;/a&gt;&amp;nbsp; &amp;nbsp;        &lt;a href= {% url 'blog:test2' %}?page={{ page_obj. previous_page_number }} &gt;上一页&lt;/a&gt;      {% endif %}      &lt;span class= current &gt;         &amp;nbsp; &amp;nbsp; {{ page_obj. number }} / {{ page_obj. paginator. num_pages }} &amp;nbsp; &amp;nbsp;      &lt;/span&gt;      {% if page_obj. has_next %}        &lt;a href= {% url 'blog:test2' %}?page={{ page_obj. next_page_number }} &gt;下一页&lt;/a&gt;&amp;nbsp; &amp;nbsp;        &lt;a href= {% url 'blog:test2' %}?page={{ page_obj. paginator. num_pages }} &gt;最后一页 &amp;raquo;&lt;/a&gt;      {% endif %}    &lt;/span&gt;  &lt;/div&gt;{% endif %}对数据进行统计过滤 aggregates annotate:  可以使用 aggregates 对想要获取的数据模型中的数据进行统计和过滤     官方解释: 官方描述: Classes to represent the definitions of aggregate functions.    其他可以进行统计的类还有: 'Aggregate', 'Avg', 'Count', 'Max', 'Min', 'StdDev', 'Sum'    使用方式是 数据集. objects. annotate() 方法中指定要进行统计的类型, 并对该类型进行过滤筛选 objects. annotate() 可以理解为 对 objects. all() 进行修饰.      官方解释: Return a query set in which the returned objects have been annotatedwith extra data or aggregations.    annotate() 返回的对象和 objects. all() 返回的对象是一样的, 所以可以继续进行筛选过滤.    blog/templatetags/blog_tags. py 123456789101112131415161718192021222324252627282930from django import template# 导入django 的数据统计模型中的 总计类# 官方描述: Classes to represent the definitions of aggregate functions. # 其他可以进行统计的类还有: 'Aggregate', 'Avg', 'Count', 'Max', 'Min', 'StdDev', 'Sum', 'Variance',from django. db. models. aggregates import Count# Library() 是 A class 用于注册模板标签和过滤器register = template. Library()# @register. simple_tag# def get_categories():#   return Category. objects. all()@register. simple_tagdef get_categories(): 	# 将Category数据集中的post 外键关联进行统计, 并将统计结果赋值给 num_posts 变量.   # category 和 post 是一对多的关系  # 然后对该变量再次进行筛选, 去掉统计结果为 0 的分类  return Category. objects. annotate(num_posts=Count('post')). filter(num_posts__gt=0) @register. simple_tagdef get_tags():  # 这里对 tag 进行数据统计的目的是可以让统计为更多被标注过的标签变得更大些.   # 这里和上面的 get_category 方法实现一模一样.   # 但是要注意, tag 和 post 表时多对多关系.   return Tag. objects. annotate(num_posts=Count('post')). filter(num_posts__gt=0) 模板 templates/base. html 1234567891011121314151617181920212223242526{% load blog_tags %}&lt;div class= widget widget-category &gt;  &lt;h3 class= widget-title &gt;分类&lt;/h3&gt;  {% get_categories as categories %}  &lt;ul&gt;    {% for cate in categories %}      &lt;li&gt;        &lt;a href= {% url 'blog:category' cate. id %}  &gt;{{ cate. name }}&lt;span            class= post-count &gt;({{ cate. post_set. count }})&lt;/span&gt;&lt;/a&gt;      &lt;/li&gt;    {% endfor %}  &lt;/ul&gt;&lt;/div&gt;&lt;div class= widget widget-tag-cloud &gt;  &lt;h3 class= widget-title &gt;标签云&lt;/h3&gt;  {% get_tags as tags %}  &lt;ul&gt;    {% for tag in tags %}      &lt;li&gt;        &lt;a href= {% url 'blog:tag' tag. id %} &gt;{{ tag. name }}&lt;/a&gt;      &lt;/li&gt;    {% endfor %}  &lt;/ul&gt;&lt;/div&gt;"
    }, {
    "id": 28,
    "url": "http://localhost:4000/Flask-framework-for-Python/",
    "title": "Flask Framework for Python",
    "body": "2019/11/10 - Flask是一个轻量级的WSGI Web应用程序框架。它旨在快速轻松地入门，并能够扩展到复杂的应用程序。它最初是围绕Werkzeug和Jinja的简单包装器，现已成为最流行的Python Web应用程序框架之一。本文我会循序渐进的介绍 Flask 一些常用的 web 开发技巧。 Flask 框架开发技巧: Chrome 快捷键: cmd + shift+ delete: 清除浏览器缓存 cmd + option + u : 查看页面原代码 cmd + option + j : 查看页面console cmd + option + i : developer tools pychar 快捷键: 双击 shift 可以快速找到想要找的文件 cmd + option + L : 代码快速格式化 cmd + fn + 左右 : 到代码的第一行和最后一行 cmd + shift + [ ] 上一个tab, 下一个 tab. cmd + option + 左右 上一个,下一个修改点 资源信息: 官网 中文官网 python命令行查看帮助 12345678import flaskhelp(flask)help(flask. Flask)help(flask. request)help(flask. make_response)help(flask. session)help(flask. redirect)help(flask. Blueprint)flask-script 监控代码修改, 配合谷歌浏览器自动刷新方案 四步: (1) 在谷歌浏览器上安装插件 Easy Auto Refresh (2) 安装flash-script 扩展包 pip3. 7 install flask-script flask-script 简介:  flask-script 是一个 flask 终端解析器, 可以在命令行启动app 并指定相应的参数 因为参数是从命令行走的, 所以代码中就不需要指定参数了, 这样就可以做到让代码可以从测试环境部署到生产环境时不改任何代码.  是通过传递不同的启动参数完成的.  需要 pip 安装1pip3. 7 install flask-script(3) 修改代码, 让 Manager 代替app run 只增加2行代码, 替换一行代码即可实现 123456789# flask-script 让代码可以从测试环境部署到生产环境时不改任何代码from flask_script import Managermgr = Manager(app) # 要将 app作为参数if __name__ == '__main__':  mgr. run()# 命令行执行:  python3. 6 manager. py runserver# 查看帮助信息: python3. 6 manager. py runserver --help(4) 启动flask 服务, 并开始刷新谷歌浏览器 1python3. 6 index. py runserver -d -r --threaded参数    -h –host # 指定主机     -p –port # 指定端口     -d     # debug模式     -r     # 监控代码自动加载     –threaded # 多线程模式     –help -?  # 帮助信息     123456789101112131415161718192021222324python3. 6 index. py runserver --helpusage: index. py runserver [-?] [-h HOST] [-p PORT] [--threaded]             [--processes PROCESSES] [--passthrough-errors] [-d]             [-D] [-r] [-R] [--ssl-crt SSL_CRT]             [--ssl-key SSL_KEY]Runs the Flask development server i. e. app. run()optional arguments: -?, --help      show this help message and exit -h HOST, --host HOST -p PORT, --port PORT --threaded --processes PROCESSES --passthrough-errors -d, --debug      enable the Werkzeug debugger (DO NOT use in production            code) -D, --no-debug    disable the Werkzeug debugger -r, --reload     monitor Python files for changes (not 100% safe for            production use) -R, --no-reload    do not monitor Python files for changes --ssl-crt SSL_CRT   Path to ssl certificate --ssl-key SSL_KEY   Path to ssl key   app. logger. info 配合 flask-script 输出log 信息: 更多日志输出教程 在代码任意位置写上 app. logger. info , 就可以在终端中输出info 信息. 1234from flask import Flaskapp = Flask(__name__)app. logger. info('info log')app. logger. warning('warning log')logging的级别主有  NOTSET、 DEBUG、 INFO、 WARNING、 ERROR CRITICALFlask 框架扩展包和使用图示: Flask 框架介绍: 简介:  非常小的一个Python web框架, 被称为微型框架 只提供了强健的核心, 其他功能通过扩展功能实现 这意味着你可以根据项目需要进行量身定制组成:  调试, 路由, WSGI系统 模板引擎 (jinja2, flask 核心人员开发)MVC:  M: Model 模型,既数据 V: View: 视图.  C: Controller, 控制器MTV:  Model 模型,既数据 Ttemplate 模板, 负责显示 View function 视图函数Flask 安装和使用: (1). pip 安装 flask 1pip3. 7 install flask(2). pycharm 创建一个flask 项目 注意: 项目目录名字不能叫做 flask ,否则python import flask 时会找项目目录,而不是找类库 (3). 创建一个视图函数文件index. py 并写如下代码: 注意: 不要将应用的名字起名为 flask. py, 否则会和框架冲突 12345678910111213141516171819from flask import Flask#创建一个flask 应用, 名字可以随意 , 比如可以是bpp, 但是后面所有的app 都要改成bpp. app = Flask(__name__) 	#这里的 __name__ 就是 字符串  __main__ # 根目录路由的处理方式@app. route('/') 		def index():  return 'Hello World!'if __name__ == '__main__':  app. run(debug=True)			#运行应用#  app. run(debug=True, threaded=True, host='0. 0. 0. 0', port=5050)  #run 输出如下:# * Running on http://127. 0. 0. 1:5000/ (Press CTRL+C to quit)启动参数/配置信息: 更多配置信息的使用技巧       启动参数   说明         debug=True   开启调试模式 (生产环境需要关闭). 代码修改会自动重启服务       threaded=True   是否开启多线程, 默认不开启       host   指定主机. 设置成 ‘0. 0. 0. 0’ 可以通过内网IP进行访问       port   指定端口   三种 debug=True 的设置方法 同样适用于 threaded=True 123456app. config['DEBUG'] = Trueapp. debug = Trueapp. run(debug=True)app. config. from_object('config') # 也可以定义到一个 config. py文件中,然后被调用(每行一个配置)app. config. from_pyfile('myconfig. cfg') #还可以定义在文件中 配置中要有 DEBUG = True# 但是如果想用 . from_pyfile() 必须提前设定app = Flask(__name__, instance_relative_config=True)config. py 内容如下:  前提是要在视图文件中设置 app. config. from_object(‘config’)12345import osSECRET_KEY = os. urandom(24)MAIL_SERVER = 'smtp. 163. com'MAIL_USERNAME = 'dalong_coo@163. com'MAIL_PASSWORD = 'xxxxxxx'flask 默认配置 - 源代码 (app. py) 12345678910111213141516171819202122232425262728293031 #: Default configuration parameters.   default_config = ImmutableDict({    'DEBUG':                get_debug_flag(default=False),    'TESTING':               False,    'PROPAGATE_EXCEPTIONS':         None,    'PRESERVE_CONTEXT_ON_EXCEPTION':    None,    'SECRET_KEY':              None,    'PERMANENT_SESSION_LIFETIME':      timedelta(days=31),    'USE_X_SENDFILE':            False,    'LOGGER_NAME':             None,    'LOGGER_HANDLER_POLICY':        'always',    'SERVER_NAME':             None,    'APPLICATION_ROOT':           None,    'SESSION_COOKIE_NAME':         'session',    'SESSION_COOKIE_DOMAIN':        None,    'SESSION_COOKIE_PATH':         None,    'SESSION_COOKIE_HTTPONLY':       True,    'SESSION_COOKIE_SECURE':        False,    'SESSION_REFRESH_EACH_REQUEST':     True,    'MAX_CONTENT_LENGTH':          None,    'SEND_FILE_MAX_AGE_DEFAULT':      timedelta(hours=12),    'TRAP_BAD_REQUEST_ERRORS':       False,    'TRAP_HTTP_EXCEPTIONS':         False,    'EXPLAIN_TEMPLATE_LOADING':       False,    'PREFERRED_URL_SCHEME':         'http',    'JSON_AS_ASCII':            True,    'JSON_SORT_KEYS':            True,    'JSONIFY_PRETTYPRINT_REGULAR':     True,    'JSONIFY_MIMETYPE':           'application/json',    'TEMPLATES_AUTO_RELOAD':        None,  })请求和响应: 变量或对象       变量/对象   上下文______   说明         current_app   程序上下文   代表当前运行的实例app 名, 也就是文件名(去掉. py)       g   程序上下文   处理请求用到的临时变量, 不同页面都可以使用(设置和获取). 每次请求会重置该变量       request   请求上下文   请求对象, 所有和HTTP请求信息都在request中. 保存了客户端HTTP请求的信息.        session   请求上下文   用户会话, 用于保存需要被’记住’的会话信息.    测试current_app, 文件名为 testEnv. py 123456789101112from flask import Flask, current_appapp = Flask(__name__)@app. route('/')def index():  return 'Hello, %s!' % current_app. nameif __name__ == '__main__':  app. run()# 输出为 Hello, testEnv!请求钩子函数: 在特定场景被触发的装饰器函数       钩子函数   说明         @app. before_first_request   第一次请求之间调用的函数       @app. before_request   每次请求之前都调用的函数       @app. after_request   每次请求之后调用. 前提是没有异常       @app. teardown_rquest   每次请求之后调用, 即使有异常页调用   视图函数: 路由介绍 @app. route(): 使用说明:  路由末尾的 / 最好还是加上, 防止路由错误 如果路由需要指定参数, 那么参数需要写在 &lt;&gt; 中, 且该参数名子需要在绑定的函数的参数名一致 如果参数需要指定数据类型(int/float/path/string),那么类型需要写在参数前面, 且用冒号隔开 &lt;int:uid&gt; 如果不指定参数类型, 默认就是 string path 类型其实也是字符串类型, 只不过 不把 / 作为分割符无参数路由 123@app. route('/')def index(): return '&lt;h1&gt;你好&lt;/h1&gt;'单参数路由 123456@app. route('/welcome/&lt;name&gt;')def welcome(name):  return '你好%s' %name# 调用方法 http://127. 0. 0. 1:5000/welcome/dalong# 页面输出: 你好dalong多参数路由 12345@app. route('/ma/&lt;name&gt;/&lt;age&gt;/')def ma(name, age):  return '%s and %s' %(name,age)#调用: http://127. 0. 0. 1:5000/ma/dalong/15#输出: dalong and 15todo 参数默认值 指定参数类型 123456# 指定参数类型, 默认是 string, 还可以是 int, float, path@app. route('/user/&lt;int:uid&gt;')def user(uid):  return  %d 号用户  %uid#调用方法: http://127. 0. 0. 1:5000/user/1#输出: 1 号用户path 类型参数 1234567# path类型, 其实仍然是字符串, 只是'/' 不再是分隔符@app. route('/path/&lt;path:p&gt;')def path(p):  return  路径是 %s  %p #调用方法: http://127. 0. 0. 1:5000/path/1/2/3/4#输出 : 路径是 1/2/3/4万能路由的例子  任何网址都可以进入这个页面123456789101112131415161718192021222324from flask import Flaskfrom flask_script import Managerfrom werkzeug. routing import BaseConverterapp = Flask(__name__)mgr = Manager(app)class RegexConverter(BaseConverter):  def __init__(self, url, *args):    self. url = url    self. regex = args[0]app. url_map. converters['regex'] = RegexConverter@app. route('/&lt;regex(r [\w\W]* ):url&gt;/', methods=['GET', 'POST'])def index(url):  return '万能路由' + urlif __name__ == '__main__':  mgr. run()获取客户端请求(request) 信息.  request. xxx(): 首先要 import 1from flask import request定义路由 123from flask import request@app. route('/request/')def req(): # 定义的函数名必须是 req()不同request 的详细说明  http://127. 0. 0. 1:5000/cat1/cat2/cat3/?a=1&amp;b=2&amp;c=3 为例      request 类型   输出样式   说明         request. url   http://127. 0. 0. 1:5000/cat1/cat2/cat3/?a=1&amp;b=2&amp;c=3   是完整的url , 不包含?号 后面get 参数       request. base_url   http://127. 0. 0. 1:5000/cat1/cat2/cat3/   是base url, 不包含任何路径信息       request. path   /cat1/cat2/cat3/   只是装饰器中的路径, 不含base url , request       request. method   GET   返回http请求类型GET/POST       request. host   127. 0. 0. 1:5000   获取访问者的IP地址 127. 0. 0. 1:5000       request. args   ImmutableMultiDict([(‘a’, ‘1’), (‘b’, ‘2’), (‘c’, ‘3’)])   获取get信息中的某个参数       request. headers   一大堆, 看下面的例子   获取客户端的请求的头信息, 比如头信息中的 User-Agent       request. environ   {'wsgi. version': (1, 0), 'wsgi. url_scheme': 'http', 'wsgi. input': &lt;_io. BufferedReader name=7&gt;, 'wsgi. errors': &lt;_io. TextIOWrapper name='&lt;stderr&gt;' mode='w' encoding='UTF-8'&gt;, 'wsgi. multithread': True, 'wsgi. multiprocess': False, 'wsgi. run_once': False, 'werkzeug. server. shutdown': &lt;function WSGIRequestHandler. make_environ. &lt;locals&gt;. shutdown_server at 0x10dc61f28&gt;, 'SERVER_SOFTWARE': 'Werkzeug/0. 13', 'REQUEST_METHOD': 'GET', 'SCRIPT_NAME': '', 'PATH_INFO': '/cat1/cat2/cat3/', 'QUERY_STRING': 'a=1&amp;b=2&amp;c=3', 'REMOTE_ADDR': '127. 0. 0. 1', 'REMOTE_PORT': 57839, 'SERVER_NAME': '127. 0. 0. 1', 'SERVER_PORT': '5000', 'SERVER_PROTOCOL': 'HTTP/1. 1', 'HTTP_HOST': '127. 0. 0. 1:5000', 'HTTP_CONNECTION': 'keep-alive', 'HTTP_CACHE_CONTROL': 'max-age=0', 'HTTP_USER_AGENT': 'Mozilla/5. 0 (Macintosh; Intel Mac OS X 10_13_2) AppleWebKit/537. 36 (KHTML, like Gecko) Chrome/63. 0. 3239. 108 Safari/537. 36', 'HTTP_UPGRADE_INSECURE_REQUESTS': '1', 'HTTP_ACCEPT': 'text/html,application/xhtml+xml,application/xml;q=0. 9,image/webp,image/apng,*/*;q=0. 8', 'HTTP_ACCEPT_ENCODING': 'gzip, deflate, br', 'HTTP_ACCEPT_LANGUAGE': 'zh-CN,zh;q=0. 9', 'HTTP_COOKIE': 'session=eyJjc3JmX3Rva2VuIjoiNGZkNTE0OTBhMGNlOGMyYjg0YWY3ODllMjFhNjdiOTU0ZTE4ZmM0ZCIsIm5hbWUiOiJkYWxvbmcifQ. DS-T-A. N8xYS4q8ApBWqBXDpB21OtOH1uk', 'werkzeug. request': &lt;Request 'http://127. 0. 0. 1:5000/cat1/cat2/cat3/?a=1&amp;b=2&amp;c=3' [GET]&gt;}   环境变量       request. form   ImmutableMultiDict([])           request. cookies   {‘session’: ‘eyJjc3JmX3Rva2VuIjoiNGZkNTE0OTBhMGNlOGMyYjg0YWY3ODllMjFhNjdiOTU0ZTE4ZmM0ZCIsIm5hbWUiOiJkYWxvbmcifQ. DS-T-A. N8xYS4q8ApBWqBXDpB21OtOH1uk’}           request. charset   utf-8       下面文件的请求地址 : http://127. 0. 0. 1:5000/ma/dalong/16/ 或者 : http://127. 0. 0. 1:5000/ma/dalong/16/?a=1&amp;b=2 123456789101112131415161718192021222324from flask import request@app. route('/ma/&lt;string:name&gt;/&lt;int:age&gt;/')def ma(name, age):return request. url 	# 是完整的url , 不包含?号 后面get 参数# 输出: http://127. 0. 0. 1:5000/ma/dalong/15/return request. base_url # 是base url, 不包含任何路径信息# 输出: http://127. 0. 0. 1:5000/ma/dalong/15/return request. path 	# 只是装饰器中的路径, 不含base url , request# 输出: /ma/dalong/15/return request. method 	# 返回http请求类型GET/POST# 输出 GETreturn request. host 	# 获取访问者的IP地址 127. 0. 0. 1:5000# 输出 127. 0. 0. 1:5000return request. args['a'] 	# 获取get信息中的某个参数#http://127. 0. 0. 1:5000/ma/dalong/16/?a=1&amp;b=2# 1return str(request. args) 	# 获取get信息中的所有参数#http://127. 0. 0. 1:5000/ma/dalong/16/?a=1&amp;b=2#输出: ImmutableMultiDict([('a', '1'), ('b', '2')])return request. headers['User-Agent'] #获取客户端请求的头User-Agent 信息#输出: Mozilla/5. 0 (Macintosh; Intel Mac OS X 10_13_2) AppleWebKit/537. 36 (KHTML, like Gecko) Chrome/63. 0. 3239. 108 Safari/537. 36return str(request. headers) # 获得所有头信息#输出: Host: 127. 0. 0. 1:5000 Connection: keep-alive Cache-Control: max-age=0 User-Agent: Mozilla/5. 0 (Macintosh; Intel Mac OS X 10_13_2) AppleWebKit/537. 36 (KHTML, like Gecko) Chrome/63. 0. 3239. 108 Safari/537. 36 Upgrade-Insecure-Requests: 1 Accept: text/html,application/xhtml+xml,application/xml;q=0. 9,image/webp,image/apng,*/*;q=0. 8 Accept-Encoding: gzip, deflate, br Accept-Language: zh-CN,zh;q=0. 9 Cookie: name=dalong; session=eyJ1c2VybmFtZSI6Ilx1NTkyN1x1OWY5OSJ9. DSzR_w. Ikn8WTP2wiNKFPrWKc0xKtfUZM8; class=1702响应(response) 的处理函数 make_response(): http 状态码  1xx 处理中 2xx 成功 3xx 跳转 4xx 客户端错误 5xx 服务端错误如果需要构造响应, 需要先from flask import make_response 包 123456789from flask import make_response@app. route('/response/')def response():  # return 'OK' 											# 最基本响应  # return 'Page 没找到啊', 404 	  	 				 # 指定状态码  # resp = make_response('我是构造响应', 404); return resp # 使用函数构造响应.     resp = make_response('我是构造响应', 404); return resp重定向 redirect(): 先要from flask import redirect 12345678910from flask import redirect@app. route('/redirect/')def old():  return redirect('/new/')  #return '老旧内容'@app. route('/new/')def new():  return  新的页面 通过函数名找到路由 url_for(): 通过视图函数名,找到函数对应的装饰器的参数.  首先导入 from flask import url_for 传递的参数 return url_for('old') 需要是另一个函数的名字123456789from flask import redirect, url_for@app. route('/redirect/')def old():  return redirect('/new/')@app. route('/new/')def new():  return url_for('old')另一种方法: 找路由并带上参数 12345678910from flask import redirect, url_for@app. route('/redirect/')def old():  return redirect(url_for('welcome', name='dalong'))  #return '老旧内容'# 指定普通参数的路由@app. route('/welcome/&lt;name&gt;')def welcome(name):  return '你好%s' %name异常终止客户请求 abort(500): 通过flask 框架 传递异常参数(404,500等)给页面,让页面输出浏览器默认的异常页面. 12345678from flask import abort@app. route('/error/')def error():  # 框架会通过传递的参数,提前给出既定的报错信息.   # abort(404) # 模拟404错误  # abort(500) # 模拟  # return '出错了'  abort(500)定制页面报错信息 errorhandler(404): 一旦定制页面报错信息(404), 定制后, app中任何404都会走这个函数  应该也可以定制500, 200123456# 定制404, 定制后, app中任何404都会走这里@app. errorhandler(404)def page_not_found(e):  return '大哥,报错了'#请求方法: http://127. 0. 0. 1:5000/abcd#页面输出: 大哥,报错了会话控制 resp. set_cookie, request. cookies. get() session['username'] = '大龙', session. get(): 首先讲一下Python的flask中session与cookies的关系  php 的 session是储存在服务器中的，cookies是储存在浏览器本地中 flask的session与cookies，session是经过加密保存在cookies中。 在flask中使用session需要先设置secret_key，根据算法加密session信息 如果你设置了 Flask. secret_key ，你可以在 Flask 应用中使用会话。 会话主要使得在页面跳转过程中保留信息成为可能。 Flask 的实现方法是使用一个签名的 cookie 。 这样，用户可以查看会话的内容，但是不能修改它，除非用户知道密钥。所以确保密钥被设置为一个复杂且无法被容易猜测的值。 之所以用 cookies. get('name') 而不是 cookies['name'] 是因为 get() 函数就算拿不到信息也不会报错. session. get() 也是一个道理设置 , 获取 和 删除 Cookie 1234567891011121314151617181920212223242526272829303132from flask import requestfrom flask import make_responseimport time# 设置 cookie@app. route('/set_cookie/')def set_cookie():  resp = make_response('cookie 已经设置了')  exps = time. time() + 100 # 设置cookie 100秒后失效  resp. set_cookie('name','dalong', expires=exps)  return resp# 获取 cookie@app. route('/get_cookie/')def get_cookie():  # 客户再次请求, cookie 在客户的request中可以找到  # return request. cookies['name']  return request. cookies. get('name') or '你是哪个二哥?'# 删除 cookie@app. route('/del_cookie/')def del_cookie():  resp = make_response('cookie 已经删除了')  resp. delete_cookie('class')  return resp #使用方法:# http://127. 0. 0. 1:5000/set_cookie/# http://127. 0. 0. 1:5000/get_cookie/# http://127. 0. 0. 1:5000/del_cookie/获取所有cookie 信息 1return str(request. cookies) or '你是哪个二哥?'还可以同时设置cookie, render_template 123resp = make_response(render_template('userprofile. html'))resp. set_cookie('username', 'the username')return resp设置 获取 和 删除 sesison  设置session 必须先设置 安全秘钥1234567891011121314151617181920212223242526272829303132from flask import session# 设置秘钥import osapp = Flask(__name__)app. config['SECRET_KEY'] = os. urandom(24) # 返回24个字节的用以加密的随机字符串# app. config['SECRET_KEY'] = '123456'# 设置 session@app. route('/set_session/')def set_session():  session['username'] = '大龙'  return 'session 已经设置'# 获取session@app. route('/get_session/')def get_session():  return session. get('username') or 'who are you'# 删除 session#可以直接使用session. pop('key',None) 即：#session. pop('name',None)#如果要删除session中所有数据使用：clear()即：#session. clear()@app. route('/del_session/')def del_session():	session. clear()	# session. pop('username',None)	return  session 已经清空 # 请求方法 # http://127. 0. 0. 1:5000/set_session/# http://127. 0. 0. 1:5000/get_session/todo 登录访问 @login_required: Flask-Login官方文档 Flask-Login 扩展可以很容易地实现登录系统。除了处理用户认证的细节，Flask-Login 提供给我们一个装饰器，它用来限制只允许登录的用户访问某些视图 一个登录过的用户才能够访问 /dashboard 路由。 1234567891011121314# app. pyfrom flask import render_templatefrom flask. ext. login import login_required, current_user@app. route('/')def index():  return render_template( index. html )@app. route('/dashboard')@login_requireddef account():  return render_template( account. html )注意:  @app. route 应该是外层的视图装饰器（换句话说，@app. route 应该在所有装饰器的最前面）。todo 缓存 Flask-Cache 扩展: Flask-Cache官方文档 目的:  将一些页面内容缓存,提高服务器的负载使用 Flask-Cache 扩展。这个扩展提供我们一个装饰器，我们可以在我们的首页视图上使用这个装饰器用来在一段时间内缓存响应。 1234567891011121314151617181920# app. pyfrom flask. ext. cache import Cachefrom flask import Flaskapp = Flask()# We'd normally include configuration settings in this callcache = Cache(app)@app. route('/')@cache. cached(timeout=60)def index():  [. . . ] # Make a few database calls to get the information we need  return render_template(    'index. html',    latest_posts=latest_posts,    recent_users=recent_users,    recent_photos=recent_photos  )蓝图 Blueprint 两步拆分视图函数: 说明:  当代码越来越复杂, 将所有视图函数放在一个文件中会比较麻烦, 查找定位问题速度会降低. 如果能根据功能模块进行划分,并拆分到不同文件中会更有效率. 步骤: (1) 创建一个新的蓝图文件reg. py 1234567891011121314from flask import Blueprint# 创建应用, user 其实和app 是一样的对象user = Blueprint('user', __name__) # 第一个参数的名字要和app名字一致. # 添加路由@user. route('/login/')def login():  return '登录'@user. route('/register/')def register():  return '注册'(2) 然后在主文件中注册一个蓝图 1234567# 注册蓝图, 顺便可以指定路由的前缀from reg import userapp. register_blueprint(user, url_prefix='/ur')#访问方法: #http://127. 0. 0. 1:5000/ur/register/#http://127. 0. 0. 1:5000/ur/login/完整的视图函数的例子: 简单的注册,登录,登出代码: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061from flask import Flaskfrom flask import requestfrom flask import make_responsefrom flask import sessionfrom flask import redirectfrom random import randintimport osimport timeapp = Flask(__name__)app. config['SECRET_KEY'] = os. urandom(24)# 主页@app. route('/')def index():  userId = session. get('userId')  userName = session. get('userName')  # print('获得的用户信息:',userId, userName)  if userId == None:    return '这是主页, 请 &lt;a href= /login/ &gt;登录&lt;/a&gt; 或&lt;a href= /register/ &gt;注册&lt;/a&gt;'  else:    return '你好 ' + str(userName) + '&lt;a href= /logout/ &gt;退出登录&lt;/a&gt;'# 注册@app. route('/register/')def register():  # todo 省略注册的页面  # 生成 randint 数字, 写到cookie中.   userId = randint(10000, 99999)  print(userId)  resp = make_response('注册成功, 请进行&lt;a href= /login/ &gt;登录&lt;/a&gt;')  resp. set_cookie('userId', str(userId))  return resp# 登录@app. route('/login/')def login():  # todo 省略登录页面  # 读 cookie, 写session  print(request. cookies. get('userId'))  if request. cookies. get('userId') == None:    return redirect('/register/')  session['userId'] = request. cookies. get('userId')  session['userName'] = '大龙'  return '登录成功, session 设置完毕, 请回到&lt;a href= / &gt;主页&lt;/a&gt;'# 登出@app. route('/logout/')def logout():  #清除cookie 和 session  resp = make_response('您已经退出系统 &lt;a href= / &gt;回到主页&lt;/a&gt;')  resp. delete_cookie('userId')  session. clear()  return respif __name__ == '__main__':  app. run(debug=True, threaded=True, host='0. 0. 0. 0', port=5050)模板引擎: 如果希望模板的修改自动加载,需要在视图函数中增加如下代码. 注意:    先把DEBUG = True 打开     在 flask-script 命令行模式下有效.  1app. config['TEMPLATES_AUTO_RELOAD'] = True模板引擎介绍: 模板的概念:  模板文件就是按照特定的语法规则书写的,负责展示效果的HTML 文件  模板引擎就是提供了规则的解析和替换的工具 Jinja2:  Flask框架中使用的默认模板引擎, 是由 Flask 开发人员开发的. Jinja2的使用 from flask import render_template return render_template('index. html'): (1) 准备工作, 先项目根目录下创建一个模板文件夹 templates 123project/			#项目根目录	templates/		#模板目录	testJinja2. py	#视图函数(2) 在模板文件夹中创建一个index. html 模板文件 1&lt;h1&gt;hello Jinja2 from template&lt;/h1&gt;(3) 在视图函数中 from flask import render_template or from flask import render_template_string 1234567891011121314151617from flask import Flaskfrom flask_script import Managerfrom flask import render_templatefrom flask import render_template_stringapp = Flask(__name__)manager = Manager(app)@app. route('/')def index():  # return render_template_string('&lt;h1&gt;hello Jinja2 from template string&lt;/h1&gt;')  return render_template('index. html')  # return 'Janja2 Hi'if __name__ == '__main__':  manager. run() return render_template('index. html') 负责渲染整个页面 return render_template_string('&lt;h1&gt;hello Jinja2 from template string&lt;/h1&gt;') 负责渲染页面局部内容render_template 接收任意多个参数 1msg. body = render_template('mailTemplate/'+template+'. txt', **kwargs)模板文件中的变量使用 {{ 变量 }} or {{ g. 变量 }}  {% 逻辑块 %}  {# 注释内容 #}: Jinja有两种分隔符。{% . . . %} 和 {{ . . . }}  {% 逻辑块 %} 用于执行类似 for 循环或者赋值的声明， {{ 变量 }} 用于输出表达的结果到模板中。 {# 注释内容 #}123456&lt;h1&gt;hello Jinja2 from template&lt;/h1&gt;&lt;h2&gt;你好 {{ name|upper }}&lt;/h2&gt;&lt;h2&gt;这个g. name也行 {{ g. name }} g变量不需要分配&lt;/h2&gt;&lt;h2&gt;这个是注释 {# 看不到 #}&lt;/h2&gt;&lt;br&gt;去掉html标签 {{ tags|safe }}12345678910111213141516from flask import Flask, gfrom flask_script import Managerfrom flask import render_templatefrom flask import render_template_stringapp = Flask(__name__)manager = Manager(app)@app. route('/')def index():  g. name = '大龙'  return render_template('index. html', name='dalong', tags='&lt;h2&gt;我带h2标签吗&lt;/h2&gt;')if __name__ == '__main__':  manager. run()g 变量的使用*: g 变量的好处是可以在视图函数和模板之间传递多个变量,而不用将g 转 return 的时候传递给模板. 用法 1234567from flask import Flask, g@app. route('/')def index():  g. name = '小龙'  g. age = 18  return render_template('testG_index. html')# 模板中使用 : &lt;h2&gt;这个g. name也行 {{ g. name }} g变量不需要分配&lt;/h2&gt;注意:  g 变量的赋值只能发生在路由函数中, 如果在外面就会报错. 在模板中对变量使用过滤器 {{ 变量|upper }}: 过滤器是一个函数，它像linux 中的管道符, 能够在 {{ . . . }} 中用于处理一个表达式的结果。在表达式结果输出到模板之前它就被调用。 用法如下 12345{{ 变量|upper }}{{ 变量|capitalize }}{{ 变量|striptags }}{{ 变量|safe }}{{ g. name|replace('da','xiao')|title }}可用的过滤器函数 过滤器完整列表。       修饰函数   说明         upper   全大写       lower   全小写       capitalize   首字母大写       title   每个单词首字母大写       trim   去两边空白       striptags   过滤掉html标签.        safe   将变量带的html标签保留. 如果不用safe, 默认变量中的html 标签会转义后展现在页面中. 但使用要谨慎. 确保安全才能使用. 比如用户从表单提交的数据要严格禁止使用safe.        abs   数字的绝对值       first/last   获取可迭代数据的第一条, 最后一条       format   相当于 python 的 format函数       min/max   最大最小值       map   将一系列内容进行逐一处理. 比如 {{ titles|map('lower')|join(', ') }} 将titles数组进行小写,然后逗号分隔组成字符串       replace   替换字符串 {{  Hello World |replace( Hello ,  Goodbye ) }}       slice   可迭代元素的切片       join   将迭代单元组合到一起. {{ [1, 2, 3]|join('|') }} -&gt; 1|2|3 , {{ [1, 2, 3]|join }} -&gt; 123   自定义过滤器: 我们可以在是视图函数中自定义过滤器, 用于将我们想要的效果作用域变量上. 语法:  使用 @app. template_filter() 装饰器来定义装饰器,然后在装饰器下面定义一个过滤器函数(普通python函数) 如果装饰器定义了参数, 那么过滤器名字就是该参数. 如果装饰器没有参数,那么函数名就是过滤器的名字定义 123@app. template_filter('rjusts')def rightjust(text):  return text. rjust(10, '-')1{{  abc |rightjust }}输出效果: ——-abc 模板中的流程控制和循环 {% if name %} {% for i in range(1,6) %}: 分支结构 12345{% if name %}  &lt;h1&gt;hello {{ name }} 是if 判断中的&lt;/h1&gt;{% else %}  &lt;h1&gt;hello world&lt;/h1&gt;{% endif %}123@app. route('/')def index():  return render_template('index. html', name='dalong')循环结构 123456{# 循环结构 #}&lt;ol&gt;{% for i in range(1,6) %}  &lt;li&gt;{{ i }}&lt;/li&gt;{% endfor %}&lt;/ol&gt;模板中的文件包含{% include '2. html' %}: 可以提前把那些会被重复使用的html 内容定义到单独的文件中, 然后在jinja模板中将定义好的html文件包含进来 主模板文件 1{% include '2. html' %}被包含的模板文件2. html 123&lt;div&gt;  &lt;h2&gt;这是被包含的文件2. html的内容&lt;/h2&gt;&lt;/div&gt;宏(模板函数)的定义和调用 {% macro show_name(name) %} {{ show_name(name) }}: 宏相当于定义一个模板函数, 为函数传递一个参数, 然后函数返回一段固定的html内容 宏的目的:  我们模板中坚持 DRY（不要重复自己）的原则，通过抽象出重复出现的代码片段到 宏 , 避免编写一大段 if . . . else语句( 将 if . . . else 写到宏里面去 ) 通常我们可以将特定的显示效果定义成宏, 然后在需要的地方导入即可. 避免相同效果的重复代码 .  比如: 将很多使用了不同css样式和文字的 &lt;a&gt; 链接定义在宏里面, 然后根据需要在不同的地方通过参数进行调用. 宏的原理:  宏采用类似python 函数定义的方法进行定义 用 python 类似 import 的形式对宏进行调用.  核函数类似的方法传递参数并进行调用. 宏定义, 调用宏 1234567{# 定义宏 #}{% macro show_name(name) %}  &lt;h1&gt;hello {{ name }} 这是宏里的内容&lt;/h1&gt;{% endmacro %}{# 调用宏 #}{{ show_name(name) }}注意:  定义宏用{%%} 调用宏用{{}} 调用宏的代码一定要放在定义宏的后面,否则会报错找不到该宏123@app. route('/')def index():  return render_template('index. html', name='dalong')宏也可以单独定义到文件中,然后导入到模板中使用 {% from '3. html' import show_name %}: 3. html 12345{# 定义宏 #}{% macro show_name(name) %}  &lt;h1&gt;hello {{ name }} 这是被导入的宏里的内容&lt;/h1&gt;{% endmacro %}主模板导入宏 12345{# 导入宏 #}{% from '3. html' import show_name %}{# 调用宏 #}{{ show_name(name) }}block的定义 及 模板的继承 {% block title %} {% extends 'parents. html' %} {{ super() }}: 目的:  用于将一些公共的模板作为父类, 子类可以继承父类,然后重写父类的部分显示效果.  大点的网站项目通常有一个顶层的 layout. html，它定义了你的应用程序的通用布局以及你的网站的每一部分。其他模板继承它block的意义: 定义html代码块  用于在html 中将小块的内容进行定义, 以便于后续的子模板进行调用和重写注意:  当重写了一个block 后发现原来的显示没有了, 很可能是没有调用 super()父模板 parents. html 12345678910&lt;!DOCTYPE html&gt;&lt;html lang= en &gt;&lt;head&gt;  &lt;meta charset= UTF-8 &gt;  &lt;title&gt;{% block ptitle %}父模板的标题{% endblock %}&lt;/title&gt;&lt;/head&gt;&lt;body&gt;  {% block pcontent %}父模板的内容{% endblock %}&lt;/body&gt;&lt;/html&gt;子模板 child. html 123456789101112{# 继承 #}{% extends 'parents. html' %}{# 完全重写父类模板 #}{% block ptitle %} 重写父母版的标题 {% endblock %}{# 部分重写模板中的内容 #}{% block pcontent %}  {{ super() }} {# 调用父类的内容 #}  部分重写父母版的标题{% endblock %}视图函数 123@app. route('/extends/')def extends():  return render_template('child. html')flask-bootstrap 支持 bootstrap: 官方文档 安装: 1pip3. 6 install flask-bootstrap使用方法: (1). 首先创建bootstrap 实例, 并用它初始化app应用. 12345678from flask import Flaskfrom flask_bootstrap import Bootstrapapp = Flask(__name__)app. config['TEMPLATES_AUTO_RELOAD'] = Truebs = Bootstrap(app)  @app. route('/bs/')def boot():  return render_template('bs. html')(2). 在程序中使用一个包含所有Bootstrap 文件的基模版。这个模版利用 Jinja2 的模版继承机制，让程序扩展一个具有基本页面结构的基模版，其中就有用来引入 Bootstrap 的元素。  Jinja2 中的 extends 指令从 Flask-Bootstrap 中导入 bootstrap/base. html，从而实现模版继承。Flask-Bootstrap 中的基模版提供了一个网页框架，引入了 Bootstrap 中的所有 CSS 和 JavaScript 文件。1234567891011{# 集成bootstrap的基础模板 #}{% extends 'bootstrap/base. html' %}{% block title %}BootStrap 用户注册页面{% endblock %}{% block content %}  &lt;div class= container &gt;欢迎注册gogog&lt;/div&gt;{% endblock %} 基模版中定义了可在衍生模版中重定义的块。block 和 endblock 指令定义的块中的内容可添加到基模版中。 上面这个示例定义了3个块，分别名为 title、navbar和content。这些块都是基模版提供的，可在衍生模版中重新定义。Flask-Bootstrap 的 base. html 模块定义的可用块。 Available blocks:       Block name   Outer Block   Purpose         doc       Outermost block.        html   doc   Contains the complete content of the &lt;html&gt; tag.        html_attribs   doc   Attributes for the HTML tag.        head   doc   Contains the complete content of the &lt;head&gt; tag.        body   doc   Contains the complete content of the &lt;body&gt; tag.        body_attribs   body   Attributes for the Body Tag.        title   head   Contains the complete content of the &lt;title&gt; tag.        styles   head   Contains all CSS style &lt;link&gt; tags inside head.        metas   head   Contains all &lt;meta&gt; tags inside head.        navbar   body   An empty block directly above content.        content   body   Convenience block inside the body. Put stuff here.        scripts   body   Contains all &lt;script&gt; tags at the end of the body.    注意:    如果使用了bootstrap , 如果重写了一个block 之后, 原来的效果消失了, 那么可能是忘了super()     上表中很多块都是 Flask-Bootstap 自用的， 如果直接重定义可能会导致一些问题。如果程序需要向已经有内容的块中添加新内容， 必须使用 Jinja2 提供的 super() 函数。例如，如果要在衍生模版中添加新的 JavaScript 文件，需要这么定义：   1234{% block scripts %}{{ super() }}&lt;script type= text/javascript  src= my-script. js &gt;&lt;/script&gt;{% endblock %}   Bootstrap 官方的基类: bootstrap/base. html 12345678910111213141516171819202122232425262728293031323334{% block doc -%}&lt;!DOCTYPE html&gt;&lt;html{% block html_attribs %}{% endblock html_attribs %}&gt;{%- block html %} &lt;head&gt;  {%- block head %}  &lt;title&gt;{% block title %}{{title|default}}{% endblock title %}&lt;/title&gt;  {%- block metas %}  &lt;meta name= viewport  content= width=device-width, initial-scale=1. 0 &gt;  {%- endblock metas %}  {%- block styles %}  &lt;!-- Bootstrap --&gt;  &lt;link href= {{bootstrap_find_resource('css/bootstrap. css', cdn='bootstrap')}}  rel= stylesheet &gt;  {%- endblock styles %}  {%- endblock head %} &lt;/head&gt; &lt;body{% block body_attribs %}{% endblock body_attribs %}&gt;  {% block body -%}  {% block navbar %}  {%- endblock navbar %}  {% block content -%}  {%- endblock content %}  {% block scripts %}  &lt;script src= {{bootstrap_find_resource('jquery. js', cdn='jquery')}} &gt;&lt;/script&gt;  &lt;script src= {{bootstrap_find_resource('js/bootstrap. js', cdn='bootstrap')}} &gt;&lt;/script&gt;  {%- endblock scripts %}  {%- endblock body %} &lt;/body&gt;{%- endblock html %}&lt;/html&gt;{% endblock doc -%} 定义项目基础模板: 说明:  一个项目中,如果很多页面相似, 只有细微差别, 那么为了避免重复工作, 通常可以为项目制定一个基础模板(继承自bootstrap 基础模板),然后其他页面对基础模板进行继承, 然后根据需要进行简单修改. 步骤: (1). 重bootcss. com上复制一个顺眼的导航条 (2). 将内容赋值到模板中 1234567891011{# 集成bootstrap的基础模板 #}{% extends 'bootstrap/base. html' %}{% block title %}BootStrap 基础模板{% endblock %}{# 定制导航条 #}{% block navbar %}	{# 将拷贝的导航条拷贝到这里 #}{% endblock %}(3). 根据需要,定制导航条内容 (4). 然后再定制页面主体部分 1234567891011121314151617181920{# 集成bootstrap的基础模板 #}{% extends 'bootstrap/base. html' %}{% block title %}BootStrap 基础模板{% endblock %}{# 定制导航条 #}{% block navbar %}	{# 将拷贝的导航条拷贝到这里 #}{% endblock %}{# 定制页面主体部分 #}{% block content %}  &lt;div class= container &gt;    {# 为页面弹框预留空间 #}    {% block popwindow %}      默认内容    {% endblock %}    {% include '2. html' %}  &lt;/div&gt;{% endblock %}一个完整的基础模板例子 base. html 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576{# 集成bootstrap的基础模板 #}{% extends 'bootstrap/base. html' %}{% block title %}BootStrap 基础模板{% endblock %}  {# 定制导航条 #}  {% block navbar %}    &lt;nav class= navbar navbar-inverse &gt;     &lt;div class= container-fluid &gt;      &lt;!-- Brand and toggle get grouped for better mobile display --&gt;      &lt;div class= navbar-header &gt;       &lt;button type= button  class= navbar-toggle collapsed  data-toggle= collapse  data-target= #bs-example-navbar-collapse-1  aria-expanded= false &gt;        &lt;span class= sr-only &gt;Toggle navigation&lt;/span&gt;        &lt;span class= icon-bar &gt;&lt;/span&gt;        &lt;span class= icon-bar &gt;&lt;/span&gt;        &lt;span class= icon-bar &gt;&lt;/span&gt;       &lt;/button&gt;       &lt;a class= navbar-brand  href= # &gt;三手店&lt;/a&gt;      &lt;/div&gt;      &lt;!-- Collect the nav links, forms, and other content for toggling --&gt;      &lt;div class= collapse navbar-collapse  id= bs-example-navbar-collapse-1 &gt;       &lt;ul class= nav navbar-nav &gt;        &lt;li class= active &gt;&lt;a href= # &gt;产品 &lt;span class= sr-only &gt;(current)&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;        &lt;li&gt;&lt;a href= # &gt;服务&lt;/a&gt;&lt;/li&gt;        &lt;li class= dropdown &gt;         &lt;a href= #  class= dropdown-toggle  data-toggle= dropdown  role= button  aria-haspopup= true  aria-expanded= false &gt;更多. . . &lt;span class= caret &gt;&lt;/span&gt;&lt;/a&gt;         &lt;ul class= dropdown-menu &gt;          &lt;li&gt;&lt;a href= # &gt;Action&lt;/a&gt;&lt;/li&gt;          &lt;li&gt;&lt;a href= # &gt;Another action&lt;/a&gt;&lt;/li&gt;          &lt;li&gt;&lt;a href= # &gt;Something else here&lt;/a&gt;&lt;/li&gt;          &lt;li role= separator  class= divider &gt;&lt;/li&gt;          &lt;li&gt;&lt;a href= # &gt;Separated link&lt;/a&gt;&lt;/li&gt;          &lt;li role= separator  class= divider &gt;&lt;/li&gt;          &lt;li&gt;&lt;a href= # &gt;One more separated link&lt;/a&gt;&lt;/li&gt;         &lt;/ul&gt;        &lt;/li&gt;       &lt;/ul&gt;       &lt;form class= navbar-form navbar-left &gt;        &lt;div class= form-group &gt;         &lt;input type= text  class= form-control  placeholder= 猪手 &gt;        &lt;/div&gt;        &lt;button type= submit  class= btn btn-default &gt;搜索&lt;/button&gt;       &lt;/form&gt;       &lt;ul class= nav navbar-nav navbar-right &gt;        &lt;li&gt;&lt;a href= # &gt;登录&lt;/a&gt;&lt;/li&gt;        &lt;li class= dropdown &gt;         &lt;a href= #  class= dropdown-toggle  data-toggle= dropdown  role= button  aria-haspopup= true  aria-expanded= false &gt;用户选项 &lt;span class= caret &gt;&lt;/span&gt;&lt;/a&gt;         &lt;ul class= dropdown-menu &gt;          &lt;li&gt;&lt;a href= # &gt;Action&lt;/a&gt;&lt;/li&gt;          &lt;li&gt;&lt;a href= # &gt;Another action&lt;/a&gt;&lt;/li&gt;          &lt;li&gt;&lt;a href= # &gt;Something else here&lt;/a&gt;&lt;/li&gt;          &lt;li role= separator  class= divider &gt;&lt;/li&gt;          &lt;li&gt;&lt;a href= # &gt;Separated link&lt;/a&gt;&lt;/li&gt;         &lt;/ul&gt;        &lt;/li&gt;       &lt;/ul&gt;      &lt;/div&gt;&lt;!-- /. navbar-collapse --&gt;     &lt;/div&gt;&lt;!-- /. container-fluid --&gt;    &lt;/nav&gt;  {% endblock %}  {# 定制页面主体部分 #}  {% block content %}    &lt;div class= container &gt;      {# 为页面弹框预留空间 #}      {% block page_content %}        默认内容      {% endblock %}    &lt;/div&gt;  {% endblock %}12345678910111213141516from flask import Flask,gfrom flask_script import Managerfrom flask import render_templatefrom flask import render_template_stringfrom flask_bootstrap import Bootstrapapp = Flask(__name__)app. config['TEMPLATES_AUTO_RELOAD'] = Truebs = Bootstrap(app)manager = Manager(app)@app. route('/base/')def base():  return render_template('base. html') if __name__ == '__main__':  manager. run()继承基础模板, 去定义一个404错误页面error. html: (1) 创建一个继承自基础模板的错误页面子模板 12345{% extends 'base. html' %}{% block title %} 出错了 404 {% endblock %}{% block page_content %} 404 臣妾做不到. . . {% endblock %}(2) 为404错误创建统一路由 123@app. errorhandler(404)def page_not_found(e):  return render_template('error. html')继承基础模板, 跳转新页面 (注册页面的例子): (1)在基础模板中为链接增加 url_for 函数 1&lt;li&gt;&lt;a href= {{ url_for('register') }} &gt;注册&lt;/a&gt;&lt;/li&gt;(2)在视图函数中创建route 注册用的路由 123@app. route('/register/')def register():  return render_template('register. html')(3)创建一个继承自基础模板的子模板 register. html 12345{% extends 'base. html' %}{% block title %} 注册页 {% endblock %}{% block page_content %} 注册了. . . {% endblock %}再看url_for() 函数: 普通构造, 参数构造, 外链构造: 普通URL构造 1&lt;li&gt;&lt;a href= {{ url_for('register') }} &gt;注册&lt;/a&gt;&lt;/li&gt;带参数的URL构造 1&lt;li&gt;&lt;a href= {{ url_for('welcome', name='xiaoming', id=3, page=5) }} &gt;欢迎页&lt;/a&gt;&lt;/li&gt; 输出结果: /welcome/xiaoming?id=3&amp;page=5, 没有主机和端口号, 只能在网站内部使用外链URL构造 1&lt;li&gt;&lt;a href= {{ url_for('welcome', name='xiaolong', id=3, page=5, _external=True) }} &gt;欢迎页&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href= {{ url_for('welcome', name='xiaolong', id=3, page=5, _external=True) }} &gt;欢迎页&lt;/a&gt;&lt;/li&gt; 如果需要构造完整的外部链接地址, 需要添加参数 _external=True 构造出的HTML为&lt;a href= http://127. 0. 0. 1:5000/welcome/xiaolong?id=3&amp;amp;page=5 &gt;欢迎页&lt;/a&gt; 没有用_external=True参数的url_for 构造出来的HTML为 &lt;a href= /register/ &gt;注册&lt;/a&gt;静态文件 图片, css, js: 静态文件就是那些不会改变的文件。在一般的应用程序中，静态文件包括 CSS 文件，JavaScript 文件以及图片。它们也可能是音频文件以及其它类似的东西。 加载静态图片 url_for('static', filename='img/timg. jpg') {% block head %}: 目录结构: 1234project/			#项目目录	static/			#静态资源	templates/		#模板文件	mnage. py		#视图函数/启动控制文件加载一个静态图片 12345678{% extends 'base. html' %}{% block title %} 加载静态资源 {% endblock %}{% block page_content %}  &lt;img src= {{ url_for('static', filename='img/timg. jpg') }}  alt=  &gt;{% endblock %}{##}找个favicon. ico 文件放到静态目录下 1234{% block head %}  {{ super() }}  &lt;link rel= icon  type= image/x-icon  href= {{ url_for('static', filename='img/favicon. ico') }} &gt;{% endblock %}注意:  {% block head %} 标签是在bootstrap 中自带的,所以不需要在父类中提前定义.  {{ supper() }} 是必须的, 因为子类需要继承并重现bootstrap 中的head 内容 favicon. ico 标签要写成 &lt;link rel= icon  type= image/x-icon  href=  &gt;     或者: &lt;link rel= shortcut icon  href=  &gt;   加载静态CSS {% block style %}: 视图函数 123@app. route('/mycss/')def mycss():  return render_template('mycss. html')mycss. html 1234567891011121314151617{% extends 'base. html' %}{% block title %}定制CSS{% endblock %}{% block page_content %}  {% block style %}    {{ super() }}    &lt;style type= text/css &gt;      . myclass{        height:100px;        width:100px;        background-color: green;      }    &lt;/style&gt;    &lt;link rel= stylesheet  type= text/css  href= {{ url_for('static', filename='css/base. css') }}  /&gt;  {% endblock %}  &lt;div class= myclass &gt;123&lt;/div&gt;  &lt;div class= myclass2 &gt;123&lt;/div&gt;{% endblock %}注意:  {{ supper() }} 是必须的, 因为子类需要继承并重现bootstrap 中的CSS样式 内容 {% block style %} 内部可以写内嵌标签,也可以引入独立的css文件.  注意 !!! 独立的css文件的名字不要和路由的名字重名,否则可能造成找不到静态文件的可能. base. css 12345. myclass2{  height:100px;  width:50px;  background-color: red;}加载静态JS {% block scripts %}: 路由函数 123@app. route('/myjs/')def myjs():  return render_template('myjs. html')myjs. html 12345678910{% extends 'base. html' %}{% block title %}测试js{% endblock %}  {% block scripts %}    {{ super() }}    &lt;script&gt;      console. log('这是自定义的内嵌js')    &lt;/script&gt;    &lt;script src= {{ url_for('static', filename='js/myjs. js') }} &gt;&lt;/script&gt;  {% endblock %}myjs. js 1console. log('这是独立js')注意:  {{ supper() }} 是必须的, 因为子类需要继承并重现bootstrap 中的js 内容 {% block scripts %} 内部可以写内嵌js,也可以引入独立的js文件. 待完善: 使用 Flask-Assets 管理静态资源: 详细使用说明 介绍  Flask-Assets 是一个用来管理你的静态文件的扩展。Flask-Assets 提供了两个非常有用的工具。首先，它可以让你们在 Python 代码中定义资源的 束/包（bundles），这些束/包（bundles）能够被一起插入到你的模板。其次，它可以让你们 预处理（pre-process） 这些文件。这就意味着你们能够合并和压缩你们的 CSS 和 JavaScript 文件，使得用户仅仅只需要加载两个压缩的文件（CSS 和 JavaScript），而不需要迫使你开发一个复杂的资源管道。你甚至可以编译来自 Sass, LESS, CoffeeScript 以及一堆其它来源的文件。 安装Flask-Assets 1pip3. 7 install Flask-Assets使用说明    首先，创建一个Environments对象，并使用它初始化Flask应用，  然后创建一个Buldle 对象  最后将Bundle对象注册到Environments对象上。 123456789from flask import Flaskfrom flask_assets import Environment, Bundleapp = Flask(__name__)assets = Environment(app) # 首先，创建一个Environments实例, 并使用它初始化Flask应用js = Bundle('jquery. js', 'base. js', 'widgets. js',      filters='jsmin', output='gen/packed. js') 	# 创建 Bundle 对象assets. register('js_all', js) 							# 将对象绑定到Assets 上 Bundle对象可以传递任意数量的源文件作为参数，使用output参数指定输出文件路径，使用filters参数指定过滤器。所有文件的路径都是相对路径，以应用的静态文件路径为基准。Flask 表单: 提交的表单数据都放在 request. form 中 FLASK 表单的使用 (原生创建表单和验证): 新建一个模板文件 login. html 1234&lt;form method= post  action= {{ url_for('check') }} &gt; 用户名: &lt;input type= text  name= username &gt; &lt;input type= submit  value= 登录 &gt;&lt;/form&gt;渲染模板文件,处理表单提交 123456789@app. route('/login/')def login(): return render_template('login. html')@app. route('/check/', methods=['POST'])def check(): return request. form['username'] 接收表单数据的路由必须添加一个数组参数.  参数: methods=['POST'] request. form数组包含了form 提交的所有数据表单渲染与提交处理可以放在一个路由中  表单的提交地址action='' 需要设置为空, 默认提交到当前路由地址 路由参数methods 需要同时有 GET 和 POST123456@app. route('/login/', methods=['POST','GET'])def login():  if request. method == 'POST':    return request. form['username']  return render_template('login. html')flask-wtf 安全表单的创建和验证: WTF 是 Write &amp; Test Form 的缩写 官方文档 中文官方文档 说明::    Flask-WTF 表单保护你免受 CSRF 威胁，你不需要有任何担心。尽管如此，如果你有不包含表单的视图，那么它们仍需要额外的保护。   是一个处理表单的扩展库, 通过了 SCRF 安全认证 可以进行字段校验, 跨站攻击保护. 安装:: 1pip3. 7 install flask-wtf使用:: (1) 首先导入flask-wtf 相关的模块  Form模块: FlaskForm , 用于被后续定义表单类继承 表单字段模块: StringField, SubmitField . . . : , 用于生成表单字段 表单验证模块: DataRequired. . ,用于对提交的表单内容进行验证1234567891011121314from flask import Flaskfrom flask_bootstrap import Bootstrapfrom flask_wtf import FlaskForm # form 模块from wtforms import StringField, SubmitField # 字段模块from wtforms. validators import DataRequired # 验证器模块app = Flask(__name__)bs = Bootstrap(app)#CSRF 需要使用秘钥app. config['SECRET_KEY'] = '123456' # 必须要为app设置SECRET_key 注意:  必须要为app设置SECRET_KEY(2) 在视图函数中定义一个form类  该类必须继承自 FlaskForm 模块 在form 类中生成 表单字段的对象 , 并为对象指定 label 名字和验证器     第一个参数: label 名称   第二个参数: 为validators 传递验证器数组   1234# 表单类: 要求继承自 FlaskFormclass NameForm(FlaskForm):  name = StringField('用户名', validators=[DataRequired()])  submit = SubmitField('提交')(3) 在路由中实例化form 类, 并将对象传递给模板 1234567891011# 原生渲染@app. route('/')def index():  form = NameForm()  return render_template('index. html', form=form)# bootstrap 渲染@app. route('/boot/')def boot():  form = NameForm()  return render_template('form. html', form=form)(4. 1) 原生渲染form 表单  通过传递进来的form 对象渲染表单字段 需要把渲染内容放到一个自己写的&lt;form&gt; 标签中1234567891011&lt;form action=  &gt;  {# CSRF 的隐藏字段 #}  {{ form. hidden_tag() }}  {# 渲染name 字段 #}  {{ form. name. label() }}{{ form. name(id='xxx', class='yyy') }}  {# 提交按钮 #}  {{ form. submit() }}&lt;/form&gt;   form. hidden_tag(): 生成安全token , 生成的隐藏表单内容如下:   1&lt;input id= csrf_token  name= csrf_token  type= hidden  value= IjRmZDUxNDkwYTBjZThjMmI4NGFmNzg5ZTIxYTY3Yjk1NGUxOGZjNGQi. DS8uzQ. 6at3jdtOQTbYEKny63J1H_HbTM4 &gt;      form. name. label(): 是name字段的标签名     form. name(id=’xxx’, class=’yyy’): 生成name字段, 并指定css样式的id 和 class  (4. 2) 通过 bootstrap 渲染 form 表单  目的: Bootstrap 自带了wtf 的渲染宏, 可以直接import 使用 使用: 将form 对象作为参数, 让宏进行调用即可12345678910111213141516171819{% extends 'bootstrap/base. html' %}{# 使用bootstrap自带的表单渲染宏 #}{% import 'bootstrap/wtf. html' as wtf %}{% block title %}  bootstrap 表单渲染{% endblock %}{% block content %}  &lt;div class= container &gt;    {% if name %}      &lt;h2&gt;{{ name }}&lt;/h2&gt;    {% endif %}    {# 渲染表单 #}    {{ wtf. quick_form(form) }}  &lt;/div&gt;{% endblock %} import 'bootstrap/wtf. html' as wtf: 用 as 关键字可以为宏起别名 wtf. quick_form(form): quick_form 是宏的一个方法, 将form 对象直接传递给该方法.  注意: bootstrap form宏 需要放在 &lt;div class= container &gt; 中进行渲染(5. 1) 表单提交后的处理 - 无跳转方案 12345678910@app. route('/', methods = ['GET', 'POST'])def index():  name = None # 为了安全, 现将存储提交数据的变量初始化为None  form = NameForm()    if form. validate_on_submit(): # 验证是否是有效提交    name = form. name. data   # 获取提交信息    form. name. data = ''		 # 为了安全, 得到信息后,就立刻清空信息源  return render_template('form. html', form=form, name=name)(5. 1) 表单提交后的处理 - redirect 跳转方案  原因: 浏览器默认会记录最后的请求状态, 如果POST后刷新页面后会提示再次提交表单.  方案: POST 重定向 GET123456789101112@app. route('/', methods = ['GET', 'POST'])def index():  form = NameForm()   if form. validate_on_submit(): # 验证是否是有效提交    session['name'] = form. name. data # 为了保存提交过来的名字, 需要将name 保存到session中    return redirect(url_for('index')) # 然后redirect 到当前页, 为的是不让刷新后的弹出框出现  name = session. get('name') # 重新从session中拿到name 变量  return render_template('form. html', form=form, name=name) #渲染模板常见表单字段类型:  使用前需要提前导入:from wtforms import StringField, SubmitField      字段   说明         StringField   普通文本字段       SubmitField   提交按钮       PasswordField   密码字段       HiddenField   隐藏字段       TextAreaField   文本域字段       DateField   文本字段,datetime. date格式       DataTimeField   文本字段,datetime. datetime格式       IntegerField   文本字段, 整数       FloatField   文本字段,浮点数格式       BooleanField   复选框, 值为True 或 False       RadioField   单选框       SelectField   下拉框       FileField   文件上传   12345__all__ = (  'BooleanField', 'DecimalField', 'DateField', 'DateTimeField', 'FieldList',  'FloatField', 'FormField', 'IntegerField', 'RadioField', 'SelectField',  'SelectMultipleField', 'StringField',)常见的字段验证器:  使用前需要提前导入: from wtforms. validators import DataRequired      验证器   说明         DataRequired   字段不为空       Email   验证邮箱地址       EqualTo   验证字段一致性       IPAddress   IP 地址验证       Length   字符串长度验证       NumberRange   数字范围验证       URL   URL 验证       Regexp   正则验证方法   1234567__all__ = (  'DataRequired', 'data_required', 'Email', 'email', 'EqualTo', 'equal_to',  'IPAddress', 'ip_address', 'InputRequired', 'input_required', 'Length',  'length', 'NumberRange', 'number_range', 'Optional', 'optional',  'Required', 'required', 'Regexp', 'regexp', 'URL', 'url', 'AnyOf',  'any_of', 'NoneOf', 'none_of', 'MacAddress', 'mac_address', 'UUID')字段的自定义校验: (1) 先导入 自定义验证抛异常模块 1from wtforms. validators import DataRequired, Length, ValidationError(2) 在表单类中定义一个新的验证函数  函数名字必须为: validate_字段名 函数的参数 field: 表示被验证的字段.  函数的报错信息需要用 raise ValidationError('用户名长度不能小于6个字符') 定义12345678910# 表单类: 要求继承自 FlaskFormclass NameForm(FlaskForm):  name = StringField('用户名', validators=[DataRequired(DataRequired(message= 不能为空 ))])  # Length(5,20,message='必须在5~20个字符之间')  submit = SubmitField('提交')  #自定义校验, 名字必须为: 'validate_字段名'  def validate_name(self,field):    if len(field. data) &lt; 6:      raise ValidationError('用户名长度不能小于6个字符')(3) 使用: 把form 对象直接传递到模板中就可以直接生效了. flash 弹出窗口 消息显示 flash() get_flashed_messages(): 目的:  当用户在页面中进行交互时, 系统可以有针对性的给用户一些提示, 警告, 引导等信息.  这些信息可以通过弹窗的形式告诉用户如何操作.  弹窗可以让用户自己关闭,或者延时自动小时. 使用方法: (1) 引入flash发送消息和获取消息模块  注意: 因为消息机制涉及到安全, 所以必须要设置SECRET_KEY123from flask import flash, get_flashed_messages # 引入模块app. config['SECRET_KEY'] = '123456' # 必须设置 SECRET_KEY(2) 在路由中设置触发消息的机制 12345@app. route('/')def index():  flash('财神到')  flash('财神天天到')  return render_template('testFlash. html')(3. 1) 普通模板中获取flash 消息  因为内容可能不止一条, 所以需要循环遍历.  如果有多个页面都需要弹出flash 消息, 可以再项目的基础模板中写一次就可以了. 123{% for message in get_flashed_messages() %}  &lt;div&gt;{{ message }}&lt;/div&gt;{% endfor %}(3. 2) bootstrap 中获取flash 消息  bootstrap 组件来自 这里1234567891011121314151617181920{% extends 'bootstrap/base. html' %}{# 使用bootstrap自带的表单渲染宏 #}{% block title %}  bootstrap flash 弹出框{% endblock %}{% block content %}  &lt;div class= container &gt;    {% for message in get_flashed_messages() %}     &lt;div class= alert alert-warning alert-dismissible  role= alert &gt;       &lt;button type= button  class= close  data-dismiss= alert  aria-label= Close &gt;&lt;span           aria-hidden= true &gt;&amp;times;&lt;/span&gt;&lt;/button&gt;       &lt;strong&gt;Warning!&lt;/strong&gt; &lt;div&gt;{{ message }}&lt;/div&gt;     &lt;/div&gt;   	{% endfor %}  &lt;/div&gt;{% endblock %}一段完整的表单生成 + 表单验证 + 弹窗 的例子: 路由文件mywtf. py 123456789101112131415161718192021222324252627282930313233343536373839404142from flask import Flask, render_template, request, session, g, flash, get_flashed_messagesfrom flask_bootstrap import Bootstrapfrom flask_script import Managerfrom flask_wtf import FlaskFormfrom wtforms import StringField, PasswordField, SubmitField, BooleanFieldfrom wtforms. validators import DataRequired, EqualTo, Email, Lengthapp = Flask(__name__)app. config['TEMPLATES_AUTO_RELOAD'] = Trueapp. config['SECRET_KEY'] = '123456'bs = Bootstrap(app)mgr = Manager(app)class myForm(FlaskForm):  useremail = StringField('邮件地址: ', validators=[DataRequired('用户名不能为空'), Email('邮箱格式不正确')])  password1 = PasswordField('密码: ', validators=[DataRequired('密码不能为空'), Length(5,15, 密码必须介于5~15位字符 )])  password2 = PasswordField('密码: ', validators=[DataRequired('密码不能为空'), Length(5,15, 密码必须介于5~15位字符 ), EqualTo('password1', message= 两次密码输入不一致 )])  confirm = BooleanField('记住我')  button = SubmitField('提交')@app. route('/', methods = ['POST', 'GET'])def index():  if request. method == 'POST':    app. logger. info('表单提交了')  form = myForm()  if form. validate_on_submit():    app. logger. info('表单验证成功')    flash( 用户名 {} 已经存在, 请尝试新的用户名 . format(form. useremail. data))    form. useremail. data=''    form. password1. data=''    form. password2. data=''    form. confirm. data=''  return render_template('mywtf. html', form=form)if __name__ == '__main__':  mgr. run()模板文件mywtf. html 12345678910111213141516171819202122232425262728293031323334353637{% extends 'bootstrap/base. html' %}{% block title %}  测试wtf{% endblock %}{% block content %}  &lt;nav class= navbar navbar-default &gt;    &lt;div class= container-fluid &gt;      &lt;!-- Brand and toggle get grouped for better mobile display --&gt;      &lt;div class= navbar-header &gt;        &lt;a class= navbar-brand  href= # &gt;用户注册&lt;/a&gt;      &lt;/div&gt;    &lt;/div&gt;&lt;!-- /. container-fluid --&gt;  &lt;/nav&gt;  &lt;div class= container &gt;    {% block page %}      {% for message in get_flashed_messages() %}        &lt;div class= alert alert-warning alert-dismissible  role= alert &gt;          &lt;button type= button  class= close  data-dismiss= alert  aria-label= Close &gt;&lt;span              aria-hidden= true &gt;&amp;times;&lt;/span&gt;&lt;/button&gt;          &lt;strong&gt;Warning!&lt;/strong&gt;          &lt;div&gt;{{ message }}&lt;/div&gt;        &lt;/div&gt;      {% endfor %}    {% endblock %}    &lt;hr/&gt;    {# Bootstrap 渲染方案 #}    {% import 'bootstrap/wtf. html' as wtf %}    {{ wtf. quick_form(form) }}  &lt;/div&gt;{% endblock %}flask-moment 负责显示时间戳: 官网 说明:  专门根据本地时间显示时间戳的扩展库, 使用非常方便(1) 安装包 1pip3. 7 install flask-moment(2) 导入模块, 创建对象 设置路由 12345678910111213141516171819202122232425from flask import Flaskfrom flask import render_templatefrom flask_script import Managerfrom flask_moment import Momentfrom datetime import datetime, timedeltafrom flask import gapp = Flask(__name__)mgr = Manager(app)#moment = Moment(app)@app. route('/')def index():  return '本地化显示'@app. route('/moment/')def mmt():  g. current_time = datetime. utcnow()  g. old_time = datetime. utcnow() + timedelta(seconds=-3600) # 一小时之前  return render_template('mement. html')if __name__ == '__main__':  mgr. run()(3) 设置模板显示时间 12345678910111213141516171819202122&lt;h1&gt;本地化时间显示&lt;/h1&gt;{# 简化的显示方式 #}&lt;div&gt;时间: {{ moment(g. current_time). format('LLLL') }}&lt;/div&gt;&lt;div&gt;时间: {{ moment(g. current_time). format('LLL') }}&lt;/div&gt;&lt;div&gt;时间: {{ moment(g. current_time). format('LL') }}&lt;/div&gt;&lt;div&gt;时间: {{ moment(g. current_time). format('L') }}&lt;/div&gt;{# 自定义显示方式 #}&lt;div&gt;自定义: {{ moment(g. current_time). format('YYYY. MM. DD') }}&lt;/div&gt;{# 显示时间差. 既某个时间距离今天多少天了. #}&lt;div&gt;发表于: {{ moment(g. old_time). fromNow() }}&lt;/div&gt;{# 包含Jquery. js 因为 moment 依赖jquery; 如果用bootstrap 渲染,就不需要引入jquery了 #}{{ moment. include_jquery() }}{# 包含moment. js #}{{ moment. include_moment() }}{# 设置中文显示 #}{{ moment. locale('zh-CN') }}扩展知识 - 为文件上传做准备: 利用环境变量设置自用的配置信息: 目的: 避免将个人隐私信息公布出来, 又可以完成原有的功能 linux命令行设置 环境变量  设置环境变量时,不要用空格12设置: export ABC=hello获取: echo $ABC代码中设置和获取环境变量 1234567891011from flask import Flaskimport osapp=Flask(__name__)app. config['SECRET_KEY'] = os. environ. get('SECRET_KEY') or '123456'@app. route('/env/')def env():  os. environ['wangzhe'] = 'jiangziya'  app. logger. info(str(os. environ))  return os. environ. get('wangzhe') or  没有ABC 变量 生成随机字符串: 123456789def randStr(lenth=32):  import random  base_str = 'abcdefghijklnmopgrstuvwxyz1234567890'  return ''. join(random. choice(base_str) for i in range(lenth))@app. route('/random/')def rand():  return randStr()生成图片的缩略图 pillow: (1) 安装包 1pip3. 7 install pillow(2) 导入 1234567891011121314from PIL import Image# 缩略图@app. route('/zoom/')def zoom():    #打开文件  img = Image. open('1. jpg')  #设置尺寸  img. thumbnail((128, 128))  #保存图片  img. save('2. jpg')  return '缩略图已经生成'Flask 文件上传: 原生文件上传: 目的:  通过设置上传文件的类型, 文件的大小以及上传文件的目录来进行文件上传. 原理:  判断上传的文件是否符合大小,类型的要求. 如果符合, 那么就为上传文件生成一个随机的文件名,然后保存在指定的目录下. 然后对该文件生成缩略图,生成的缩略图覆盖原文件. 之后将该图片展示在网页中. 步骤: (1) 写一个路由函数, 再写个HTML模板, 允许上传文件 123@app. route('/', methods=['POST', 'GET'])def index():	return render_template('upload. html', img_url=img_url)123456789{% if img_url %}  &lt;img src= {{ img_url }}  alt=  &gt;{% endif %}&lt;form action=   method= post  enctype= multipart/form-data &gt;  文件上传  &lt;input type= file  name= photo &gt;  &lt;input type= submit  value= 上传 &gt;&lt;/form&gt;(2) 设置文件的大小, 目录,文件类型 的配置信息 12345678# 文件大小的限制, 最大不能超过8M. 只写这行代码就可以限制了, 不需要做其他代码修改app. config['MAX_CONTENT_LENGTH'] = 1024 * 1024 * 8# 上传图片的保存路径. 在项目的根目录上需要创建一个 upload 目录app. config['UPLOAD_FOLDER'] = os. path. join(os. getcwd(), 'upload')# 允许的文件后缀ALLOW_EXTENSIONS = set(['png', 'jpg', 'jpeg', 'gif'])(3) 写两个函数, 用于生成随机文件名 和 判断上传文件名是否符合规范 123456789# 判断文件中是否有点, 并且后缀是否在 ALLOW_EXTENSIONS 中def allow_file(filename):  return '. ' in filename and filename. rsplit('. ', 1)[1] in ALLOW_EXTENSIONS# 随机生成文件名def randStr(length=32):  import random  base_str = 'abcdefghijklnmopgrstuvwxyz1234567890'  return ''. join(random. choice(base_str) for i in range(length))(4) 在路由函数中进行判断,  如果满足要求     就对文件生成随机文件名   保存   生成缩略图, 覆盖原文件   展示图片   1234567891011121314151617181920212223if request. method == 'POST':    app. logger. info('图片提交了')    file = request. files. get('photo')    if file and allow_file(file. filename): # 判断文件上传了, 且文件名满足上传条件      # 获取文件后缀, 用于拼接新的随机文件名      postfix = os. path. splitext(file. filename)[1]      # 随机文件名      filename = randStr()+postfix      # 拼接完整路径名      pathname = os. path. join(app. config['UPLOAD_FOLDER'], filename)      # 保存文件      file. save(pathname)      app. logger. info('图片上传成功了')      # 生成缩略图      img = Image. open(pathname)      img. thumbnail((128,128))      img. save(pathname)      # 构造图片URL      img_url = url_for('uploaded', filename=filename)      return render_template('upload. html', img_url=img_url)    else:      app. logger. info('图片上传失败了')      return '图片上传失败'完整视图函数文件 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768from flask import Flask, render_template, request, send_from_directory, url_forfrom flask_script import Managerimport osfrom PIL import Imageapp = Flask(__name__)app. config['TEMPLATES_AUTO_RELOAD'] = Truemgr = Manager(app)# 文件大小的限制, 最大不能超过8M. 只写这行代码就可以限制了, 不需要做其他代码修改app. config['MAX_CONTENT_LENGTH'] = 1024 * 1024 * 8# 上传图片的保存路径. 在项目的根目录上需要创建一个 upload 目录app. config['UPLOAD_FOLDER'] = os. path. join(os. getcwd(), 'upload')# 允许的文件后缀ALLOW_EXTENSIONS = set(['png', 'jpg', 'jpeg', 'gif'])# 判断文件中是否有点, 并且后缀是否在 ALLOW_EXTENSIONS 中def allow_file(filename):  return '. ' in filename and filename. rsplit('. ', 1)[1] in ALLOW_EXTENSIONS# 随机生成文件名def randStr(length=32):  import random  base_str = 'abcdefghijklnmopgrstuvwxyz1234567890'  return ''. join(random. choice(base_str) for i in range(length))@app. route('/', methods=['POST', 'GET'])def index():  if request. method == 'POST':    app. logger. info('图片提交了')    file = request. files. get('photo')    if file and allow_file(file. filename): # 判断文件上传了, 且文件名满足上传条件      # 获取文件后缀, 用于拼接新的随机文件名      postfix = os. path. splitext(file. filename)[1]      # 随机文件名      filename = randStr()+postfix      # 拼接完整路径名      pathname = os. path. join(app. config['UPLOAD_FOLDER'], filename)      # 保存文件      file. save(pathname)      app. logger. info('图片上传成功了')      # 生成缩略图      img = Image. open(pathname)      img. thumbnail((128,128))      img. save(pathname)      # 构造图片URL      img_url = url_for('uploaded', filename=filename)      return render_template('upload. html', img_url=img_url)    else:      app. logger. info('图片上传失败了')      return '图片上传失败'  return render_template('upload. html')# 访问上传图片@app. route('/uploaded/&lt;filename&gt;')def uploaded(filename):  # flask 自带的 安全的发送静态文件 http://127. 0. 0. 1:5000/uploaded/timg-4. jpeg  return send_from_directory(app. config['UPLOAD_FOLDER'], filename)if __name__ == '__main__':  mgr. run()注意:  设定上传文件大小需要设定 app. config['MAX_CONTENT_LENGTH'] = 1024 * 1024 * 8 接收上传的路由要指定 methods=['POST', 'GET'] 上传的文件需要从 file = request. files. get('photo') 获取 文件名 是 file. filename 文件保存需要用 file. save() 保存路径需要自己拼接 file. save(os. path. join(app. config['UPLOAD_FOLDER'], file. filename)) 用安全的方式在页面展示图片 用 send_from_directory模板文件 123456789101112131415161718192021222324&lt;!DOCTYPE html&gt;&lt;html lang= en &gt;&lt;head&gt;  &lt;meta charset= UTF-8 &gt;  &lt;title&gt;文件上传&lt;/title&gt;  &lt;style type= text/css &gt;  &lt;/style&gt;&lt;/head&gt;&lt;body&gt;{% if img_url %}  &lt;img src= {{ img_url }}  alt=  &gt;{% endif %}&lt;form action=   method= post  enctype= multipart/form-data &gt;  文件上传  &lt;input type= file  name= photo &gt;  &lt;input type= submit  value= 上传 &gt;&lt;/form&gt;&lt;/body&gt;&lt;/html&gt;注意:    form 提交方法必须是 post   上传文件时, &lt;form&gt; 标签要指定 enctype= multipart/form-data  input 类型必须是 file, 且需要指定 name 属性flask-uploads 扩展库可以帮助我们上传文件: 官方文档 安装 1pip3. 7 install flask-uploads完整视图文件 12345678910111213141516171819202122232425262728293031323334353637383940414243from flask import Flask, render_template, requestfrom flask_script import Managerfrom flask_uploads import UploadSet, IMAGES, configure_uploads, patch_request_classimport osapp = Flask(__name__)app. config['TEMPLATES_AUTO_LOAD'] = Truemgr = Manager(app)# flask 自己的上传文件位置设置. 中间名字要和 UploadSet('ABC', IMAGES) 第一个参数一致app. config['UPLOADED_ABC_DEST'] = os. path. join(os. getcwd(), 'upload', 'ABC')# flask 自己的文件大小限制. 该设置是flask_upload 中设置文件大小的上限app. config['MAX_CONTENT_LENGTH'] = 8 * 1024 * 1024# 创建 flask_upload 上传限制文件上传大小, 默认64M, 如果size=None 则采取 MAX_CONTENT_LENGTH, 也可以自己设定大小. patch_request_class(app, size=None)# 创建 flask_upload 上传对象, 第一个参数匹配到 app. config['UPLOADED_ABC_DEST'] 中间名(大小写可以无所谓). # IMAGES = tuple('jpg jpe jpeg png gif svg bmp'. split())# 除了 IMAGES 之外, 还可以是: TEXT, DOCUMENTS, AUDIO, DATA, SCRIPTS, ARCHIVES, EXECUTABLES# 不设置第二个参数, 默认是 DEFAULTS = TEXT + DOCUMENTS + IMAGES + DATApicUp = UploadSet('ABC', IMAGES)# 上传对象对app做初始化configure_uploads(app, picUp)@app. route('/', methods=['POST', 'GET'])def index():  img_url = None  if request. method == 'POST' and 'photo' in request. files: #photo是form中的文件域    # 保存文件 , 调用    filename = picUp. save(request. files['photo']) # 保存文件同时返回文件名给变量. 该函数可以指定保存路径和保存的文件名    # 获取上传的文件名    img_url = picUp. url(filename)  return render_template('testFlaskUpload. html', img_url=img_url)if __name__ == '__main__':  mgr. run()注意:  模板文件同上面的原生模板一样,没有区别 上传文件的访问路由无需添加, 扩展库自动添加了一个路由   需要先配置上传文件的路径和文件大小   picUp. url(filename) 输出: http://127. 0. 0. 1:5000/_uploads/ABC/timg-4_1. jpeg picUp. path(filename) 输出: /Users/dalong/code/python1702/flaskprojects/upload/ABC/timg-4_1. jpeg picUp. get_basename(filename) 输出: timg-4_1. jpeg模板文件 12345678910111213141516171819202122&lt;!DOCTYPE html&gt;&lt;html lang= en &gt;&lt;head&gt;  &lt;meta charset= UTF-8 &gt;  &lt;title&gt;文件上传&lt;/title&gt;  &lt;style type= text/css &gt;  &lt;/style&gt;&lt;/head&gt;&lt;body&gt;{% if img_url %}  &lt;img src= {{ img_url }}  alt=  &gt;{% endif %}&lt;form action=   method= post  enctype= multipart/form-data &gt;  文件上传  &lt;input type= file  name= photo &gt;  &lt;input type= submit  value= 上传 &gt;&lt;/form&gt;&lt;/body&gt;&lt;/html&gt;完整上传的代码, 包含flask_upload, flask_wtf, flask_bootstrap: 视图文件 uploadFinal. py 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263from flask import Flask, render_templatefrom flask_script import Managerfrom flask_bootstrap import Bootstrapfrom flask_wtf import FlaskFormfrom wtforms import SubmitFieldfrom flask_wtf. file import FileField, FileRequired, FileAllowedfrom flask_uploads import UploadSet, IMAGESfrom flask_uploads import configure_uploads, patch_request_classimport osfrom PIL import Imageapp = Flask(__name__)manager = Manager(app)bootstrap = Bootstrap(app)app. config['SECRET_KEY'] = os. environ. get('SECRET_KEY') or '123456'app. config['UPLOADED_PHOTOS_DEST'] = os. getcwd()app. config['MAX_CONTENT_LENGTH'] = 8 * 1024 * 1024photos = UploadSet('photos', IMAGES)configure_uploads(app, photos)patch_request_class(app, size=None)class UploadForm(FlaskForm):  photo = FileField('头像', validators=[FileRequired(message='文件未选择'), FileAllowed(photos, message='只能上传图片')])  submit = SubmitField('上传')def random_string(length=32):  import random  base_str = 'abcdefghijklmnopqrstuvwxyz1234567890'  return ''. join(random. choice(base_str) for i in range(length))@app. route('/', methods=['GET', 'POST'])def upload():  img_url = None  form = UploadForm()  if form. validate_on_submit():    # 获取文件后缀    suffix = os. path. splitext(form. photo. data. filename)[1]    # 生成随机文件名    filename = random_string() + suffix    # 保存文件    photos. save(form. photo. data, name=filename)    # 拼接完整路径名    pathname = os. path. join(app. config['UPLOADED_PHOTOS_DEST'], filename)    # 打开文件    img = Image. open(pathname)    # 设置尺寸    img. thumbnail((128, 128))    # 保存文件    img. save(pathname)    # 获取URL    img_url = photos. url(filename)  return render_template('uploadFinal. html', form=form, img_url=img_url)if __name__ == '__main__':  manager. run()模板文件 uploadFinal. html 123456789101112131415{% extends 'bootstrap/base. html' %}{% from 'bootstrap/wtf. html' import quick_form %}{% block title %}完整的文件上传{% endblock %}{% block content %}  &lt;div class= container &gt;    &lt;h1&gt;文件上传&lt;/h1&gt;    {% if img_url %}      &lt;img src= {{ img_url }} &gt;    {% endif %}    {{ quick_form(form) }}  &lt;/div&gt;{% endblock %}flask-mail 邮件发送: 官方文档 说明: 是一个邮件发送的扩展库, 使用非常简单 安装: 1pip3. 7 install flask-mail基本的发送功能: (1) 导入类库: 一个是邮箱类, 一个是邮件类 1from flask_mail import Mail, Message(2) 设置基本的邮箱的配置信息, 创建邮箱对象: 邮箱地址, 用户名, 授权码 1234567# 邮箱的配置, 配置要放在mail 对象前面app. config['MAIL_SERVER'] = 'smtp. 163. com' # 邮箱服务器app. config['MAIL_USERNAME'] = 'dalong_coo@163. com' # 邮箱地址app. config['MAIL_PASSWORD'] = '*******' # 邮箱服务器密码或者授权码# 创建邮箱对象mail = Mail(app)(3) 在路由中设置邮件发送对象并发送邮件 123456789# 创建邮件对象, 包含标题和接收人msg = Message('账户激活', recipients=['37016175@qq. com'])# 发送者msg. sender = app. config['MAIL_USERNAME']# 邮件主体, 包含html 和 纯文本两种, 看收件人邮箱默认选择看哪一种. msg. html = '&lt;h1&gt;hello dalong, 激活点击右边连接&lt;/h1&gt;'msg. body = 'hello dalong'# 发送邮件mail. send(msg)完整代码 123456789101112131415161718192021222324252627282930313233343536from flask import Flaskfrom flask_script import Managerfrom flask_mail import Mail, Messageimport osapp = Flask(__name__)app. config['TEMPLATES_AUTO_RELOAD'] = Truemgr = Manager(app)# 邮箱的配置, 配置要放在mail 对象前面app. config['MAIL_SERVER'] = 'smtp. 163. com' # 邮箱服务器app. config['MAIL_USERNAME'] = 'dalong_coo@163. com' # 邮箱地址app. config['MAIL_PASSWORD'] = '*******' # 邮箱服务器密码或者授权码# 创建邮箱对象mail = Mail(app)@app. route('/')def index(): 	# 创建邮件对象 	msg = Message(    subject='账户激活',    recipients=['37016175@qq. com'],    body='hello dalong',    html='&lt;h1&gt;hello dalong, 激活点击右边连接&lt;/h1&gt;',    sender=app. config['MAIL_USERNAME']  )  # 发送邮件  mail. send(msg)  return '邮件已经发送'if __name__ == '__main__':  mgr. run()重构,在刚才的基础上, 把邮件内容封装函数使用: 目的: 需要更灵活的使用邮件发送功能,使用不同模板, 传递不通过参数 原理: 只是将邮件发送部分抽象出来,作为独立函数, 可以接受若干基本参数:  收件人地址 邮件标题 邮件主题用的模板 模板中用到的若干变量实现步骤 (1) 将原来的邮件主体做成两个邮件模板文件. 一个是html格式, 一个是纯文本格式  文件内的一些可变文字用变量替换password. html 1&lt;h1&gt;hello {{ name }}, 激活点击右边连接&lt;/h1&gt;password. txt hello {{ name }}, 激活点击右边连接(2) 将发送邮件对象的部分合并到一个函数中, 用函数参数代替实体内容. 12345678910111213# 封装函数def send_mail(to, subject, template, **kwargs):  # 创建邮件对象  msg = Message(    subject=subject,    recipients=[to],    body=render_template('mailTemplate/'+template+'. txt', **kwargs),    html=render_template('mailTemplate/'+template+'. html', **kwargs),    sender=app. config['MAIL_USERNAME']  )  # 发送邮件  mail. send(msg)(3) 路由中调用邮件发送函数, 并灵活传递参数 1send_mail('37016175@qq. com', '找回密码', 'password', name= 大龙 )完整代码 1234567891011121314151617181920212223242526272829303132333435363738394041from flask import Flask, render_templatefrom flask_script import Managerfrom flask_mail import Mail, Messageimport osapp = Flask(__name__)app. config['TEMPLATES_AUTO_LOAD'] = Truemgr = Manager(app)app. config['MAIL_SERVER'] = 'smtp. 163. com' # 邮箱服务器app. config['MAIL_USERNAME'] = 'dalong_coo@163. com' # 邮箱地址app. config['MAIL_PASSWORD'] = '*******' # 邮箱服务器密码或者授权码# 创建邮箱 对象mail = Mail(app)# 封装函数def send_mail(to, subject, template, **kwargs):  # 创建邮件对象  msg = Message(    subject=subject,    recipients=[to],    body=render_template('mailTemplate/'+template+'. txt', **kwargs),    html=render_template('mailTemplate/'+template+'. html', **kwargs),    sender=app. config['MAIL_USERNAME']  )  # 发送邮件  mail. send(msg)@app. route('/')def index():  send_mail('37016175@qq. com', '找回密码', 'password', name= 大龙 )  return '邮件已经发送'if __name__ == '__main__':  mgr. run()重构, 将发送邮件做到线程里面: 目的: 让浏览器触发的邮件发送不再同步等待发送结果, 也可以同时发送多封邮件 原理:通过建立线程, 主线程不再需要等待邮件发送结束. 实现步骤: (1) 导入线程 和 current_app 类库, 并创建一个可以被线程执行的函数.  该函数的作用     模拟一个app上下文   然后发送邮件.     两个参数     app: 第一个参数用于模拟app上下文   msg: 发送邮件的对象   123456789from flask import current_appfrom threading import Thread# 异步发送邮件def async_send_mail(app, msg):  # 必须在程序的上下文中才能发送邮件. 新建的线程没有, 因此需要手动创建  with app. app_context(): # 创建程序上下文    # 在程序上下文中发送邮件    mail. send(msg)(2) 实例化线程对象,传递可以上下文的app对象 和 msg 对象, 启动线程 12345# 获取原始的app实例, 因为是发邮件是线程, 而线程需要实例上下文才能发送邮件, 所以需要创建一个带有上下文的app实例, 传递给邮件线程函数. app = current_app. _get_current_object()# 创建线程对象, 并启动线程thr = Thread(target=async_send_mail, args=(app, msg))thr. start()完整代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253from flask import Flask, render_template, current_appfrom flask_script import Managerfrom flask_mail import Mail, Messageimport osfrom threading import Thread # 导入线程类库app = Flask(__name__)app. config['TEMPLATES_AUTO_RELOAD'] = Truemgr = Manager(app)app. config['MAIL_SERVER'] = 'smtp. 163. com' # 邮箱服务器app. config['MAIL_USERNAME'] = 'dalong_coo@163. com' # 邮箱地址app. config['MAIL_PASSWORD'] = '*******' # 邮箱服务器密码或者授权码# 创建email 对象mail = Mail(app)# 异步发送邮件def async_send_mail(app, msg):  # 必须在程序的上下文中才能发送邮件. 新建的线程没有, 因此需要手动创建  with app. app_context(): # 创建程序上下文    # 在程序上下文中发送邮件    mail. send(msg)# 封装函数def send_mail(to, subject, template, **kwargs):  # 获取原始的app实例, 因为是发邮件是线程, 而线程需要实例上下文才能发送邮件, 所以需要创建一个带有上下文的app实例, 传递给邮件线程函数.   app = current_app. _get_current_object()  # 创建邮件对象  msg = Message(    subject=subject,    recipients=[to],    body=render_template('mailTemplate/'+template+'. txt', **kwargs),    html=render_template('mailTemplate/'+template+'. html', **kwargs),    sender=app. config['MAIL_USERNAME']  )  # 创建线程对象, 并启动线程  Thread(target=async_send_mail, args=(app, msg)). start()# 发送邮件的配置, 配置要放在mail 对象前面@app. route('/')def index():  send_mail('37016175@qq. com', '修改邮箱', 'password', name= 小龙 )  return '邮件已经发送'if __name__ == '__main__':  mgr. run()password. html 1&lt;h1&gt;hello {{ name }}, 激活点击右边连接&lt;/h1&gt;password. txt hello {{ name }}, 激活点击右边连接Flask-SQLAlchemy 操作数据库: 中文文档 官方文档 数据库回顾:    分类：   关系型数据库：MySQL、sqlite、oracle、MariaDB…   非关系型数据库：Redis、MongoDB、…     选择：   数据库本身没有好坏，需要根据项目的需要进行选择。  sqlite介绍::  基本命令和 mysql一样 不需要单独起服务, 直接可以使用 不需要创建一个库, 就在一个库中操作各种表sqlite命令行:: 1234567891011121314151617181920212223242526272829sqlite3 --helpUsage: sqlite3 [OPTIONS] FILENAME [SQL]FILENAME is the name of an SQLite database. A new database is createdif the file does not previously exist. OPTIONS include:  -ascii        set output mode to 'ascii'  -bail        stop after hitting an error  -batch        force batch I/O  -column       set output mode to 'column'  -cmd COMMAND     run  COMMAND  before reading stdin  -csv         set output mode to 'csv'  -echo        print commands before execution  -init FILENAME    read/process named file  -[no]header     turn headers on or off  -help        show this message  -html        set output mode to HTML  -interactive     force interactive I/O  -line        set output mode to 'line'  -list        set output mode to 'list'  -lookaside SIZE N  use N entries of SZ bytes for lookaside memory  -mmap N       default mmap size set to N  -newline SEP     set output row separator. Default: '\n'  -nullvalue TEXT   set text string for NULL values. Default ''  -pagecache SIZE N  use N slots of SZ bytes each for page cache memory  -scratch SIZE N   use N slots of SZ bytes each for scratch memory  -separator SEP    set output column separator. Default: '|'  -stats        print memory stats before each finalize  -version       show SQLite version  -vfs NAME      use NAME as the default VFS   不输入任何文件名作为参数, 就会默认在当前目录创建一个数据库文件     就是一个文件, 直接用python函数调用就可以了  可以在命令行直接用 sqlite 查看创建的库文件和表内容 123456cd /Users/dalong/code/python1702/testDB/sqlite3 data. sqlite# 或者不指定参数, 而是直接 sqlite3 进入数据库终端, 然后 输入 . open data. sqlite 获取数据库sqlite3基本的sqlite 命令操作 12345678. open data. sqlitecreate table mytest(  . . . &gt; id int(3),  . . . &gt; name String(10),  . . . &gt; age Integer(5)  . . . &gt; );insert into mytest(id,name,age) values('1','dalong',15);select * from mytest; # 输出 1|dalong|15sqlite 数据库命令 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576sqlite&gt; . help. auth ON|OFF      Show authorizer callbacks. backup ?DB? FILE   Backup DB (default  main ) to FILE. bail on|off      Stop after hitting an error.  Default OFF. binary on|off     Turn binary output on or off.  Default OFF. cd DIRECTORY     Change the working directory to DIRECTORY. changes on|off    Show number of rows changed by SQL. check GLOB      Fail if output since . testcase does not match. clone NEWDB      Clone data into NEWDB from the existing database. databases       List names and files of attached databases. dbinfo ?DB?      Show status information about the database. dump ?TABLE? . . .    Dump the database in an SQL text format             If TABLE specified, only dump tables matching             LIKE pattern TABLE. . echo on|off      Turn command echo on or off. eqp on|off|full    Enable or disable automatic EXPLAIN QUERY PLAN. exit         Exit this program. explain ?on|off|auto? Turn EXPLAIN output mode on or off or to automatic. fullschema ?--indent? Show schema and the content of sqlite_stat tables. headers on|off    Turn display of headers on or off. help         Show this message. import FILE TABLE   Import data from FILE into TABLE. imposter INDEX TABLE Create imposter table TABLE on index INDEX. indexes ?TABLE?    Show names of all indexes             If TABLE specified, only show indexes for tables             matching LIKE pattern TABLE. . limit ?LIMIT? ?VAL?  Display or change the value of an SQLITE_LIMIT. lint OPTIONS     Report potential schema issues. Options:             fkey-indexes   Find missing foreign key indexes. log FILE|off     Turn logging on or off.  FILE can be stderr/stdout. mode MODE ?TABLE?   Set output mode where MODE is one of:             ascii  Columns/rows delimited by 0x1F and 0x1E             csv   Comma-separated values             column  Left-aligned columns.  (See . width)             html   HTML &lt;table&gt; code             insert  SQL insert statements for TABLE             line   One value per line             list   Values delimited by  |              quote  Escape answers as for SQL             tabs   Tab-separated values             tcl   TCL list elements. nullvalue STRING   Use STRING in place of NULL values. once FILENAME     Output for the next SQL command only to FILENAME. open ?OPTIONS? ?FILE? Close existing database and reopen FILE             The --new option starts with an empty file. output ?FILENAME?   Send output to FILENAME or stdout. print STRING. . .    Print literal STRING. prompt MAIN CONTINUE Replace the standard prompts. quit         Exit this program. read FILENAME     Execute SQL in FILENAME. restore ?DB? FILE   Restore content of DB (default  main ) from FILE. save FILE       Write in-memory database into FILE. scanstats on|off   Turn sqlite3_stmt_scanstatus() metrics on or off. schema ?PATTERN?   Show the CREATE statements matching PATTERN             Add --indent for pretty-printing. selftest ?--init?   Run tests defined in the SELFTEST table. separator COL ?ROW?  Change the column separator and optionally the row             separator for both the output mode and . import. session CMD . . .    Create or control sessions. sha3sum ?OPTIONS. . . ? Compute a SHA3 hash of database content. shell CMD ARGS. . .   Run CMD ARGS. . . in a system shell. show         Show the current values for various settings. stats ?on|off?    Show stats or turn stats on or off. system CMD ARGS. . .   Run CMD ARGS. . . in a system shell. tables ?TABLE?    List names of tables             If TABLE specified, only list tables matching             LIKE pattern TABLE. . testcase NAME     Begin redirecting output to 'testcase-out. txt'. timeout MS      Try opening locked tables for MS milliseconds. timer on|off     Turn SQL timer on or off. trace FILE|off    Output each SQL statement as it is run. vfsinfo ?AUX?     Information about the top-level VFS. vfslist        List all available VFSes. vfsname ?AUX?     Print the name of the VFS stack. width NUM1 NUM2 . . .  Set column widths for  column  mode             Negative values right-justifyflask-sqlalchemy: 说明：  通过了大多数的关系型数据库的支持，而且提供了ORM支持。安装： 1pip install flask-sqlalchemypython 中 连接配置语法： 12345MySQL：mysql://username:password@host:port/database	# 如果是mysql, 那么数据库需要提前创建好SQLite：	windows：sqlite:///c:/path/to/database	linux：sqlite:////path/to/database配置选项：SQLALCHEMY_DATABASE_URI使用方法： (1) 导入 SQLAlchemy 包,并做基本配置,然后初始化: 123456789101112# 导入类库from flask_sqlalchemy import SQLAlchemy# 连接地址base_dir = os. path. abspath(os. path. dirname(__file__)) # 输出: /Users/dalong/code/python1702/testDB# 配置URIapp. config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:///' + os. path. join(base_dir, 'data. sqlite')# 是否允许sqlite追踪数据库的改变，发出警告，不需要可以禁用(默认开启)app. config['SQLALCHEMY_TRACK_MODIFICATIONS'] = False# 创建对象db = SQLAlchemy(app)   以上代码可以在 项目目录下直接创建一个 data. sqlite 文件.     uri: sqlite:////Users/dalong/code/python1702/testDB/data. sqlite  (2) 设计数据表 数据模型：:  创建一个类, 类需要继承自 db. Model 每行对应一个字段. 12345678910# 定义数据模型类class User(db. Model):  # 若不指定表名，则默认将模型类名转为'小写+下划线'的形式  # 例一: 类名(UserModel)，对应的表名(user_model)  # 例二: 类名(User)，对应的表名(user)  # 若想指定表名，通过__tablename__属性  __tablename__ = 'users'  id = db. Column(db. Integer, primary_key=True)  username = db. Column(db. String(32), unique=True)  email = db. Column(db. String(64), unique=True)(3) 通过路由 创建删除表：: 12345678910111213@app. route('/create/')def create():  # 若表已经存在，则不会再次创建，可以先删除再创建  db. drop_all()  # 创建所有的表  db. create_all()  return '数据表已创建'@app. route('/drop/')def drop():  # 删除所有的表  db. drop_all()  return '数据表已删除' 创建好后可以通过sqlite3 命令行查看数据库内容 sqlite3 data. sqlite 也可以在pycharm 中将项目中的data. sqlite文件拖拽到 database 视图中查看 第一次创建时, 也会创建一些管理表.  如果表已经存在, 则创建不会成功. (4) 命令行创建删除表, 可以有人机交互 prompt_bool：:  因为通过路由操作数据库有一定风险,尤其是删除操作, 这里提供了另一种方法 通过 flask_script 中的Manager实例, 为需要通过命令行才能操作的数据库从操作命令加一个装饰器 @manager. command 在pycharm 的 terminal中直接删除或者创建     python manage. py createall   python manage. py dropall   1234567891011121314from flask_script import prompt_bool # 用于命令行人机交互.  @manager. commanddef createall():  db. drop_all()  db. create_all()  return '数据表已创建'@manager. commanddef dropall():  if prompt_bool('您确定删库跑路吗?'): # 命令行会提示 : 您确定删库跑路吗?, 回答yes删,否则不删.     db. drop_all()    return '数据表已删除'  return '删库有风险，操作需谨慎'自定义 flask_script 的 shell命令:  flask_script 自带了一个 shell 方法, 可以直接在终端里面和代码进行交互.  shell 执行方法: python manage. py shell 但是这个方法有一些局限性, 因为默认的shell命令没有任何数据(需要手动导入) 所以如果需要的话, 可以让shell 带入自定义的上下文 shell 可以改成任何命令名字123456789from flask_script import Shell# 自定义shell命令，因为默认的shell命令没有任何数据(需要手动导入)def make_shell_context():  # 必须返回一个字典，字典中的所有数据都可以在shell中使用  return dict(db=db, User=User)# 下面代码是用自定义的 'shell' 代替原有shell, 新的shell 带有一个字典参数,包含了需要导入的数据对象manager. add_command('shell', Shell(make_context=make_shell_context))SQLite 数据的CURD操作: 增加数据 db. session. add(feng), db. session. add_all([zhong, fei, hui]) , db. session. commit():  添加/修改/删除后必须 commit 才能真正提交 SQLALCHEMY_COMMIT_ON_TEARDOWN 设置成 True 之后就不需要每次 commit了1234567891011121314151617181920# 在每次请求之后自动提交，否则每次都需要手动提交app. config['SQLALCHEMY_COMMIT_ON_TEARDOWN'] = True@app. route('/insert/')def insert():  # 创建对象  # feng = User(username='feng', email='feng@163. com')  # 添加一天数据到数据库  # db. session. add(feng)  # 一次添加多条数据  zhong = User(username='zhong', email='zhong@163. com')  fei = User(username='fei', email='fei@163. com')  hui = User(username='hui', email='hui@163. com')  db. session. add_all([zhong, fei, hui])  # 提交操作(执行前面的SQL)，每次都要提交，  # 除非设置SQLALCHEMY_COMMIT_ON_TEARDOWN  # db. session. commit()  return '数据已添加'根据主键(primary_key)查询数据 User. query. get(uid): 1234567@app. route('/select/&lt;uid&gt;')def select(uid):  # 根据主键进行查询  u = User. query. get(uid)  if u:    return u. username  return '查无此人'多种条件查询 User. query. get(4), User. query. all(), User. query. filter_by(username='hui'). first(),: 查询主键是4的数据 get(4)  get() 用户查主键id12u = User. query. get(4)return u. username查询所有数据 all()  all() 用户查所有12users = User. query. all()return ','. join(u. username for u in users查询用户名是’hui’的用户,只取第一条数据 filter_by()  filter_by() 只能用于等值条件(xxx=’yyy’) filter_by(). filter_by(). filter_by() 也可以 first() 用于返回第一条 查询结果可以是多条结果 first() 可以改成 all()12u = User. query. filter_by(username='hui'). first()return u. email查询用户ID 是4的用户/大于4的用户 filter()  filter() 可以用于非等值条件查询 filter 和 filter_by 的区别是filter_by 参数是字段名, filter 参数是数据类名. 字段名 filter() 可以连续使用. 比如filter(). filter(). filter()123u = User. query. filter(User. id == 4). first()u = User. query. filter(User. id &gt; 4). first()return u. username找到就返回，没有就报错(404)  没有数据就让路由返回404 错误1234u = User. query. get_or_404(8)u = User. query. filter_by(id=4). first_or_404()return u. username统计数据条数  返回的数据只能是字符串 还可以统计最大值, 最小值等数据12count = User. query. count()return str(count)完整测试代码 更多操作, and, or, like, in 123456789101112131415161718192021222324252627@app. route('/find/')def find():  # 根据主键进行查询  # u = User. query. get(4)  # return u. username  # 查询所有数据  # users = User. query. all()  # return ','. join(u. username for u in users)  # 指定过滤条件（只能是等值条件，可以是多个）  # u = User. query. filter_by(username='hui'). first()  # return u. email  # 指定过滤条件（可以指定非等值条件）  # u = User. query. filter(User. id == 4). first()  # u = User. query. filter(User. id &gt; 4). first()  # return u. username  # 找到就返回，没有就报错(404)  # u = User. query. get_or_404(8)  # u = User. query. filter_by(id=4). first_or_404()  # return u. username  # 统计  count = User. query. count()  return str(count)todo: 自行测试：limit、offset、order_by、group_by、paginate 更新数据 db. session. add(u):  更新操作和新添加数据一样,用 db. session. add() 参数对象u 修改后, 用 db. session. add() 操作, db 会判断u是否有primary_key. 如果u没有主键 那么就新建, 有就update123456789@app. route('/update/&lt;uid&gt;')def update(uid):  u = User. query. get(uid)  if u:    u. email = 'xxx@163. com'    # 再次添加数据自动会识别为更新操作    db. session. add(u)    return '数据已修改'  return '查无此人'删除数据 db. session. delete(u): 1234567@app. route('/delete/&lt;uid&gt;')def delete(uid):  u = User. query. get(uid)  if u:    db. session. delete(u)    return '数据已删除'  return '查无此人' 真实项目中, 数据库几乎不做物理删除，大多数是通过设置标志位来进行逻辑删除完成特定功能的。 SQLite 表字段类型 和 字段参数: 常见字段类型       字段类型   Python类型   说明         Integer   int   32位       SmallInteger   int   16位       BigInteger   int/long   不受限制的整数       Float   float   浮点数       String   str   变长字符串       Text   str   变长字符串，做了优化       Boolean   bool   布尔值       Date   datetime. date   日期       Time   datetime. time   时间       DateTime   datetime. datetime   日期时间       Interval   datetime. timedelta   时间间隔   常见字段参数       选项   说明         primary_key   字段是否作为主键索引，默认为False       unique   字段是否作为唯一索引，默认为False       index   字段是否作为普通索引，默认为False       nullable   字段是否可以为空，默认为True       default   为字段指定默认值   SQLAlchemy 建表和插数据技巧二则：: 121. 插入数据时可以不传值的情况有三种：自增的主键、有默认值的、可以为空的2. flask-sqlalchemy中要求，每一张表都必须有一个主键，默认为id数据库的迁移 flask-migrate: 概念 ::  当数据模型更改时，需要将更改已有的数据库，这个过程叫数据库迁移. 要求：  直接删除然后再创建有点简单粗暴，副作用有点大(原有的数据全部丢失)； 要求既能将数据模型的更改应用到数据库，又保留原有的数据安装和配置: 安装 flask-migrate： 1pip3. 7 install flask-migrate配置：  先初始化migrate 对象 然后将MigrateCommand 加到命令行 dbmgrt 是我们给命令起的名字, 叫什么都可以123456# 导入类库from flask_migrate import Migrate, MigrateCommand# 创建对象migrate = Migrate(app, db)# 添加数据库迁移命令manager. add_command('dbmgrt', MigrateCommand)开始迁移分三步：: 注意:  以下命令都是在pycharm 中的 Terminal 中完成 不是每次迁移都会成功(比如有冲突:修改字段类型,但是数据还没有改变)，若失败需要手动解决(1) 初始化数据库迁移的仓库 init  如果重来没迁移过, 那么第一次执行一次就行了   以后的迁移操作都是2和3结合使用   执行后,会发现项目目录下多了一个migrations 的目录1python3. 7 manage. py dbmgrt init(2) 生成迁移SQL 脚本，会根据数据模型与数据表的差异生成SQL语句 migrate  执行后, 在 migrations/versions 下面会生成新的迁移用的脚本1python3. 7 manage. py dbmgrt migrate(3) 执行迁移，就是执行上面生成的SQL语句 1python3. 7 manage. py dbmgrt upgrade练习:  flask项目的目录结构(具有扩展型) 试着写一下用户的注册、激活、登录等项目练习: 项目需求:  用户注册登录 用户信息管理 博客发表回复 博客列表展示 博客详情展示 分别显示内容 点赞, 收藏, 搜索, 统计, 排序…(1) 利用虚拟环境创建一个pycharm项目, 并创建目录: pycharm 创建创建虚拟s:  方法(1)     创建虚拟环境 : 在 pycharm 的 terminal 中进行下面的命令   12345pip3. 7 install virtualenv # 安装# 进入项目目录,然后执行下面命令virtualenv venv # 创建source . /venv/bin/activate # 激活deactivate # 退出虚拟环境如果要指定虚拟环境的python版本,就需要用 -p 1virtualenv -p /Library/Frameworks/Python. framework/Versions/3. 7/bin/python3. 7 env 方法(2)         在pycharm 中创建一个新的项目, 直接指定编译环境使用虚拟环境.           创建按后再pycharm 中的 terminal 中 pip3. 6 list 查看包个数, 没什么东西就证明可用了.       为虚拟环境做依赖的配置信息文件: 123pip3. 7 freeze     # 查看 所有的Pip 环境安装的依赖包列表pip3. 7 freeze &gt; requirements. txt  # 将所有的Pip 环境安装的依赖包导出到文件中pip3. 7 install -r requirements. txt # 读取安装文件来安装 pip 包为项目创建目录结构: 项目结构:  做一个项目, 最好提前将项目结构确定好, 项目启动后就不太容易改了. 123456789101112131415161718192021222324252627blog/ 				# 项目根目录	app/			# 程序包目录		__init__. py		templates/	# 模板夹			common/		# 被包含,被继承用的			errors/		# 报错类的模板			email/ 		# 邮件模板			marcos/		# 宏函数    static/		# 静态文件夹      img/      css/      js/      upload/      favicon. ico		views/		# 蓝本视图函数文件		models/		# 数据模型文件		forms/		# form表单类		config. py	# 配置文件		email. py	# 其他一些需要用的类或者函数		extensions. py # 将所有添加的扩展放到一起. 比如 from xxx import xxx 这种代码		helper. py	# 放通用小函数的文件	migrations/		# 数据库迁移文件	tests/			# 测试文件目录	venv/			# 虚拟环境目录	requirements. txt# 项目依赖包列表文件. 里面列出来需要 pip 安装的所有包信息	manage. py		# 启动控制文件	(1) 设置项目配置信息 app/config. py: 配置文件app/config. py: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849import osbase_dir = os. path. abspath(os. path. dirname(__file__))# 通用配置class Config:  TEMPLATES_AUTO_RELOAD = True  # 秘钥  SECRET_KEY = os. urandom(24)  # 数据库  SQLALCHEMY_COMMIT_ON_TEARDOWN = True  SQLALCHEMY_TRACK_MODIFICATIONS = False  # 邮件  MAIL_SERVER = 'smtp. 163. com'  MAIL_USERNAME = 'dalong_coo@163. com'  MAIL_PASSWORD = '*******'  #上传文件  MAX_CONTENT_LENGTH = 8*1024*1024  UPLOADED_PHOTOS_DEST = os. path. join(base_dir, 'static/upload')  # 初始化操作, 完成特定环境的初始化  @staticmethod  def init_app(app):    pass# 开发环境 - 继承自基本配置class DevelopmentConfig(Config):  SQLALCHEMY_DATABASE_URI = 'sqlite:///' + os. path. join(base_dir, 'blog-dev. sqlite')# 测试环境class TestConfig(Config):  SQLALCHEMY_DATABASE_URI = 'sqlite:///' + os. path. join(base_dir, 'blog-test. sqlite')# 生产环境class ProductionConfig(Config):  SQLALCHEMY_DATABASE_URI = 'sqlite:///' + os. path. join(base_dir, 'blog. sqlite')  # 邮件. 因为生产环境有些配置不同,这里要把部分内容重写.   MAIL_USE_SSL = True  MAIL_PORT = 465# 配置字典config = {  'development': DevelopmentConfig,  'testing': TestConfig,  'production': ProductionConfig,  # 默认配置  'default': DevelopmentConfig}(2) 添加扩展类库: app/extensions. py 中导入扩展并初始化: 12345678910111213141516171819202122232425262728293031323334# 导入类库from flask_bootstrap import Bootstrapfrom flask_wtf import FlaskFormfrom flask_sqlalchemy import SQLAlchemyfrom flask_mail import Mailfrom flask_uploads import UploadSetfrom flask_moment import Momentfrom flask_migrate import Migratefrom flask_login import LoginManager# 创建对象bs = Bootstrap()db = SQLAlchemy()mail = Mail()moment = Moment()migrate = Migrate(db=db)login_manager = LoginManager()# 初始化扩展def config_extensions(app):  bs. init_app(app)  db. init_app(app)  mail. init_app(app)  moment. init_app(app)  migrate. init_app(app)  # 用户登录认证  login_manager. init_app(app)  # 指定登陆失败的跳转的路由的端点  login_manager. login_view = 'user. login'  # 设置登录失败的提示信息  login_manager. login_message = '需要登录才能访问'  # 设置session 的保护级别  # None: 不使用session 保护, basic: 基本的保护, strong: 最严格的保护  login_manager. session_protection = 'strong'(3) 配置蓝图, 在蓝图中增加路由: app/views/ 下创建蓝本: main. py 1234567from flask import Blueprint, render_templatemain = Blueprint('main', __name__)@main. route('/')def index():  return render_template('main/index. html')创建 app/views/__init__. py: 12345678910111213141516from . main import mainfrom . user import user# 蓝本默认配置DEFAULT_BLUEPRINT = (  # (蓝本, '前缀')  (main, ''),  (user, '/user'))def config_blueprint(app):  for blueprint, url_prefix in DEFAULT_BLUEPRINT:    app. register_blueprint(blueprint, url_prefix=url_prefix)(4) 初始化配置 app/__init__. py:  __init__. py 用于封装 create_app 工厂方法1234567891011121314151617181920212223242526272829from flask import Flask, render_templatefrom app. config import configfrom app. extensions import config_extensionsfrom app. views import config_blueprint# 封装一个工厂方法创建app, 并获取配置信息def create_app(config_name='default'):  # 创建实例  app = Flask(__name__)  app. config. from_object(config[config_name])  # 调用初始化配置  config[config_name]. init_app(app)  # 初始化扩展库  config_extensions(app)  # 配置蓝本  config_blueprint(app)  # 定制报错信息  config_errorhandler(app)  # 返回实例  return appdef config_errorhandler(app):  # 再蓝图中定制的报错信息只在蓝图中有效. 如果想全局有效, 需要在 app/__init__. py中定制.   @app. errorhandler(404)  def err404(e):    return render_template('errors/404. html')(5) 创建项目基础模板: 在 app/templates/common 中创建base. html 是最基本的模板: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071{% extends 'bootstrap/base. html' %}{% from 'bootstrap/wtf. html' import quick_form %}{% block title %}默认标题{% endblock %}{# 定制导航条 #}{% block navbar %}  &lt;nav class= navbar navbar-inverse  style= border-radius: 0px; &gt;    &lt;div class= container &gt;      &lt;!-- Brand and toggle get grouped for better mobile display --&gt;      &lt;div class= navbar-header &gt;        &lt;button type= button  class= navbar-toggle collapsed  data-toggle= collapse             data-target= . navbar-collapse  aria-expanded= false &gt;          &lt;span class= sr-only &gt;Toggle navigation&lt;/span&gt;          &lt;span class= icon-bar &gt;&lt;/span&gt;          &lt;span class= icon-bar &gt;&lt;/span&gt;          &lt;span class= icon-bar &gt;&lt;/span&gt;        &lt;/button&gt;        &lt;a class= navbar-brand  href= {{ url_for('main. index') }} &gt;首页&lt;/a&gt;      &lt;/div&gt;      &lt;!-- Collect the nav links, forms, and other content for toggling --&gt;      &lt;div class= collapse navbar-collapse &gt;        &lt;ul class= nav navbar-nav &gt;          &lt;li&gt;&lt;a href= # &gt;板块1&lt;/a&gt;&lt;/li&gt;          &lt;li&gt;&lt;a href= # &gt;板块2&lt;/a&gt;&lt;/li&gt;        &lt;/ul&gt;        &lt;ul class= nav navbar-nav navbar-right &gt;        {% if current_user. is_authenticated %}          &lt;li&gt;&lt;a href= {{ url_for('user. logout') }} &gt;退出&lt;/a&gt;&lt;/li&gt;          &lt;li class= dropdown &gt;            &lt;a href= #  class= dropdown-toggle  data-toggle= dropdown  role= button  aria-haspopup= true               aria-expanded= false &gt;{{ current_user. username }} &lt;span class= caret &gt;&lt;/span&gt;&lt;/a&gt;            &lt;ul class= dropdown-menu &gt;              &lt;li&gt;&lt;a href= {{ url_for('user. profile') }} &gt;个人信息&lt;/a&gt;&lt;/li&gt;              &lt;li&gt;&lt;a href= {{ url_for('user. change_password') }} &gt;修改密码&lt;/a&gt;&lt;/li&gt;              &lt;li&gt;&lt;a href= # &gt;修改邮箱&lt;/a&gt;&lt;/li&gt;              &lt;li&gt;&lt;a href= # &gt;修改头像&lt;/a&gt;&lt;/li&gt;            &lt;/ul&gt;          &lt;/li&gt;        {% else %}          &lt;li&gt;&lt;a href= {{ url_for('user. login') }} &gt;登录&lt;/a&gt;&lt;/li&gt;          &lt;li&gt;&lt;a href= {{ url_for('user. register') }} &gt;注册&lt;/a&gt;&lt;/li&gt;          &lt;li&gt;&lt;a href= {{ url_for('user. forget_password') }} &gt;找回密码&lt;/a&gt;&lt;/li&gt;        {% endif %}        &lt;/ul&gt;      &lt;/div&gt;&lt;!-- /. navbar-collapse --&gt;    &lt;/div&gt;&lt;!-- /. container --&gt;  &lt;/nav&gt;{% endblock %}{# 定制内容 #}{% block content %}  &lt;div class= container &gt;    {% for message in get_flashed_messages() %}      &lt;div class= alert alert-warning alert-dismissible  role= alert &gt;        &lt;button type= button  class= close  data-dismiss= alert  aria-label= Close &gt;&lt;span aria-hidden= true &gt;&amp;times;&lt;/span&gt;        &lt;/button&gt;        {{ message }}      &lt;/div&gt;    {% endfor %}    {% block page %}默认内容{% endblock %}  &lt;/div&gt;{% endblock %}{% block head %}  {{ super() }}  &lt;link rel= icon  type= image/x-icon  href= {{ url_for('static', filename='favicon. ico') }}  /&gt;{% endblock %}创建 404 模板 templates/errors/404. page: 1234567891011{% extends 'common/base. html' %}{% block title %}404报错了{% endblock %}{% block content %}  {% block page %}  {% endblock %}  &lt;h2&gt;报错了&lt;/h2&gt;{% endblock %}在 app/templates/main 中创建 index. html 继承 base. html: 12345678910{% extends 'common/base. html' %}{% block title %}  首页{% endblock %}{% block page %}  &lt;h2&gt;首页&lt;/h2&gt;  &lt;br&gt;{% endblock %}在 app/views/main. py 中创建路由 调用 index. html: 12345678from flask import Blueprint, render_templatemain = Blueprint('main', __name__)@main. route('/')def index():  return render_template('main/index. html')(6) manage. py 中 调用 create_app函数, 然后启动: 123456789101112131415from app import create_appfrom flask_script import Managerimport os# 调用工厂方法创建实例config_name = os. environ. get('FLASK_CONFIG') or 'default' # 可以在环境变量中设置 FLASK_CONFIG = production. 目的是为了不改代码app = create_app(config_name)#@app. route('/')#def index():#  return 'ok'if __name__ == '__main__':  mgr = Manager(app)  mgr. run()启动服务 1python3. 7 manage. py runserver -r -d -h 0. 0. 0. 0邮件发送: app/email. py 创建 email 代码 , 导入相关依赖 1234567891011121314151617181920212223242526272829from flask import current_app, render_templatefrom app. extensions import mailfrom flask_mail import Messagefrom threading import Thread# 异步发送邮件def async_send_mail(app, msg):  # 必须在程序上下文中才能发送邮件，新建的线程没有，因此需要手动创建  with app. app_context():    # 发送邮件    mail. send(msg)def send_mail(to, subject, template, **kwargs):  # 获取原始的app实例, 因为是发邮件是线程, 而线程需要实例上下文才能发送邮件, 所以需要创建一个带有上下文的app实例, 传递给邮件线程函数.   app = current_app. _get_current_object()  # 创建邮件对象  msg = Message(    subject=subject,    recipients=[to],    body=render_template('email/'+template+'. txt', **kwargs),    html=render_template('email/'+template+'. html', **kwargs),    sender=app. config['MAIL_USERNAME']  )  # 创建线程对象, 并启动线程  return Thread(target=async_send_mail, args=(app, msg)). start()用户管理模块: 需求: 注册-&gt;激活-&gt;登录认证-&gt;用户信息管理 用户注册, 登录, 找回密码, 修改密码, 个人中心: 添加注册的视图文件 app/views/user. py  我犯的错误: form = RegisterForm() 忘了括号123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153from flask import Blueprint, url_for, render_template, flash, redirect, current_app, requestfrom app. forms import RegisterForm, LoginForm, ChangePWDForm, ForgetPWDForm, RenewPWDFormfrom itsdangerous import TimedJSONWebSignatureSerializer as Serializerfrom app. models import Userfrom app. extensions import dbfrom app. email import send_mailfrom flask_login import login_user, current_user, logout_user, login_requiredimport osuser = Blueprint('user' , __name__)@user. route('/register/', methods=['GET', 'POST'])def register():  form = RegisterForm()  if form. validate_on_submit():    # 根据表单数据创建用户对象    u = User(username=form. username. data, password=form. password. data, email=form. email. data)    # 将用户提交数据保存到数据库中    db. session. add(u) # 此时还没有ID    db. session. commit() # 手动提交    # 生成token    token = u. generate_activate_token()    # 发送激活邮件到注册邮箱, 需要用户点击激活连接    send_mail(u. email, '账户激活', 'activate', username=u. username, token=token)    # flash 用户, 提示下一步操作    flash('注册成功, 请到邮箱激活,完成注册')    return redirect(url_for('main. index'))  return render_template('user/register. html', form=form)# 用户激活@user. route('/active/&lt;token&gt;')def active(token):  if User. check_token_for_user_active(token):    flash('激活成功')    return redirect(url_for('user. login'))  else:    flash('激活失败')    return redirect(url_for('main. index'))# 用户登录@user. route('/login/', methods=['POST', 'GET'])def login():  form = LoginForm()  if form. validate_on_submit():    # 根据用户名查找用户    u = User. query. filter_by(username=form. username. data). first()    if not u:      flash('用户不存在')    elif not u. confirmed:      flash('账户尚未激活, 请激活后再登录')    elif u. verify_password(form. password. data):      # 登录, 并完成状态记录中      login_user(u, remember=form. remember_me. data)      flash('登录成功')      # 跳转到URL中 next 参数对应的路由      return redirect(request. args. get('next') or url_for('main. index'))    else:      flash('无效的密码')  return render_template('user/login. html', form=form)# 退出登录@user. route('/logout/')def logout():  # 退出当前已经登录的用户  logout_user()  flash('您已经退出登录')  return redirect(url_for('main. index'))# 个人中心@user. route('/profile/')# 登录路由保护@login_requireddef profile():  return render_template('user/profile. html')# 修改密码@user. route('/change_password/', methods=['POST', 'GET'])@login_requireddef change_password():  form = ChangePWDForm()  if form. validate_on_submit():    # 匹配原密码是否正确    if current_user. verify_password(form. oldpwd. data):      current_app. logger. info('原密码:'+form. oldpwd. data)      current_user. password = form. password. data      db. session. add(current_user)      flash('密码修改成功')      return redirect(url_for('main. index'))  return render_template('user/change_password. html', form=form)# 找回密码@user. route('/forget_password/', methods=['POST','GET'])def forget_password():  form = ForgetPWDForm()  if form. validate_on_submit():    u = User. query. filter_by(email=form. email. data). first()    current_app. logger. info(form. email. data)    current_app. logger. info(str(u))    if u:      # 发送重置密码的函数      # 生成token      token = u. generate_activate_token()      # 发送重置密码邮件到注册邮箱, 需要用户点击重置连接      send_mail(u. email, '密码重置','forget_password', username=u. username, token=token)      flash('重置密码邮件已经发送到您的邮箱, 请及时查收')      return redirect(url_for('main. index'))    else:      flash('您输入的邮箱信息有误,请重新尝试输入您注册时用的邮箱')  return render_template('user/forget_password. html', form=form)# 重置新密码: 确认用户token ,如果正确,跳转到密码重置页. @user. route('/renew_token_check/&lt;token&gt;')def renew_token_check(token):  # 验证token  u = User. check_token_for_renew_password(token)  if u:    # 登录    login_user(u)    flash('请您尽快更新您的密码')    return redirect(url_for('user. renew_password'))  return  您的token 信息有误, 请重新尝试找回密码 # 重置密码@user. route('/renew_password/', methods=['POST','GET'])@login_requireddef renew_password():  form = RenewPWDForm()  if form. validate_on_submit():    # 匹配原密码是否正确      current_user. password = form. password. data      db. session. add(current_user)      flash('密码修改成功')      return redirect(url_for('main. index'))  return render_template('user/renew_password. html', form=form)添加表单类 app/forms/user. py 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354from flask_wtf import FlaskFormfrom wtforms import StringField, PasswordField, SubmitField, BooleanFieldfrom wtforms. validators import DataRequired, EqualTo, Length, Email, ValidationErrorfrom app. models import User# 用户注册表单class RegisterForm(FlaskForm):  username = StringField(label='用户名', validators=[DataRequired(message= 用户名不能为空 ), Length(1,12, message= 字符只能是1~12位字符之间 )])  password = PasswordField(label='密码', validators=[DataRequired(message= 密码不能为空 ), Length(1,20, message= 密码长度是1~20字符之间 )])  confirm = PasswordField(label='确认密码', validators=[DataRequired(message= 确认密码不能为空 ), Length(1,20, message= 确认密码长度是1~20字符之间 ), EqualTo('password', message= 两次密码不一致 )])  email = StringField(label= 邮箱 , validators=[Email(message= 邮箱不能为空 )])  submit = SubmitField(label= 提交 )  # 自定义用户名验证器  def validate_username(self, field):    if User. query. filter_by(username=field. data). first():      raise ValidationError('用户名已注册,请选用其他名称')  # 自定义邮箱验证器  def validate_email(self, field):    if User. query. filter_by(email=field. data). first():      raise ValidationError('邮箱已注册使用,请选用其他名称')# 用户登录表单class LoginForm(FlaskForm):  username = StringField(label='用户名', validators=[DataRequired(message= 用户名不能为空 ), Length(1,12, message= 字符只能是1~12位字符之间 )])  password = PasswordField(label='密码', validators=[DataRequired(message= 密码不能为空 ), Length(1,20, message= 密码长度是1~20字符之间 )])  remember_me = BooleanField(label='记住我')  submit = SubmitField(label= 提交 )# 修改密码class ChangePWDForm(FlaskForm):  oldpwd = PasswordField(label='原密码', validators=[DataRequired(message='请输入原密码'), Length(1,12,message= 密码长度为1~12个字符之间 )])  password = PasswordField(label='密码',validators=[DataRequired(message= 密码不能为空 ), Length(1, 20, message= 密码长度是1~20字符之间 )])  confirm = PasswordField(label='确认密码',validators=[DataRequired(message= 确认密码不能为空 ), Length(1, 20, message= 确认密码长度是1~20字符之间 ),EqualTo('password', message= 两次密码不一致 )])  submit = SubmitField(label= 提交 )# 重置密码class RenewPWDForm(FlaskForm):  password = PasswordField(label='密码',validators=[DataRequired(message= 密码不能为空 ), Length(1, 20, message= 密码长度是1~20字符之间 )])  confirm = PasswordField(label='确认密码',validators=[DataRequired(message= 确认密码不能为空 ), Length(1, 20, message= 确认密码长度是1~20字符之间 ),EqualTo('password', message= 两次密码不一致 )])  submit = SubmitField(label= 提交 )# 找回密码class ForgetPWDForm(FlaskForm):  email = StringField(label='请输入注册时使用的邮箱地址', validators=[Email(message= 邮箱不能为空 )])  submit = SubmitField(label='提交')app/forms/init. py 文件 12from . user import RegisterForm, LoginForm, ChangePWDForm, ForgetPWDForm, RenewPWDFormtemplates/user/register. html  我犯的错误: {{ quick_form(form) }} 用双大括号12345678910111213{% extends 'common/base. html' %}{% block title %}  注册{% endblock %}{% block page %}  &lt;h2&gt;注册&lt;/h2&gt;  &lt;br&gt;  {{ quick_form(form) }}{% endblock %}templates/user/login. html 1234567891011{% extends 'common/base. html' %}{% block title %}  登录{% endblock %}{% block page %}  &lt;h2&gt;登录&lt;/h2&gt;  &lt;br&gt;  {{ quick_form(form) }}{% endblock %}templates/user/forget_password. html 12345678910{% extends 'common/base. html' %}{% block title %}找回密码{% endblock %}{% block page %}  &lt;h2&gt;找回密码&lt;/h2&gt;  &lt;hr&gt;  {{ quick_form(form) }}{% endblock %}templates/user/change_password. html 12345678910{% extends 'common/base. html' %}{% block title %}修改密码{% endblock %}{% block page %}  &lt;h2&gt;修改密码&lt;/h2&gt;  &lt;hr&gt;  {{ quick_form(form) }}{% endblock %}templates/user/renew_password. html 123456789{% extends 'common/base. html' %}{% block title %}重置密码{% endblock %}{% block page %}  &lt;h2&gt;重置密码&lt;/h2&gt;  &lt;hr&gt;  {{ quick_form(form) }}{% endblock %}templates/user/profile. html 123456789101112131415161718{% extends 'common/base. html' %}{% block title %}  个人中心{% endblock %}{% block page %}  &lt;h1&gt;个人信息显示&lt;/h1&gt;  &lt;div class= form-group &gt;    &lt;label for= username &gt;Password&lt;/label&gt;    &lt;input type= text  class= form-control  id= username  value= {{ current_user. username }}  readonly&gt;  &lt;/div&gt;  &lt;div class= form-group &gt;    &lt;label for= email &gt;邮箱&lt;/label&gt;    &lt;input type= email  class= form-control  id= email  value= {{ current_user. email }}  readonly&gt;  &lt;/div&gt;{% endblock %}/models/user. py 添加数据库模型, 并对密码的存储进行单独设置  这里的密码hash 是单向机密,不能解密 还需要将账户激活的在这里定义123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687# 老师说这里要单独导入db 扩展, 因为下面代码要用到from app. extensions import db, login_manager# 一个python 自带的密码加密包, 感觉代替了传统的md5加密from werkzeug. security import generate_password_hash, check_password_hashfrom itsdangerous import TimedJSONWebSignatureSerializer as Serializerfrom flask import current_appfrom flask_login import UserMixinimport osclass User(UserMixin, db. Model):  __tablename__ = 'users'  id = db. Column(db. Integer, primary_key=True)  username = db. Column(db. String(20), unique=True)  password_hash = db. Column(db. String(128))  email = db. Column(db. String(64), unique=True)  confirmed = db. Column(db. Boolean, default=False)  # 密码属性的保护  @property  def password(self):    raise AttributeError('密码是不可读的')  # 设置密码加密, 加密后保存  @password. setter  def password(self, password):    self. password_hash = generate_password_hash(password)  # 校验密码的函数  def verify_password(self, password):    # 如果校验成功返回True , 否则返回 False    return check_password_hash(self. password_hash, password)  # 生成账户激活的token  def generate_activate_token(self, expires_in=3600):    # 生成一个加密字符串    s = Serializer(current_app. config['SECRET_KEY'], expires_in=expires_in)    return s. dumps({'id': self. id})  # 获取token, 解密token, 然后激活账户  # 这里需要设置成静态方法, 因为激活前,还不知道用户是谁  @staticmethod  def check_token_for_user_active(token):    u = User. check_active_token(token)    if not u:      # 没找到用户信息,异常情况      return False    # 确认没激活的时候就激活    if not u. confirmed:      u. confirmed = True      db. session. add(u)    return True  # 获取token, 解密token, 然后重置密码  @staticmethod  def check_token_for_renew_password(token):    u = User. check_active_token(token)    if not u:      # 没找到用户信息,异常情况      return False    # 确认是用户本人的token, 返回u, 让用户自己去修改密码    return u  @staticmethod  def check_active_token(token):    s = Serializer(current_app. config['SECRET_KEY'])    try:      # 解密字符串, 可以生成一个字典      data = s. loads(token)    except:      return False    return User. query. get(data['id'])# 登录认证的回调函数, 通过UID 获得用户的完整信息@login_manager. user_loaderdef loader_user(uid):  return User. query. get(int(uid))注意:  设计完数据模型, 不要忘记迁移操作 新建的模型必须要让外部看到，否则迁移时没有办法实现创建 app/models/__init__. py 1from . user import User为 email 函数创建创建邮件模板 templates/email/activate. html 12&lt;h1&gt;hello {{ username }} &lt;/h1&gt;&lt;p&gt;请点击右侧 &lt;a href= {{ url_for('user. active', token=token, _external=True) }} &gt;连接&lt;/a&gt; 激活&lt;/p&gt;templates/email/activate. text 12hello {{ username }}请复制右侧 {{ url_for('user. active', token=token, _external=True) }} 连接 激活templates/email/forget_password. html 123&lt;h2&gt;{{ username }} 您好. 请点击下面链接重置您的密码&lt;/h2&gt;&lt;a href= {{ url_for('user. renew_token_check', token=token, _external=True) }} &gt;重置密码&lt;/a&gt;templates/email/forget_password. txt 12{{ username }} 您好. 请拷贝下面链接到您的浏览器中完成重置密码{{ url_for('user. renew_token_check', token=token, _external=True) }}执行数据迁移命令:  如果第一步或者第二部失败了, 需要手动删掉 migrations 目录,然后再重头来 迁移成功可以看到migrtions/versions 和 app/blog-dev. sqlite123python3. 7 manage. py dbmgrt initpython3. 7 manage. py dbmgrt migratepython3. 7 manage. py dbmgrt upgrade用户登录认证: 用户认证的逻辑: (1)    添加点击跳转链接     添加登录视图函数     添加模板文件     视图函数添加校验逻辑     flask-login   配置：   12345678910111213login_manager = LoginManager()def config_extensions(app):	. . . 	# 用户登录认证管理  login_manager. init_app(app)  # 指定登录的端点(视图函数)  login_manager. login_view = 'user. login'  # 需要登录的提示信息  login_manager. login_message = '需要登录才可访问'  # 设置session的保护级别  # None：不使用session保护；'basic'：基本的(默认)；'strong'：最严格的  login_manager. session_protection = 'strong'    修改model类：   123456789from flask_login import UserMixinclass User(UserMixin, db. Model):	pass  # 登录认证的回调@login_manager. user_loaderdef loader_user(uid):  return User. query. get(int(uid))       完整的登录处理   12345678910111213141516171819@user. route('/login/', methods=['GET', 'POST'])def login():  form = LoginForm()  if form. validate_on_submit():    # 根据用户名查找用户    u = User. query. filter_by(username=form. username. data). first()    if not u:      flash('无效的用户名')    elif not u. confirmed:      flash('账户尚未激活，请激活后再登录')    elif u. verify_password(form. password. data):      # 用户登录，顺便可以完成'记住我'的功能      login_user(u, remember=form. remember. data)      flash('登录成功')      return redirect(request. args. get('next') or               url_for('main. index'))    else:      flash('无效的密码')  return render_template('user/login. html', form=form)      退出登录及路由保护   12345678910111213# 退出登录@user. route('/logout/')def logout():  # 退出当前登录的用户  logout_user()  flash('您已退出登录')  return redirect(url_for('main. index'))   @user. route('/profile/')# 路由保护(需要登录才可访问)@login_requireddef profile():  return render_template('user/profile. html')    flask_login总结:  获取状态:    登录认证: is_authenticated  匿名状态: is_anonymous  改变状态:    login_user: 未登录 -&gt; 登录  logout_user: 登录 -&gt; 未登录  路由保护:    login_required  记住我:    login_user(u, remember=form. remember_me. data) 练习:  修改密码: 找回密码: 需要带着一个加密的字符串(带上用户信息), 在登录位置添加 修改邮箱: 需要邮箱认证   修改头像: 需要修改数据模型     修改密码   12341. 添加点击跳转链接2. 添加对应的视图函数，渲染指定的模板文件3. 添加模板文件，渲染一个表单(原密码、新密码、确认密码)4. 校验后修改密码      找回密码   123451. 在登录页面添加点击跳转的链接2. 添加视图函数渲染指定模板(用户名或邮箱)3. 提交后校验确定用户，向其注册的邮箱发送一封邮件(携带个人信息)4. 点击邮件中的链接跳转处理，校验token后，渲染一个设置密码的表单5. 点击提交后保存新密码      修改邮箱   12341. 添加点击跳转的链接，添加视图函数，渲染指定模板2. 模板中显示一个表单，填入新的邮箱地址(为了安全可以要求输入用户密码)3. 提交后检验，向新的邮箱发送一封邮件(包含用户信息、新的邮箱)4. 点击邮件中的链接，校验token，将邮箱更换    换头像的代码:: 上传头像 123451. 添加点击链接、添加视图函数、渲染指定模板2. 添加flask-uploads扩展配置3. 创建头像上传表单类4. 上传文件处理(随机文件名、缩略图、保存到数据库，删除原来头像)5. 修改用户模型(添加icon字段用于保存头像文件名)视图函数 app/views/user. py： 123456789101112131415161718192021222324252627282930from app. helper import random_stringfrom app. extensions import photosfrom PIL import Image# 上传/修改头像@user. route('/icon/', methods=['POST','GET'])def icon():  form = IconForm()  if form. validate_on_submit():    # 获取上传头像, 获取随机文件名, 保存新头像    filename = random_string(5) + os. path. splitext(form. icon. data. filename)[1]    photos. save(form. icon. data, name=filename)    current_app. logger. info(photos. path(filename))    # 生成缩略图    img = Image. open(photos. path(filename))    img. thumbnail((128,128))    img. save(photos. path(filename))    # 删除原有头像    if current_user. icon != 'default_icon. jpeg':      os. remove(os. path. join(current_app. config['UPLOADED_PHOTOS_DEST'], current_user. icon))    # 保存新头像到数据库    current_user. icon = filename    db. session. add(current_user)    # 提示上传完成    flash('头像已经成功保存')    return redirect(url_for('user. icon'))  img_url = photos. url(current_user. icon)  return render_template('user/icon. html', form=form, img_url=img_url)app/templates/user/icon. html 12345678910{% extends 'common/base. html' %}{% block title %}  用户头像{% endblock %}{% block page %}  &lt;h2&gt;用户头像&lt;/h2&gt;  &lt;br&gt;  {% if img_url %}&lt;img src= {{ img_url }} &gt;{% endif %}  {{ quick_form(form) }}{% endblock %}修改扩展包信息 app/extensions. py 12345678from flask_uploads import UploadSet, IMAGES, configure_uploads, patch_request_classphotos = UploadSet('photos', IMAGES)# 初始化扩展def config_extensions(app):  # 上传文件  configure_uploads(app, photos)  patch_request_class(app, size=None)修改配置信息 app/config. py 123#上传文件MAX_CONTENT_LENGTH = 8*1024*1024UPLOADED_PHOTOS_DEST = os. path. join(base_dir, 'static/upload')上传头像表单app/forms/user. py： 12345678from flask_wtf. file import FileField, FileAllowed, FileRequiredfrom app. extensions import photosclass IconForm(FlaskForm):  icon = FileField('头像',           validators=[FileRequired(message='请选择文件'),           FileAllowed(photos, message='只能上传图片')])  submit = SubmitField('上传')修改用户模型app/models/user. py： 1234class User(UserMixin, db. Model):  . . .   # 头像	icon = db. Column(db. String(64), default='default_icon. jpeg')迁移数据库 12python3. 7 manage. py db migratepython3. 7 manage. py db upgrade博客管理:: 步骤: 添加博客表数据模型 app/models/posts. py 12345678910111213from app. extensions import dbfrom datetime import datetimeclass Posts(db. Model):  __tablename__ = 'posts'  id = db. Column(db. Integer, primary_key=True)  rid = db. Column(db. Integer, default=0)  content = db. Column(db. Text)  timestamp = db. Column(db. DateTime, default=datetime. utcnow)  # 外键关联user表, 有了外键, users 表就可以做引用了.   uid = db. Column(db. Integer, db. ForeignKey('users. id')) # 和users. id 进行外键关联app/models/user. py 123456789# 在关联模型中添加反向引用, 模型是Post, 只有在需要数据的时候才读取, 所以是 dynamic  # 参数1: 关联模型  # 参数2: 反向引用的名称, 比如: p. user. icon 就是在调用博客信息是,直接可以从博客的表中获取user的图标信息  # 参数3: 加载方式, dynamic 表示动态加载  # 参数4: 设定表关系, 该参数可选  # 关系: 1对1(uselist=False), 1对多(uselist=True), 多对多  # 使用方法: u. posts. all()  posts = db. relationship('Posts', backref='user', lazy='dynamic', uselist=True)app/models/init. py 1from . posts import Posts迁移 123python3. 7 manage. pdb migratepython3. 7 manage. pdb upgradeapp/forms/posts. py 12345678from flask_wtf import FlaskFormfrom wtforms import TextAreaField, SubmitFieldfrom wtforms. validators import DataRequired, Lengthclass PostsForm(FlaskForm):  # render_kw: 要渲染的html标签字段的属性字典  content = TextAreaField(label= 这一刻的想法 , render_kw={'placeholder': '这一刻的想法. . . '}, validators=[DataRequired(message= 内容不能为空 ), Length(3, 140, message='字符限定在3 ~ 140个字符之间')])  submit = SubmitField(label='提交')app/forms/init. py 1from . posts import PostsFormapp/views/main. py 1234567891011121314151617181920212223242526from flask import Blueprint, render_template, flash, redirect, url_forfrom app. forms import PostsFormfrom app. models import Postsfrom flask_login import current_userfrom app. extensions import dbmain = Blueprint('main', __name__)@main. route('/', methods=['POST', 'GET'])def index():  form = PostsForm()  if form. validate_on_submit():    if current_user. is_authenticated:      u = current_user. _get_current_object()      p = Posts(content=form. content. data, user=u)      db. session. add(p)      flash('发表成功')      return redirect(url_for('main. index'))    else:      flash('请先登录再发表')      return redirect(url_for('user. login'))  posts = Posts. query. filter_by(rid=0). order_by(Posts. timestamp. desc()). all()  return render_template('main/index. html', form=form, posts=posts)app/templates/main/index. html 12345678910111213141516171819202122232425262728{% extends 'common/base. html' %}{% block title %}  首页{% endblock %}{% block page %}  {{ quick_form(form) }}  {% for p in posts %}    &lt;div class= media &gt;      &lt;div class= media-left &gt;        &lt;a href= # &gt;          &lt;img class= media-object  src= {{ url_for('static', filename='upload/'+p. user. icon) }}  alt= icon &gt;        &lt;/a&gt;      &lt;/div&gt;      &lt;div class= media-body &gt;        &lt;div style= float: right &gt;发表于: {{ moment(p. timestamp). fromNow() }}&lt;/div&gt;        &lt;h4 class= media-heading &gt;{{ p. user. username }}&lt;/h4&gt;        {{ p. content }}      &lt;/div&gt;    &lt;/div&gt;  {% endfor %}{% endblock %}{% block scripts %}  {{ super() }}  {{ moment. include_moment() }}  {{ moment. locale('zh-CN') }}{% endblock %}分页功能- flask_SQLalchemy 的 paginate 扩展包: 分页查询介绍:paginate: 概念:paginate 是一个 SQLAlchemy 的一个方法, 会返回一个分页对象  返回结果是一个分页对象,叫做 Pagination 包含了所有分页信息参数:  page: 必选, 当前页号 per_page: 可选, 每页显示条数, 不指定,默认是20条 error_out: 可选, 当页码超出可选范围时是否报错, 默认是True 404错误. 可改成 False 返回值: 返回分页对象 Pagination     Pagination 属性         items: 当前页数据     page: 当前页号     pages: 总共多少页     total: 总记录条数     per_page: 每页显示条数     prev_num: 上页页码     next_num: 下页页码     has_prev: 是否有上一页     has_next: 是否有下一页          Pagination 方法         prev: 上一页的分页对象     next: 下一页的分页对象     iter_pages: 是一个迭代器, 返回的是导航条上显示的页码号, 如果页数太多, 显示不完的页码, 显示None          将分页封装做成一个宏, 用于显示分页条: 创建宏 app/templates/common/marco. html 1234567891011121314151617181920212223242526272829{# 分页宏 #}{% macro pagination_show(pagination, endpoint) %}  &lt;nav aria-label= Page navigation &gt;    &lt;ul class= pagination &gt;      {# 上一页的按钮 #}      &lt;li {% if not pagination. has_prev %}class= disabled {% endif %}&gt;        &lt;a href= {% if pagination. has_prev %}{{ url_for(endpoint, page=pagination. prev_num, **kwargs) }} {% else %}#{% endif %}   aria-label= Previous &gt;          &lt;span aria-hidden= true &gt;&amp;laquo;&lt;/span&gt;        &lt;/a&gt;      &lt;/li&gt;      {# 中间页码 #}      {% for p in pagination. iter_pages() %}        {% if p %}          &lt;li {% if p==pagination. page %} class= active  {% endif %} &gt;&lt;a href= {{ url_for(endpoint, page=p, **kwargs) }} &gt;{{ p }}&lt;/a&gt;&lt;/li&gt;        {% else %}          &lt;li&gt;&lt;a href= # &gt;&amp;hellip;&lt;/a&gt;&lt;/li&gt;        {% endif %}      {% endfor %}      {# 下一页的按钮 #}      &lt;li {% if not pagination. has_next %}class= disabled {% endif %}&gt;        &lt;a href= {% if pagination. has_next %}{{ url_for(endpoint, page=pagination. next_num, **kwargs) }}{% else %}#{% endif %}  aria-label= Next &gt;          &lt;span aria-hidden= true &gt;&amp;raquo;&lt;/span&gt;        &lt;/a&gt;      &lt;/li&gt;    &lt;/ul&gt;  &lt;/nav&gt;{% endmacro %}调用宏 app/templates/main/index. html 1234567891011121314151617181920212223242526272829303132333435{% extends 'common/base. html' %}{% from 'common/macro. html' import pagination_show %}{% block title %}  首页{% endblock %}{% block page %}  {{ quick_form(form) }}  {% for p in posts %}    &lt;div class= media &gt;      &lt;div class= media-left &gt;        &lt;a href= # &gt;          &lt;img height= 50 , width= 50  class= media-object  src= {{ url_for('static', filename='upload/'+p. user. icon) }}  alt= icon &gt;        &lt;/a&gt;      &lt;/div&gt;      &lt;div class= media-body &gt;        &lt;div style= float: right &gt;发表于: {{ moment(p. timestamp). fromNow() }}&lt;/div&gt;        &lt;h4 class= media-heading &gt;{{ p. user. username }}&lt;/h4&gt;        {{ p. content }}      &lt;/div&gt;    &lt;/div&gt;  {% endfor %}  {# 展示分页导航条 #}  {{ pagination_show(pagination, 'main. index') }}{% endblock %}{% block scripts %}  {{ super() }}  {{ moment. include_moment() }}  {{ moment. locale('zh-CN') }}{% endblock %}视图函数 app/views/main. py 123456789101112131415161718192021222324252627282930from flask import Blueprint, render_template, flash, redirect, url_for, requestfrom app. forms import PostsFormfrom app. models import Postsfrom flask_login import current_userfrom app. extensions import dbmain = Blueprint('main', __name__)@main. route('/', methods=['POST', 'GET'])def index():  form = PostsForm()  if form. validate_on_submit():    if current_user. is_authenticated:      u = current_user. _get_current_object()      p = Posts(content=form. content. data, user=u)      db. session. add(p)      flash('发表成功')      return redirect(url_for('main. index'))    else:      flash('请先登录再发表')      return redirect(url_for('user. login'))  # 获取页码,默认为1, 强制转换成整型  page = request. args. get('page', 1, type=int)  pagination = Posts. query. filter_by(rid=0). order_by(Posts. timestamp. desc()). paginate(page=page, per_page=5, error_out=False)  posts = pagination. items  # posts = Posts. query. filter_by(rid=0). order_by(Posts. timestamp. desc()). paginate()  # posts = Posts. query. filter_by(rid=0). order_by(Posts. timestamp. desc()). all()  return render_template('main/index. html', form=form, posts=posts, pagination=pagination)练习:  点击博客内容, 跳转到博客详情(博客内容, 所有回复, 发表回复) 点击头像,或者电机用户名, 跳转到用户发表的博客列表页面() 要用分页显示 导航条上增加”我发表的”, 查看我自己发表的所有博客 添加博客收藏功能(多对多的关系, 老师明天讲). 博客收藏功能:重点 - 表的多对多关系: 新建一个表,存储用户和帖子的收藏关系 多对多的例子: 收藏帖子, 学生选课 收藏博客实现步骤: (1) 添加中间关联表: app/models/init. py  db. Table 用于建立多对多关系的中间表 用户收藏帖子关联表, 这张表不需要我们手动维护, 是SQLAchemy 自动维护的1234567from app. extensions import db# 用户收藏帖子关联表, 这张表不需要我们手动维护, 是SQLAchemy 自动维护的collections = db. Table('collections',  db. Column('user_id', db. Integer, db. ForeignKey('users. id')),  db. Column('posts_id', db. Integer, db. ForeignKey('posts. id')))(2) 在users 表中添加 favorites 字段, app/models/user. py  这个字段反向引用 Posts secondry 用于指定当前表 User 和 反向引用表 Posts 的中间表名 backref 用于反向引用, 用于让post 对象也可以拿到users 中的收藏信息.  比如: 获取收藏数: {{ p. abc. count() }}1234567class User(UserMixin, db. Model):	. . .  # 添加收藏功能的反向引用 # secondry 表示中间表 # backref()用于定义一个backrefer对象,并放到 relationship 中使用. 该函数可以接受一个名字和任意多个参数 # 这里的users 可以是任何名字, 可以改成abc favorites = db. relationship('Posts', secondary='collections', backref=db. backref('abc', lazy='dynamic'), lazy='dynamic')(3) 迁移数据 12python3. 7 manage. py db migratepython3. 7 manage. py db upgrade(4) 定义相关函数: 添加收藏, 去雄收藏, 判断是否收藏 app/models/user. py 123456789101112131415161718# 添加收藏def add_favorite(self, pid):  p = Posts. query. get(pid)  self. favorites. append(p) # 这里会直接操作中间表,添加新的数据到collections中# 取消收藏def del_favorite(self, pid):  p = Posts. query. get(pid)  self. favorites. remove(p) # 这里会直接操作中间表,删除指定的数据# 判断是否收藏过def is_favorite(self, pid):  # 获取所有已经收藏的博客  favorites = self. favorites. all()  posts = list(filter(lambda p: p. id == pid, favorites))  if len(posts) &gt; 0:    return True  return False(5)添加收藏按钮, 用 ajax 进行请求 templtes/main/index. html  ajax 请求的优势是局部刷新, 消耗服务器资源小123{% if current_user. is_authenticated %}  &lt;span class= collect  style= cursor: pointer;  url= {{ url_for( 'posts. collect', pid=p. id) }} &gt;{% if current_user. is_favorite(p. id) %}取消收藏{% else %}收藏{% endif %}&lt;/span&gt;{% endif %}(6) 做 ajax 请求 app/templates/main/index. html 12345678910111213141516171819202122{% block scripts %}  {{ super() }}  {{ moment. include_moment() }}  {{ moment. locale('zh-CN') }}  &lt;script&gt;    $(function(){      $('. collect'). click(function () {        // this 在回调函数中没有意义. 需要将this 临时保存在另一个变量中.         _this = this        $. get($(_this). attr('url'), function(){          if ($(_this). text() == '收藏'){            $(_this). text('取消收藏')          }else{            $(_this). text('收藏')          }        })      })    })  &lt;/script&gt;{% endblock %}(7) 添加新的蓝本文件 app/views/posts. py 123456789101112131415161718from flask import Blueprint, jsonifyfrom flask_login import current_userposts = Blueprint('posts', __name__)# 这里需要将pid类型限制成整型, 否则可能找不到@posts. route('/collect/&lt;int:pid&gt;')def collect(pid):  # 判断是否收藏  if current_user. is_favorite(pid):    # 取消收藏    current_user. del_favorite(pid)  else:    # 添加收藏    current_user. add_favorite(pid)  # jsonify 的作用是将python字典转成json 字符串  return jsonify({'result': 'ok'})项目总结::  用户管理     注册   登录注册   信息展示   上传头像   信息修改    博客管理     发表博客   展示博客   分页   回复   收藏   统计显示   RESTFul API 开发: 概念::  RESTFUL: 一种软件架构风格、设计风格，而不是标准，只是提供了一组设计原则和约束条件。它主要用于客户端和服务器交互类的软件。基于这个风格设计的软件可以更简洁，更有层次，更易于实现缓存等机制。  REST: 即表述性状态传递（英文：Representational State Transfer，简称REST）是Roy Fielding博士在2000年他的博士论文中提出来的一种软件架构风格。它是一种针对网络应用的设计和开发方式，可以降低开发的复杂性，提高系统的可伸缩性。  API: 应用程序接口. 符合restful 风格的应用程序接口叫做restful api  Restful的接口都是围绕着资源以及对资源的操作展开的 资源:  网络上存在的任意实体,即使是一条消息也是资源 操作:  所谓的操作就是对资源的CURD.  对网络的任意操作都可以抽象为增删改查  RestFul 对网络资源的操作抽象为 HTTP的各种方法(POST, GET, DELETE,PUT等), 以完成对资源的特定操作.       方法   操作   例子         GET   获取全部资源   http://127. 0. 0. 1:5000/source           获取指定资源   http://127. 0. 0. 1:5000/source/250       POST   创建新的资源   http://127. 0. 0. 1:5000/source       PUT   更新指定资源   http://127. 0. 0. 1:5000/250       DELETE   删除指定资源   http://127. 0. 0. 1:5000/250   数据: JSON  通常传输数据的格式是JSON, 有时也会结合GET传递参数    JSON 指的是 JavaScript 对象表示法（JavaScript Object Notation）  JSON 是轻量级的文本数据交换格式  JSON 独立于语言 *  JSON 具有自我描述性，更易理解  JSON 是存储和交换文本信息的语法。类似 XML。  JSON 比 XML 更小、更快，更易解析。 使用的工具Postman 模拟 ResuFul 请求  说明: postman是一款非常好用的API测试工具, 可以轻松模拟各种请求.  提示: 下载(getpostman. com)安装包, 一路next 原生实现: 准备测试使用的数据和路由 123456789101112131415161718192021222324252627282930313233343536from flask import Flask, jsonify, abortfrom flask_script import Managerapp = Flask(__name__)# 定义测试数据(博客资源)posts = [  {    'id': 1,    'title': 'Python语法',    'content': '听别人说很简单, 但是想用好不容易'  },  {    'id': 2,    'title': 'HTML',    'content': 'H5, CSS, JS, JQuery, bootstrap 都很简单'  },  {    'id': 3,    'title': 'FLask',    'content': 'Flask 各种框架'  }]@app. route('/')def index():  return 'restful api 开发'if __name__ == '__main__':  mgr = Manager(app)  app. config['TEMPLATES_AUTO_RELOAD'] = True  mgr. run()GET请求- 获取所有资源: 1234# 获取所有资源@app. route('/posts')def get_posts_list():  return jsonify({'posts': posts})请求方法 http://127. 0. 0. 1:5000/posts/ GET 请求 - 获取指定资源: 1234567# 获取指定资源@app. route('/posts/&lt;int:pid&gt;')def get_posts(pid):  p = list(filter(lambda p:p['id'] == pid, posts))  if len(p) == 0:    abort(404)  return jsonify({'posts': p[0]})请求方法 http://127. 0. 0. 1:5000/posts/2 POST 创建新的资源: 123456789101112131415# 创建新的资源@app. route('/posts', methods=['POST'])def create_posts():  # 判断是否有json数据传过来, 并判断title 和 content key 是否存在在json对象中  if not request. json or 'title' not in request. json or 'content' not in request. json:    abort(400) # 400 bad request表示传过来的数据格式不正确  # 新建资源  p = {    'id': posts[-1]['id'] + 1,    'title': request. json['title'],    'content': request. json['content']  }  # 保存资源  posts. append(p)  return jsonify({'posts': p}), 201 # 201 代表 created请求方法:  请求地址: http://127. 0. 0. 1:5000/posts  请求body:  1234{   title :  Django ,   content :  Django 各种框架 }    这里有一个坑: 需要双引号来把内容括起来  请求头: Content-Type application/json PUT 修改指定资源: 12345678910@app. route('/posts/&lt;int:pid&gt;', methods=['PUT'])def update_posts(pid):  p = list(filter(lambda p: p['id'] == pid, posts))  if len(p) == 0:    abort(404)  if 'title' in request. json:    p[0]['title'] = request. json['title']  if 'content' in request. json:    p[0]['content'] = request. json['content']  return jsonify({'posts': p[0]}) 请求方法: http://127. 0. 0. 1:5000/posts/1  请求Body:  12345{   id  :  3 ,   title :  改成Django ,   content :  改成Django 各种框架 }  请求头: Content-Type application/json DELETE 删除指定资源: 1234567@app. route('/posts/&lt;int:pid&gt;', methods=['DELETE'])def delete_posts(pid):  p = list(filter(lambda p: p['id'] == pid, posts))  if len(p) == 0:    abort(404)  posts. remove(p[0])  return jsonify({'result': '资源已删除'})请求方法: http://127. 0. 0. 1:5000/posts/3 错误定制: 如果不定制错误, 那么就会返回浏览器默认的错误格式. 返回浏览器指定的错误也有好处, 浏览器返回的错误给出了具体的问题所在, 所以方便调试 12345678910111213# 错误定制. 如果不定制错误, 那么就会返回浏览器默认的错误格式# &lt;!DOCTYPE HTML PUBLIC  -//W3C//DTD HTML 3. 2 Final//EN &gt;# &lt;title&gt;404 Not Found&lt;/title&gt;# &lt;h1&gt;Not Found&lt;/h1&gt;# &lt;p&gt;The requested URL was not found on the server.  If you entered the URL manually please check your spelling and try again. &lt;/p&gt;@app. errorhandler(404)def page_not_found(e):  return jsonify({'error': '页面没找到'}), 404@app. errorhandler(400)def bad_request(e):  return jsonify({'error': '坏请求'}), 400身份认证: 安装 1pip3. 7 install flask-httpauth代码实现: 三个步骤  (1) 在需要认证保护的路由上面加上 @auth. login_required装饰器, 这个装饰器会调用 @auth. verify_password 所定义的验证方法 (2) 定义一个 @auth. verify_password 验证用户名和密码. 这里大多数情况下需要从数据库中匹配用户名密码. 如果验证失败了,会调用 @auth. error_handler 装饰器定义的方法 (3) 定义 @auth. error_handler 方法处理错误 注意: 上面装饰器的开头都是 @auth 开头, 不是@app1234567891011121314151617181920212223# 导入类库from flask_httpauth import HTTPBasicAuth# 创建对象auth = HTTPBasicAuth()# 认证的回调函数@auth. verify_passworddef verify_password(username, password):  # 此处理论上应该查询数据，这里模拟使用  if username == 'Jerry' and password == '123456':    return True  return False# 定制认证错误显示@auth. error_handlerdef unauthorized():  return jsonify({'result': 'Unauthorized Access'}), 403 @app. route('/posts')# 路由保护@auth. login_requireddef get_posts_list():  return jsonify({'posts': posts})使用方法:  postman 在认证中设置用户名密码, 才能通过认证 完整代码: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134from flask import Flask, jsonify, abort, requestfrom flask_script import Managerfrom flask_httpauth import HTTPBasicAuth# 创建认证对象auth = HTTPBasicAuth()# 认证回调函数@auth. verify_passworddef verify_password(username,password):  # 此处是模拟功能, 真正场景需要查询数据库  if username == 'Jerry' and password == '123456':    return True  return False@auth. error_handlerdef unauthorized():  return jsonify({'result': 'unautheried user'})app = Flask(__name__)# 定义测试数据(博客资源)posts = [  {    'id': 1,    'title': 'Python语法',    'content': '听别人说很简单, 但是想用好不容易'  },  {    'id': 2,    'title': 'HTML',    'content': 'H5, CSS, JS, JQuery, bootstrap 都很简单'  },  {    'id': 3,    'title': 'FLask',    'content': 'Flask 各种框架'  }]# 错误定制. 如果不定制错误, 那么就会返回浏览器默认的错误格式# 返回浏览器指定的错误也有好处, 浏览器返回的错误给出了具体的问题所在, 所以方便调试# &lt;!DOCTYPE HTML PUBLIC  -//W3C//DTD HTML 3. 2 Final//EN &gt;# &lt;title&gt;404 Not Found&lt;/title&gt;# &lt;h1&gt;Not Found&lt;/h1&gt;# &lt;p&gt;The requested URL was not found on the server.  If you entered the URL manually please check your spelling and try again. &lt;/p&gt;@app. errorhandler(404)def page_not_found(e):  return jsonify({'error': 'page not found'}), 404@app. errorhandler(400)def bad_request(e):  return jsonify({'error': 'bad request'}), 400# 获取所有资源@app. route('/posts')# # 路由保护@auth. login_required # 这里有一个认证保护的例子def get_posts_list():  return jsonify({'posts': posts})# 获取指定资源@app. route('/posts/&lt;int:pid&gt;')def get_posts(pid):  p = list(filter(lambda p:p['id'] == pid, posts))  if len(p) == 0:    abort(404)  return jsonify({'posts': p[0]})# 创建新的资源@app. route('/posts', methods=['POST'])def create_posts():  # 判断是否有json数据传过来, 并判断title 和 content key 是否存在在json对象中  app. logger. info(str(request. json))  if not request. json or 'title' not in request. json or 'content' not in request. json:    abort(400) # 400 bad request表示传过来的数据格式不正确  # 新建资源  p = {    'id': posts[-1]['id'] + 1,    'title': request. json['title'],    'content': request. json['content']  }  # 保存资源  posts. append(p)  return jsonify({'posts': p}), 201 # 201 代表 created# 修改指定的资源@app. route('/posts/&lt;int:pid&gt;', methods=['PUT'])def update_posts(pid):  p = list(filter(lambda p: p['id'] == pid, posts))  if len(p) == 0:    abort(404)  if 'title' in request. json:    p[0]['title'] = request. json['title']  if 'content' in request. json:    p[0]['content'] = request. json['content']  return jsonify({'posts': p[0]})# 删除指定资源@app. route('/posts/&lt;int:pid&gt;', methods=['DELETE'])def delete_posts(pid):  p = list(filter(lambda p: p['id'] == pid, posts))  if len(p) == 0:    abort(404)  posts. remove(p[0])  return jsonify({'result': '数据已删除'})@app. route('/')def index():  return 'restful api 开发'if __name__ == '__main__':  mgr = Manager(app)  app. config['TEMPLATES_AUTO_RELOAD'] = True  mgr. run()flask-restful 实现RestFul: 安装:: 1pip3. 7 install flask-restful完整代码: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394from flask import Flask, jsonify, gfrom flask_script import Managerfrom flask_httpauth import HTTPBasicAuth# 导入类库from flask_restful import Api, Resourcefrom itsdangerous import TimedJSONWebSignatureSerializer as Serializerauth = HTTPBasicAuth()@auth. verify_passworddef verify_password(username_or_token, password):  if username_or_token == 'Jerry' and password == '123456':    g. username = username_or_token    return True  # 如果是token, 解密token  s = Serializer(app. config['SECRET_KEY'])  try:    data = s. loads(username_or_token)    g. username = data['username']    return True  except:    return False@auth. error_handlerdef unauthorized():  return jsonify({'result': 'unauthorized access'}), 403app = Flask(__name__)app. config['SECRET_KEY'] = '123456'manager = Manager(app)# 创建对象api = Api(app)# 创建一个资源类，一个资源的完整操作一般需要两个资源类# 需要写一个类继承 Resource, 一个资源的完整操作需要定义两个类, 一个负责带参数, 另一个负责不带参数class UserAPI(Resource):  # 添加认证, 只要写一行, 对类中的所有方法都有效  decorators = [auth. login_required]  # 定义和restful 请求方法同名的方法  def get(self, uid):    # return {'User': 'GET'}    return {'User': g. username}  def put(self, uid):    return {'User': 'PUT'}  def delete(self, uid):    return {'User': 'DELETE'}class UserListAPI(Resource):  # 添加认证, 只要写一行, 对类中的所有方法都有效  decorators = [auth. login_required]  def get(self):    return {'UserList': 'GET'}  def post(self):    return {'UserList': 'POST'}# 添加资源，第一个参数是类名, 第二个参数是路由地址, 第三个参数是其他可以进来的路由. api. add_resource(UserAPI, '/users/&lt;int:uid&gt;', '/u/&lt;int:uid&gt;')api. add_resource(UserListAPI, '/users')# 若不是在创建Api对象时设置app，那么init_app调用要放在添加资源之后# 如果需要将创建API对象和初始化app分离, 那么初始化app时一定要将初始化代码放在创建的restful类的后面# api. init_app(app)#api. init_app(app)@app. route('/')@auth. login_requireddef index():  return 'flask-restful'# 生成一段乱码发给客户, 乱码中带有token@app. route('/get_token')@auth. login_requireddef get_token():  s = Serializer(app. config['SECRET_KEY'], expires_in=3600)  return s. dumps({'username': g. username})if __name__ == '__main__':  manager. run()请求方法:  (1). 首先在postman中输入用户名密码, 然后请求 http://127. 0. 0. 1:5000/get_token , GET 方法就可以.  返回的body中会带回来一个字符串, 类似于: eyJhbGciOiJIUzI1NiIsImlhdCI6MTUxNTY2ODgxMCwiZXhwIjoxNTE1NjcyNDEwfQ. eyJ1c2VybmFtZSI6IkplcnJ5In0. uBn42cOTVW9mEjFrpdRA3qZXj-5rIhMv6mhlwcLcya8  (2). 将字符串放在 postman 用户名里, 密码留空, 然后再请求  http://127. 0. 0. 1:5000 Flask 项目部署: Web 工作原理  客户端(浏览器) &lt;=&gt; web服务端(nginx) &lt;=&gt; WSGI &lt;=&gt; Python(Flask) &lt;=&gt; 数据库  WSGI 是一个 PYthon 通用标准协议, uWSGI 是一个实现  Fast-CGI 是 PHP 通用标准协议 nginx 安装: 123456# Nginx 安装yum install -y nginxsed -i 's/listen    80 default_server/listen    8081 default_server/g' /etc/nginx/nginx. confsed -i 's/listen    \[::\]:80 default_server/listen    \[::\]:8081 default_server/g' /etc/nginx/nginx. confsystemctl start nginx. servicesystemctl enable nginx. servicenginx 配置文件和web 目录  查看启动状态: systemctl status nginx web 目录: /usr/share/nginx/html 配置文件: /etc/nginx/nginx. conf(1)配置Nginx 虚拟主机:    /etc/nginx/nginx. conf 最后一个大括号前面添加   1include vhost/*. conf;      创建vhost目录   1mkdir /etc/nginx/vhost      在该目录下建立配置文件 www. blog. com. conf   12345678910server {    listen 8082;    server_name www. blog. com blog. com;    location / {        root html/blog;        index index. html;    }}      创建虚拟主机目录和文件   12mkdir /usr/share/nginx/html/blog echo 'hello blog' &gt; /usr/share/nginx/html/blog       做本地域名劫持 vim /etc/hosts, 增加一行. 前面的IP 是自己在局域网的IP   110. 11. 56. 210 www. blog. com   (2)添加博客内容: 在blog文件夹下建立一个 test. py 文件 1234567891011121314from flask import Flaskfrom flask_script import Managerapp = Flask(__name__)@app. route('/')def index():    return 'welcome flask'if __name == '__main__':    mgr = Manager(app)    app. config['TEMPLATES_AUTO_RELOAD']=True    app. run()运行服务, 并在客户端访问 1python3. 7 test. py runserver -r -d -h0. 0. 0. 0(3)安装 uWSGI: 说明: uWSGI 是实现了WSGI协议的应用程序 安装运行后就不需要在用 python3. 6 test. py -r -d -h0. 0. 0. 0 来启动服务了 123456yum install uwsgisystemctl start uwsgi# 另一种方法# pip3. 7 install uwsgi# ln -s /usr/local/python3. 7/bin/uwsgi /usr/bin/uwsgi配置参数::  更多参数 uwsgi –help12345678http		# 采用http协议socket		# 使用 socketwsgi-file 	# 指定接收到的数据交给那个文件处理 manage. pycallable	# 指定交给那个对象来处理 appchdir 		# 网站根目录 /usr/share/nginx/html/blogdaemonize	# 后台启动, 需要指定一个日志文件processes	# 指定进程数threads		# 指定线程数. 总的线程数是进程数 * 线程数WSGI 以 http 启动方法:: 12cd /usr/share/nginx/html/bloguwsgi --http 10. 11. 56. 210:5000 --wsgi-file test. py --callable app然后到浏览器中访问: http://10. 11. 56. 210:5000 WSGI 以 socket 启动:: 1vim /etc/nginx/vhost/www. blog. com. conf123456789101112server {    listen 8082;    server_name www. blog. com blog. com;    location / {    #    root html/blog;    #    index index. html;        include uwsgi_params;    # 包含需要的参数        uwsgi_pass 127. 0. 0. 1:5000;  # 指定的转发地址(socket 协议)    }}启动 12systemctl restart nginxuwsgi --socket 127. 0. 0. 1:5000 --wsgi-file test. py --callable app为了简化配置命令, 也可以写uwsgi配置文件 , 在blog目录下创建uwsgi. ini 123456[uwsgi]socket = 127. 0. 0. 1:5000wsgi-file = test. pycallable = appdaemonize = /var/log/uwsgi. log# 如果用多线程, 需要制定threads 参数再次启动 1uwsgi uwsgi. ini静态文件处理: 修改nginx 配置, 让静态内容不进行转发 vim /etc/nginx/vhost/www. blog. com. conf 12345678910111213141516server {    listen 8082;    server_name www. blog. com blog. com;    location /static { 				# 如果网址路径匹配到 /static         root html/blog/;		# 那么就获取 web服务器 目录的html/blog 的内容        #alias /html/blog/statics/    }    location / {    #    root html/blog;    #    index index. html;        include uwsgi_params;        uwsgi_pass 127. 0. 0. 1:5000;    }} 如果多个路由可以匹配到进来的path, nginx采用最大前缀匹配原则,流程回顾:: (1) 浏览器获取输入的www. blog. com(2) /etc/hosts 劫持了这个域名, 进行本机域名解析, ip 为 10. 11. 56. 210(3) nginx 开始查找这个域名如何处理, 检索 /etc/nginx/nginx. conf 文件(4) nginx 在 文件尾部找到 include vhost/*. conf; 而后在/etc/nginx/vhost 目录下找到www. blog. com. conf 文件(5) www. blog. com. conf 文件找到 www. blog. com 的处理方法.  监听8082端口的信息 将处理www. blog. com 的请求.  路由/static 将内容转发到 html/blog/下面的内容返回.  路由/  将内容转发给uwsgi , 转发的地址是127. 0. 0. 1:5000123456789101112131415server {    listen 8082;    server_name www. blog. com blog. com;    location /static {        root html/blog/;    }    location / {    #    root html/blog;    #    index index. html;        include uwsgi_params; # 带上 /etc/nginx/uwsgi_params 文件中的参数        uwsgi_pass 127. 0. 0. 1:5000;    }}(6) uwsgi --socket 127. 0. 0. 1:5000 --wsgi-file test. py --callable app , 启动的 uwsgi 服务正在监听 127. 0. 0. 1:5000, 将接收到的信息发给 test. py 程序, 并呼叫app 这个程序来处理. (7) 接下来就是flask 的逻辑以及返回了. "
    }, {
    "id": 29,
    "url": "http://localhost:4000/Python-Crowler-In-Actrion/",
    "title": "Python Crowler In Action",
    "body": "2018/09/24 - 网络爬网是一种强大的技术，通过查找一个或多个域的所有 URL 来从网络收集数据。Python有几个流行的网络爬虫库和框架。 在本文中，我们将介绍不同的框架, 抓取策略和用例。使用 Python 从头开始构建简单的网络爬虫, 并告诉你为什么最好使用像 Scrapy 这样的网络爬虫框架。 爬虫[TOC] 课程大纲内容:  用到的Python 库     urllib. request   urllib. parse   requests    解析内容:     正则表达式   xpath   bs4   jsonpath    采集动态内容     selenium+phantomjs         网页中有些内容是动态加载的, 比如通过 js 或者 ajax 加载产生的数据     因为如果只对静态内容爬取,可能导致获取内容不全, 所以需要模拟浏览器的请求, 获取动态内容部分.            scrapy 框架     高性能的异步网络爬虫框架    分布式爬虫     scrapy-redis 组件, 通过多台电脑, 同时爬取数据, 结合redis 数据库存储数据    反爬虫     某些数据比较敏感, 不希望爬虫爬取的方法.  是爬虫和反爬虫的博弈   手段:         User-Agent: 浏览器的标志信息, 会通过请求头传递给服务器端, 用于说明访问网站的客户端信息. 服务端会检查UA是否合法. 不合法就会禁止爬虫访问     代理: 服务端通过检查请求客户端IP 来确定是否是爬虫. 解决方法是去找代理开改变自己的IP地址.      验证码访问: 服务端通过验证码来判断访问者是人还是爬虫. 如果想解决, 就可能需要半自动的方式 – 手工输入验证码, 然后继续爬取     验证加密: 服务端将内容加密, 客户端就无法读取内容          爬虫概览: 核心: 数据分析: 难点: 爬虫和反爬虫的博弈: 爬虫相关开发语言: Python, Java, PHP. (Python 更灵活, 有 scrapy 框架): 爬虫分类:: 一 , 通用爬虫: 比如百度, 360, google, sougou等搜索引: 访问网页-&gt;抓取数据-&gt;数据存储-&gt;数据处理-&gt;提供检索服务 爬取方式:  给定起始URL, 放入爬取队列.  从队列中获取URL 对象, 开始爬取 分析网页, 获取该网页内所有URL, 再加入队列, 继续重复这第2部操作如何让百度爬取到你的网页  主动提交到搜索引擎 在其他网站设置友情链接 百度和CDN服务商合作, 只要有域名, 就会被百度爬取如何限制爬虫抓取你的网站    robots. txt 可以进行限制, 比如 https://www. taobao. com/robots. txt   User-agent: BaiduspiderAllow: /articleAllow: /oshtmlDisallow: /product/Disallow: /User-Agent: GooglebotAllow: /articleAllow: /oshtmlAllow: /productAllow: /spuAllow: /dianpuAllow: /overseaAllow: /listDisallow: /User-agent: BingbotAllow: /articleAllow: /oshtmlAllow: /productAllow: /spuAllow: /dianpuAllow: /overseaAllow: /listDisallow: /User-Agent: 360SpiderAllow: /articleAllow: /oshtmlDisallow: /User-Agent: YisouspiderAllow: /articleAllow: /oshtmlDisallow: /User-Agent: SogouspiderAllow: /articleAllow: /oshtmlAllow: /productDisallow: /User-Agent: Yahoo! SlurpAllow: /productAllow: /spuAllow: /dianpuAllow: /overseaAllow: /listDisallow: /User-Agent: *Disallow: / SEO , SEM 排名: 通过 pagerank 排名 和 付费排名 二, 专项爬虫: 根据自己的需求, 实现爬虫程序, 抓取需要的数据: 网页特征: 1. http, https , 2. html 内容, 3. 有独立的url 地址 爬取步骤: 确定html -&gt; 模拟浏览器访问URL, 获取网页的html 内容 -&gt; 解析html 字符串, 按规则获取数据 请求头信息::  accept:text/html,application/xhtml+xml,application/xml;q=0. 9,image/webp,image/apng,/;q=0. 8 可接受的请求类型 request url: https://www. baidu. com/ 请求地址 request method: Get/ Post status code:200 ok , 服务端响应码 remote address: 服务器IP地址+端口号 referer: 前一个页面地址 accept-encoding: gzip 内容压缩方式, 浏览器会自动解压缩, 我们爬虫不需要设置这个 accept-language: 可接受的请求语言 cookie: BAIDU 可接受的cookie 信息(用户不敏感信息, 用户名, session id 等)响应头信息::  connection: keep-alive 连接方式长连接 content-encoding: gzip 内容压缩方式, 浏览器会自动解压缩, 我们爬虫不需要设置这个 content-type: text/html;charset=utf-8 响应文件内容类型, 可以是 json , 可以是 html, xml date: 服务器日期 server: 服务器版本 set-cookie: 设置cookie值使用工具 - fiddler:  安装方法: http://www. cocoachina. com/apple/20170704/19729. html     运行命令: sudo mono --arch=32 /Applications/fiddler-mac/Fiddler. exe    Fiddler     右侧 inspectors 按钮: 查看主要请求和响应的信息内容. 如果内容未解码, 点击黄色长条按钮就可以解码   右侧 Statistics 面板: 查看统计信息   Urllib 库: 是python 提供的 网页获取库 urllib. request 负责请求数据:  read() 读取全部信息. b’…. ’ 表示为二进制信息  readline()  按行读取, 返回一个列表  getcode() 获取状态码  geturl() 获取url  getheaders() 获取头信息 urllib. parse 用于处理获取的数据 读取URL urllib. request. urlopen(), resp. read(), resp. geturl(), resp. getcode(), resp. getheaders(): 1234567891011121314151617import urllib. requestimport sslssl. _create_default_https_context = ssl. _create_unverified_contexturl='https://www. baidu. com'# 尽量保证运行代码时不要开启fiddlerresp = urllib. request. urlopen(url=url)print(type(resp))print(resp. getcode())print(resp. geturl())print(resp. getheaders())# print(resp. read())        # 读取全部信息. b'. . . . ' 表示为二进制信息print(resp. read(). decode('utf-8')) # 解码utf-8保存读取内容到文件 urllib. request. urlretrieve(): 1urllib. request. urlretrieve(url=url, filename='baidu. com')URL 中汉字参数的处理 urllib. parse. urlencode(): 1234567891011121314# 汉字不能作为URL 内容, 必须先进行编码和解码# http://tool. chinaz. com/Tools/urlencode. aspxurl2 = 'https://www. baidu. com/s?'source = {  'wd':'美女'}url3 = urllib. parse. urlencode(source)url = url2+url3resp = urllib. request. urlopen(url)print(url)print(resp. getcode())模拟浏览器请求 urllib. request. Request(): Request() 可以帮我们定制 请求的 headers 将User-Agent 放到请求中 更多user-agent 12345678910111213# 模拟user-agent http://blog. csdn. net/tao_627/article/details/42297443# 使用 urllib. open# safari 浏览器# Mozilla/5. 0 (Windows NT 6. 1; WOW64) AppleWebKit/534. 57. 2 (KHTML, like Gecko) Version/5. 1. 7 Safari/534. 57. 2headers = {  'User-Agent': 'Mozilla/5. 0 (Windows NT 6. 1; WOW64) AppleWebKit/534. 57. 2 (KHTML, like Gecko) Version/5. 1. 7 Safari/534. 57. 2',}req = urllib. request. Request(url = 'http://www. baidu. com/', headers=headers)resp = urllib. request. urlopen(req)print(resp. getcode())print(resp. read(). decode('utf-8'))POST 请求: POST 请求需要通过代码方式将参数带入请求中. 案例: 百度翻译 fanyi. baidu. com  第一步: 抓包分析百度翻译, 查看网址有没有跟搜索内容相关的单词显示.  猜测是GET 还是 POST 请求 如果是GET, 青花瓷 Querystring 中会带参数, 如果是POST, 青花瓷 form 可以找到所有参数123456789101112131415161718192021222324252627282930313233343536373839404142434445import urllib. requestimport urllib. parseimport sslssl. _create_default_https_context = ssl. _create_unverified_context# 通过抓包工具解析到URLpost_url = 'http://fanyi. baidu. com/sug/'# 通过抓包工具找到参数data = {  'kw':'baby'}# 配置post 参数, 需要把data 进行encode 转换. # urlencode 先转汉字, encode 再转字符data = urllib. parse. urlencode(data). encode('utf-8')headers = {  'User-Agent': 'Mozilla/5. 0 (Windows NT 6. 1; WOW64) AppleWebKit/534. 57. 2 (KHTML, like Gecko) Version/5. 1. 7 Safari/534. 57. 2',}req = urllib. request. Request(url=post_url, data=data, headers=headers)resp = urllib. request. urlopen(req)# print(resp. getcode())# print(resp. read(). decode('utf-8'))# 最后将结果放在网页的json 读取器里面就可以正常显示返回的中文内容了# 转换成 JSONimport jsonjson_obj= json. loads(resp. read(). decode('utf-8'))answer = json. dumps(json_obj,ensure_ascii=False)with open('fanyi. json','w',encoding='utf-8') as fp:  fp. write(answer)AJAX 请求: 案例: 豆瓣电影排行榜 GET 需求: 由用户指定页码, 进行抓取豆瓣电影动作排行榜的电影信息. 步骤: 抓包工具分析请求页面的AJAX 请求 信息 和响应信息 12345678910111213141516171819202122232425262728293031323334353637383940# 豆瓣电影分析import urllib. requestimport urllib. parseimport sslssl. _create_default_https_context = ssl. _create_unverified_contexturl1 = 'https://movie. douban. com/j/chart/top_list?type=5&amp;interval_id=100%3A90&amp;action=&amp;start=0&amp;limit=1'url2 = 'https://movie. douban. com/j/chart/top_list?type=5&amp;interval_id=100%3A90&amp;action=&amp;start=0&amp;limit=20'base_url = 'https://movie. douban. com/j/chart/top_list?type=5&amp;interval_id=100%3A90&amp;action=&amp;limit=20&amp;'def main():  start_page = int(input('请输入要查看的页码'))  data = {    'start':(start_page-1)*20  }  data = urllib. parse. urlencode(data)  url = base_url+data  download_message(url)  print(url)def download_message(url):  headers = {    'User-Agent': 'Mozilla/5. 0 (Windows NT 6. 1; WOW64) AppleWebKit/534. 57. 2 (KHTML, like Gecko) Version/5. 1. 7 Safari/534. 57. 2',  }  req = urllib. request. Request(url=url, headers=headers)  resp = urllib. request. urlopen(req)  content = str(resp. read(). decode('utf-8'))  with open('douban. json','w',encoding='utf-8') as fp:    fp. write(content)if __name__ == '__main__':  main()案例: 肯德基餐厅查询 POST 需求: 分析点击页码后的ajax 请求 , 查看 抓包工具的 X-Requested-With: XMLHttpRequest 12345678910111213141516171819202122232425262728293031# 肯德基餐厅分析import urllib. requestimport urllib. parseimport sslssl. _create_default_https_context = ssl. _create_unverified_contextpost_url = 'http://www. kfc. com. cn/kfccda/ashx/GetStoreList. ashx?op=cname'page = 10data = {  'cname':'上海',  'pageIndex':	'4',  'pageSize':	'10',}data = urllib. parse. urlencode(data). encode('utf-8')headers = {  'User-Agent': 'Mozilla/5. 0 (Windows NT 6. 1; WOW64) AppleWebKit/534. 57. 2 (KHTML, like Gecko) Version/5. 1. 7 Safari/534. 57. 2',}req = urllib. request. Request(url=post_url, data=data, headers=headers)resp = urllib. request. urlopen(req)print(resp. getcode())print(resp. read(). decode('utf-8'))案例: 由用户录入贴吧的名字, 并输入起始页和结束页, 将获取到的信息保存到文件中, 每页为一个html 文件. 1234567891011121314151617181920212223242526272829303132# 获取百度贴吧信息 -- 科幻电影吧# 由用户录入贴吧的名字, 并输入起始页和结束页, 将获取到的信息保存到文件中, 每页为一个html 文件. import urllib. requestimport urllib. parseimport sslssl. _create_default_https_context = ssl. _create_unverified_contextdef download_message(url,pn):  urllib. request. urlretrieve(url, str(pn)+'tieba. html')bar_name = input('请输入贴吧名称: ')page_start = input('请输入起始页码:')page_end  = input('请输入结束页码:')pns = []for i in range(int(page_end)-int(page_start)):  pns. append((int(page_start)+i)*50)post_url = 'http://tieba. baidu. com/f?&amp;ie=utf-8&amp;'for pn in pns:  data = {    'pn':pn,    'kw':bar_name,  }  data = urllib. parse. urlencode(data)  url = post_url + data  # print(url)  download_message(url,pn)URLError / HTTPError: 处理网页爬取过程中的异常情况 1234567891011121314151617181920212223242526# HTTP 异常处理import urllib. requestimport urllib. error as errorurl = 'http://sh. meituan. com/meishi123/'headers = {  'User-Agent': 'Mozilla/5. 0 (Windows NT 6. 1; WOW64) AppleWebKit/534. 57. 2 (KHTML, like Gecko) Version/5. 1. 7 Safari/534. 57. 2',}req = urllib. request. Request(url=url, headers=headers)try:  resp = urllib. request. urlopen(req)except error. HTTPError as e:  print(1)  print(e)except error. URLError as e:  print(2)  print(e)except Exception as e:  print(3)  print(e)Cookie 处理: 模拟网站对于一些数据在浏览器的存储操作 比如网站登录后, 会通过cookie 记住用户登录状态. 爬虫可以制造cookie 模仿登录状态. handler  HTTPHandler CookieHandler – 定制cookie ProxyHandler – 定制代理使用 handler 访问数据 123456789101112131415161718192021222324252627282930# Cookie 处理import urllib. requestimport urllib. parseimport sslssl. _create_default_https_context = ssl. _create_unverified_contexturl = 'https://www. baidu. com'# 创建 handler 对象handler = urllib. request. HTTPHandler()# 构建 opener 对象 , 可以取代 urllib. request. urlopen()openner = urllib. request. build_opener(handler)# 定制请求对象headers = {'User-Agent': 'Mozilla/5. 0 (Windows NT 6. 1; WOW64) AppleWebKit/534. 57. 2 (KHTML, like Gecko) Version/5. 1. 7 Safari/534. 57. 2',}req = urllib. request. Request(url=url, headers=headers)# 使用 opener 发送请求resp = openner. open(req)print(resp. read(). decode('utf-8'))定制 cookie对象 1234567891011121314151617181920212223242526272829303132333435# Cookie 处理import urllib. requestimport urllib. parseimport http. cookiejarimport sslssl. _create_default_https_context = ssl. _create_unverified_context# 构建cookie对象, 这个对象可以帮助我们保存服务器想浏览器cookie 写入的所有信息cookie = http. cookiejar. CookieJar()# 使用cookie对象来构建 handler对象handler = urllib. request. HTTPCookieProcessor(cookie)# 使用 handler 构建 opener 对象opener = urllib. request. build_opener(handler)# 定制请求对象post_url = 'http://www. renren. com/ajaxLogin/1=1&amp;uniqueTimestamp=2018221554272'data = {}headers = {  'User-Agent': 'Mozilla/5. 0 (Windows NT 6. 1; WOW64) AppleWebKit/534. 57. 2 (KHTML, like Gecko) Version/5. 1. 7 Safari/534. 57. 2',}req = urllib. request. Request(url=post_url,headers=headers, data=data)resp = opener. open(req). read(). decode('utf-8')print('登录成功')get_url = 'http://www. renren. com/224549540/profile'req1 = urllib. request. Request(url=get_url,headers=headers)resp1 = opener. open(req1)print(resp1. read(). decode('utf-8'))代理服务器: 概念: 通过代理去爬取数据, 伪装自己的IP. 代理常用功能:  突破自身IP瓶颈, 访问外国站点 访问一些单位或团体内部资源 提高访问速度 隐藏真实IP配置方法:  本地浏览器配置代理 代码配置代理国内主要免费的代理: 西次代理 主要的两种匿名代理:  透明代理 – 告诉服务器我是代理, 但是不告诉你我的真实IP 高匿代理 – 带个面具, 不告诉你我是代理, 也让你不知道我的真实IP. 查看自己IP的方法: 百度中搜索 IP Mac 设置代理的方法: 设置后, 在百度搜索IP 查看地址是否变化, 变了代理就成功了. 用代码配置爬虫代理: 1234567891011121314151617181920212223242526# 为爬虫设置代理import urllib. request as requestimport urllib. parse as parseimport sslssl. _create_default_https_context = ssl. _create_unverified_context# 构建代理ProxyHandler, 该对象帮我们使用代理来访问服务器handler = request. ProxyHandler(proxies={'http':'27. 197. 109. 149:8118'})openner = request. build_opener(handler)url = 'https://www. baidu. com/s?wd=ip'headers = {  'User-Agent': 'Mozilla/5. 0 (Windows NT 6. 1; WOW64) AppleWebKit/534. 57. 2 (KHTML, like Gecko) Version/5. 1. 7 Safari/534. 57. 2',}req = request. Request(url=url, headers=headers)resp = openner. open(req)print(resp. read(). decode('utf-8'))# 结果可以看到代理用正则表达式爬取数据: 案例: 糗事百科图片抓取 步骤    先在网页中拿几张图片的html 标签, 比较查看一下规律   1&lt;img src= //pic. qiushibaike. com/system/pictures/12011/120116488/medium/app120116488. jpg  alt= 糗事#120116488  class= illustration  width= 100%  height= auto &gt;    ​  123456789101112131415161718192021222324252627282930313233343536373839404142import reimport urllib. requestimport urllib. parseimport sslssl. _create_default_https_context = ssl. _create_unverified_contextstr = '''&lt;div class= thumb &gt;&lt;a href= /article/120116488  target= _blank &gt;&lt;img src= //pic. qiushibaike. com/system/pictures/12011/120116488/medium/app120116488. jpg  alt= 糗事#120116488  class= illustration  width= 100%  height= auto &gt;&lt;/a&gt;&lt;/div&gt;'''url = 'https://www. qiushibaike. com/'headers = {  'User-Agent': 'Mozilla/5. 0 (Windows NT 6. 1; WOW64) AppleWebKit/534. 57. 2 (KHTML, like Gecko) Version/5. 1. 7 Safari/534. 57. 2',}req = urllib. request. Request(url=url, headers=headers)resp = urllib. request. urlopen(req)pt = re. compile(r'&lt;div class= thumb &gt;. *?&lt;img src= (. *?)  alt=. *?&lt;/div&gt;',re. S)src_list = pt. findall(resp. read(). decode('utf-8'))print(src_list)# 下载图片到本地目录中print('开始下载图片. . . ')for src in src_list:  url = 'http:'+src  img_name = src. split('/')[-1]  file_path= 'qiubai/' + img_name  urllib. request. urlretrieve(url, file_path)  print(img_name+'下载完成')print('全部图片下载完成')XPath: w3scholl文档 html 是一种特殊的XML XPath 用 xml 的方式获取 html 中的内容路径, 并抓取到需要的数据.  节点 node : xml 中的一个标签的整段内容, 可能包含父节点,子节点, 同胞节点等 属性 : 文本:XPath 基本语法       表达式   描述         bookstore   查找此节点的所有子节点.        /bookstore   查找根节点节点是bookstore的标签直接子节点       //bookstore   查找所有是 bookstore 的标签       //@lang   查找所有包含 lang 的属性的值       //title[@lang]   查找包含lang 属性的 title 节点       //title[@lang=’en’]   查找包含lang 属性, 且属性值是 en 的 title 节点       /bookstore/book[‘price&gt;35. 00’]   在bookstore元素的直接子节点中查找price大于35的所有 book 元素,       //book/title | //book/price   查找所有book下面的title 和 price 直接子标签       //title | //price   查找所有title 和 price 节点.        //div[@id=’head’ and @class=’s_down’]   查找所有 id=head, 并且 class=s_down 的所有 div 节点       //div[contains(@id, ‘he’)]   查找属性包含 he 的所有 div 标签       //div[starts-with(@id,’he’)]   查找属性以 he 开头的所有 div 标签       //div[ends-with(@id,’he’)]   查找属性以 he 结尾的所有 div 标签       //div/h1/text()   查找所有div下直接子节点是 h1 的内容.    Xpath 扩展工具的安装和使用:  去 chrome store 下载 xpath. crx 文件, 并安装到谷歌浏览器中 control + shift +x 快捷键 打开xparth 插件 按住shift 键 + 移动鼠标, 就可以查看网页元素的xpath 了 可以在插件的 query 输入框中编辑自动生成的xpath语句安装 lxml 库: 1pip3. 6 install lxmletree. parse() 和 etree. HTML() 可以让我们对网页转成 xpath 树状数据, 让我们可以通过 xpath 语法进行解析 12etree. parse('xx. html')etree. HTML(resp. read(). decode('utf-8'))案例一: 爬取本地古诗html test. html 123456789101112131415161718192021222324252627282930&lt;!DOCTYPE html&gt;&lt;html lang= en &gt;&lt;head&gt;	&lt;title&gt;Document&lt;/title&gt;&lt;/head&gt;&lt;body&gt;	&lt;h1&gt;这是一个无聊至极的网页&lt;/h1&gt;	&lt;div class= gushi &gt;		&lt;ul&gt;			&lt;li id= first &gt;桃花坞里桃花庵&lt;/li&gt;			&lt;li class= haha &gt;桃花庵里桃花仙&lt;/li&gt;			&lt;li class= hehe &gt;桃花仙人种桃树&lt;/li&gt;			&lt;li class= heihei &gt;再卖桃花换酒钱&lt;/li&gt;		&lt;/ul&gt;		&lt;div id= author  class= tang &gt;			&lt;a href= http://www. baidu. com &gt;唐伯虎&lt;/a&gt;		&lt;/div&gt;	&lt;/div&gt;	&lt;ol&gt;		&lt;li class= go &gt;世间只有你最好&lt;/li&gt;		&lt;li class= love &gt;有一种喜欢叫那个下午阳光很好，而你正好穿了一件白衬衫&lt;/li&gt;		&lt;li class= cray &gt;45°也无法抹去我心中的创伤&lt;/li&gt;		&lt;li&gt;夏至殇&lt;/li&gt;		&lt;li class= iii &gt;天才是百分之百的灵感&lt;/li&gt;	&lt;/ol&gt;	&lt;div&gt;		&lt;p&gt;这里再没有下文了&lt;/p&gt;	&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;123456789101112131415161718192021222324252627# XPathfrom lxml import etree# 获取本地HTML 文件, 创建一个tree 对象# 是一个属性结构, 支持使用 xpath 进行路径定位html_tree = etree. parse('test. html')# 返回一个符合路径定位的资源列表, 是一个 list# 参数就是xpath 路径# 查找古诗所有语句gushi1 = html_tree. xpath('//div[@class= gushi ]/ul/li/text()')# 查找语句gushi2 = html_tree. xpath('//div[@class= gushi ]/ul/li[@class= haha ]/text() ')# 查找作者author = html_tree. xpath('//div[@class= gushi ]/div[@id= author ]/a/text()')# 查找作者链接link = html_tree. xpath('//div[@class= gushi ]/div[@id= author ]/a/@href')# 找所有带有 class 的 li 标签的内容lis = html_tree. xpath('//li[@class]/text()')print(gushi1)print(gushi2)print(author)print(link)print(lis)案例二: 站长素材图片抓取 需求: 将制定网址的图片下载, 并保存到制定文件夹, 以图片标题名为文件名 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960import urllib. requestimport urllib. parsefrom lxml import etreeimport os# 图片路径xpath_src = '//div[@id= container ]/div/div/a/img/@src2'# 标题路径xpath_alt = '//div[@id= container ]/div/div/a/@alt'# 请求数据base_url = 'http://sc. chinaz. com/tupian/shuaigetupian'# http://sc. chinaz. com/tupian/shuaigetupian_2. htmldef conection_with_url(url):	headers = {		'User-Agent': 'Mozilla/5. 0 (Windows NT 10. 0; WOW64) AppleWebKit/537. 36 (KHTML, like Gecko) Chrome/65. 0. 3325. 181 Safari/537. 36'	}	request = urllib. request. Request(url=url,headers=headers)	response = urllib. request. urlopen(request)	return responsedef parse_data(response,page):	html_string = response. read(). decode('utf-8')	# 加载为tree对象	html_tree = etree. HTML(html_string)	# 使用xpath解析	alt_list = html_tree. xpath(xpath_alt)	src_list = html_tree. xpath(xpath_src)	print('开始下载第%d页'%page)	for index in range(len(alt_list)):		src = src_list[index]		suffix = os. path. splitext(src)[-1]		img_name = alt_list[index]		img_path = 'shuaige/' + img_name + suffix		urllib. request. urlretrieve(src,img_path)		print(img_name+'下载完成')	print('第%d页下载完成'%page)def main():	start_page = int(input('请输入起始页：'))	end_page = int(input('请输入结束页：'))	for page in range(start_page,end_page+1):		if page == 1:			url = base_url + '. html'		else:			url = base_url + '_' + str(page) + '. html'		# 请求数据		response = conection_with_url(url)		# 解析并保存数据		parse_data(response,page)	print('全部下载结束')if __name__ == '__main__':	main()案例三: 内涵段子用户头像抓取 12345678910111213141516171819202122232425262728293031323334import urllib. requestimport urllib. parsefrom lxml import etreesrc_xpath = '//div[@class= content ]/ul[@id= detail-list ]/li/div/div/a/img/@data-src'name_xpath = '//div[@class= content ]/ul[@id= detail-list ]/li/div/div/a/div/span[@class= name ]/text()'url = 'http://neihanshequ. com/'headers = {	'User-Agent': 'Mozilla/5. 0 (Windows NT 10. 0; WOW64) AppleWebKit/537. 36 (KHTML, like Gecko) Chrome/65. 0. 3325. 181 Safari/537. 36'}request = urllib. request. Request(url=url,headers=headers)response = urllib. request. urlopen(request)html_string = response. read(). decode('utf-8')html_tree = etree. HTML(html_string)src_list = html_tree. xpath(src_xpath)name_list = html_tree. xpath(name_xpath)print(len(src_list))print(len(name_list))for i in range(len(src_list)):	url = src_list[i]	if url. endswith('. jpg') == False:		file_path = 'neihan/' + name_list[i] + '. jpg'	else:		file_path = 'neihan/' + str(i) + name_list[i] + '. jpg'	urllib. request. urlretrieve(url,file_path)Beautiful soap 爬取网页: 需求: 通过制定的职位关键字, 城市, 页码来爬取智联招聘的网站的职位信息, 并将爬取的数据写成json格式保存到文件中. bs4 是对Xpath 的封装 安装: 1pip3. 7 install bs4常用函数  find() 返回一个对象     find('a') 返回第一个找到的a 标签   find('a', title='xxx')   find('a', class_='xxx')    find_all() 返回一个列表     find_all('a') 返回所有a 标签   find_all(['a','span']) 返回所有 a 标签和 span 标签   find_all('a', limit=2) 返回前两个 a 标签    select() 根据选择器得到节点对象 , 推荐常用属性  contents 返回子孙节点的列表 descendants 返回子孙节点的生成器选择器的语法       选择器   将匹配         soup. select(‘div’)   所有名为&lt;div&gt;的元素       soup. select(‘#author’)   带有 id 属性为 author 的元素       soup. select(‘. notice’)   所有使用 CSS class 属性名为 notice 的元素       soup. select(‘div span’)   所有在&lt;div&gt;元素之内的&lt;span&gt;元素       soup. select(‘div &gt; span’)   所有直接在&lt;div&gt;元素之内的&lt;span&gt;元素，中间没有其他元素       soup. select(‘input[name]’)   所有名为&lt;input&gt;，并有一个 name 属性，其值无所谓的元素       soup. select(‘input[type=”button”]’)   所有名为&lt;input&gt;，并有一个 type 属性，其值为 button 的元素   案例一: 1234567891011121314151617181920212223242526272829303132&lt;!DOCTYPE html&gt;&lt;html lang= en &gt;&lt;head&gt;	&lt;title&gt;bs4 测试文档&lt;/title&gt;&lt;/head&gt;&lt;body&gt;	&lt;div&gt;		&lt;h1&gt;金庸武侠杀人技能鉴赏&lt;/h1&gt;		&lt;div class= content &gt;			&lt;ul id= wugongmiji &gt;				&lt;li title= first  class= kuihua &gt;葵花宝典&lt;/li&gt;				&lt;!-- &lt;li name= jianfa  id= bixie &gt;&lt;!-- 辟邪剑法 --&gt;&lt;/li&gt; --&gt;				&lt;li age= 29  class= jiuyang &gt;九阳神功&lt;/li&gt;				&lt;li id= shengong  class= jiuyin &gt;九阴真经&lt;/li&gt;				&lt;li class= xixing  title= dafa &gt;吸星大法&lt;/li&gt;			&lt;/ul&gt;			&lt;a href= http://www. baidu. com &gt;百度一下&lt;/a&gt;		&lt;/div&gt;		&lt;ul&gt;			&lt;p&gt;&lt;b&gt; 你所熟悉的武侠人物 &lt;/b&gt;&lt;/p&gt;			&lt;li&gt;张无忌&lt;/li&gt;			&lt;li&gt;尹志平&lt;/li&gt;			&lt;li&gt;令狐冲&lt;/li&gt;			&lt;li&gt;郭靖&lt;/li&gt;			&lt;li&gt;穆念慈&lt;/li&gt;		&lt;/ul&gt;		&lt;a href= http://www. 360. com &gt;&lt;/a&gt;	&lt;/div&gt;	&lt;/body&gt;&lt;/html&gt;123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778from bs4 import BeautifulSoupimport bs4# 打开本地html文档，加载成BeatifullSoup对象soup = BeautifulSoup(open('bstext. html',encoding='utf-8'),'lxml')print(type(soup))# # 获取单个节点-------------- # # 只能找到第一个# print(soup. li)# # 查找第一个li标签# print(soup. find('li'))# # 查找title=dafa的li标签,属性名字不需要单双引号# print(soup. find('li',title='dafa'))# # 查找name属性,error，不能查找name属性# # print(soup. find('li',name='jianfa'))# # 查找class属性等于jiuyang的li标签，注意class后面有下划线_# print(soup. find('li',class_='jiuyang'))# # 获取多个节点--------------# print(soup. find_all('a'))# # 注意什么是节点，节点里有什么# print(soup. find_all('ul'))# # 查找所有的a和h1标签# print(soup. find_all(['a','h1']))# # 如果获取到的标签节点仍然存在子节点，依然可以使用如上方法获取子节点# ul_node = soup. find('ul')# print(type(ul_node))# print(ul_node. find('li'))# # 限定find_all查找的个数# print(soup. find_all('li',limit=3))# # 使用CSS选择器定位节点 -------------# # 注意：选择器返回的是一个列表对象# # 使用标签名访问节点# print(soup. select('h1')[0])# # 使用class选择节点 . 等价于 class=# print(soup. select('. jiuyang'))# # 使用id选择节点 # 等价于 id=# print(soup. select('#shengong'))# # 使用属性查找 age=29# print(soup. select('[age= 29 ]'))# # 使用CSS选择器匹配路径# print(soup. select('. content #wugongmiji . jiuyang'))# 节点的类型# bs4. BeautifulSoup 根节点类型# bs4. element. NavigableString 连接类型# bs4. element. Tag 节点类型# bs4. element. Comment 注释类型# 查找子孙节点ul_list = soup. find('div',class_= content ). descendantsfor ul in ul_list:	print(type(ul),str(ul))	# if type(ul) == bs4. element. Tag:	# 	print(ul)# # 获取节点内容 -----------------------# node = soup. select('. content #wugongmiji . jiuyang')[0]# # 使用string# print(node. string)# # 使用get_text()# print(node. get_text())# # 使用attrs获取节点所有属性，返回 一个字典对象# print(node. attrs)# # 获取节点的单一属性值# print(node. attrs. get('age'))# print(node. get('age'))# print(node['age'])案例二: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879from bs4 import BeautifulSoupimport urllib. request as requestimport urllib. parse as parseimport jsondef request_data(location, keyword, page):  handler = request. HTTPHandler() # 定制handler  opener = request. build_opener(handler) #  url = 'http://sou. zhaopin. com/jobs/searchresult. ashx?'  # url = 'http://sou. zhaopin. com/jobs/searchresult. ashx?jl=%E4%B8%8A%E6%B5%B7&amp;kw=%E7%88%AC%E8%99%AB&amp;p=1'  data = {    'jl':location,    'kw':keyword,    'p':page,  }  data = parse. urlencode(data)  url = url+data  print(url)  headers = {    'User-Agent': 'Mozilla/5. 0 (Windows NT 6. 1; WOW64) AppleWebKit/534. 57. 2 (KHTML, like Gecko) Version/5. 1. 7 Safari/534. 57. 2',  }  req = request. Request(url=url, headers=headers)  resp = opener. open(req)  return resp# print(resp. read(). decode('utf-8'))# 使用 bs4 加载网页信息# 解析数据def parse_data(resp):  soup = BeautifulSoup(resp. read(). decode('utf-8'), 'lxml')  print(type(soup))  # 在网页中先看源代码, 观察到需要找的职位的div 的 class 名字是 newlist_list  table_list = soup. select('. newlist_list &gt; . newlist_list_content table. newlist')  # print(len(table_list))  datalist=[]  for node in table_list[1:]:    # print(type(node))    zwmc = node. select('. zwmc div a')[0]. get_text()    # print(zwmc)    gsmc = node. select('. gsmc a')[0]. get_text()    # print(gsmc)    # zwyx = node. find_all('td', class_='zwyx')[0]. get_text()    zwyx = node. select('. zwyx')[0]. get_text()    # print(zwyx)    gzdd = node. select('. gzdd')[0]. get_text()    # print(gzdd)    dic = {'职位名称':zwmc,'公司名称':gsmc,'职位月薪':zwyx,'工作地点':gzdd}    datalist. append(dic)  return datalist# 存储数据def save_data(data):  json. dump(data,open('text. json','w',encoding='utf-8'),ensure_ascii=False)def main():  lo = input('城市')  kw = input('请输入关键字')  pn = input('请输入页码')  resp = request_data(lo, kw, pn)  data = parse_data(resp)  save_data(data)if __name__ == '__main__':  main()selenium 模仿浏览器操作行为: 模拟浏览器, 可以执行网页中的js, 实现动态加载. 可以做一些代码无法实现的功能 安装方法: 第一步: 1pip3. 7 install selenium第二步: 下载chrome 内核文件 chromedriver http://blog. csdn. net/huilan_same/article/details/51896672 http://chromedriver. storage. googleapis. com/index. html 第三步: 写代码进行测试: 12345678910111213141516171819202122232425262728293031# Selenium 模拟浏览器行为from selenium import webdriverimport time# 谷歌浏览器内核路径(全局路径,)# 注意, 绝对路径需要用 r''path = '/Users/dalong/code/python1702/spider/chromedriver'# 创建浏览器驱动对象# 传入的参数也需要是谷歌浏览器的驱动browser = webdriver. Chrome(path)url = 'http://www. baidu. com'browser. get(url)time. sleep(3)# 查找百度首页的输入框input_box = browser. find_element_by_id('kw')# 输入关键词input_box. send_keys('美女')# 点击按钮btn = browser. find_element_by_id('su')btn. click()time. sleep(3)# 退出# browser. quit()phantomJS 配合 selenium 进行无界面浏览器的模拟浏览: phantomJS 是一个无界面浏览器 支持网页元素查找, js的执行等 由于不进行css, 和 UI 渲染, 所以执行效率更高 phantomJS 已经停止更新了, 新的代替品马上出来. 安装: http://phantomjs. org/download. html 案例一: 模拟百度输入关键词, 点击按钮后将搜索结果截图 1234567891011121314151617181920212223242526272829from selenium import webdriverimport timepath = '/Users/dalong/code/python1702/spider/phantomjs'browser = webdriver. PhantomJS(path)url = 'http://www. baidu. com'browser. get(url)time. sleep(2)# 截图browser. save_screenshot('baidu. png')# # 查找百度首页的输入框input_box = browser. find_element_by_id('kw')# 输入关键词input_box. send_keys('美女')# 点击按钮btn = browser. find_element_by_id('su')# print(btn)btn. submit()time. sleep(5)# # 截图browser. save_screenshot('girl. png')browser. quit()      Selenium 查找网页元素的方法如下:   例子:         find_element_by_id           find_elements_by_name           find_elements_by_xpath           find_elements_by_tag_name           find_elements_by_class_name           find_elements_by_css_selector   browser. find_elements_by_css_selector(‘#su’)[0]       find_elements_by_link_text       案例二: 将百度首页的源码获取, 并保存在本地 12345678910111213141516171819from selenium import webdriverimport timepath = '/Users/dalong/code/python1702/spider/phantomjs'browser = webdriver. PhantomJS(path)url = 'http://www. baidu. com'browser. get(url)time. sleep(2)browser. save_screenshot('bd. png') # 截图# page_source 可以拿到网页源代码, 相当于urllib. request. urlopen()print(browser. page_source)with open('baidu. html','w',encoding='utf-8') as fp:  fp. write(browser. page_source)browser. quit()案例三: 模拟ajax自动滚动效果查看今日头条 12345678910111213141516171819202122232425# 今日头条 ajax 请求from selenium import webdriverimport timefrom bs4 import BeautifulSoupfrom lxml import etreeimport sslssl. _create_default_https_context = ssl. _create_unverified_contextpath = '/Users/dalong/code/python1702/spider/phantomjs'browser = webdriver. PhantomJS(path)url = 'https://www. toutiao. com/'time. sleep(5)browser. get(url)browser. save_screenshot('jinri1. png')# 模仿滚动, 再来张截图js = 'document. body. scrollTop=10000' # scrollTop=10000 是往下滚10000个像素点browser. execute_script(js)time. sleep(5)browser. save_screenshot('jinri2. png')browser. quit()jsonPath: 更多教程 和 xpath 类似的用法 安装: pip install jsonpath json 对象的转换:  json. loads() json. dumps() json. load() json. dump()todo 代码: 1#requests 库: requests 可以认为是 urllib. request 的封装, 提供了简洁易用的API 功能更加完善 使用更多的是 requests 安装: pip3. 6 install requests 官方文档 案例一: 通过 requests 获取基本网页信息 123456789101112131415161718192021222324252627282930# 使用 requests 模块import requestsurl = 'https://www. baidu. com'# 配置请求头信息headers = {  'User-Agent': 'Mozilla/5. 0 (Windows NT 6. 1; WOW64) AppleWebKit/534. 57. 2 (KHTML, like Gecko) Version/5. 1. 7 Safari/534. 57. 2',}resp = requests. get(url, headers=headers)# 获取编码集print(resp. encoding)# 指定编码集resp. encoding = 'utf-8'print(resp. encoding)# 获取源代码结果print(resp. text)# 获取二进制字节类型源代码print(resp. content)# 获取URLprint(resp. url)# 获取状态码print(resp. status_code)# 获取响应头信息print(resp. headers)案例二: GET 方法传递多个参数 1234567891011121314151617181920212223242526# 使用 requests 模块import requestsurl = 'https://www. baidu. com/?'data = {  'wd': '北京'}# 配置请求头信息headers = {  'User-Agent': 'Mozilla/5. 0 (Windows NT 6. 1; WOW64) AppleWebKit/534. 57. 2 (KHTML, like Gecko) Version/5. 1. 7 Safari/534. 57. 2',}# 注意, requests 中如果要给url增加多个参数, 使用 paramsresp = requests. get(url=url, params=data, headers=headers)# 指定编码集resp. encoding = 'utf-8'print(resp. text)# 获取源代码结果with open('baidu2. html', 'w', encoding='utf-8') as fp:  fp. write(resp. text)案例三: requests 模拟 POST 请求 12345678910111213141516171819# 学习 requests 的 POST 请求import requestsurl = 'https://www. bing. com/ttranslationlookup?&amp;IG=4AF03420760D45238FD255B397E7AFC3&amp;IID=translator. 5035. 6'data = {  'text' : 'dancer',  'from': 'en',  'to': 'zh-CHS'}headers = {  'User-Agent': 'Mozilla/5. 0 (Windows NT 6. 1; WOW64) AppleWebKit/534. 57. 2 (KHTML, like Gecko) Version/5. 1. 7 Safari/534. 57. 2',}resp = requests. post(url=url, data=data, headers=headers)print(resp. text)案例四: requests 使用代理爬取数据 12345678910111213141516171819202122232425# 定制proxyimport requestsurl='http://www. baidu. com/s?'data={  'wd':'ip'}headers = {  'User-Agent': 'Mozilla/5. 0 (Windows NT 6. 1; WOW64) AppleWebKit/534. 57. 2 (KHTML, like Gecko) Version/5. 1. 7 Safari/534. 57. 2',}# 注意: 这里如果是http, 那么 url 中的地址也需要是http# 不要一边是http, 另一边是 httpsproxies={  'http': '124. 88. 84. 154:8080',}# 在参数中配置proxy 参数, 注意参数写法resp = requests. get(url=url, params=data, headers=headers, proxies=proxies)print(resp. status_code)print(resp. text. find('124. 88. 84. 154'))案例五: 用 requests 处理 session cookie , 模拟登录 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748import requestsimport sslssl. _create_default_https_context = ssl. _create_unverified_context# 案例五: 用 requests 处理 cookie, 模拟登陆. # www. quanshuwang. com# 用户名:dancerpython# 密码: *****# 首页first_page_url = 'http://www. quanshuwang. com/book_269. html'# 登录login_url = 'http://www. qianshuwang. com/login. php?do=submit'# 登录成功后查看的收藏地址favorite_url = 'http://www. quanshuwang. com/modules/article/bookcase. php'headers = {  'User-Agent': 'Mozilla/5. 0 (Windows NT 6. 1; WOW64) AppleWebKit/534. 57. 2 (KHTML, like Gecko) Version/5. 1. 7 Safari/534. 57. 2',}# 数据来自抓包工具data = {  'username': 'dancerpython',  'password': '123457',  'action': 'login',}# 构建session 对象, 该对象可以自动保存于服务器的对话内容, 包括了 cookie信息session = requests. Session()# session 对象可以直对任何页面发起请求, 无论get还是post, session都可以得到会话内容. # session. get()# session. post()# 第一步, 登录收藏地址, 看看是否可以正常访问. resp = session. get(url=favorite_url, headers=headers)resp. encoding = 'gbk'print( 请输入用户名  in resp. text) # 返回True, 因为还未登录, 所以结果是需要进行登录# 第二步: 获取cookie, 模拟登录resp = session. post(url=login_url, headers=headers, data=data)resp. encoding = 'gbk'print(resp. text)案例六: 模拟笑话网站的登录 123456789101112131415161718192021222324252627282930import requestslog_url = 'http://www. jokeji. cn/user/c. asp?'member_center= 'http://www. jokeji. cn/User/MemberCenter. asp'data = {  'u':'dancerpython',  'p':'123457abc',  't':'big'}headers = {  'User-Agent': 'Mozilla/5. 0 (Windows NT 6. 1; WOW64) AppleWebKit/534. 57. 2 (KHTML, like Gecko) Version/5. 1. 7 Safari/534. 57. 2',}# 模拟登录resp = requests. get(url=log_url, params=data, headers = headers)print(resp. text)# 登录后普通访问无效member_resp = requests. get(url=member_center, headers = headers)print(member_resp. text)# 用 session 对象访问就可以了session = requests. Session()m = session. get(url=member_center, headers = headers)print(m. text)案例七: 古诗文网 模拟验证码的登录 打码平台 可以提供自动识别验证码的服务, 是付费服务. 用户名: dalong_co 密码: ** 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152import requestsfrom bs4 import BeautifulSoupimport urllib. requestbase_url = 'http://so. gushiwen. org'favorite_url= 'http://so. gushiwen. org/user/login. aspx?form=http://so. gushiwen. org/user/collect. aspx'headers = {  'User-Agent': 'Mozilla/5. 0 (Windows NT 6. 1; WOW64) AppleWebKit/534. 57. 2 (KHTML, like Gecko) Version/5. 1. 7 Safari/534. 57. 2',}# 先从favorite_url 获取验证码图片s = requests. Session()r = s. get(url=favorite_url, headers=headers)soup = BeautifulSoup(r. text, 'lxml')src = soup. select('#imgCode')[0]. attrs. get('src')# 验证码图片的全路径img_url = base_url+src# 下载图片urllib. request. urlretrieve(img_url, 'yanzheng. png')# 人工输入图片中的验证码yanzheng_code = input('请输入验证码数字')login_url = 'http://so. gushiwen. org/user/login. aspx?from=http%3a%2f%2fwww. gushiwen. org%2fdefault. aspx'a = soup. select('#__VIEWSTATE')[0]. attrs. get('value')b = soup. select('#__VIEWSTATEGENERATOR')[0]. attrs. get('value')print(a)print(b)print(yanzheng_code)data = {  '__VIEWSTATE':a,  '__VIEWSTATEGENERATOR':b,  'from':'http://so. gushiwen. org/',  'email':'37016175@qq. com',  'pwd':'ldl5506',  'code':yanzheng_code,  'denglu':'登录',}headers = {  'User-Agent': 'Mozilla/5. 0 (Macintosh; Intel Mac OS X 10_13_3) AppleWebKit/604. 5. 6 (KHTML, like Gecko) Version/11. 0. 3 Safari/604. 5. 6',}ret = s. post(url=login_url, headers=headers, data=data)print(ret. text)案例: 内涵段子抓取 12345678910111213141516171819# 内涵段子 数据抓取 (xpath 配合 requests)import requestsfrom lxml import etree# 获取网页内容url = 'http://neihanshequ. com/'headers = {  'User-Agent': 'Mozilla/5. 0 (Macintosh; Intel Mac OS X 10_13_3) AppleWebKit/604. 5. 6 (KHTML, like Gecko) Version/11. 0. 3 Safari/604. 5. 6',}resp = requests. get(url=url, headers=headers)# 通过xpath 获取要抓取的数据xpath = '//ul[@id= detail-list ]/li//span[@class= name ]/text()'tree = etree. HTML(resp. text)name_list = tree. xpath(xpath)print(name_list)Scrapy: 官网 中文文档 概念与安装: Scrapy 是一个为了怕去网站数据, 提取结构性数据而编写的框架. 安装:    pip3. 6 install scrapy     安装好后就可以在命令行使用 scrapy 命令了.     命令使用方法:   1234567891011121314151617181920212223# scrapy --helpScrapy 1. 5. 0 - project: FirstSpiUsage: scrapy &lt;command&gt; [options] [args]Available commands: bench     Run quick benchmark test check     Check spider contracts crawl     Run a spider edit     Edit spider fetch     Fetch a URL using the Scrapy downloader genspider   Generate new spider using pre-defined templates list     List available spiders parse     Parse URL (using its spider) and print the results runspider   Run a self-contained spider (without creating a project) settings   Get settings values shell     Interactive scraping console startproject Create new project version    Print Scrapy version view     Open URL in browser, as seen by ScrapyUse  scrapy &lt;command&gt; -h  to see more info about a command   框架组成:  引擎 下载器 spiders ( 我们自己的代码主要放在这里 ) 调度器 管道( Item Pipeline)工作原理: 创建项目:  cd 到 一个目录 scrapy startproject FirstSpi 就生成了一个项目目录 目录结构如下:创建爬虫命令:    scrapy genspider qiubai  www. qiushibaike. com    注意, 爬虫名字不能和项目名字重名.  命令执行后, 会生成一个新的目录 spiders , 目录中会有qiubai. py 文件 接下来的代码编写主要围绕 qiubai. py 和 items. py 进行开始爬取 糗百: scrapy crawl qiubai  代码写完后用就可以用爬取命令进行数据抓取.  修改 qiubai. py 文件中的 parse 函数 , 然后重复执行上面的命令导出文件命令  scrapy crawl qiubai -o qiubai. csv实战一: 糗事百科, 拿作者头像, 名称和内容: 修改 items. py 文件 1234567891011121314151617181920# -*- coding: utf-8 -*-# Define here the models for your scraped items## See documentation in:# https://doc. scrapy. org/en/latest/topics/items. htmlimport scrapy# FirstspiItem 是一个字典类型, 需要用字典的方式访问这个类的数据class FirstspiItem(scrapy. Item):  # define the fields for your item here like:  # name = scrapy. Field()  # 创建三个变量, 分别保存用户名和头像的图片链接和文本内容  name = scrapy. Field()  img = scrapy. Field()  content = scrapy. Field()修改 qiubai. py 文件 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253# -*- coding: utf-8 -*-import scrapy# 这个文件就是 spider 文件. 是被 scrapy genspider qiubai  www. qiushibaike. com  创建出来的# 此文件中除了parse 之外, 其他的预设的函数名, 类名, 变量名都不要修改. class QiubaiSpider(scrapy. Spider):  # 爬虫名字, 运行爬虫命令时需要用到  name = 'qiubai'  # 允许爬取的域名列表  allowed_domains = ['www. qiushibaike. com']  # 起始的爬取URLs, 由spiders 主动提交给引擎开始爬取.   start_urls = ['http://www. qiushibaike. com/']  # 解析函数, 是一个回调函数.   # 该解析函数必须返回一个可迭代对象, 不写不会报错, 但无法导出数据为文件  def parse(self, response):    # print( ============= )    # print(type(response))    # print(response. xpath())    # print( ============= )    # response. xpath 获取到的并不是etree 一样的数据,而是一个 Selector 对象    # Selector 对象 可以使用 extract 方法进行解析, 获取到里面的数据    # Selector 对象也可以使用 xpath 函数    div_list = response. xpath('//div[starts-with(@class,  author )]')    print( ============= )    # items = []    for div in div_list:      item = {}      # extract 函数可以对selector 的列表执行, 也可以对单个selector 执行.       # 获取图片url和作者名      name = div. xpath('. //h2/text()'). extract()[0]      name = name. strip('\r\n')      # print(name)      img = div. xpath('. //img/@src'). extract()[0]      # print(img)      content = div. xpath('. . //a/div[@class= content ]/span[1]/text()'). extract()[0]      content = content. strip('\r\n')      # item字典的数据来自items. py 中的属性定义      item['name'] = name      item['img'] = img      item['content'] = content      # 使用 yield 可以优化内存开销, 建议使用 yield 代替 return       yield item      # items. append(item)    # print(items)    # print( ============= )    # return items执行 scrapy crawl qiubai -o qiubai. csv 后会在目录中生成 qiubai. csv 文件, 也可以导出 json 或 xml 文件 123scrapy crawl qiubai -o qiubai. csvscrapy crawl qiubai -o qiubai. xmlscrapy crawl qiubai -o qiubai. jsonScrapy Shell 介绍: Scrapy 的命令行工具, 可以快速获取想要的爬取内容 查看百度首页 1234mkdir ScrapyShellcd ScrapyShell/scrapy shell  www. baidu. com 进入ipython 命令行 用 xpath 获取数据1234response. text # 查看百度首页所有返回的文本response. xpath('//div[@id= lg ]/img/@src') # 查看百度logo 的xpath 元素response. xpath('//div[@id= lg ]/img/@src'). extract_first() # 获取百度 logo的 图片地址查看视频首页的视频来源 1scrapy shell  365yg. com 12response. text # 查看视频首页所有返回的文本response. xpath('//li[starts-with(@class, item )]//a[@target= _blank ]/text()'). extract()用response. css 定位数据1scrapy shell  https://sou. zhaopin. com/jobs/searchresult. ashx?jl=%E4%B8%8A%E6%B5%B7&amp;kw=python&amp;sm=0&amp;p=1 ::text 是拿内容 ::attr(“”) 是拿属性 123response. css('table[class= newlist ] td[class= zwmc ]&gt;div&gt;a::text'). extract()response. css('table[class= newlist ] td[class= zwmc ]&gt;div&gt;a::attr( href )'). extract()pipelines. py 的作用: 12345678910111213141516171819202122232425262728293031# -*- coding: utf-8 -*-# Define your item pipelines here## Don't forget to add your pipeline to the ITEM_PIPELINES setting# See: https://doc. scrapy. org/en/latest/topics/item-pipeline. html# pipelines 用于处理及存储数据# 生命周期函数, 就是构造函数和解构函数等# 如果需要让 pipeline 生效, 必须开启 settings. py 中的  ITEM_PIPELINES  开关class FirstspiPipeline(object):  # process_item 是必须实现的方法  # 每次获取到数据, 都会自动调用这个方法, 在此处对数据进行加工处理  def process_item(self, item, spider):    return item  # 可选方法  # 开始执行spider 程序时, 自动触发这个方法. 相当于构造函数, 该函数只会被执行一次  # 一般用于初始化数据库, 打开文件等  def open_spider(self,spider):    pass  # 可选方法  # 爬虫结束时, 自动调用. 只会被调用一次  # 解构函数, 用于关闭数据库, 关闭文件等.   def colose_spider(self,spider):    pass实例分析 – 抓取百度首页的文字: 第一步: 创建项目和爬虫 12scrapy startproject baiduscrapy genspider homepage  www. baidu. com 第二步: 修改配置文件settings. py, homepage. py, pipelines. py, items. py settings. py 12345678910111213141516171819202122BOT_NAME = 'baidu'SPIDER_MODULES = ['baidu. spiders']NEWSPIDER_MODULE = 'baidu. spiders'# Crawl responsibly by identifying yourself (and your website) on the user-agentUSER_AGENT = 'Mozilla/5. 0 (Macintosh; Intel Mac OS X 10_13_3) AppleWebKit/604. 5. 6 (KHTML, like Gecko) Version/11. 0. 3 Safari/604. 5. 6'# Obey robots. txt rulesROBOTSTXT_OBEY = False# Configure item pipelines# See https://doc. scrapy. org/en/latest/topics/item-pipeline. htmlITEM_PIPELINES = {  'baidu. pipelines. BaiduPipeline': 300,}homepage. py 12345678910111213141516171819# -*- coding: utf-8 -*-import scrapyclass HomepageSpider(scrapy. Spider):  name = 'homepage'  allowed_domains = ['www. baidu. com']  start_urls = ['http://www. baidu. com/']  def parse(self, response):    item = {}    name = response. xpath('//map//@title'). extract()    print('=========')    print(name)    print('=========')    item['name'] = name    # 必须返回一个字典, 否则pipeline 无法获取输入数据    yield itempipelines. py 123456789101112131415161718192021222324252627# -*- coding: utf-8 -*-# Define your item pipelines here## Don't forget to add your pipeline to the ITEM_PIPELINES setting# See: https://doc. scrapy. org/en/latest/topics/item-pipeline. htmlclass BaiduPipeline(object):  # process_item 是必须实现的方法  # 每次获取到数据, 都会自动调用这个方法, 在此处对数据进行加工处理  def process_item(self, item, spider):    print( =====process_item====== )    return item  # 可选方法  # 开始执行spider 程序时, 自动触发这个方法. 相当于构造函数, 该函数只会被执行一次  # 一般用于初始化数据库, 打开文件等  def open_spider(self,spider):    print('==========open_spider========')  # 可选方法  # 爬虫结束时, 自动调用. 只会被调用一次  # 解构函数, 用于关闭数据库, 关闭文件等.   def close_spider(self,spider):    print('+++++++colose_spider+++++++')items. py 123456789import scrapyclass BaiduItem(scrapy. Item):  # define the fields for your item here like:  name = scrapy. Field()  # pass第三步: 执行爬取命令 1scrapy crawl homepage案例二: 抓取电影信息: 创建项目和爬虫 123scrapy startproject filmProjectcd filmProjectscrapy genspider film  www. id97. com/movie 修改基本信息 settings. py 1234567891011121314151617181920212223242526BOT_NAME = 'filmProject'SPIDER_MODULES = ['filmProject. spiders']NEWSPIDER_MODULE = 'filmProject. spiders'# Crawl responsibly by identifying yourself (and your website) on the user-agentUSER_AGENT = 'Mozilla/5. 0 (Macintosh; Intel Mac OS X 10_13_3) AppleWebKit/604. 5. 6 (KHTML, like Gecko) Version/11. 0. 3 Safari/604. 5. 6'# Obey robots. txt rulesROBOTSTXT_OBEY = FalseDEFAULT_REQUEST_HEADERS = { 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0. 9,*/*;q=0. 8', # 'Accept-Language': 'en',}ITEM_PIPELINES = {  'filmProject. pipelines. FilmprojectPipeline': 300,}items. py 12345678910import scrapyclass FilmprojectItem(scrapy. Item):  # define the fields for your item here like:  name = scrapy. Field()  img_url = scrapy. Field()  score = scrapy. Field()  director = scrapy. Field()  # passfilm. py 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253import scrapyclass FilmSpider(scrapy. Spider):  name = 'film'  allowed_domains = ['www. id97. com/movie']  start_urls = ['http://www. id97. com/movie/?page='+str(x) for x in range(1,5)]  print(start_urls, ================= )  def parse(self, response):    # 电影名 //div[contains(@class, movie-item )]//a[@style]/@title    # 图片 //div[contains(@class, movie-item )]//img/@data-original    # 评分 //div[contains(@class, movie-item )]//div[@class= meta ]//em/text()    # div_list = response. xpath('//div[contains(@class, movie-item-in )]')    name = response. xpath('//div[contains(@class, movie-item-in )]//a[@style]/@title'). extract()    img_url = response. xpath('//div[contains(@class, movie-item-in )]//img/@data-original'). extract()    score = response. xpath('//div[contains(@class, movie-item-in )]//div[@class= meta ]//em/text()'). extract()    detail_url = response. xpath('//div[contains(@class, movie-item-in )]/a/@href'). extract()    print(detail_url)    for i in range(len(name)):      # 也可以使用 items中的类创建对象, 用字典对象也没有问题.       item = {}      item['name'] = name[i]      item['img_url'] = img_url[i]      item['score'] = score[i]      # 导演名字在详情页, 需要去详情页抓取      # 返回一个request给引擎, 这个request负责请求电影详情页的导演信息. cllback 函数负责导演名字的解析      # meta参数, 是用来在此函数和自定义函数 parse_info 之间传递参数, 以字典形式设置      # call back 函数在引擎获得下载信息后被触发      yield scrapy. Request(url=detail_url[i], callback=self. parse_director, meta={'item': item}, dont_filter=True)      # 将数据返回给 pipelines. py      # print(item)      # yield item    # # 解析一页后, 把下一页再传给引擎    # next_url = 'http://www. id97. com/movie/' + '?page' + str(self. page)    # self. page = self. page+1    # yield scrapy. Request(url=next_url, callback=self. parse)  def parse_director(self, response):    # 先从 response 中获取返回的item 对象    item = response. meta['item']    # response. text 是详情页的源代码    # 所以就可以用详情页的解析逻辑来解析导演名称    director = response. xpath('//table/tbody/tr[1]/td[2]/a/text()'). extract_first()    # 拿了导演姓名, 就可以把信息写入 item 对象中了    item['director'] = director    # 到此, item 就完整了.     yield item创建图片下载目录, 并执行爬虫 12mkdir imgsscrapy crawl film链接提取器 Link Extractors: 文档 作用是在页面原地阿妈中提取链接, 既 a 标签的内容 每个LinkExtractor有唯一的公共方法是 extract_links ,它接收一个 Response 对象,并返回一个 scrapy. link. Link 对象｡Link Extractors,要实例化一次并且 extract_links 方法会根据不同的response调用多次提取链接｡ 导入方法: from scrapy. linkextractors import LinkExtractor 创建对象 link = LinkExtractor(提取规则) 提取规则参数: (建议使用 xpath )  allow 正则 deny 不提取正则的规则 restrict_xpaths 用 xpath 提取链接 restrict_css 用 css 选择器提取链接返回的提取链接 links. extract_links(response) 例子: 电影天堂 提取国内电影的链接 使用 scrapy shell 进行爬去 12scrapy shell  http://www. ygdy8. net/html/gndy/china/index. html 123456789101112131415161718192021from scrapy. linkextractors import LinkExtractor# 使用正则提取链接linkor = LinkExtractor(allow=r'list_4_\d+?. html')linkor. extract_links(response)# 输入结果如是一个链接的数组:# 使用xpath 提取链接# 注意: xpaths 只需要提取到a 标签外层的元素即可. 就可以获得所有的链接links = LinkExtractor(restrict_xpaths='//div[@class= x ]')links. extract_links(response)# 输出结果和上面相同# 使用 css 访问链接linksl = LinkExtractor(restrict_css='. x')linksl. extract_links(response)# 输出结果同上输出结果 1234567[Link(url='http://www. ygdy8. net/html/gndy/china/list_4_2. html', text='[2]', fragment='', nofollow=False), Link(url='http://www. ygdy8. net/html/gndy/china/list_4_3. html', text='[3]', fragment='', nofollow=False), Link(url='http://www. ygdy8. net/html/gndy/china/list_4_4. html', text='[4]', fragment='', nofollow=False), Link(url='http://www. ygdy8. net/html/gndy/china/list_4_5. html', text='[5]', fragment='', nofollow=False), Link(url='http://www. ygdy8. net/html/gndy/china/list_4_6. html', text='[6]', fragment='', nofollow=False), Link(url='http://www. ygdy8. net/html/gndy/china/list_4_7. html', text='[7]', fragment='', nofollow=False), Link(url='http://www. ygdy8. net/html/gndy/china/list_4_101. html', text='末页', fragment='', nofollow=False)] 案例二: 读书网爬取 创建项目: 123scrapy startproject dushuProjectcd dushuProjectscrapy genspider -t crawl read  www. dushu. com/book/1081. html 修改settings. py 123456789101112131415161718192021222324252627282930BOT_NAME = 'dushuProject'SPIDER_MODULES = ['dushuProject. spiders']NEWSPIDER_MODULE = 'dushuProject. spiders'# Crawl responsibly by identifying yourself (and your website) on the user-agentUSER_AGENT = 'Mozilla/5. 0 (Macintosh; Intel Mac OS X 10_13_3) AppleWebKit/604. 5. 6 (KHTML, like Gecko) Version/11. 0. 3 Safari/604. 5. 6'# Obey robots. txt rulesROBOTSTXT_OBEY = False# Override the default request headers:DEFAULT_REQUEST_HEADERS = { 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0. 9,*/*;q=0. 8', 'Accept-Language': 'en',}# Configure item pipelines# See https://doc. scrapy. org/en/latest/topics/item-pipeline. htmlITEM_PIPELINES = {  'dushuProject. pipelines. DushuprojectPipeline': 300,}修改 read. py 1234567891011121314151617181920212223242526272829303132333435363738# -*- coding: utf-8 -*-import scrapy# 链接提取器from scrapy. linkextractors import LinkExtractor# 导入 spiderfrom scrapy. spiders import CrawlSpider, Rule# 这里的类继承了 CrawlSpider 子类, 而CrawlSpider 继承自 scrapy. spider 基类class ReadSpider(CrawlSpider):  name = 'read'  allowed_domains = ['www. dushu. com']  start_urls = ['https://www. dushu. com/book/1081. html']  # 建议不要重写父类的parses方法  # 父类的逻辑  # def parse(self, response):  #  pass  # 子类实现一个 rules 元组, 框架会自动调用这个rules  # 包含了一个链接提取器对象.   # callback 是回调函数  # follow 指定是否跟进链接(继续爬取链接中的链接)  # LinkExtractor对象的提取规则需要我们制定  rules = (    Rule(LinkExtractor(allow=r'/book/1081_\d+?. html'), callback='parse_item', follow=True),  )  # 回调函数  def parse_item(self, response):    book_list = response. xpath('//div[@class= bookslist ]/ul/li')    for book in book_list:      i = {}      i['title'] = book. xpath('. /div[@class= book-info ]/h3/a/@title'). extract_first()      print(i)      yield i修改 pipelines. py 12345678910111213141516171819202122# -*- coding: utf-8 -*-# Define your item pipelines here## Don't forget to add your pipeline to the ITEM_PIPELINES setting# See: https://doc. scrapy. org/en/latest/topics/item-pipeline. htmlimport jsonclass DushuprojectPipeline(object):  def open_spider(self, spider):    self. fp = open('read. json','w',encoding='utf-8')  def close_spider(self, spider):    self. fp. close()  def process_item(self, item, spider):    string = json. dumps(item,ensure_ascii=False)    self. fp. write(string + '\n')    return item爬虫的日志信息: 级别:  严重 CRITICAL 一般 ERROR 警告 WORNING 一般信息 DEBUG ​settings. py 文件增加如下代码: 1234567# 设置日志登记# 不需要打印调试信息, 就可以调试完后关闭# 因为打印语句会比较消耗爬取效率和系统资源, 所以需要注意#LOG_LEVEL =  ERROR LOG_LEVEL =  INFO LOG_FILE = 'read. log'POST 请求: 创建项目 123scrapy startproject fanyiprojectcd fanyiprojectscrapy genspider fanyi  fanyi. baidu. com 修改 fenyi. py 1234567891011121314151617181920212223242526272829303132# -*- coding: utf-8 -*-import scrapyimport jsonclass FanyiSpider(scrapy. Spider):  name = 'fanyi'  allowed_domains = ['fanyi. baidu. com']  # start_urls = ['http://fanyi. baidu. com/']  # 默认是以 GET 方式提交请求, 引擎会自动把 start_urls 里面的url 封装成请求  # def parse(self, response):  #  pass  # POST 的请求就需要重写start_requests 方法  def start_requests(self):    post_url='http://fanyi. baidu. com/sug'    data = {      'kw': 'baby',    }    # 提交post 请求    # url post地址    # formdata post 的参数    # callback 回调函数, 引擎会把response 对象回传给这个指定的函数    # FormRequest 用于提交 POST 请求    yield scrapy. FormRequest(url=post_url, formdata=data, callback=self. parse_info)  def parse_info(self,response):    # 解析内容即可    obj = json. loads(response. text, encoding='utf-8')    string = json. dumps(obj, ensure_ascii=False)    print(string)小练习: 爬取王者荣耀官网: 需求: 将英雄信息和图片写到数据库中 "
    }, {
    "id": 30,
    "url": "http://localhost:4000/Connect-DataBases-with-Python/",
    "title": "Python and Databases(Mysql, Redia, MongoDB) Basic",
    "body": "2018/03/12 - Python编程语言具有强大的数据库编程功能。Python支持各种数据库，如SQLite，MySQL，Redis, MongoDB, Oracle，Sybase，PostgreSQL等。本文是从几种常用的数据库的基本操作入手, 教大家如何对 Mysql, Redis, MongoDB 进行日常操作, 以及如何使用 Python 对他们进行基本的操作。 Python 与 数据库[TOC]  数据库的概念: 数据库分为关系型数据库和非关系型数据库  关系型数据库: 一般使用 mysql, mariaDB. 一般公司使用 mysql 配合 radis 一起使用 非关系型数据库: radis 和 mongodb. 关系型数据库:  优点:     复杂查询: 可以使用 SQL 语句在一个或多个表之间进行复杂查询操作   事物处理支持: 可以对中断的事物进行回滚操作   Mysql数据库: mysql 基本命令 mysql 官方手册 mariaDB 官方手册 官方 MySQL dmg 包的下载和使用: 从官方下载 mysql 安装 dmg 包, 记住安装完毕时提示的密码 到 mac 的系统设置界面中启动 mysql 在终端命令行输入如下命令 /usr/local/mysql/bin/mysql -u root -pxxxxxx 进入数据库后修改 密码为自己熟悉的密码 set password for root@localhost = password('xxxxxx'); XAMPP安装说明 (推荐! 因为安装的是 MariaDB):    下载 xampp     安装并启动数据库服务     访问 mysql   123cd /Applications/XAMPP/xamppfiles/bin. /mysql -uroot -234567 #    ​  Mysql客户端 启动和连接命令: 12345mysql -h主机名orIP地址 -P端口号 -u用户名 -p密码 -Ssocket文件 -D指定数据库/Applications/XAMPP/xamppfiles/bin/mysql -uroot -pxxxxx# 如果访问的本机的 mysql, -h 参数可以不加Mysql 服务端 配置和命令:  配置文件 配置文件 my. cnf  查看Mysql 的运行服务, 以及数据文件存放位置, error log 日志位置, socket 文件位置 :     首先找到 mysqld 的执行进程 ps aux|grep mysql 如下   –datadir 就是数据文件存放位置   –log-error 是错误日志位置   –socket 是 socket 文件位置   1_mysql      26761  0. 0 0. 1 4574220 12404  ?? S   9:28AM  0:02. 73 /Applications/XAMPP/xamppfiles/sbin/mysqld --basedir=/Applications/XAMPP/xamppfiles --datadir=/Applications/XAMPP/xamppfiles/var/mysql --plugin-dir=/Applications/XAMPP/xamppfiles/lib/mysql/plugin/ --user=mysql --log-error=/Applications/XAMPP/xamppfiles/var/mysql/localhost. err --pid-file=/Applications/XAMPP/xamppfiles/var/mysql/localhost. pid --socket=/Applications/XAMPP/xamppfiles/var/mysql/mysql. sock --port=3308基本命令: 技巧:  在 mysql 命令行中要使用 HELP 接命令来了解不同的命令用法.  在 mysql 命令行中写 sql 语句的时候, 用大写来写命令 , 这样就可以用双击 tab 来获取 mysql 的命令提示 mysql 中的SQL是不区分大小写的, 但 数据库, 表名是大小写敏感的 sql 命令需要用 ; 结束. 如果不用;就回车, 就进入多行编辑模式 mysql 中的每个数据库是以文件形式存在于磁盘上的 字符集: 建库的时候就就要把charset 设置成 utf8, 这样建表,建字段的时候才会默认是 utf8 开启不严谨报错: 默认 myql 的报错为严谨报错, 只要数据除了问题,就会报错, 开启不严谨报错后, 当数据超出范围后, 自动街区为最大值. 可以在 mysql. ini 文件中将原来的 sql-mode 注释掉,重启 mysql, 不严谨报错生效. sql-mode= NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION ##==================  系统命令  =================help # \h 查看 mysql 的基本快捷命令 比如 \h 就是 help 的快捷命令. help 后面 可以再接需要了解的命令status # \s 可以查看当前数据库的版本, socket, 当前用户, 启动时间, slow query 等source # \. 可以执行一个外部的 sql 文件system # \! 执行一个系统命令clear # \c 作为当前sql语句的撤销动作ego # \G 发送命令, 并将结果垂直显示(不用表格来显示)select version(), current_date, now(); # 查看版本和当前日期select user(); # 查看 mysql 的用户GRANT ALL ON menagerie. * TO 'your_mysql_name'@'your_client_host'; # 给某用户可以访问某个数据库的所有权限. your_mysql_name 表示用户名, your_client_host 表示用户自己机器的 host ip 地址, menagerie 是一个数据库的名字quit # \q or exit 都可以退出数据库show warnings \G # 执行 sql 语句的时候偶尔会有警告,但是并不显示出来, 可以通过这个命令来查看. warnings只记录上一次执行的提示, 所以遇到后要马上看, 否则会被其他的 warning 覆盖掉show errors \G # 查看报错信息#=================== 数据库操作命令 =================### 数据库操作命令, 数据库的名字是大小写敏感的show databases; # 显示数据库use test # 选择并进入 test 数据库create database py1702 character set utf8; # 创建数据库并设置字符集create database if not exists py1703; # 如果库不存在 就创建. 不存在的话,会创建成功. 存在的话, 也会成功, 但是会报一个 warning show create database py1702\G # 查看一个创建了的数据库alter database py1702 charset latin1; # 修改字符集drop database py1702; # 删除数据库drop database if exists py1703; # 如果库存在就删. 存在会删除成功, 不存在也会成功, 但是会有一个 warning. #==================== 表操作命令 ===================#show tables; # 显示表列表select database(); # 查看当前所在库是哪个库show create table pet; # 查看创建的表desc pet; # 查看表的字段信息 可以写 describe 表名load data local infile '/Users/dalong/code/python1702/homework/pets. txt' into table pet; # 从本地文件中 load 数据到表中. 前提是需要创建一个文件, 每一行表示一条记录, 每行的单个数据需要用 tab 键隔开. 空值用 \N 代替. 可以用 LINES TERMINATED BY '\r\n' 来决定每行(每条记录)的分割符. 用fields terminated by ',' 决定字段的分割符DROP user if exists; # 删表user,如果存在就删,否则也不会报错, 但有 warning 数字类型字段表1-1: 数字字段类型       类型   大小   范围(有符号)   范围(无符号)   描述   用途         tinyint   1字节   -128 到 +127   0 到 255   最小整数值   可用于存储年龄       int   4字节   -2147483648 到 +2147483647   0 到 4294967295   可以存10位数的10进制数字           float   4字节   float       单精度浮点数   别存钱       double   8字节           双精度浮点值   别存钱       decimal   取决于你当前存储的值           decimal 在 mysql 中其实是字符串类型存储   可以用于存钱                                创建一个 mynumber 表, 把字段分别定义成 int 无符号, tinyint 无符号, tinyint 有符号, 浮点, 双精, decimal 的类型进行超出范围测试 int(4), tinyint(2) 的时候并不能真正的去限制存储数据的长度, 只是会影响显示. 只有配合 zerofill的时候才有意义 在有存储准确性要求的时候, 建议使用 deccimal 来存储数值CREATE TABLE `mynumber` ( `id` int(10) unsigned DEFAULT NULL, `age` tinyint(3) unsigned DEFAULT NULL, `unage` tinyint(4) DEFAULT NULL, `money1` float(6,2) DEFAULT NULL, `money2` double(6,2) DEFAULT NULL, `money3` decimal(6,2) DEFAULT NULL) ENGINE=InnoDB DEFAULT CHARSET=utf8; insert into mynumber(id) values(1234567891); #插入10位数,正常insert into mynumber(id) values(12345678912); #插入11位数,保存10位最大数insert into mynumber(age) values(255); #插入255, 正常insert into mynumber(age) values(257); #插入大于255,保存最大值insert into mynumber(unage) values(10000); # 超出范围保存最大值insert into mynumber(money1, money2,money3) values(1,2,3); # 正常insert into mynumber(money1, money2,money3) values(1234,1234,1234); # 正常insert into mynumber(money1, money2,money3) values(12345,12345,12345);# 超出范围保存最大值insert into mynumber(money1, money2,money3) values(1234. 345,1234. 345,1234. 345); # 超出小数范围,四舍五入insert into mynumber(money1, money2,money3) values(1234. 523,1234. 523,1234. 525); # 超出小数范围,四舍五入CREATE TABLE `aa` ( `zerotime` int(5) unsigned DEFAULT NULL, `zerotime2` int(5) unsigned zerofill DEFAULT NULL) ENGINE=InnoDB DEFAULT CHARSET=utf8; insert into aa(zerotime) value(123456); #正常insert into aa(zerotime2) value(123456); #正常insert into aa(zerotime2) value(1); # 用0填充日期和时间类型字段表1-2: 时间字段类型       类型   大小   范围   格式   用途         date   3   1000-01-01 到 9999-12-31   YYYY-MM-DD   存储日期       time   3   -838:59:59 到 838:59:59   HH:MM:SS   时间值       year   1   1901 到 2155   YYYY   年       datetime   8   1000-01-01 00:00:00 到 9999-12-31 23:59:59   YYYY-MM-DD HH:MM:SS   年月日时分秒    创建一个时间字段的表, 并做实验 注意: 在存储日期是, 可以用 int 型字段来存储时间戳, 这样方便时间的计算和转换. CREATE TABLE `mytime` ( `mydate` date DEFAULT NULL, `mytime` time DEFAULT NULL, `myyear` year(4) DEFAULT NULL, `mydatetime` datetime DEFAULT NULL) ENGINE=InnoDB DEFAULT CHARSET=utf8;insert into mytime values( 2017-11-28 ,  15:26:00 , 2017 , now()); TIMESTAMP 的使用 CREATE TABLE t(	id int,	ts TIMESTAMP); # 创建时间戳类型字段 # 字段 Default CURRENT_TIMESTAMP # 字段 Extra on update CURRENT_TIMESTinsert into t(id) values(1),(2); # 插入数据后, 默认会在 ts 字段放入当前时间 2017-12-02 17:51:27udpate t set id=5 where id=1; # 更新数据后时间也会变create table t2(	id int, 	ts TIMESTAMP NULL ON UPDATE CURRENT_TIMESTAMP); # 如果建表时设置了 TIMESTAMP 类型字段,默认值是 NULL, 那么第一次插入数据是, ts 字段值为 null. 当被更新时, 该字段会被更新. # 时间 - 时间戳转换SELECT ts, UNIX_TIMESTAMP(ts) FROM t; +---------------------+--------------------+| ts         | UNIX_TIMESTAMP(ts) |+---------------------+--------------------+| 2013-07-22 12:50:05 |     1374490205 || 2013-07-22 12:50:05 |     1374490205 || 2013-07-22 12:51:56 |     1374490316 || 2001-07-22 12:12:12 |     995796732 |+---------------------+--------------------+字符串类型字段 char 比 barchar 高效, varchar 比 char 更节省空间 char和 varchar 存储的最大值的范围是0 - 255 char 在插入数据小于指定长度时,会使用空格填补到指定的长度 当存储的数据长度超出了指定的长度, 会截取(开启不严谨报错的前提下)      类型   大小   描述   用途         char   0 到 255字节   定长字符串, 固定长度, 但效率高. 不够长度用空格占位   手机号       varchar   0 到 65535字节   可变长度字符串, 一旦超出限制, mysql 会自动转成 text 类型, 节省空间, 但效率低           text   0 到 65535字节   长文本数据   存储大块的文本       enum(‘f’,’m’)   65535个成员   枚举类型, 提前固定好可选的选项成员, 每个成员的值都是字符串           set(‘f’,’m’)   64个成员   集合类型, 可赋予多个集合成员, 多个成员用逗号隔开       CREATE TABLE `mystring` ( `phone` char(11) DEFAULT NULL, `username` varchar(7) DEFAULT NULL, `sex` enum('w','m') DEFAULT NULL, `hoby` set('watchtv','coding','dota','lol','wz') DEFAULT NULL) ENGINE=InnoDB DEFAULT CHARSET=utf8;insert into mystring(phone) value(13061611151); # 正常插入insert into mystring(phone) value(1306161115111); # 超出范围, 截取最大范围insert into mystring(username) values(' 坚持到无能为力,拼搏到感动自己'); # 超出范围, 截取最大范围insert into mystring(sex) values('w'); #正常插入insert into mystring(sex) values('wm'); #错误插入, 值为空insert into mystring(hoby) values('lol'); # 正常insert into mystring(hoby) values('lol,wz,dota'); #正常insert into mystring(hoby) values('lol,wz,lol,wz,dota'); # 自动去重, 和 python 的集合的特性一样insert into mystring(hoby) values('lol,wangzhe'); #成功,但是只有 lol 存入, 因为 wangzhe没有在建表时被定义. 字段约束      字段约束   描述   用法         unsigned   无符号(正数)   只能设置数值类型, 不允许负数出现, 最大存储长度会扩大一倍       zerofill   零填充   只能用于设置数值类型, 当位数不够,用0在左侧填充       auto_increment   自增   用于设置字段的自增属性,每增长一条数据, 该字段数值自动加一. delete 记录不会影响 AUTO_INCREMENT=4 . drop table 才会将重置 AUTO_INCREMENT=0       default   默认值   可通过此属性来设置字段默认值. 如果未设置默认值, 默认值为 Null       Null / Not Null   可选/ 必选   指定该字段是否是必填项或可选项, 但强行插入, 还是会成功, 但是会有 warning.    Null 注意事项  null 意味着没有值或者未知值 可以测试某个值是否为null, 要用 is null 进行测试 对 null 进行算数运算, 结果还是 nullCREATE TABLE `myfield` ( `id` int(10) unsigned NOT NULL AUTO_INCREMENT, `age` tinyint(3) unsigned zerofill DEFAULT NULL, `username` varchar(20) DEFAULT 'xxx', `hobby` varchar(20) NOT NULL DEFAULT 'play', PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8;insert into myfield values(null, 1, 'zhang san', 'coding');insert into myfield(age) values(2);alter table myfield add nn varchar(10) not null;insert into myfield(age) values(3);select 3+ 'a'; # 3select 3+ '1a'; # 4select 3 + 'a1'; # 3select 3 + null; # nullselect 3 * null; # null索引HELP INDEX索引分三种  主键索引 primary key 唯一索引 unique 常规索引 index 全文索引 fulltext (了解), 以后全文索引都用第三方全文索引. 主键索引    是关系数据库中最常见的索引类型, 主要作用是记录表中每条数据的位置.     设置方法: 在字段后面添加 primary key 关键字     一个字段如果被设置成 auto_increment, 那么这个字段会被设置成 主键     最好为每个表设定主键, 但不是必须设定.     每个表只能有一个主键     主键可以有多个候选约束, not null, auto_increment   ALTER TABLE MYINDEX3 ADD PRIMARY KEY(NAME);   ALTER TABLE MYINDEX3 DROP PRIMARY KEY; # 主键只有一个,所以不需要指定名字  唯一索引    唯一索引保证该字段的数据是不重复的     每个表中可以存在多个唯一索引     设置方法: 在字段后面添加 unique 关键字   ALTER TABLE MYINDEX3 ADD UNIQUE(AGE);   ALTER TABLE MYINDEX3 DROP INDEX AGE; # 删除 unique 索引需要用 index 关键字     给一个已有的表某个字段做 unique 设置时要先确保要字段已有数据没有重复, 否则会报错 类似于 Duplicate entry '' for key 'job'  常规索引    index 是关系数据库中查询最重要的技术.     如果要提升数据库的性能,首先想到的是常规索引     缺点: 1. 多占磁盘空间. 2. 减慢增删改的效率.     索引的创建, 可以使用 index 或者 key 来创建常规索引   ALTER TABLE MYINDEX3 ADD INDEX(SEX);   ALTER TABLE MYINDEX3 DROP INDEX SEX;  create table myindex( # 不起索引名的创建索引方式username varchar(20),sex tinyint unsigned, unique(username), # 为 username 创建唯一索引 index(sex) # 为 sex 创建普通索引, (没起名字) );create table myindex2(	# 起索引名字的创建索引方式 username varchar(20), sex tinyint unsigned, unique uname (username), # 为 username 起索引名字 uname index isex(sex) # 为sex 创建普通索引, 名字是 isex );desc myindex;+----------+---------------------+------+-----+---------+-------+| Field  | Type        | Null | Key | Default | Extra |+----------+---------------------+------+-----+---------+-------+| username | varchar(20)     | YES | UNI | NULL  |    || sex   | tinyint(3) unsigned | YES | MUL | NULL  |    |+----------+---------------------+------+-----+---------+-------+desc myindex2;+----------+---------------------+------+-----+---------+-------+| Field  | Type        | Null | Key | Default | Extra |+----------+---------------------+------+-----+---------+-------+| username | varchar(20)     | YES | UNI | NULL  |    || sex   | tinyint(3) unsigned | YES | MUL | NULL  |    |+----------+---------------------+------+-----+---------+-------+show create table myindex;CREATE TABLE `myindex` ( `username` varchar(20) DEFAULT NULL, `sex` tinyint(3) unsigned DEFAULT NULL, UNIQUE KEY `username` (`username`), # 如果设置 index 名字,默认使用字段名 KEY `sex` (`sex`) # 如果设置 index 名字,默认使用字段名) ENGINE=InnoDB DEFAULT CHARSET=utf8 show create table myindex2;CREATE TABLE `myindex2` ( `username` varchar(20) DEFAULT NULL, `sex` tinyint(3) unsigned DEFAULT NULL, UNIQUE KEY `uname` (`username`), # 设置了自己的索引名 KEY `isex` (`sex`) # 设置了自己的索引名) ENGINE=InnoDB DEFAULT CHARSET=utf8insert into myindex2 values('张三', 18); # 成功insert into myindex2 values('张三', 18); # 失败 ERROR 1062 (23000): Duplicate entry '张三' for key 'uname'数据存储类型(MyISAM 和 InnoDB)以及位置 myisam 不支持事物, innodb 支持 myisam 引擎处理效率不高 myisam 产生三个文件, innodb 两个 myisam 不支持外键, innodb 支持 存储区别: myisam 或者 innodb 都会产生一个叫 . frm 文件, 用来存储表结构框架 innodb 存储文件用 . bd, 存储索引和表数据 myisam 存储文件 . myd 存储表数据的文件; myi 存储表的索引的文件. alter table myindex2 engine = myisam;alter table myindex2 engine=innodb;transaction 事物处理HELP START TRANSACTION如下步骤完成事物的处理  用 mysql 做事物之前,需要先确认所用的到表示 innodb 存储类型. show create table myindex2; 如果不是, 修改为 innodb. alter table myindex2 engine=innodb; 修改当前数据库的提交方式为手动提交. 查看方式 select @@autocommit; 1 为自动提交, 0 为手动提交 ; 自动提交的意思是当数据一旦更改, mysql 就自动将更改数据写到文件中 . 修改当前数据库的手动提交 set autocommit=0; 经过测试,设置成0这个步骤应该是不需要的 开启事物: begin; 或者使用 START TRANSACTION; 写 sql 的操作 提交 or 回滚: commit; 是提交 ; rollback; 是回滚begin;insert into user values('zhangsan');select * from user; # 看到了新增数据rollback work; # 执行回滚操作, 取消新增数据select * from user; # 新增数据消失insert into user values('lisi'); commit work; # 手动提交数据 work 是可选的关键字alter 对表结构的操作HELP ALTER TABLE   增加字段: add column     增加索引: add index()/primary key()/unique()     修改表名: rename     修改字段名: change 也可以用于重新设置表的所有属性信息     修改字段DEFAULT: alter, 基本只用来设置 default 值 or 删掉 default 值 set default , drop default     修改字段TYPE 和 Null: modify , 需要同时设置 Type 和 Null     删除字段 drop column     删除索引 drop index/primary key   ​   修改表名 ALTER TABLE MYINDEX4 RENAME MYINDEX3;  修改字段名 alter table 表名 change 原字段名 新字段名 约束条件 ALTER TABLE MYINDEX3 CHANGE COLUMN SEX SIX VARCHAR(3); # 需要补充上字段的属性信息 ALTER TABLE MYNUM CHANGE COLUMN F2 F2 INT(4) ZEROFILL UNSIGNED NOT NULL DEFAULT 1 UNIQUE; # change 基本上可以修改所有字段属性,还能添加索引. 注意, not null 需要放在 zerofill 和 unsigned 属性的后面  为表添加一个新的字段 ALTER TABLE 表名 ADD 字段名 约束条件 ALTER TABLE MYINDEX3 ADD COLUMN SEX CHAR(3) NOT NULL;  为一个字段修改默认值 ALTER TABLE MYINDEX3 ALTER NAME SET DEFAULT 1;  删除表字段 alter table 表名 drop column 字段名 ALTER TABLE MYINDEX3 DROP COLUMN SEX;  修改表字段的数据类型  如果想修改 not null, 需要把字段 type 也一起修改ALTER TABLE MYINDEX3 MODIFY SIX VARCHAR(3) NOT NULL;  添加/删除索引 alter table 表名 add 索引类型(索引字段) 索引名 ALTER TABLE MYINDEX3 ADD PRIMARY KEY(NAME); alter table 表名 add 索引类型(索引字段) alter table USERS2 add index(NAME); alter table 表名 drop key 索引名 alter table USERS2 drop key NAME; insert 添加数据HELP INSERT增INSERT 删DELETE 改UPDATE 查SELECT 增:    指定字段添加:   insert into 表名(字段1,字段2…) values(‘值1’,’值2’…)     不指定字段为表加数据:   需要为表中的每个字段添加数据   insert into 表名 values(‘值1’,’值2’…)     插入多条记录   insert into 表名 values(‘值1’,’值2’…),(‘值1’,’值2’…)   insert into bb values('zs1',2),('ls',3),('ww',5);     快速将一个表插入多条记录作为练习用.   insert into bb select * from bb;  select 查询HELP SELECT   不指定字段的查询(不建议)   select * from 表名     指定字段来查询数据   select 字段1, 字段2 from 表名     给字段起别名   select 字段1 as 别名 from 表名   select 字段1 别名 from 表名  #####where 条件 比较运算符: &gt;, &lt;, =, &gt;=, &lt;=, != or &lt;&gt;, =  update a set age = age-40 where id between 4 and 7;  where age &gt;= 102 逻辑运算符: and, or, between A and B, not between A and B, in, not in  where ange &lt;=71 and sex = 'w'  where age between 10 and 30 取10 到 30之间的数据, 包含10 和 30 这两个数字  where age not between 10 and 30 取在10到30之外的数据.  where age in (10,20,30)  where age not in (10,20,30)  where age &lt; 30 and classid in (1707,1708) 子查询 也就是 where 中也有 select 语句 (了解, 效率低)  select * from 表名, where name in (select name …. ) order by 排序  默认为升序 asc, desc 为降序  order by 字段1, 字段2 is, is not 与 NULL  因为 null 是特殊字符,不能用 =, 只能用 is .  is 不能用于除了 null 之外的数据  where 字段名 is null  where 字段名 is not null limit 限制返回条数  limit 开始位(默认为0), 条数.  select * from a order by age desc limit 1;  select * from a order by id desc limit 5,5; # 从第五条开始取5条  分页公式: limit (当前页-1)*每页条数, 每页条数  where username is not null and age between 10 and 20 order by age desc limit 2. group by 分组   group by 什么字段, select 中也要有同一个字段  group by 后面可接多个字段,逗号隔开. 比如先按班级分,再按那女分, 那么就会有 班级数*2条结果, 分别显示每个班男生数量和女生数量.   group by 后面如果要接条件, 需要用 having, 不能用 where select sex, count(*) as total from a group by sex; # 统计男女分别有多少人select classid,count(*) as total from a group by classid; # 统计每班分别有多少人select classid, sex, count(*) as total from a group by classid, sex; #统计每班和每班男女分别有多少人. select classid, sex, count(*) as total from a group by classid, sex having total &gt; 10; # 分组统计后, 只拿人数大于10的记录. select classid, count(*) as t from a group by classid having classid in ('py1706','py1707') and t&gt;2; # 统计在1706, 1707两个班的学生人数, 只取 group by 后结果大于2人的数据. like 模糊查询HELP LIKE包含: like ‘%字符%’ where username like '%张%' 以. . 字符开头: like ‘张%’ where username like '张%' 以. . 字符结尾: like ‘%张’ where username like '%张' _ 代表一个任意字符 where username like '李_龙' where username like '___day' # 可以匹配到 Monday, Friday Sunday delete 删除HELP DELETEdelete from 表名 where 条件  需要加 where 条件, 否则就清空表.  删除之前,最好先 改成 select 看看有多少条数据会被删除. delete from a where username is null; update 更新数据HELP UPDATEupdate 表名 set 字段名=值, 字段名=值 where 条件  更新之前, 最好先 改成 select 看看有多少条数据会被更新. update a set age = age-10 where age &gt; 10; join 的使用注意:  隐式和显示内连接为同一链接, 只会把两个表关联起来的数据查出来, 未关联数据不会显示 左连接以左表为主表,右表为辅表, 会把主表所有数据查出来, 辅表没有关联上的数据使用 NULL占位 右连接以右表为主表,左表为辅表, 会把主表所有数据查出来, 辅表没有关联上的数据使用 NULL 占位select u. id t, u. username, g. goodname from user u, goods g where u. id = g. uid and u. id = 1;select u. id t, u. username, g. goodname from user u inner join goods g on u. id = g. uid;select * from user u inner join goods g on u. id = g. uid;select * from user u left join goods g on u. id = g. uid and g. uid = 1;select * from user u right join goods g on u. id = g. uid;Mysql 的聚合函数max() 查询字段最大值 min() 查询字段最小值 avg() 查询字段平均值 sum() 求和 select max(age), min(age), count(*), sum(age), avg(age) from python. a; mysql 的导入和导出数据库的导出  cd 到需要备份的目录 执行命令 mysqldump -uroot -234567 库名&gt; 文件名. sql1/Applications/XAMPP/xamppfiles/bin/mysqldump -uroot -234567 py1702&gt;py1702. sql数据库的导入  cd 到有 . sql 的备份文件对应的目录 执行命令 mysql -uroot -234567 库名&lt;p. sql1/Applications/XAMPP/xamppfiles/bin/mysql -uroot -234567 py1702&lt;py1702. sqlmysql的权限管理权限管理全面你介绍 修改密码 set password for 用户@localhost = password(‘新密码’) set password for root@localhost = password('234567'); # 把 root 的用户的密码改成 234567 创建用户  新建用户如果不授予库操作权限 使用 mysql 库: use mysql 查看当前库都有哪些用户: select user from user 创建用户: create user dalong identified by '123456' # identified by 指定了密码授予权限    grant all on py1702. * to dalong #给大龙授予 py1702库的所有权限   all: 代表增删改查所有权限   grant all on *. * to 'mrldl'@'localhost'; 给大龙授予所有库的所有权限     revoke all on py1702. * from dalong  查看某用户权限 show grants for 'mrldl'@'localhost'; 删除用户 drop user dalong; # 删除用户后, 被删用户需要在下次进入 mysql 客户端后才会被禁止进入 刷新权限 flush privileges; # 刷新权限后, 之前对权限的修改可以立即生效 用 python 操作 mysql:  使用 pymysql 模块 来操作 mysql 数据库1pip3. 7 install pymysql步骤:    设置连接 db = pymysql. connect(主机名,用户名,密码,数据库)     设置字符集: db. set_charset('utf8')     创建 cursor 对象, 对数据进行增删改查操作: cursor = db. cursor()   注意: 一旦创建了游标对象的时候就开启了事物, 对于数据的任何修改操作, 最后都要 commit() 或者 rollback() 进行操作     准备 sql 语句: sql = '. . . '     执行语句: cursor. execute(sql)     处理结果集: cursor. fetchall() ; cursor. fetchone()     获取当前sql语句所影响的行数: cursor. rowcount()     关闭数据库连接: cursor. close()  12345678910111213141516171819202122232425# mysql 连接import pymysql# 打开数据库连接db = pymysql. connect(host='localhost',          unix_socket='/Applications/XAMPP/xamppfiles/var/mysql/mysql. sock',          user='root',          passwd='234567',          db='py1702',          charset='utf8')# 使用 cursor() 方法创建一个游标对象 cursorcursor = db. cursor()sql = 'select now()'try:  cursor. execute(sql)  # db. commit()  # 使用 fetchone() 方法获取单条数据.   data = cursor. fetchone()  print(data)except Exception as e:  print ( Error: unable to get time data ### SQL : )  print(e)  db. close()1234567891011121314151617181920212223import pymysql# 获取数据库对象db = pymysql. connect(host='localhost',          unix_socket='/Applications/XAMPP/xamppfiles/var/mysql/mysql. sock',          user='root',          passwd='234567',          db='py1702',          charset='utf8')print(db)print(db. charset)# 创建游标对象cursor = db. cursor()print(cursor)# 准备 sqlsql = 'select * from myindex3'cursor. execute(sql)# 获取结果集print(cursor. fetchall())# 获取影响行数print(cursor. rowcount)# 关闭数据库db. close()非关系数据库:    性能: 非关系型数据库是基于键值对, 不需要 sql 层面的解析, 所以说性能非常高     可扩展性: 同样是基于键值对的存在, 所以水平扩展性比较好   ​  redis 数据库:  redis在线交互教学 redis官网命令 redis 命令中文参考 redis 在线书 redis 全部命令redis mac 客户端安装 12ruby -e  $(curl -fsSL https://raw. githubusercontent. com/Homebrew/install/master/install)  &lt; /dev/null 2&gt; /dev/null ; brew install caskroom/cask/brew-cask 2&gt; /dev/null1brew cask install rdm安装完后可以看到 rdm app 了 redis 数据库介绍:  redis 是非关系型远程内存数据库,性能强劲 发布订阅, 主从复制, 持久化, 支持事物 五种数据存储类型     string 字符串   list 列表   set 集合   hash 散列表   zset 有序集合   redis 数据库的安装:  从 redis官网 下载最新的 redis 安装包 下载解压后放到 /usr/local/ 目录下 , 会需要 root 权限123cd /usr/local/redis-4. 0. 2/sudo make 启动 redis 服务: 安装成功后在命令运行如下命令 redis-server , 运行后不要退出. 可以看到服务的欢迎界面和端口号, 默认端口是 Port:63791src/redis-server 启动 redis 客户端链接 redis 服务器:另外开一个 terminal 窗口执行如下命令 redis-cli , 并测试是否可以进入 redis 命令行12345src/redis-cliredis&gt; set foo barOKredis&gt; get foo bar redis 数据结构: 查看当前是几号数据库, 切换数据库  可以参看客户端命令行的提示 127. 0. 0. 1:6379[3]&gt; 表示3号库  没有中括号,表示 0 号库 127. 0. 0. 1:6379&gt;  select 1 切换到1号库       结构类型   结构存储的值   结构的读写能力         string   字符串,可以包含字符串,整数,浮点数   可以自增, 自减       list   列表,每个元素可以包含一个字符串   推入,弹出,读取,查找       set   集合,包含字符串,唯一性,无序性   可以进行位运算,交集,并集,差集等       hash   字典, 无序的键值对集合   添加,修改,删除       zset   有序集合, 排序由score的大小决定   添加,删除   Key 相关命令注意:  redis 中 系统命令大小写不敏感 redis 中 key 的名字大小写敏感 ```python redishelp # 查看任何命令的介绍command # 列出所有命令l # 输入部分命令后连续按 tab 键可以补全和推荐命令. 因为 list 命令多数以 l 开头, set 命令多数以 s 开头, hash 命令多数以 h 开头, zset 命令多数以 z 开头, 所以输入第一个字母,按 tab 键可以看到大部分相关的命令. 123456789101112131415161718192021222324252627| 命令    | 解释                    || --------- | ---------------------------------------- || keys 简单正则 | 获取按正则可以匹配到的所有的 key. `*` 表示任意多个(0到无穷)任意字符, `?` 表示单个任意字符, `[]`表示多个选项中选一个, `[^]` 表示不在此范围内的其它任意单个字符. `[]` 语法和正则一样.  没有匹配到,返回空列表 or 空集合 || exists  | 判断 key 是否存在 ,0不存在 1存在. 如果同时匹配多个 keys, 则返回匹配到的 key 的个数 || type   | 看 key 的类型                || del    | 删除 key, 成功返回1, 失败返回0 . 可以一次删除多个 key, 成功返回删除成功的个数 || expire  | 给 key 设置过期时间. 成功返回1, 失败返回0        || ttl    | 查看 key 的过期时间,单位秒. 如果返回值为-1, 表示该 key 永不过期 || pttl   | 查看 key 设置过期时间,单位毫秒. 如果返回值为-1, 表示该 key 永不过期 || persist  | 为 key 设置永久不过期. 设置成功返回1, 失败返回0.      || flushdb  | 删除当前库中所有key               || flushall | 删除所有库中所有key. redis 库是从0 ~ 15, 每个库有自己独立的 keys || rename  | 修改 key 名. 成功返回 ok, 原 key不存在返回 error. 如果 目标 Key 存在,则目标 key 被覆盖 || move   | 将 key 移到某库中               || randomkey | 随机返回一个 key. 没有任何 keys 则返回 nil      |```python rediskeys * 		# 返回所有 keyskeys *a* 	# 返回包含 a 的所有 keykeys a*		# 返回以 a 开头的所有 keykeys a?		# 返回 a开头的第二个字符是任意字符的两个字符的 keyskeys *a*e 	# 匹配中间有a, 结尾是 e 的所有 keyskeys h[ae]llo # 匹配第二个字符是 a或 e, 的以 h 开头, llo 结尾的keyskeys h[^a-z]llo # 匹配第二个字符不是 a-z 的所有以 h 开头, llo 结尾的keysstring 字符串字符串命令       命令   描述         get   根据 key 获取键中的值, 不存在返回 nil       set   存值到键中, 返回成功失败       del   删除指定键中的值       setex   设置键值及过期时间, 以秒为单位, 成功返回 ok       ttl   查看有效时间, 以秒为单位       setnx   只有在 key 不存在的时候设置健值, 成功返回1, 失败返回0       mset   设置多个值, 返回成功失败       setrange   覆盖已有字符串值, 重置重 offset 开始其中一部分. offset 值范围以0 开始. 成功返回新的字符串长度       mget   通过多个 keys, 一次获取多个健值, 不存在 返回 nil       getrange   获取某字符串变量中的一段, 返回子串. offset 从0开始, -1 结束. 如果 end 的值大于字符串长度, 会返回到 字符串结尾的字符串       getset   返回已有变量的值,再给该变量重设新值       incr   对变量中的数字型字符串加一, 返回新值       decr   对变量中的数字型字符串减一, 返回新值       incrby   对变量中的数字型字符串加整数,返回新值       decrby   对变量中的数字型字符串减整数, 返回新值. 值可以使负数       append   对字符串右侧拼接新字符串, 返回str 长度       strlen   获取字符串长度. 如果 key不存在, 返回0   ```python redisset name dalong # 设置 dalong 到 name中 , 返回 ok 表示成功 get name # 按键获取 value, 返回 “dalong “ del name # 按键名删除, 返回的1 代表成功的数量 get name # 获取空 键或返回 nil. python 会转成 None setex name 10 ‘dalong’ # 设置 name 变量, 10秒后过期 ttl name # 查看变量name 所剩过期时间 setnx name ‘dalong’# 如果变量 name 不存在, 则设置. 成功返回1, 失败返回0mset name ‘dalong’ sex ‘f’ # 同时设置多个变量 set key ‘hello world’ setrange key 6 ‘Redis’ # 从 key 的偏移量6开始替换成 Redis 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647##### list 列表redis 列表是简单的字符串列表, 按照插入顺序排序, 可以在列表左侧右侧添加新元素list命令| 命令   | 描述                    || ------- | ---------------------------------------- || rpush  | 将值在列表右侧依次推入, 成功返回1, 否则0. 没有 key 就创建一个新的 list || rpushx | Append a value to a list, only if the list exists || lpush  | 将值在列表左侧依次推入, 成功返回1, 否则0. 没有 key 就创建一个新的 list || lpushx | Prepend a value to a list, only if the list exists || lrange | 获取指定范围内的列表值               || lindex | 按索引返回列表中的值                || lpop  | 从列表左侧弹出一个值,并返回              || linsert | 在列表某元素值得前\|后插入新值.  Insert an element before or after another element in a list || lset  | 覆盖指定索引位置值为新值.  如果索引不存在,则报错 ERR index out of range || rpop  | 从列表右侧弹出一个值,并返回              || lrem  | count &gt; 0 : 从表头向表尾搜索,删除掉 COUNT 个 value 元素&lt;br /&gt;count &lt; 0 : 从表尾向表头搜索,删除掉 COUNT 个 value 元素&lt;br /&gt;count = 0 : 删除表中所有与 VALUE 相等的元素 || ltrim  | 将原来的List 进行剪裁. 范围外的扔掉. 语法和 lrange 一样.  Trim a list to the specified range || llen  | 返回列表元素个数                 |```python redisrpush mylist v1 # 连续 push 三个值,两个相同rpush mylist v2rpush mylist v1lrange mylist 0 -1 # 获取从头到尾的所有值lindex mylist 0 # 按索引查看每一个值lindex mylist 1lindex mylist 2lindex mylist 3lpop mylist # 弹出最左边的值lrange mylist 0 -1 # 再查看一遍所有值linsert myl after a c # 在 myl 中的 a 后面插入 cset 集合set 命令       命令   描述         sadd   为集合添加一个或多个元素, 若成功,则返回添加元素个数. 如果集合不存在, 创建新的集合       smembers   返回集合所有元素       sismember   检查某个值是否已在集合中, 找到返回1, 否则0       srem   从集合中按值删除一个或多个元素, 成功返回1, 否则0       scard   返回集合元素个数       spop   随机从集合中删除一个元素,并返回该元素. 如果集合为空, 返回 nil       sinter   求交集       sdiff   求差集, 注意比较顺序       sunion   求合集       srandmember   随即返回一个集合元素   ```python redissadd myset item1 # 为集合增加元素 sadd myset item2 sadd myset item3 smembers myset # 返回集合所有元素 sismember myset item4 # 测试一个元素是否属于集合, 返回0 sismember myset item3 # 返回1 srem myset item2 # 删除一个集合 smembers myset # 再次查看集合 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253#### hash-key 散列/字典- hash-key 包含多个带键值对的元素 field-value- key 中有多个 fields, 每个 field 有对应的值- key 中的 field 是不会重复的- 适用于保存实例化的对象. 可以将 python 中的对象进行保存. `print(a. __dict__)` - 和 python 字典概念类似hk 命令| 命令      | 描述                    || ------------ | ---------------------------------------- || hset     | 设置 hk. `hset key field value`. 如果 key 不存在 创建新的 hk. 如果key 已经存在, 但是 field 不存在, 创建新的 field. 如果key ,field 都存在, 那么覆盖原有 value. || hget     | 获取指定 field 的 value            || hgetall   | 返回所有 key 中的 fields 和 values       || hdel     | 移除 指定 field               || hmset    | 一次设置多个fields 的值, 成功返回 ok         || hincrby   | 对键值对的值增加整数, 成功返回新的 数值. 不能增加浮点数      || hincrbyfloat | 对键值对的值增加浮点数 ,成功返回新的数值. 如果 field不存在 也会添加. 可添加整数. || hsetnx    | 当field不存在变量时设置              || hmget    | 一次获取多个fields 的值             || hlen     | 返回 key 中的键值对个数, 注意要和 hstrlen       || hexists   | 看一个变量的 field 是否存在 ,存在返回1, 否则0      || hdel     | 删除某 key 的一个或多个 fields          || hstrlen   | 获取某变量 field 对应 value 的长度. **注意**要和 hlen 区别开 || hkeys    | 获取所有 fields               || hvals    | 获取所有 values of fields          |```python redishset myhk k1 v1 # 为hk 创建元素 , 返回1hset myhk k2 v2hset myhk k1 v1 # 已存在同名的 key, 返回0hmset hash name 'zs' age 18 sex 'w' # 设置多个值hgetall myhk # 获取所有 hkhdel myhk k2 # 按 key 删除一个 hkhgetall myhk # 再次获取所有 hkHINCRBY hash age 10 # 对 hash 的 age 增加10 HINCRBYFLOAT hash money 10. 1 zset 有序集合:  zset 是有序集合, 元素具有唯一性,不重复  zk 的排序是由元素的score来决定的  zk 的值被称为 score 必须是浮点型的  zk 是唯一可以根据成员来访问元素的数据类型       命令   描述         zadd   为 集合 添加元素, 如果集合不存在,创建新集合. 成功返回添加个数, 失败返回0       zrange   根据 range 的索引值范围 获取多个元素(0 -1 表示从第一个索引返回到最后一个索引). 可以通过 WITHSCORES 关键字同时返回权重值       zrangebyscore   根据 score 的取值范围 获取多个元素. 可以通过 WITHSCORES 关键字同时返回权重值.        zscore   获取指定元素对应的 score 分数       zrem   移除指定元素       zincrby   对集合中指定元素的分数加整数权重       zcard   返回集合内元素个数       zcount   返回集合内指定score 范围元素个数.  Count the members in a sorted set with scores within the given v   zadd mysk 728 k1 # 为zk添加元素, key 是 k1, score 是 728zadd mysk 982 k2zrange mysk 0 -1 withscores #返回从头到尾的所有元素, 不加 withscores 参数, 将值返回 key, 否则返回 key,valuezrangebyscore mysk 0 800 withscores # 只返回 value 在0-800之间的元素. 有 withscore 参数则返回 key value, 否则只返回 key zrem mysk k1 # 按 key删除 元素zrange mysk 0 -1 withscores # 再次显示所有 元素ZINCRBY z1 10 a # 为 z1集合的 a 的 score 增加10点权重 用 Python 连接 redis: 连接方法  先导入 redis 模块 pip3. 7 install redis 写如下链接测试代码1234567import redisr = redis. Redis(host='localhost',port=6379,db=0)r. set('name','dalong')print(r. get('name')) # 返回 b'dalong'1234567891011121314import redisr = redis. StrictRedis('localhost', db=0)r. set('s1', 'dalong') # 成功返回 tureprint(r. get('s1'))  # 获取 s1 的值print(r. get('s1'). decode('UTF-8')) # 获取解码的 s1 值r. setex('s1', 10, 'xiaolong')r. setnx('s2' , 'zhangsan')r. mset({'s3':'lisi', 's4':'wangwu'})r. mset(s5='lisi', s6='wangwu')r. setrange('s6',2 , 'xxx')print(r. mget('s1','s2')) # 返回一个列表print(r. getrange('s1',0,-1). decode('UTF-8'))print(r. getset('s7',10). decode('utf-8'))print(r. incr('s7',10))MongoDB 数据库: mongo 官方文档 了解 JSON: JSON 是一个轻量级的数据传输格式 用在web前台, 移动端, 第三方接口和后台的数据交互中. 类似于 python 的字典 导入 JSON 模块 12345678910111213141516171819202122232425262728import json# json 化后写入文件#json. dump()#json. load()# json 化后返回字符串#json. dumps()#json. loads()myDict = {'name':'lisi', 'age':18}with open('. /test. txt', 'w') as f	json. dump(myDict,f)with open('. /test. txt', 'r') as f	json. load(myDict,f)print(json. loads(json. dumps(myDict)))#序列化后写入文件#pickle. dump()#pickle. load()#序列化后返回字符串#pickle. dumps()#pickle. loads()安装MongoDB: 官方安装文档  下载和安装 mongodb1brew install mongodb --with-openssl 安装成功后运行如下命令创建数据库需要存储数据文件的目录和权限123sudo mkdir -p /data/dbsudo chown dalong -u /data/db 分别启动 mongodb 服务端 mongod 和 客户端 mongo, 客户端需要另外开一个 terminal 窗口. 12345# 新建 terminal 窗口, 启动后不要关掉. 启动后可以看到服务端口号. 默认端口号 27017mongod# 新建 terminal 窗口mongo 尝试在 Mongo客户端输入命令 show dbs 测试是否成功1show dbs # 输出两个数据库, admin localMongo DB 介绍:    mongo 严格区分大小写, 所有命令都要小写     mongodb 将数据存为文档, 数据结构有 key value 组成     mongo 文档类似于 josn 字段值     字段值可以包含其他文档或者是数组   {'name':{'name':'value'}}  mongo 存储的概念(区别)      SQL 术语/ 概念   mongo 术语/概念   解释说明         database   dbs   数据库       table   collection   表       row   document   行/文档       column   field   字段/域       primary key   primary key   主键索引/mongo 会自动将_id 设置成主键索引                   对库的操作手册 12345help # 查看 help 相关的帮助信息db. help() # 数据库相关帮助; 任何命令后面加上 . help() 可以获取这个命令的帮助信息db. users. help() # 获取集合相关帮助. d # 输入部分命令后双击 tab 可以补全db.  # 输入命令加点后双击 tab 可以看到该命令的子命令 库命名规范:  库名全部小写，禁止使用任何_以外的特殊字符，禁止使用数字打头的库名，如：123_abc；1234567891011121314151617show dbs # 查看所有的库 use users # 使用一个库, 没有就创建#当不存在的库创建以后, 需要使用`db. createCollection('goods')` 在这个库中创建一个新的 collection 后才能用 `show dbs` 看到新的库db. getName() #可以看到当前正在使用的数据库名. db 就代表了 usersdb. getMongo() # 查看当前客户端连接的服务器信息对象db. hostInfo() # 获取服务器信息db. stats() # 查看当前库的统计报告db. version() # 查看当前 mongo 版本信息db. dropDatabase() # 删掉数据库, 删除之前最好先 use 一下,确认删除的是当前数据库对于集合的操作手册 1db. collection. help() # 看和 表 相关的帮助技巧 db. 集合名. + 双击 tab 可以获取对集合的所有操作  集合命名规范  禁止使用任何_以外的特殊字符，禁止使用数字打头的集合 集合名全部小写，名，如：123_abcshow collections # 查看所有集合 # db. createCollection('集合名字') # 创建集合db. createCollection('user') # 创建一个叫做 user 的集合.  # db. 集合名. drop # 删除集合db. user. drop()INSERT/SAVE 文档的添加/修改(文档覆盖)_id 相关的资料  如果往一个不存在的 collection 中插入一条记录, 那么等于同时创建了这个 collection. 123456789101112131415161718 # insert 添加文档 # db. 集合名. insert(文档)db. users. insertOne({'name':'zhangsan','age':18}) # 插入1条文档 ,成功会返回  insertedId  : ObjectId( 5a1fd3ba07e9cb5855b8e747 ) 表示成功插入一条. 返回自动生成的 id, key 的引号可有可无 db. products. insertOne( { _id: 10, item:  box , qty: 20 } ) # 也可以自己生成 id 进行插入, _id 是必须的id格式db. users. find() # 查看所有文档 也可以写成 db['users']. find()db. users. find({'age':'43'}); # find() 函数可以带上条件, 查看 age 是43的所有数据db. goods. find({ _id  : ObjectId( 5a1fcd1d07e9cb5855b8e733 )}) # 按照 id 查,可以查出唯一数据# db. 集合名. insert([文档1,文档2. . . ]) # 插入多条数据db. users. insertMany([{name:'dalong',age:'43'},{name:'xiaoyu',age:12}]) # 插入多条数据, 多条数据用[]括起来db. users. save({name:'zhaoliu',age:20}) # 修改,也可以作为添加. 不给 id 就是添加,和 insert 一样db. users. save({ _id  : ObjectId( 5a1fb66260f017e8a99f054d ), name:'zhaoliu',age:'21'}) # 这是修改或者叫做覆盖, 返回信息 {  nMatched  : 1,  nUpserted  : 0,  nModified  : 1 } 表示 找到一条, 修改了一条. Update 修改官方解释 主体结构: db. collection. update(  &lt;query&gt;,  &lt;update&gt;,  {   upsert: &lt;boolean&gt;, # 默认 false, 当 true 时,无存在就创建   multi: &lt;boolean&gt;, # 默认 false, 当 true 时,匹配多条就更新多条   writeConcern: &lt;document&gt;, # 这个先不用管   collation: &lt;document&gt; # 这个先不用管  })# 修改一条db. users. updateOne(	{name:'dalong'},	{$set:{age:10}},	{upsert:true})# 修改多条db. goods. updateMany( 	{name:'dalong'}, 	{$set:{age:10}}, 	{upsert:true} )修改的操作符  $inc 可以对文档的某个值为数字型（只能为满足要求的数字）的键进行增减的操作 $set 用来指定一个键并更新键值. set 可以设定任意类型的值,修改方式就是直接覆盖掉原来的值(比如将原来的字符串类型值换成列表型). 若键不存在就创建.  # inc 的例子db. createCollection('b') db. b. insert({ uid : 201203 , type : 1 ,size:10})db. b. find()db. b. update({ uid  :  201203 },{ $inc :{ size  : 1}})db. b. find()db. b. update({ uid  :  201203 },{ $inc :{ size  : 2}})db. b. find()db. b. update({ uid  :  201203 },{ $inc :{ size  : -1}})db. b. find() # set 的例子db. goods. updateMany( 	{name:'dalong'}, 	{$set :{age:18,goods:['books','food','money']}} );db. users. updateMany(	{name:'mike'}, 	{$inc:{age:10}}, 	true,true)find 查找所有db. users. find(). help() 可以看和 find 相关的命令 123456db. users. find() 	# 默认返回所有记录db. users. find({name:'taotao'}) # 只查名字是涛涛的数据db. users. find({},{name:1}) # 第一个大括号是查询条件,第二个大括号是要显示哪些字段, name:1 表示只显示 name 字段信息db. users. find({},{age:0}) # age:0 表示不显示 age 字段, 其他都显示db. users. find({},{name:1,_id:0}) # 这个是正确的, 因为_id 是系统提供的db. users. find({},{age:0,name:1}) # 这是错误写法,因为矛盾了(,除了_id 可以,其他都不行)findOne 查一条 1db. users. findOne({},{name:1,_id:0}) # 查看一条,并只显示名字count 统计条数 1db. users. find(). count() # 统计所有信息条数, 可以加条件pretty 展开查看 1db. users. find(). pretty() # 以容易让人看的样式展示数据查询条件的操作符  $gt 大于 db. users. find({age:{$gt:10}})     大于某 ID db. col. find({ _id :{$gt:ObjectId( 5a22b67a60f017e8a99f0558 )}}). pretty()    $gte 大于等于 db. users. find({age:{$gte:10}}) $lt 小于 db. users. find({age:{$lt:10}}) $lte 小于等于 db. users. find({age:{$lte:10}}) $ne 不等于 db. users. find({age:{$ne:10}}) 等于 {field:值} db. users. find({age:10}) 使用 id 来查询 db. col. find({ _id :ObjectId( 5a22b67a60f017e8a99f0558 )}). pretty() /数据/ 模糊查询  db. users. find(name:/da/) /^数据/ 以. . 开头 db. users. find({name:/^da/}) /数据$/ 以. . 结尾 db. users. find({name:/ng$/}) $in 在 . . 之内  db. users. find({age:{$in:[18,20]}}) $nin 不在… 之内 db. users. find({age:{$nin:[18,20]}})and 查询 12345678910111213# db. collection. find({key1:val2,key2,val2}) 语法db. users. find({name:'dalong',age:10}) #找名字是大龙,年龄是10岁得人db. users. find({name:'dalong',age:{$gte:10}}) # 找到名字是大龙,年龄大于10岁得人db. users. updateOne({name:'dalong', age:10}, {$set:{age:20} }) # 找到名字是大龙, 年龄是10岁的人,把他的名字更新成20 db. users. find({name:'dalong',age:{$gte:10,$lte:20}}) #找名字是大龙, 年龄在10到20之间的数据db. users. find({name:'dalong',name:'zhangsan'}) # 后面的条件会覆盖前面的条件, 不建议使用这样的写法db. users. find({name:{$in:['dalong','zhangsan']}}) # 查找名字是大龙或张三的数据or 查询 要用 $or 关键字 12345db. users. find({$or:[{name:'dalong'},{name:'zhangsan'}]}) # 查询 neme 是大龙或者张三的数据db. users. find({name:'zhangsan',$or:[{age:{$gte:10}},{age:{$lte:30}}]}) # 查询名字是张三, 年龄大于10 或 小于 30的数据and or 组合写法 1db. users. find({name:'zhangsan',$or:[{age:{$gte:10}},{age:{$lte:30}}]},{name:1,_id:0}) # 查询名字是张三, 年龄大于10 或 小于 30的数据limit 限制返回条数 12db. users. find(). limit(3) # 从零开始取3条skip 跳过前 n 条数据后再取值 1234db. users. find(). skip(5) # 从第5条开始去db. users. find(). skip(2). limit(3) # 从第二条开始取, 取三条db. users. find(). limit(3). skip(2) # 返回来写也一样db. users. find(). skip(2). limit(3). count() # 统计sort 排序 12345db. users. find(). sort({age:1}) # 升序db. users. find(). sort({age:-1}) # 降序db. users. find(). sort({age:-1}). limit(1) # 取年龄最大的一条记录remove 删除db. collection. remove({条件},1) # 1234567db. users. remove({name:'zhaoliu'}) # 删掉符合条件的数据db. users. remove({name:'taotao'},1) # 只删除匹配到的第一条数据db. users. remove({name:'taotao'},{justOne:true})db. users. remove({}) # 删除所有数据db. dropDatabase() # 删掉数据库, 删除之前最好先 use 一下,确认删除的是当前数据库数据库的备份和恢复1mongodump --help备份数据  先启动 mongod 服务 cd 到想要存储备份数据的目录, 打开一个终端 输入如下命令 会发现有一个新的目录 dump , 可以看到备份的库数据1mongodump删除数据库 用来测试接下来 mongorestore ```python bashuse users # drop 库之前必须先 use db. dropDatabase() # 干掉use pydb. dropDatabase() 1234567891011**恢复数据**1. 进入dump 所在目录2. 运行如下命令3. 再次进入 mongo 客户端, 查看之前删掉的库是否重现```bashmongorestore为 python 安装 mongo 模块: 1pip3. 7 install pymongopython 中使用 pymongo 模块: pymongo 使用手册 123456789import pymongoclient = pymongo. MongoClient('localhost', 27017)db = client. pyprint(db)print(db. name)print(db. users)# print(db. users. insert_one({'name':'taotao', 'age':14}). inserted_id)for item in db. users. find():  print(item['name'])12345678910111213141516import pprintimport datetime					from pymongo import MongoClient # 导入 mongo 客户端client = MongoClient('localhost', 27017) # 连接服务器, 默认端口号27017db = client. users				# 连接 users 数据库post = { author :  Mike ,		# 建立一个字典     text :  My first blog post! ,     tags : [ mongodb ,  python ,  pymongo ],     date : datetime. datetime. utcnow()}posts = db. posts				# 为 users 库建立一个叫做 posts 的表 post_id = posts. insert_one(post). inserted_id	# 将字典插入 posts 表中并将 id 赋值到 post_id 变量中print(post_id) 					# 打印该 idpprint. pprint(posts. find_one()) # 打印 posts 表中的一条记录 , 这里没有加条件, 但是可以加搜索条件pprint. pprint(posts. find_one({ author :  Mike })) # 加了搜索条件pprint. pprint(posts. find_one({ _id : post_id})) # 用刚才获取的 id 来搜索文档添加 insert : 插入一条或多条, 返回单个 id 或列表 id . 不建议使用 insert_one : 插入一条, 返回一个对象, 需要通过 obj. inserted_id 获取 ID 值 insert_many : 插入多条, 返回一个对象, 需要通过 obj. inserted_ids 获取一个 ID 列表 123456789101112131415161718192021from pymongo import MongoClientcli = MongoClient('localhost', 27017)db = cli. pydata = db. users. find()# 添加一条文档print(db. users. insert({'name':'ZHANGSAN','age':18, 'sex':'w'}))# 返回 创建的 id# 添加多条文档print(db. users. insert([{'name':'LISI','age':28, 'sex':'w'},{'name':'WANGWU','age':28, 'sex':'w'}]))# 以列表形式返回创建的 id 信息# insertOneinsertObj = db. users. insert_one({'name':'CAILIU','age':18, 'sex':'w'})# 返回一个pymongo 对象, 需要进行解析print(insertObj. inserted_id)# insertManyinsertObjs = db. users. insert_many([{'name':'LISI','age':28, 'sex':'w'},{'name':'WANGWU','age':28, 'sex':'w'}])print(insertObjs. inserted_ids)** find 文档查询** 注意:  当需要按照 id 来查询是, 需要导入 from bson import ObjectId , 避免报错 当需要模糊查询时, 需要导入 import re , 并用 re. compile('da') 来代替原来的 /da/ 当进行排序的时候, 在 python 中的写法 sort('age',1) 和 mongo 命令行中的写法 sort({age:1}) 不一样示例代码 123456789101112131415161718192021222324252627282930313233343536373839404142from pymongo import MongoClientfrom bson import ObjectIdimport refrom collections import Iteratorcli = MongoClient('localhost', 27017)db = cli. pydata = db. users. find() # 返回一个迭代器, 通过 next 方法可以进行遍历取值for x in data:  print(x['name'],x['age'])data = db. users. find_one() # 返回一条字典print(data)# 返回一个带条件的搜索data = db. users. find({'name':'ZHANGSAN','age':{'$gt':'17'}})print(data)# 通过 id 查询一个文档 , 需要导入 from bson import ObjectId , 避免错误data = db. users. find({ _id  : ObjectId( 5a20f48a421aa99cd374b7b9 )})print(data)# 模糊查询  需要导入正则模块 import re, 用 re. compile 来搜索data = db. users. find({'name':re. compile('da')})print(data)# 排序 , 注意 最后面的 sort('age',1) 和 mongo 命令行中的写法 sort({age:1}) 不一样data = db. users. find(). sort('age',1)print(data)# count 统计条数data = db. users. find(). count()print(data)# skip 跳过前四条,获取剩下的data = db. users. find(). skip(4)print(data)# limit 从第4条开始取, 取4条文档data = db. users. find(). skip(4). limit(4)print(data)update 文档更新 1234567891011121314151617181920from pymongo import MongoClientfrom bson import ObjectIdimport refrom collections import Iteratorcli = MongoClient('localhost', 27017)db = cli. py# update 更新数据 , 成功后返回一个影响数据的字典result = db. users. update({'name':'LISI'}, {'$set':{'age':20}})result = db. users. update({'name':'LISI'}, {'$inc':{'age':1}})print(result)# updateOne 更新一条数据 ,成功后返回一个对象, 可以使用 modified_count 获取改动条数data = db. users. update_one({'name':'LISI'}, {'$set':{'sex':'f'}})data = db. users. update_one({'name':'LISI'}, {'$inc':{'age':1}})print(data. modified_count)# updateMany 更新多条数据, 成功后返回一个对象, 可以使用 modified_count 获取改动条数data = db. users. update_many({'name':'LISI'}, {'$set':{'sex':'f'}})print(data. modified_count)remove 删除 1234567891011121314151617181920212223from pymongo import MongoClientfrom bson import ObjectIdimport refrom collections import Iteratorcli = MongoClient('localhost', 27017)db = cli. py# 按条件删除result = db. users. remove({'name':'WANGWU'})print(result)# 删除一条数据, 返回一个对象, 需要解析, result. deleted_count 得到删除的条数result = db. users. delete_one({'name':'WANGWU'})print(result. deleted_count)# 删除多条, 返回一个对象, 需要解析, result. deleted_count 得到删除的条数result = db. users. delete_many({'name':'WANGWU'})print(result. deleted_count)# 删除表中的所有数据(清空)result = db. users. remove()result = db. users. delete_many({}) # 删除成功返回对象,result. deleted_count 得到删除的条数关闭数据库 1db. close()"
    }, {
    "id": 31,
    "url": "http://localhost:4000/Beginning-Python-From-Novice-to-Professional-Part-2/",
    "title": "Learning Python From Scratch (Part2)",
    "body": "2017/11/05 - Python编程语言可能是使用最广泛的语言之一，因此不断发展。如果一个人想充分发挥其潜力，同时又想在专业上保持竞争力，那么学习 Python 是必不可少的。本文是我很早之前学习 Python 基础时的笔记, 希望可以为那些不了解该Python的人提供了快速入门的必要信息. python 高级[TOC] 查看一个对象的所有内置方法 / 查看一个类的所有内置方法  先进入python 命令行 然后创建一个类, 并将类实例化. 123456&gt;&gt;&gt; class A:. . .   def AA(self):. . .       pass. . . &gt;&gt;&gt; &gt;&gt;&gt; a = A()  查看对象所有内置方法: 命令行输入 a. __ 然后双击 tab12345678&gt;&gt;&gt; a. __a. __class__(     a. __ge__(      a. __lt__(      a. __setattr__(   a. __delattr__(    a. __getattribute__( a. __module__     a. __sizeof__(    a. __dict__      a. __gt__(      a. __ne__(      a. __str__(     a. __dir__(      a. __hash__(     a. __new__(      a. __subclasshook__( a. __doc__      a. __init__(     a. __reduce__(    a. __weakref__    a. __eq__(      a. __init_subclass__( a. __reduce_ex__(  a. __format__(    a. __le__(      a. __repr__(    查看类所有的内置方法: 命令行输入 A. __ 然后双击 tab123456789101112&gt;&gt;&gt; A. __A. __abstractmethods__ A. __eq__(       A. __le__(       A. __repr__(     A. __base__(      A. __flags__      A. __lt__(       A. __setattr__(    A. __bases__      A. __format__(     A. __module__     A. __sizeof__(    A. __basicsize__    A. __ge__(       A. __mro__       A. __str__(      A. __call__(      A. __getattribute__(  A. __name__      A. __subclasscheck__( A. __class__(     A. __gt__(       A. __ne__(       A. __subclasses__(  A. __delattr__(    A. __hash__(      A. __new__(      A. __subclasshook__( A. __dict__      A. __init__(      A. __prepare__(    A. __text_signature__ A. __dictoffset__   A. __init_subclass__( A. __qualname__    A. __weakref__    A. __dir__(      A. __instancecheck__( A. __reduce__(     A. __weakrefoffset__ A. __doc__       A. __itemsize__    A. __reduce_ex__(   2. 1面向对象 OOP: OOP 是一种思想, 面向过程也是一种思想通过面向对象思想, 抽象一个类, 通过这个类,创建对应的对象,让对象执行类里面的方法 生活角度解释面向对象  类: 一群拥有相识特点的事物集合, 是概念 , 人类, 汽车, 水杯, 手机等 对象: 精确到某一个事物, 我手里的 iphoneX, 涛哥 面向过程是数学逻辑的映射 面向对象是生活逻辑的映射 以后写 python 都要用面向对象的思想. 多个对象协同工作完成你的功能.    面向过程: 买砖, 打地基, 封顶, 装修, 买家具       面向对象: 包工头搞定      官方的 类和对象的概念  类: 对象的抽象对象: 类的具象 面向对象和函数,属性  函数是向面向对象思想的靠拢面向对象是在函数的基础上, 对多个函数进行封装, 更抽象还有属性,( 可以是变量,字典等 ) 面向对象语言三大特点:  封装: 将特定的函数,属性封装到类中, 并可以控制权限 继承: 如果想使用其他类的一些方法,属性,可以直接继承过来 多态: 多种形态, 不同的对象调用同一个函数接口, 得到的响应是不同的. 2. 1. 1创建和使用类: 类名  用 class 关键字定义类 类名的首字母大写 类名后面的小括号() 用来作为继承使用, 如果不需要继承, 小括号可以省略 小括号中可以放多个父类, 称之为多继承, 只放一个父类称之为单继承self  成员方法必须有一个参数, 这个参数我们一般命名为 self, 当然写成别的名字, 比如 a 也可以, 这个 self 就是创建的对象.  self 地址就是当前对象的地址. 哪个对象调用这个成员方法, 那么 self 就代表谁,  目的是可以让实例化的对象可以访问自己内部的变量和方法 self 放在参数第一个位置, 其他参数需要放在 self 后面实例化的过程: 小明, 老王为例  创建小明 - 先在内存中创建一个小明对象空间, 然后再创建一个叫小明的变量指针指向小明实例化对象空间 创建老王 - 先在内存中创建一个新的老王对象空间, 然后再创建一个叫老王的变量指针指向老王实例化对象空间 实例化一个类和实例化一个自定义函数一样, 通过第一个字母的大小写区分是类还是函数 . 可以让实例化的对象访问类中的属性(attribute)和方法(method)成员方法:  和普通函数的定义没有区别, 只是必须把 self 参数放在最前面 _前缀: 在成员方法和属性的名字前面加上下划线 表示是私有属性或方法 在类中调用成员方法 需要 用 self 关键字 self. wangwang() self. __init__() 也可以work, 含义是需要重新初始化信息. 但是这并不常见, 一般都是在初始化类时自动初始化. 不进行人工调用. 12345678910111213141516class People:  def eat(self,food = '酸菜鱼'):    print('我爱吃' + food)  def run(self):    print('我喜欢跑步')if __name__ == '__main__':  xiaoming = People()  laowang = People()  print(type(xiaoming))  xiaoming. eat('糖醋排骨')  laowang. eat()成员属性:  成员属性可以动态添加, 刚开始没有在类中定义, 对象直接动态添加了几个属性.  _前缀: 在成员方法和属性的名字前面加上下划线 表示是私有属性或方法 后缀_: 在成员属性的后面加上下划线, 表示避免和某个保留关键字冲突 定义方法: self. 属性名 = 属性值 属性名可以是任意类型变量, 包括实例化的对象 如果不用 self. 添加属性, 那么实例化对象就不能访问到这个属性, 类中的其他方法也不能访问这个变量.  类内调用方法: self. 属性名 对象调用方法: 对象名. 属性名 动态添加属性不好. 当创建一个类时, 最好可以直接把属性定义到类中. _ or __ 私有属性, 私有方法  保护属性和方法:  python 语法中比较鸡肋, 基本上不用这玩意. 因为 python 没有访问控制符的概念, 所以也就不存在什么 私有, 公开, 受保护的 控制符.  python 中, 类的属性和方法时可以在外部随意访问的, 一般不做限制.  如果使用单下划线作为前缀, 可以告诉别人这个属性或者方法时内部使用的, 但只是命名约定,不做强制限制 如果使用 双下划线 作为前缀, 外部是不能访问的. 需要通过定义方法来调用双下划线开头的属性或者方法, 这类的方法名定义一般叫做 set_xxx, get_xxx __dict__ 可以用于访问类中的成员属性 , print(a. __dict__) 可以打印出如下内容{'_internal': '私有变量', 'public': '公有变量', '_A__private': '新的隐私'} , 所以可以用 print(a. _A__private) 访问__private 属性12345678910111213141516171819202122232425262728293031class A:  def __init__(self):    self. _internal = '私有变量'    self. public = '公有变量'    self. __private = '隐私'  def _internal_method(self):    print('私有方法')  def public_method(self):    print('公有方法')  def __private_method(self):    print(self. __private)  def get_private(self):    self. __private_method()  def set_private(self, value):    self. __private = valuea = A()print(a. _internal)print(a. public)print(a. _internal_method())print(a. public_method())print(a. get_private())a. set_private('新的隐私')print(a. get_private())print(a. __dict__)类属性:  直接在类中, 方法的外面定义的属性,成为类属性 调用方式是  类名. 属性名 , 也可以使用 对象名. 属性名 但只推荐第一种方式 当类属性和对象属性同名时, 用 对象名. 属性名 方式会优先调用对象属性名 类属性和对象属性可以同时存在(访问, 添加, 修改), 互不影响.  最后一句话: 哥们, 用类名调用类属性, 用对象名调用对象属性, 别乱套1234567891011121314class Wolf:  # 写到这里的叫做类属性, 类属性的意思是这个属性不是属于某个对象的,而是属于整个类的, 它的调用方式是通过 类名. 属性名, 当然也可以通过 对象名. 属性名  age = 100  def __init__(self, name):    # 写在 self. 后面的属性, 称之为成员属性, 或者对象属性, 实例属性    self. name = namelang = Wolf('小灰灰')print(Wolf. age)Wolf. age = 50lang. age = 80print(Wolf. age)print(lang. age)类方法和静态方法:  类方法: 在方法前面定义了装饰器 @classmethod 的方法叫做类方法. 类方法可以通过 类名. 类方法() 调用 类方法定义时, 需要用 def heng(cls): 来定义 , 类方法必须有一个参数 cls 是用来代表类自己.  类方法中禁止出现 self 关键字, 因为类方法是通过类名进行调用的. 对象是实例化后的东西, 类方法是对类的操作 在类方法中直接通过 cls. 直接调用类属性和类方法 在类方法中可以通过 cls. 创建对象, 并封装一层, 最终通过类方法给外部提供简洁的借口 最好不要在一个类中定义同名的类方法和对象方法, 如果一定要这样做, 那么把对象方法的定义放在类方法的前面1234567891011121314151617181920212223242526272829303132#类方法class Pig:  # 这种事对象方法, 或成员方法, 通过对象地址调用  age = 100  def __init__(self, name):    self. name = name  def gong(self):    print('喜欢拱又白又嫩的白菜')  def heng(self):    print('对象方法的哼哼')  # 看见这个装饰器, 代表下面的方法时类方法. 要通过类名调用  @classmethod  def heng(cls):  # 这里的 cls 代表类本身, cls 也可以换成其他的名字, 但是一般都写成 cls    print(cls)    print(cls. age)    p = cls('净坛使者')  # 类方法创建了一个自己的实例对象    p. gong()       # 新建的对象调用了对象方法    print('类方法的哼哼')    return p       # 并将对象返回bajie = Pig('八戒')# bajie. gong()# bajie. heng()Pig. heng()print(Pig. heng())静态方法 @staticmethod: 静态方法和类方法基本一致, 只是定义静态方法时不需要cls 和 self 123456class Pig:  @staticmethod  def jingtai():    print('这是静态方法')Pig. jingtai()常用属性方法 __name__, __dict__ , __bases__:  __name__ : 通过类名调用, 获取类名字符串 通过类名. __dict__查看当前类的信息, 包含类属性,类方法,静态方法等 通过对象名. __dict__ 获取所有对象的属性 通过 self. __dict__ 和通过对象名调用一样 通过 类名. __bases__ 查看所有的父类, 注意 , __bases__ 只能显示上一级的父类, 不能显示更高层的父类123456789101112131415161718192021class Tiger:  age = 100  height = 200  def __init__(self,name):    self. name = name  def haha(self):    pass  @classmethod  def leifangfa(cls):    pass  @staticmethod  def jingtai():    passprint(Tiger. __name__) #print(Tiger. __dict__) # 通过类名. __dict__查看当前类的信息, 包含类属性,类方法,静态方法等t = Tiger( 狗 )print(t. __dict__)   # 通过对象名. __dict__ 获取所有对象的属性12345678910111213class Base:  passclass A(Base):  passclass B(Base):  passclass C(A,B):  passprint(C. __bases__)  # 输出 (&lt;class '__main__. A'&gt;, &lt;class '__main__. B'&gt;)限制属性动态添加 __slots__: 指出了可以添加的属性,其他的不能加. 包含构造方法也不例外 123456789#限制属性动态添加class Shikelang:  __slots__ = ('name', 'age' , 'height') # 指出了可以添加的属性,其他的不能加. 包含构造方法也不例外xiaoqiang = Shikelang()xiaoqiang. name = '小强'xiaoqiang. age = 1xiaoqiang. height = 0. 1析构方法 __del__:  和构造方法相反 当对象的内存被销毁的时候自动调用 当对象的内存的空间被释放的时候, 就会自动调用 __del__ 方法 __del__ 方法中经常放一些清理工作, 比如关闭文件, 关闭数据库等123456789101112class Pig:  def __init__(self,name):    self. name = name  def __del__(self): # 当内存中的空间被释放的时候,就会调用 __del__ 方法    print('这头猪被宰了')print(100)p = Pig('八戒')   # 开辟了内存空间, 并将 p 指向了这个内存空间ershixiong = Pig('小八戒') # 第二个对象指向了同一个内存空间p = 'hello'     # 对象 p 被赋予了新的值del ershixiong   # 销毁了二师兄对象,同时也释放了内存print(200)多态:  多态, 就是多种形态, 在 python 也是鸡肋 重写就是一种多态 多态的定义: 当调用对象的同一个方法, 得到不同的结果的形态叫做多态 多态的本质是给某对象的同一个方法传递不同的对象, 这些不同的对象拥有同一个对象方法(继承自同一个父类), 被传递的不同对象的方法会在某方法中被调用,从而得到不同的结果123456789101112131415161718192021222324252627# 多态# 例子: 豆豆殴打小动物class Animal: # 这是一个父类  def bebeat(self): #这是一个接口方法, 所有继承 Animal 的类都会自动有 bebeat 方法    passclass Pig(Animal):  def bebeat(self):    print('猪哼哼两声跑向了白菜地')class Dog(Animal):  def bebeat(self):    print('狗朝你旺旺两声, 开始攻击你')class Person:  def __init__(self,name):    self. name = name  def beat(self, animal: 'Annimal'): #这里的第二个参数要限制必须是 Annimal 类的对象    print('豆豆正在殴打小动物')    animal. bebeat()doudou = Person('王豆豆')bajie = Pig()doudou. beat(bajie)wangwang = Dog()doudou. beat(wangwang)序列化, 反序列化 Pickle:  在内存中, 程序运行结束后, 对象就销毁了.  pickle 模块可以帮助我们对 python 对象进行序列化 python 中 一切都是对象 将对象写到文件中, 或者写到数据库中的行为, 叫做持久化保存, 下次当需要继续使用对象时, 可以再把对象从文件或者数据库中读出来.  作用: 将对象序列化, 这里的对象指的是所有的对象, 包括列表, 字典, 集合, 自定义对象 序列化只能保存对象的属性, 不会保存对象的方法 如果要使用反序列化的对象调用属性或者方法, 必须要把该对象用到的类import 到当前文件 pickle. dump( data, f) 函数将内容序列化, 并放到文件中 , 打开文件必须加上 b pickle. dumps( data ) 函数将内容序列化, 并转到字符串中 pickle. load(f) 从文件中读取序列化内容, 并进行反序列化, 并自动执行 ( 反- pickle. loads(s) 从字符串反序列化 序列化的可以是 类, 函数, 接口, 变量 等等 ) , 所以, 是有风险的, 千万不要对任何来路不明的序列化文件做 load() 操作 如果需要保存多个对象, 可以把多个对象放到列表中或者字典中,然后进行序列化 可以顺序的把需要序列化的内容存到文件中, 读取时也是顺序的读取, 如果取出的次数大于放入的次数, 会报错EOFError: Ran out of input , 因为已经读到文件最底部了,没有能够在读取序列化内容了对字符串, 列表,字典进行序列化 1234567891011import picklef = open('. /somedata', 'wb' )pickle. dump([1, 2, 3, 4], f)pickle. dump('hello', f)pickle. dump({'Apple', 'Pear', 'Banana'}, f)f. close()f = open('somedata', 'rb')print(pickle. load(f)) # [1, 2, 3, 4]print(pickle. load(f)) #'hello'print(pickle. load(f)) # {'Apple', 'Pear', 'Banana'}print(pickle. load(f)) # 会报错 EOFError: Ran out of input , 因为已经读到文件最底部了,没有能够在读取序列化内容了对对象进行序列化 123456789101112131415161718192021222324252627# 序列化对象import pickleclass GirlFriend:  def __init__(self,name,age,height,weight):    self. name = name    self. height = height    self. age = age    self. weight = weight  def wash(self):    print('我喜欢给你洗衣服')# 序列化# 如果需要保存多个对象, 那就需要把多个对象放到列表中或者字典中,然后进行序列化# fen = GirlFriend('刘淑芬', 16, 162, 50)# fp = open('. /fen. txt', 'wb')# pickle. dump(fen, fp)# 反序列化fp = open('. /fen. txt' , 'rb')# 如果要使用反序列化的对象调用属性或者方法, 必须要把该对象用到的类import 到当前文件中fen = pickle. load(fp)print(fen. weight)fen. wash()@property 可管理的属性:  如果想管理一个属性的访问权限, 就使用 @property 的方式 对于私有属性, 我们一般需要通过 set, get 方法来对属性进行访问 我们也可以通过 python 官方提供的 装饰器 @property 和 @age. setter 来实现两个方法, 可以模拟访问正常属性的格式 先定义一个属性, 再为这个属性定义三个名子相同的方法     getter ,获取属性值   setter ,设置属性值   deleter, 删除属性值    三个方法的前面都需要加上装饰器     @property   @属性名. setter   @属性名. deleter   12345678910111213141516171819202122232425262728293031323334class Cat:  def __init__(self, name):    self. name = name    self. __age = 3    self. __weight = 10  # def get_age(self):  #   return self. __age  #  # def set_age(self,age):  #   self. __age = age  @property    # 用了 property, 那么下面的方法就可以当做属性来调用  def age(self):    return self. __age  @age. setter   # 用类名  def age(self,age):    self. __age = age  @property  def weight(self):    return self. __weight  @weight. setter  def weight(self,weight):    self. __weight = weightcat = Cat('多多')cat. age=100print(cat. age)# print(cat. get_age())# cat. set_age(5)# print(cat. get_age()) deleter 方法1234567891011121314151617181920212223242526class Person:  def __init__(self):    self. __name = '大龙'  # Getter function  @property  def name(self):    return self. __name  # Setter function  @name. setter  def name(self, value):    if not isinstance(value, str):      raise TypeError('需要一个字符串名字')    self. __name = value  # Deleter function (optional)  @name. deleter  def name(self):    raise AttributeError( 名字不允许被删除 )dalong = Person()print(dalong. name)dalong. name='小龙'print(dalong. name)del dalong. name构造方法:  创建对象时被调用的方法称为构造方法.  构造方法的功能就是对属性进行初始化, 以及对象创建成功后的初始化方法的执行.  __init__ 是初始化方法, 实例化对象时会被自动执行.  构造方法不能写 return 任何东西, 只能返回 None 构造方法中有一个隐藏的 return , 初始化类时,返回对象本身. 12345678910111213141516class GirlFriends:  def __init__(self):    print('我被调用了')if __name__ == '__main__':  dilireba = GirlFriends()  dilireba. name = '迪丽热巴'  dilireba. age = 18  print(dilireba. name, dilireba. age)  yuanyuan = GirlFriends()  yuanyuan. name = '高圆圆'  yuanyuan. height = 165  print(yuanyuan. name, yuanyuan. height)__str__ 打印对象:  当 print 一个对象时, 默认是打印这个对象的内存地址 &lt;__main__. Dog object at 0x104818828&gt; 可以用定义一个 __str__(self): 方法, 并返回一个字符串, 来代替默认返回的内存地址.  __str__(self): 方法只有在 print(对象) 的时候才会被调用, 类内的其他方法调用__ str__ 方法不会生效. 12def __str__(self):  return  这个是print(对象) 时返回的信息 __repr__ 和 __str__ 类似, 只是更面向程序员友好:  当使用 __repr__(self): 方法, 返回一个字符串, 二返回的内容在 python 交互命令行中输入对象名可以直接显示 __repr__ 定义的返回内容 下面的例子打印出类的实例化使用方法  12def __repr__(self): return 'RunningCar(total_distance:num ,speed: num) -&gt; None :'   with 语句:  打开文件有异常, 需要关闭文件等. 这样比较麻烦, 所以 with 可以很方便的帮助打开和关闭内容. 123with open('fen. txt', 'rb') as fp:  content = fp. read()print(content)__enter__ 和 __exit__ 方法:  有一些任务，可能事先需要设置，事后做清理工作.  这种时候需要用到 with 进行处理.  with 的工作原理就是:     在执行对象方法之前会先调用 __enter__ 方法, 并将__ enter__ 返回的值赋给 as 后面的变量   with 语句块里面的代码开始执行, 执行完毕后再调用 __exit__ 方法   就算 with 语句块的代码发生异常, enter 和 exit 方法内的代码也会执行.     那我们需要做的就是为类来创建 __enter__ 和 __exit__ 方法来做事先设置和事后的清理工作 例子: 我们每次想把执行代码的 log 记录在一个log 日志中, 就可以考虑在执行前打开 log 日志, 运行完代码后, 写入日志文件并关闭日志文件. 伪代码 12with 对象() as 变量:	变量. 方法()一段包含__ enter__, __exit__ 的完整代码 1234567891011121314151617class TestforEnterAndExit:  def __init__(self,logfile='. /test. txt', mode='a+'):    self. logfile = logfile    self. mode = mode  def __enter__(self):    print( __enter__:Open %s  % self. logfile)    self. fp = open(self. logfile, self. mode , encoding='utf-8')    return self. fp  def __exit__(self, exc_type, exc_val, exc_tb):    print( __exit__:Close %s  % self. logfile)    self. fp. close()with TestforEnterAndExit() as t:  t. write('随便写一些日志 \n')print('End')把对象当参数进行传递:  老王开车     对象: 老王, 老王的车, 借的车   动作: 开, 跑   1234567891011121314151617181920212223242526272829class Person:  def __init__(self, name, age, car):    self. name = name    self. age = age    self. car = car  def travel(self):    pass  def social(self,car=None):    print('%s要出没了, 女孩们要注意' %self. name)    # 在这里做了个判断, 如果老王借到宝马,就开宝马去, 如果没有借到, 就开自己的车去    if car:      car. run()    else:      self. car. run()class Car:  def __init__(self,brand, color, price):    self. brand = brand    self. color = color    self. price = price  def run(self):    print('%s%s汽车在奔跑, 该车价格%s!' %(self. brand,self. color,self. price))laowang_car = Car( 普桑 , '黑色', '1万')borrow_car = Car( BMW , '骚粉', '100万')laowang = Person('老王', 40, laowang_car)laowang. social()laowang. social(borrow_car)练习: 小明与牌题目:  小明手里有两张牌, 左手是♣️K, 右手是♦️A 小明交换了手里的两张牌, 左手是♦️A, 右手是♣️K分析:  5个对象, 一个人, 两个手, 两张牌 3个类: 人, 手, 牌设计:  牌类     属性: 花色, 大小   方法: 无    手类     属性: 牌   方法: 拿牌    人类     属性: 姓名, 左手(对象), 右手(对象)   方法: 换牌, 看牌, 抓牌   1234567891011121314151617181920212223242526272829303132333435363738394041424344class Poker:  def __init__(self, color, number):    self. color = color    self. number = number  def __str__(self):    return '%s%s' %(self. color, self. number)class Hand:  def __init__(self, poker=None):    self. poker = poker  def hold_poker(self, poker):    self. poker = pokerclass Person:  def __init__(self, name, age, lhand, rhand):    self. name = name    self. age = age    self. lhand = lhand    self. rhand =rhand  def catch_poker(self, poker1, poker2):    self. lhand. hold_poker(poker1)    self. rhand. hold_poker(poker2)  def show(self):    print('我的左手%s' %self. lhand. poker)    print('我的右手%s' %self. rhand. poker)  def exchange_poker(self):    self. lhand. poker, self. rhand. poker = self. rhand. poker, self. lhand. pokerpoker1 = Poker('♣️', 'K')poker2 = Poker('♦️', 'A')lhand = Hand()rhand = Hand()xiaoming = Person('小明',20,lhand,rhand)xiaoming. catch_poker(poker1,poker2)xiaoming. show()print('*'*50)xiaoming. exchange_poker()xiaoming. show()2. 1. 2继承: 基本概念  一般情况系父子类是直接归属关系, 当然也可以有多层的父子关系 子类属性大于等于父类的属性和方法 基本概念     父子类其他叫法: 基类(父类), 派生类(子类), 超类(父类)   继承: 从子类的角度说, 子类继承了父类. 动物继承自生物   派生: 从父类的角度说, 父类派生了子类. 生物派生出动物   单继承: 子类只能有一个父类, python 支持多继承, 但一般情况只使用单继承.    多继承: 子类有多个父类, 多继承会出现混乱情况, 所以不建议使用   父子类关系  只要子类继承了父类,就拥有了父类的所有属性和方法 创建子类时，父类必须包含在当前文件中，且位于子类前面 定义子类时，必须在子类的括号内指定父类的 名称 所有的类的基类是 object , 如果一个类不写父类, 那么默认继承自 object 基类 一个子类如果没写构造方法, 就会直接继承父类的构造方法, 如果父类的构造方法要求必须要传递参数, 子类的实例就必须传递参数 ;1234567891011121314151617class Animal:  def sleep(self):    print('动物都喜欢睡觉')class Person(Animal):  def __init__(self,name, age):    self. name = name    self. age = age  def eat(self):    print('人都喜欢吃饭')class GirlFriend(Person):  passxiaoli = GirlFriend('小丽',28)print(xiaoli. eat())super() 是一个特殊函数，帮助子类和父类关联起来 123456789101112131415161718192021class People:  def __init__(self,name):    self. name = name    print('平民'+ self. name +'被调用了')  def drink(self):    print(self. name + '在喝水')class Police(People):  def __init__(self,name,gun):    super(). __init__(name)    self. gun = gun  def shooting(self):    print('%s在用%s开枪' %(self. name,self. gun))if __name__ == '__main__':  xiaoming = People('小明')  laowang = Police('老王','沙漠之鹰')  laowang. drink()  laowang. shooting()2. 1. 3重写:  子类可以继承父类的所有方法和属性, 但是如果子类发现继承来的内容不适合自己, 子类可以修改, 这时需要子类重写一遍父类的属性和方法.  完全重写: 子类将父类的方法推翻了, 自己重新写了一个和父类方法名一样的方法 ( 参数名和个数可以完全不同) , 子类对象调用的是子类的新重写的方法 增加功能重写(增量重写): 父类的方法对于子类不是完全没用, 子类需要在父类方法的基础上再增加一定的功能. 需要用 super() 关键字调用父类原来的方法.      增量重写时, 尽量保持子类和父类的参数名和参数个数保持一致   调用父类方法的三种格式         super(). work() # python3 推荐     super(Son, self). work() # python2 的方式     Father. work(self) #特殊情况使用          12345678910111213141516171819202122232425class Father:  def work(self):    print('我每天辛苦工作14个小时')  def jump(self, num):    print('我能跳%. 2f米' %num)class Son(Father):  #完全重写  def jump(self):    print('我能跳3米')  #增量重写  def work(self):    #调用父类的方法    super(). work()     # 最推荐的调用父类方法的方式    super(Son,self). work() # 功能同上,不推荐    print('我在工作之余, 和旁边的妹子建立了深切的革命友谊')# mingba = Father()# mingba. jump(5)xiaoming = Son()xiaoming. jump()xiaoming. work()构造方法的重写  如果子类中有一部分属性是父类初始化过的, 子类自己还需要做另外一些属性的初始化, 那么子类可以直接调用父类的初始化构造方法 好的习惯是在写初始化的构造方法时, 调用父类的构造方法```pythonclass Father: def init(self,name, age):   self. name = name   self. age = ageclass Son(Father):  def init(self, name, age, height, weight):    # 父类会对 name, age 初始化,所以子类就调用父类的初始化就可以了    super(). init(name, age)    self. height = height    self. weight = weight 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758### 2. 1. 4多继承骡子和马驴的关系的例子- 继承时,需要把多个父类写到新类的括号中, 逗号隔开- 如果多个父类中有同一个方法被子类继承, 那么子类查找顺序就是继承顺顺序, 先找到哪个父类,先继承哪个父类的方法. - 对多个父类的构造方法的继承的优先顺序和上面一样- 子类如果完全重写一个方法, 就会覆盖多个父类的方法- 如果子类需要对指定的某个父类的方法进增量重写, 需要用 `某个父类名. 方法名(self)` 的方式调用这个父类的方法   - 多层增量继承时, 不要用 `某个父类名. 方法名(self)` , 因为这会造成顶层父类的方法会被多次执行.  - 比如 A,B 分别继承了 Base, 而 C 同时继承了 A和B, 那么执行 C 的某个方法时可能会执两次 Base 类中的方法.  - 所以多层增量继承时, 使用 `super(). 方法名()` 可以避免此类问题. - 通过 `print(子类名. __mro__)` 可以看到子类对多个父类的继承优先级顺序 , 输出内容类似于: `(&lt;class '__main__. Mule'&gt;, &lt;class '__main__. Horse'&gt;, &lt;class '__main__. Donkey'&gt;, &lt;class 'object'&gt;)`  - Python 会在 MRO 列表上从左到右开始查找基类，直到找到第一 个匹配这个属性的类为止。  - 子类会先于父类被检查  - 多个父类会根据它们在列表中的顺序被检查  - 如果存在多个合法的选择，选择第一个父类```python#驴class Donkey:  def __init__(self, name, age):    self. name = name    self. age = age  def power(self):    print('推磨能推三天三夜不嫌累, 喝口红牛接着干')  def eat(self):    print('喜欢吃胡萝卜')#马class Horse:  def __init__(self, height):    self. height = height  def baofa(self):    print('百米起步只需要2. 3秒')  def eat(self):    print('喜欢吃青青草')#骡子class Mule(Horse, Donkey):  def eat(self):    Donkey. eat(self)   #1    print('骡子还喜欢吃大米')xiaoluo = Mule(180)xiaoluo. power()xiaoluo. baofa()xiaoluo. eat() # 子类优先继承括号中最前面的父类的方法print(Mule. __mro__) # __mro__#1 这里如果使用 super(). eat() 会按照 MRO 列表中的优先顺序而使用 Hourse 的 eat 方法. 所以这里指定了要继承Donkey 的 eat 方法. 2. 15 Interface 接口: 父类中定义了必须让继承的子类必须重写的方法 123456789class Animal:  def speak(self):    raise NotImplementedError( Not implemented ple add an anamal sound )    class Cat(Animal):  pass henry = Cat()henry. speak() # 因为没有重写speak 方法， 所以会报NotImplementedError错误2. 2 异常处理: 常见异常类型  什么是错误: 在代码运行之前 有语法错误, 叫做错误. 比如, 语法格式, 缩进, 小括号等低级错误 什么是异常: 在代码运行过程中出现的错, 叫做异常. 比如, NameError, ZeroDivisionError 异常,12345print(a)print(1/0)print(1 + 'a')fp = open('xxx. txt' , 'r')捕获异常:  将有可能出现的异常代码块放到 try 中 try 的 语法结构: try . . . except . . .  一个 try 至少对应 一个 except 一旦捕获到异常, 那么这行后面的 try 块就不再执行了, 而是直接跳到 except 块中的代码 except 后面如果不指定错误类型,那么 except 捕获所有类型异常 except Exception: 就等于 except: except 可以指定需要捕获的类型, 一旦指定, 那么只会捕获指定类型的 error 类型的错误 python 里面有一个官方异常类的基类, 名字是 Exception , 所有类型的error 都是 Exception 的子类123456try:  print('君不见黄河之水天上来')  print(a) # 一旦捕获到异常, 那么这行后面的 try 块就不再执行了, 而是直接跳到 except 块中的代码  print(' 奔流到海不复回')except:  print('这个异常捕获了')打印捕获到的异常对象内容:  用 as 来把异常捕获到的异常对象保存到一个变量中, 一般用 e  12345try: print(a) except NameError as e: # 将异常对象保存下来放到变量 e 中 print('这个异常捕获了') print(e)   多个异常捕获:  一个 try 可以跟多个 except , 但只能有一个 except 捕获到异常.  一旦第一个异常被捕获, 其他的异常就不会再被捕获了 多个 except 中, 要把 except Exception: or except : 放在最后, 因为它是模糊捕获, 精确捕获要放在前面多个 except 1234567891011try:  a  a = 1/0  b = 1 + 'a'except NameError as e :  print(e)except ZeroDivisionError as e:  print(e)except Exception:  print('报错了')合并捕获:  也可以将多个精确捕获放在一个元祖中,合并捕获1234567try:  a  a = 1/0  b = 1 + 'a'except (NameError, ZeroDivisionError): # 将  print('命名错误 或者是 被除数为0错误')else:  try中的语句, 如果有异常, 进入 except 块, 如果 try中没有异常, 那么try执行完毕后,就会执行 else 块123456try:  100except Exception as e:  print(e)else:    # try中的语句, 如果有异常, 进入 except 块, 如果 try中没有异常, 那么try执行完毕后,就会执行 else 块  print('这是 else 语句')finally:  异常捕获中的 finally: 无论有没有异常,都会执行finally. finally 一般用来做清理工作, 比如关闭文件, 关闭数据库等, 释放内存, 释放锁等123456try:  aexcept Exception as e:  print(e)finally:  #无论有没有异常,都会执行  print('Finally 在这里')raise 语句  可以自主的抛出异常 语法: raise 错误类型('描述错误') 一般错误类型指定 Exception 如果 raise 放在 except 块中, 那么意味着再次抛出异常给系统```pythontry: print(100) raise NameError(‘这是一个 name 错误’) print(200)except Exception as e: # raise print(e)123456789101112131415161718192021222324### **异常嵌套**- try中可以嵌套 try- except 中可以接着 try- 异常可以提高我们程序的健壮性, 你可以直接处理这个异常, 你也可以记录日志,将日常- - 记录下来等等, 提高代码健壮性操作```pythonprint('我要去拉萨')try:  print('我准备坐飞机去')  raise Exception('由于大雾, 飞机不能起飞了')  print('我倒拉萨了, 真漂亮')except Exception as e:  print(e)  try:    print('我准备坐火车过去了')    raise Exception('由于修路, 火车停运24小时')    print('我倒拉萨了')  except Exception as e:    print(e)    print('我骑自行车过去')    print('我倒拉萨啦')自定义异常:  要创建一个新的类来继承自官方的 Exception 异常类 写构造方法时需要初始化父类的构造函数 需要写一个__str__() 方法 打印异常信息 可以写一些方法可以对异常信息做一些处理 注意, 自定义异常只能用 raise 关键字抛出异常 , raise 语法是: raise 自定义异常类的名字('错误描述') except 语法是: except 自定义异常类的名字 as e:1234567891011121314151617181920class MyException(Exception):  def __init__(self,msg):    super(MyException,self). __init__()    self. msg = msg  def __str__(self):    return self. msg  def handle(self):    print('出现异常的处理函数, 处理掉了')try:  print('代码开始执行')  raise MyException('这里报错了')  print('代码结束')except MyException as e:  print(e)  print(e. handle())2. 3正则表达式: 文档官方文档 http://qiniu. cuiqingcai. com/wp-content/uploads/2015/02/20130515113723855. png 2. 3. 1概念: regular expression:    通过写一个规则, 让规则在目标文本中匹配一类字符串的方法. 这一类字符串都会通过规则被找到并处理.   根据不同的需求, 需要设计不同的正则表达式, 所以在不清楚别人的需求之前不要去看懂别人的正则表达式  世界最难看懂的三样东西: 医生的处方, 道士的符, 程序员的正则  使用原则: 如果能使用官方字符串函数搞定的, 尽量不用正则, 因为正则的效率不是很高.  使用场景举例:    匹配邮箱, url,  爬虫获取网页指定内容,  nginx 的配置文件 2. 3. 2 re 模块的几个方法:  需要 import re 模块 常用的一些匹配模式: 12r'&lt;. *?&gt;' # 常用来匹配一个 html 标签 r'^(?![a-zA-z]+$)(?!\d+$)(?![!@#$%^&amp;*]+$)(?![a-zA-z\d]+$)(?![a-zA-z!@#$%^&amp;*]+$)(?![\d!@#$%^&amp;*]+$)[a-zA-Z\d!@#$%^&amp;*]+$' # 包含字母,数字,特殊字符的强密码match():    如果找到了, 返回匹配到的对象, 如果没找到,返回 None  match 默认用 ^ 号  通过对象. group() 获取匹配到的对象  通过 对象. span() 获取匹配到的对象在目标文本中的位置, 是一个元组  match 函数是从目标文本的第一个字符串开始超找匹配的, 如果查找内容不在目标文本第一个位置, 那么就找不到.   返回正则对象   12345import rezm = re. match('love','love you baby') # 如果找到了, 返回匹配到的对象, 如果没找到,返回 Noneprint(zm)print(zm. group())print(zm. span())      search():    从目标文本中任何位置进行查找, 找不到返回 None, 找到返回匹配找到的字符串  只能查找到匹配到的第一个符合规则的字符串  返回正则对象 12345import rerm = re. search('love',' love you baby, do you love me') print(rm)print(rm. group())print(rm. span()) findall():    从目标文本中任意位置查找符合规则的所有匹配到的文本, 并将所有找到的内容通过列表返回.   findall 返回的不是对象,而是一个 list 1234567import rerm = re. findall('love',' love you baby, do you love me') print(rm) re. compile(r'\d\d\d-\d\d\d-\d\d\d\d'). findall('Cell: 415-555-9999 Work: 212-555-0000') # 返回列表['415-555-9999', '212-555-0000']re. compile(r'(\d\d\d)-(\d\d\d)-(\d\d\d\d)'). findall('Cell: 415-555-9999 Work: 212-555-0000') # 返回 列表元组 [('415', '555', '9999'), ('212', '555', '0000')]compile():    先建立一个正则的规则对象,然后再用这个对象去使用 match(), search(), findall() 来操作目标文本compile() 中的表达式不能随便加空格 1234import rept = re. compile('love')rm = pt. findall(' love you baby, do you love me')print(rm)finditer():     2. 3. 3 正则表达式的规则: 单字符匹配 . [] \d \D \w \W \s \S:    一个字母, 一个汉字都是单字符 . 能匹配换行符\n 之外的任意单个字符 1234import repattern = re. compile('. . . ') # . 能匹配换行符\n 之外的**任意**单个字符ret = pattern. match('l你ove you very much好') # match 通过. 匹配目标文本的第一个字符print(ret. group())[abcdefghijklmnopqrstuvwxyz]可以匹配到任意指定的集合内的单个字符    如果中间用 - , 必须在 - 两边放置的是连续的 ascii 字符区间  '[][][]' 代表的是三个单个指定集合的单个字符串  [^abc] : 取非中括号中的内容, 取反 1234import repattern = re. compile('[aol][xo][你]') # . 能匹配换行符\n 之外的任意单个字符ret = pattern. match('lo你ve you very much好') # match 通过. 匹配目标文本的第一个字符print(ret. group())123456'[aol][xo][你]' # 匹配前三个字符'[a-z]' # 匹配任意小写字母 , '[A-Z]' # 大写'[0-9]' # 数字'[0-9a-zA-Z-_]' # 所有数字,字母,中划线,下划线'[\n]' # 匹配换行符1re. compile(r'&lt;div&gt;([^\da-z]*)&lt;/div&gt;'). findall('&lt;html&gt;&lt;body&gt;&lt;div&gt;abasdfasd&lt;/div&gt;&lt;div&gt;123456&lt;/div&gt;&lt;div&gt;&amp;^%$* _(())&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;') # 返回 ['&amp;^%$* _(())'] \d 代表 匹配0-9的单个字符 相当于 [0-9] \D  代表小d 的取反, 就是除0-9 数字意外的单个字符 相当于[^\d] \s  代表空白字符, 包括空格, \t, \n table 等键 的单个字符 ,相当于 [ \t\r\n\f\v] 这段前面有个空格 \S 小 s 的取反, 代表除了空白字符之外的所有单个字符 相当于[^\s] \w 所有中英文字母, 数字, 下划线_, 的单个字符 相当于[A-Za-z0-9_] \W 小 w的取反, 空格, 中划线-, 各种符号等的单个字符 相当于[^\w]原始正则 r:    如果原始文本中有 \d , \n 等符号,我们需要在普通的正在 \d 前加二根斜线 或三根斜线 '\\\\n' 才能匹配到  可以在普通正在前面加 r 就可以用一根斜线进行转义 \d 了  r，可以将该字符串标记为原始字符串，它不包括转义字符  正则前面加 r 和 普通字符串前面加 r 是不一样的, 普通字符串前面加 r 是让字符串中的特殊字符失去意义. 但在正则前面加 r 的意思是帮我们省略了几个转义斜线.   只要写正则的 patten, 需要习惯写上 r 123456789import repattern = re. compile(r'\\d')ret = pattern. search('1lo你ve\d you very much好')print(ret. group())#另一种pattern = re. compile('\\\d')ret = pattern. search('1lo你ve\d you very much好')print(ret. group())数量词 * + ? {}:    数量词用来修饰前面的单个字符可以出现多少次  () 组在正则中也可以被数量词来修饰 {} : 来定义{}前面的单个字符的匹配次数 , 大括号里面填数字表示匹配次数 12print(re. compile(r'\d{3}'). match('123love you'). group()) # 指定三个连续的数字print(re. compile(r'\w{3}'). match('lov-e you'). group()) # 指定三个连续的文字{m,} : 定义{}前面的单个字符的匹配次数大于等于m次 1re. compile(r'[a-z]{4,}'). search('abcd love'). group(){,n} : 定义{}前面的字符的匹配次数是0-n 次, 不太实用,所以不用记 {m,n} : 定义{}前面的单个字符的匹配次数大于等于m次小于等于n次    能多则多原则, 1re. compile(r'[a-z]{4,6}'). search('abcd love'). group() # 返回 abcdef? : 用来表示前面的单个字符或者组是可选的 , 表示0次或1次 12345re. compile(r'n(um)?b'). search('My number nb is 415-555-4242'). group() # 返回 numbre. compile(r'\d?'). match('12ddd345abcdef love'). group() # 返回1re. compile(r'\d+[a-z]?\w?'). match('12ddd345abcdef love'). group() # 返回12ddre. compile(r'\d*[a-z]?[\w-]*'). match('12ddd34-5ab-cd_f love'). group() # 返回 12ddd34-5ab-cd_f* :用来表示前面的单个字符或者组是0次到无数次    相当于{0,}  这里要注意的是 0 次, 既没有匹配到不会报错, 而是返回空,  能多则多原则   12re. compile(r'\d*'). match('12345abcdef love'). group() #返回 12345re. compile(r'\d{3}[a-z]*'). match('12345abcdef love'). group() # 返回123      + :用来表示前面的单个字符或者组是1次或无数次    这里要记得, 需要最少要有一次.   等同于 {1,}  能多则多原则 12345re. compile(r'\d*[a-z ]+'). match('12ddd345abcdef love'). group() #返回 12dddre. compile(r'[a-z]{2}\d+'). search('love you100 baby'). group() #返回 ou100re. compile(r'[\sa-z]{2,}\d+'). search('_ou=100 d-angou a\tou100 baby'). group() # 返回angou a\tou100边界修饰符 , 词边界 ^ $ \b \B: ^ : 用你写的正则表达式 匹配字符串的开头, 如果多行模式 re. M 时, 匹配每一行的开头 12re. compile(r'^love'). search('love u baby'). group() # 返回 love, 这个和 matchre. compile(r'^\s[a-z]{2}\d+'). search('\tou100 baby'). group() # 返回 \tou100$ : 用你写的正则表达式 匹配字符串的末尾, 如果多行模式 re. M 时, 匹配每一行的末尾 1re. compile(r'[\sa-z]{2,}\d+$'). search('_ou=100 d-angou a\tou100'). group() # 返回 angou a\tou100^$ : 同时使用 , 这种模式就是全字符串匹配, 必须完整匹配 123re. compile(r'^[\sa-z]{2,}\d+$'). search('ouydangouaou100'). group() # 返回'ouydangouaou100're. compile(r'^[\sa-z]{2,}\d+$'). search('ouydan100gouaou100'). group() # 报错, 因为没有办法匹配到中间的100\b \B : 词边界, 分割单词的边界 , 用的很少    词边界: 除了 \w 都是词边界(\W 等于是词边界)  \b : 词边界  \B : 非词边界 123re. compile(r'\bis\b'). findall('izslaisnd is isi s island') # 词边界, 返回 两边是空白(词边界)re. compile(r'\Bis\B'). search('izslaisnd is isi s island'). group() # 非词边界, 返回两边是字母的那个 is 或 |:                      通过管道符      可以在两个选项中匹配最先找到的对象                管道符总是从最左边的规则开始去匹配, 一旦匹配到了就跳过其他右边的表达式  管道符如果在() 中出现, 那么只在()中有效  如果相匹配到真正的管道符, 就需要用 \| 来匹配了```pythonre. compile(r’555|415|4242’). search(‘My number is 415-555-4242’). group() # 返回415 ,       re. compile(r’\d+   [a-z]+’). search(‘abc123’). group() # 返回 abc         re. compile(r’ab123   456de’). search(‘ab456de’). group() # 返回 456de   re. compile(r’ab(123|456)de’). search(‘ab456de’). group() # 返回 ab456de 123456789101112#### 分组 `()`&gt; - 如果希望匹配到的内容可以分组, 就会用到分组方法&gt; - 小括号的内部视为一个整体, 后面有修饰次数或者跟着边界符, 是对这个括号的修饰- 正则表达式字符串中的第一对括号是第 1 组。第二对括号是第 2 组- 向 `group()` 方法传入参数 1 或 2，就可以取得匹配文本的不同部分- 向 `group()`方法传 入 0 或不传入参数，将返回整个匹配的文本- 用 `groups()` 可以返回匹配到的多个对象的一个元组集合```pythonre. compile(r'([a-z]\d)+\w+'). search('abc123llaa__--==$$aa'). group() # 返回 c123llaa__1234re. compile(r'(\d{3})-(\d{3})-(\d{4})'). search('My number is 415-555-4242'). group() # 返回415-555-4242re. compile(r'(\d{3})-(\d{3})-(\d{4})'). search('My number is 415-555-4242'). group(1) # 返回415re. compile(r'(\d{3})-(\d{3})-(\d{4})'). search('My number is 415-555-4242'). group(2) # 返回555re. compile(r'(\d{3})-(\d{3})-(\d{4})'). search('My number is 415-555-4242'). group(3) # 返回4242(正则1|正则2)+ : 分组或, 意思是前面的分组可以是 正则1, 也可以是2, 1re. compile(r'([a-z]\d|[-=#]-)+\w+'). search('abc123llaa__--==$$aa'). group() # 返回 c123llaa__   将多个返回的匹配对象同时赋值 1a,b,c = re. compile(r'(\d{3})-(\d{3})-(\d{4})'). search('My number is 415-555-4242'). groups()子模式 \num:  正则表达式的小括号的第一个意思是它是一个整体 第二个意思, 如果正则表达式中出现了小括号, 且匹配成功了, 那么第一个小括号匹配到的内容用 \1 表示, 第二个小括号匹配到的内容用 \2 表示. 如果不一样就会报错 子模式可以用 group(num) 函数来显示匹配到的每个小括号中的内容, 参数1 代表第一个小括号匹配到的内容, 参数2 表示第二个小括号中的内容 group() 表示匹配到的整个内容 没有子模式, 如果用 group(num) 就报错了1234re. compile(r'(\d)(. *)(\d)'). search('a1ca1c'). group() re. compile(r'(\d)(. *)\1'). search('a1ca1c'). group() #1#1 上面两行代码的执行结果类似, 第二种只有两个 group , 第一种有三个 group, 估计这就是区别, 可以少写很多括号和冗余正则表达式1234567import repattern = re. compile(r'&lt;([a-z]+)&gt;&lt;(\w+)&gt;\w+&lt;/\2&gt;&lt;/\1&gt;')re = pattern. search('hahaha&lt;div&gt;&lt;span&gt;今天学习的正则表达式很简答啊&lt;/span&gt;&lt;/div&gt;'). group() # 返回 &lt;div&gt;&lt;span&gt;今天学习的正则表达式很简答啊&lt;/span&gt;&lt;/div&gt;re = pattern. search('hahaha&lt;div&gt;&lt;span&gt;今天学习的正则表达式很简答啊&lt;/span&gt;&lt;/div&gt;'). group(1) # 返回 divre = pattern. search('hahaha&lt;div&gt;&lt;span&gt;今天学习的正则表达式很简答啊&lt;/span&gt;&lt;/div&gt;'). group(2) # 返回 spanprint(re)(?P&lt;goudan&gt;正则) 和 (?P=goudan): 和上面的子模式一个意思, 就是给小括号起个名字,  不过还是推荐用 \1, \2, 不推荐使用这种方式,因为看起来比较乱 下面的代码和上面的代码相同  123456import repattern = re. compile(r'&lt;(?P&lt;goudan&gt;[a-z]+)&gt;&lt;(?P&lt;dan&gt;\w+)&gt;\w+&lt;/(?P=dan)&gt;&lt;/(?P=goudan)&gt;')re = pattern. search('hahaha&lt;div&gt;&lt;span&gt;今天学习的正则表达式很简答啊&lt;/span&gt;&lt;/div&gt;'). group()re = pattern. search('hahaha&lt;div&gt;&lt;span&gt;今天学习的正则表达式很简答啊&lt;/span&gt;&lt;/div&gt;'). group(1)re = pattern. search('hahaha&lt;div&gt;&lt;span&gt;今天学习的正则表达式很简答啊&lt;/span&gt;&lt;/div&gt;'). group(2)print(re)   贪婪和非贪婪:  正则表达式默认都是贪婪模式, 指的是数量修饰词 + * {2,6} , 能多匹配就多匹配 不希望用贪婪模式的时候需要在数量词的后面用? 表示取消贪婪     *? +? ?? {m,n}?   123re. compile(r'&lt;div&gt;(. *)&lt;/div&gt;'). search('&lt;div&gt;狗带最漂亮&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;'). group(1) # 贪婪模式 返回 '狗带最漂亮&lt;/div&gt;&lt;/div&gt;'re. compile(r'&lt;div&gt;(. +?)&lt;/div&gt;'). search('&lt;div&gt;狗带最漂亮&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;'). group(1) # 取消贪婪 , 返回 '狗带最漂亮'模式修正:  compile 有两个参数, 第一个参数是正则表达式, 第二个参数就是对第一个参数的修正, 下面是一些常用的修正符 re. I : 忽略大小写 1re. compile(r'LOVE', re. I). search('I love you very much'). group() # 返回 lovere. M : 多行匹配模式 12345string = '''Iove youlove shelove he'''re. compile(r'^love', re. M). search(string). group() # 返回 lovere. S : 单行模式, 把目标文本都放到一行里面,然后用正则表达式来匹配 123456string = '''&lt;div&gt;iloveyou&lt;/div&gt;'''re. compile(r'&lt;div&gt;(. *?)&lt;/div&gt;', re. S). search(string). group() # 返回 '&lt;div&gt;i\nlove\nyou\n&lt;/div&gt;'re. DOTALL : 让 . 可以匹配到换行符, 这种一般都是 . * 一起用 1re. compile(r'. *', re. DOTALL). search('Serve the public trust. \nProtect the innocent. \nUphold the law'). group() # 返回整个字符串; 如果不用 re. DOTALL, 则只能返回第一个换行符前面的部分. re. VERBOSE : 让你的正则容易看懂  注意,这里需要用 ''' ''' 来把正则表达式扩上, 这样就可以使用多行来表示了.  123456789import rephoneRegex = re. compile(r'''((\d{3}|\(\d{3}\))?       # area code (\s|-|\. )?           # separator\d{3}              # first 3 digits(\s|-|\. )            # separator\d{4}              # last 4 digits(\s*(ext|x|ext. )\s*\d{2,5})?  # extension)''', re. VERBOSE) | 让多个修正符一起用  compile 只允许有两个参数, 当我们需要用到多个修正符的时候, 就需要用管道符来帮忙了可以使用管道字符|将变量组合起来，从而绕过这个限 制 1someRegexValue = re. compile('foo', re. IGNORECASE | re. DOTALL | re. VERBOSE)正则替换 sub: pattern. sub() : 将正则匹配到的内容用字符串进行替换  第一个参数可以是替换的字符, 也可以是一个函数, 该函数的返回值必须是一个字符串 如果使用函数作为参数, 那么函数的返回值需要是 string 类型, 如果不是, 则需要通过 str() 函数来转换1re. compile(r'\d+'). sub('35','python 班级里面的学生是34人') # 返回 python 班级里面的学生是35人 用分组的数字 \1 \2 进行替换 , 例子是替换间谍名字12re. compile(r'Agent \w*'). findall('Agent Alice told Agent Carol that Agent Eve knew Agent Bob was a double agent. ') # 返回 list ['Agent Alice', 'Agent Carol', 'Agent Eve', 'Agent Bob']re. compile(r'Agent (\w)\w*'). sub(r'\1****','Agent Alice told Agent Carol that Agent Eve knew Agent Bob was a double agent. ') # 返回 字符串 'A**** told C**** that E**** knew B**** was a double agent. ' 使用函数作为参数例子1: 单纯的返回一个替换内容, 和 inline 方式一致```pythonimport redef fn(a):			# 参数a 是默认传递进方法的参数, 是用search()匹配到的对象  return “200” ret = re. compile(r’\d+’). sub(fn, ‘python 班级里学生为100人’)print(ret) 12345678910- 使用函数作为参数例子2: 将正则表达式匹配到的内容做处理后再替换```pythonimport re		strdef fn(a):		# 参数a 是默认传递进方法的参数, 是用search()匹配到的对象  return str(int(a. group())+1)  # 将正则表达式匹配到的内容'100'做整型转换后加一后再返回ret = re. compile(r'\d+'). sub(fn, 'python 班级里学生为100人')print(ret)正则切割 split: pattern. split() : 按照正则进行匹配, 并用匹配到的内容对元字符串进行切分, 返回一个数组 1re. compile(r'\d'). split('python 1班级里2面的学3生是3人') # 返回['python ', '班级里', '面的学', '生是', '人']Look Around 环顾模式:  xxx可以是单个字符, 也可以是一个分组以某个模式 xxx 为中心, 分为四种:    xxx(?=pattern) :positive look ahead: xxx右是, 匹配到的内容不占空间  xxx(?!pattern) :negative look ahead xxx右非, 匹配到的内容不占空间  (?&lt;=pattern)xxx :positive look behind xxx左是, 匹配到的内容不占空间  (?&lt;!pattern)xxx :negative look behind xxx左非, 匹配到的内容不占空间  括号内的算是子模式 xxx(?=pattern)Look Ahead 右是 12re. compile(r'\w+,'). findall('Serve the, public, trust') # 匹配多个字母和一个逗号的连续 . 返回 ['the,', 'public,']re. compile(r'\w+(?=,)'). findall('Serve the, public, trust') # 和上一个一样, 只是不占用逗号. 返回 ['the', 'public']xxx(?!pattern)Look Ahead 右非 123456789re. compile(r'\w+(?!,)'). findall('Serve the, public, trust') # 捕获连续字符,但字符右侧不是逗号的所有匹配. 返回 ['Serve', 'th', 'publi', 'trust']result = re. compile(r'Jahn(?!\sBmith)'). finditer('I shall rather out go out with Jahn McLume than with Jahn Bmith or Jahn Con Movi')for i in result:	print i. start(),i. end()#31 35 # 第一个 Jahn 的起始位置#67 71 # 第二个 Jahn 的起始位置(?&lt;=pattern)xxx 左是 1re. compile(r'(?&lt;=Jahn\s)McLume'). findall('I shall rather out go out with Jahn McLume than with Jahn Bmith McLume or Jahn Con Movi') # 返回左边是 Jahn 的那个 McLume # 返回 ['McLume'](?&lt;!pattern)xxx 左非 1re. compile(r'(?&lt;!Jahn\s)Boe'). findall('Jahn Boe, Calvin Boe, Hobbey Boe') # 返回Boe 的左边不是 Jahn 的 Boe, 返回两个['Boe', 'Boe']一个强密码的例子: 包含字母,数字特殊字符的密码 12345678910111213141516import reprint(re. compile(r'''(^(?![a-zA-z]+$)     # 从目标文本开头开始,到末尾, 右侧不能仅仅是大小写字母(?!\d+$)        # 从目标文本开始到结束, 右侧不能仅仅是数字(?![!@#$%^&amp;*]+$)    # . . 不能仅仅是特殊字符(?![a-zA-z\d]+$)    # . . 不能仅仅是字母和数字(?![a-zA-z!@#$%^&amp;*]+$) # . . 不能仅仅是字母和特殊字符(?![\d!@#$%^&amp;*]+$)   # . . 不能仅仅是数字和特殊字符[a-zA-Z\d!@#$%^&amp;*]+$  # 的 包含字母,数字和特殊字符的多个, 直到末尾)''', re. VERBOSE | re. M). findall('''Satoshi Nakamotox21#$Alice NakamotoRoboCop Nakamoto'''))# 返回 x21#$几道练习题:  写一个正则表达式，匹配每 3 位就有一个逗号的数字?它必须匹配以下数字:‘42’‘1,234’‘6,368,745’但不会匹配:‘12,34,567’ (逗号之间只有两位数字) ‘1234’ (缺少逗号) 12345678import reprint(re. compile(r'''(^\d{1,3}		#1(,\d{3})*$		 #2)''', re. VERBOSE). match('6,368,745'). group()) #1 用^$来全字符串匹配,防止只匹配到部分数字的情况; 第一个逗号左边的三位数可以是1-3位#2 逗号和后面位数字是一个整体,会一起出现,且是0次到无数次 如何写一个正则表达式，匹配姓 Nakamoto 的完整姓名?你可以假定名字 总是出现在姓前面，是一个大写字母开头的单词。该正则表达式必须匹配: ‘Satoshi Nakamoto’ ‘Alice Nakamoto’ ‘RoboCop Nakamoto’ 但不匹配: ‘satoshi Nakamoto’(名字没有大写首字母) ‘Mr. Nakamoto’(前面的单词包含非字母字符) ‘Nakamoto’ (没有名字) ‘Satoshi nakamoto’(姓没有首字母大写) 1234567891011121314import reprint(re. compile(r'''(^[A-Z][A-Za-z]+\sNakamoto$)''', re. VERBOSE | re. M). findall('''Satoshi NakamotoAlice NakamotoRoboCop Nakamotosatoshi NakamotoMr. NakamotoNakamotoSatoshi nakamoto''')) 如何编写一个正则表达式匹配一个句子，它的第一个词是 Alice、Bob 或Carol，第二个词是 eats、pets 或 throws，第三个词是 apples、cats 或 baseballs。该句 子以句点结束。这个正则表达式应该不区分大小写。它必须匹配:‘Alice eats apples. ’‘Bob pets cats. ’‘Carol throws baseballs. ’‘Alice throws Apples. ’‘BOB EATS CATS. ’但不匹配:‘RoboCop eats apples. ’‘ALICE THROWS FOOTBALLS. ’‘Carol eats 7 cats. ’ 123456789101112131415import reprint(re. compile(r'''(^(Alice|Bob|Carol)\s+(eats|pets|throws)\s+(apples|cats|baseballs)\. $)''', re. VERBOSE | re. M | re. I). findall('''Alice eats apples. Bob pets cats. Carol throws baseballs. Alice throws Apples. BOB EATS CATS. RoboCop eats apples. ALICE THROWS FOOTBALLS. Carol eats 7 cats. ''')) 强密码: 写一个函数，它使用正则表达式，确保传入的口令字符串是强口令。强口令的 定义是:长度不少于 8 个字符，同时包含大写和小写字符，至少有一位数字。你可 能需要用多个正则表达式来测试该字符串，以保证它的强度。 2. 4 Web 数据爬取: urllib - 网页内容抓取:  需要导入 urllib 模块 from urllib import request or import urllib. request 然后再用正则来过滤数据, import re , 这里面最常用正则函数是 findall() urllib 与 MAC 电脑: 因为证书问题, 打开 https 会报错, 需要加入下面的代码来防止这个问题12import sslssl. _create_default_https_context = ssl. _create_unverified_context####urlopen的基本使用方法  注意: 当读取的数据是进制形式, 那么需要用 decode() 函数取解码, 网页用什么进行的编码, 那么就要用什么去解码.  网页上的编码在 HTML 中 &lt;meta charset='utf-8'&gt;获取http 状态码 和 URL 地址: http请求传递给web 服务器后，web服务器会返回给我们一个状态码&lt;meta charset='utf-8'&gt; 200s: 成功 消息300s: 跳转消息400s: 客户端错误消息500s: 服务器端错误消息 123456789101112131415from urllib import requesturl = 'http://bbs. tianya. cn/m/post-140-393974-6. shtml'response = request. urlopen(url, timeout=1) # 返回一个 URL open 对象#print(response. read(). decode('utf-8')) # 默认是 rb 形式读取, 需要指定 读取的code 编码#读取数据# response. read()        # 读取全部数据# response. readline()      # 读取一行数据# data = response. readlines()   # 读取全部数据,返回一个列表# for line in data:#   print(line. decode('utf-8')) # 遍历返回装有数据的列表并进行解码print(response. getcode())    # 获取返回的 http 状态码print(response. geturl())    # 获取 url 地址 . 异常捕获- timeout: 12345from urllib import requesttry:  res = request. urlopen(url, timeout=1) # 返回一个 URL 对象 , 如果在 timeout 规定时间内没有返回, 就报错except:  print('我们都知道你有异常了')URL 的 解码: 123456from urllib import requesturl = 'https://image. baidu. com/search/index?tn=baiduimage&amp;ct=201326592&amp;lm=-1&amp;cl=2&amp;ie=gbk&amp;word=%C3%C0%C5%AE&amp;fr=ala&amp;ala=1&amp;alatpl=adress&amp;pos=0&amp;hs=2&amp;xthttps=111111'myUrl = request. unquote(url)request. quote(myUrl)print(myUrl)GET POST 请求URL 地址的拼接: 域名 + 路径 + ? 参数名 = 参数值 &amp; 参数名 = 参数值 …. GET 1234567891011121314from urllib import request, parse # Base URL being accessedimport sslssl. _create_default_https_context = ssl. _create_unverified_contexturl = 'http://httpbin. org/get'# Dictionary of query parameters (if any)parms = {'name1' : 'value1', 'name2' : 'value2'}# Encode the query stringquerystring = parse. urlencode(parms) # 处理get 需要传递的参数# Make a GET request and read the responseu = request. urlopen(url+'?' + querystring) # 拼接参数resp = u. read(). decode('utf-8')print(resp)POST 123456789101112131415from urllib import request, parse # Base URL being accessedimport sslssl. _create_default_https_context = ssl. _create_unverified_contexturl = 'http://httpbin. org/post'# Dictionary of query parameters (if any)parms = {'name1' : 'value1', 'name2' : 'value2'}# Encode the query stringquerystring = parse. urlencode(parms) # 处理post 需要传递的参数# Make a GET request and read the response# u = request. urlopen(url+'?' + querystring) # 拼接参数u = request. urlopen(url, querystring. encode('ascii'))resp = u. read(). decode('utf-8')print(resp)urlretrieve() 图片下载: 通过函数下载图片url 是图片网址path 是文件保存路径 1request. urlretrieve(url,path)练习:: 1. 匹配手机, 邮箱, QQ号 1234567891011import refrom urllib import requesturl = 'http://bbs. tianya. cn/m/post-140-393974-6. shtml'response = request. urlopen(url)data = response. read(). decode('utf-8')#匹配手机号print(re. compile('[1][3-8][0-9]{9}'). findall(data))#匹配邮箱print(re. compile('\w+@\w+\. \w+'). findall(data))#匹配 QQ号print(re. compile('(qq:|QQ:)([1-9]\d{5,10})'). findall(data))2. 匹配图书的 书名 和 封面的图片, 并写入到新的 html 文件中  网页爬取技巧:    如果可能,先把需要爬取的目标 网页的源代码通过 data = request. urlopen(url). read(). decode('utf-8') 打印出来,然后存到一个临时的文件中.   先找到要匹配的html内容的 samples , 如果是重复的, 那就多找个, 放在代码中注释掉, 这样比较方便进行观察, 写出对应的正则表达式  把 sample 直接放到正则表达式的 compile 中.   通过 pycharm 高亮可以看到需要把正则表达式中的什么字符进行转义.  1234567891011121314151617181920212223242526272829303132333435from urllib import requestimport reimport sslssl. _create_default_https_context = ssl. _create_unverified_contexturl = 'https://book. douban. com/latest?icn=index-latesbook-all'data = request. urlopen(url). read(). decode('utf-8')# print(data)#书名的'''&lt;a href= https://book. douban. com/subject/27156672/ &gt;梁光正的光&lt;/a&gt;&lt;a href= https://book. douban. com/subject/27115000/ &gt;高效能阅读&lt;/a&gt;&lt;a href= https://book. douban. com/subject/27156306/ &gt;现实不似你所见&lt;/a&gt;'''booknames = re. compile(r'&lt;a href= https://book\. douban\. com/subject/\d+/ &gt;. *&lt;/a&gt;'). findall(data)print(booknames)#图片的标签'''&lt;img src= https://img3. doubanio. com/mpic/s29610044. jpg /&gt;&lt;img src= https://img3. doubanio. com/mpic/s29563532. jpg /&gt;&lt;img src= https://img3. doubanio. com/mpic/s29560886. jpg /&gt;'''print('#'*100)imgs = re. compile(r'&lt;img src= https://img\d\. doubanio\. com/mpic/s\d+\. jpg /&gt;'). findall(data)print(imgs)path = '. /books. html'f = open(path, 'w')i = 0for img in imgs:  f. write(imgs[i])  f. write(booknames[i])  i += 1f. close()3. 股票信息抓取 123456789101112131415161718192021#股票抓取from urllib import requestimport reimport sslssl. _create_default_https_context = ssl. _create_unverified_context# 多页抓取 1 - 46页urls = [ 'http://quote. stockstar. com/stock/sha_3_1_' + str(i+1) + '. html' for i in range(46)]for url in urls:  rowdata = request. urlopen(url). read(). decode('gb2312')  htmlData= re. compile(r'&lt;tbody class= tbody_right  id= datalist &gt;(. *?)&lt;/tbody&gt;', re. DOTALL). findall(rowdata)  stockData = re. compile(r'&gt;(. *?)&lt;', re. DOTALL). findall(htmlData[0])  while '' in stockData:    stockData. remove('')  i = 1  while stockData:    print(stockData[0], end =  , )    stockData. remove(stockData[0])    if i%12 == 0:      print('')    i += 14. 图片下载 12345678910111213141516171819# 图片下载from urllib import requestimport reimport sslimport osssl. _create_default_https_context = ssl. _create_unverified_contexturl = 'https://book. douban. com/latest?icn=index-latesbook-all'data = request. urlopen(url). read(). decode('utf-8')imgData = re. compile(r'&lt;img src= (https://img3\. doubanio\. com/mpic/s\d+\. jpg) /&gt;'). findall(data)print(imgData)path = '. /downloadImgs'if not os. path. exists(path):  os. mkdir(path)for url in imgData:  filename = url. split('/')[-1]  newFile = os. path. join(path,filename)  request. urlretrieve(url,newFile)模拟浏览器请求    有的时候我们去爬取一些数据, 发现网页会返回错误: 403 forbidden , 如果加上 User -Agent 参数, 就可以正常访问了.   查看浏览器 User-Agent 的方式            Safari: 进入 Develop 菜单, 查看 User Agent          1234567891011# 模拟 Safari 的 User Agent 去访问网页from urllib import requestimport reimport osimport sslssl. _create_default_https_context = ssl. _create_unverified_contexturl = 'http://www. webometrics. info/en/Asia'myAgent = {'User-Agent': 'Mozilla/5. 0 (Macintosh; Intel Mac OS X 10_13_1) AppleWebKit/604. 3. 5 (KHTML, like Gecko) Version/11. 0. 1 Safari/604. 3. 5'}req = request. Request(url, headers=myAgent)res = request. urlopen(req)print(res. read(). decode('utf-8'))requests 模块 获取网页内容:  和 urllib 模块的 request 类似, 都可以用来抓取网页 好处是对于 mac 电脑, 不会遇到ssl 报错, 也不需要 import ssl 了123456789101112# 打开网页, 获取内容,并写入到文件中import requestsurl = 'http://bbs. tianya. cn/m/post-140-393974-6. shtml'res = requests. get(url)code = res. status_codeprint(res. text[:250])print(code)path = '. /temp1. txt'f = open(path, 'wb')  # 文件需要以二进制形式写入for chunk in res. iter_content(100000): # iter_content 是一个迭代器,可以按块返回二进制形式网页内容  f. write(chunk)f. close()用 BeautifulSoup 模块 配合 requests 模块, 解析 HTML:  beautifulsoup 模块是用来快速提取 html 标签中的信息的, 可以降低复杂的正则表达式来匹配数据的工作量 简称 bs4 该模块需要通过 pip 进行安装扩展后才能通过 import bs4 在代码中导入 bs4 的 select() 方法使用范例      选择器   将匹配         soup. select(‘div’)   所有名为&lt;div&gt;的元素       soup. select(‘#author’)   带有 id 属性为 author 的元素       soup. select(‘. notice’)   所有使用 CSS class 属性名为 notice 的元素       soup. select(‘div span’)   所有在&lt;div&gt;元素之内的&lt;span&gt;元素       soup. select(‘div &gt; span’)   所有直接在&lt;div&gt;元素之内的&lt;span&gt;元素，中间没有其他元素       soup. select(‘input[name]’)   所有名为&lt;input&gt;，并有一个 name 属性，其值无所谓的元素       soup. select(‘input[type=”button”]’)   所有名为&lt;input&gt;，并有一个 type 属性，其值为 button 的元素   1pip3. 7 install beautifulsoup412345678910111213141516import bs4, requestsurl = 'https://book. douban. com/latest?icn=index-latesbook-all'res = requests. get(url)res. raise_for_status()data = bs4. BeautifulSoup(res. text, html. parser )#获取书名booknames = data. select('. detail-frame &gt; h2 &gt; a')#获取书的图片bookimgs = data. select('. cover &gt; img')print(booknames) # 返回一个list 每个元素都类似于 &lt;a href= https://book. douban. com/subject/27156672/ &gt;梁光正的光&lt;/a&gt;print(bookimgs)  # 返回一个list, 每个元素都类似于 &lt;img src= https://img3. doubanio. com/mpic/s29563532. jpg /&gt;print('#'*100)print(booknames[0]. getText()) # 获取第一个书元素的文本信息, 返回 梁光正的光print(bookimgs[0]. attrs)  # 获取第一个书元素的属性信息, 返回字典 {'src': 'https://img3. doubanio. com/mpic/s29596185. jpg'}print(str(booknames[0])) # 将 html 元素对象转换成字符串对象 返回 &lt;a href= https://book. douban. com/subject/27156672/ &gt;梁光正的光&lt;/a&gt;print(bookimgs[0]. get('src')) # 获取一个元素的属性值 , 返回 https://img3. doubanio. com/mpic/s29596185. jpgwebbrowser – 打开操作系统默认浏览器: 官方文档  这个模块，可以方便地调用系统默认浏览器，并打开/刷新页面如果使用shell 命令行调用webbrowser 需要确认 pip 是否安装了这个模块 pip3. 7 install webbrowser  命令行调用webbrowser 的方法1python -m webbrowser -t  http://www. python. org 代码中使用 12345678import webbrowserurl = 'http://docs. python. org/'# Open URL in a new tab, if a browser window is already open. webbrowser. open_new_tab(url)# Open URL in new window, raising the window if possible. webbrowser. open_new(url)用 selenium 模块控制浏览器: 官方网址  selenium 可以配合其他爬虫模块(urllib, requests 等)爬取哪些需要登录认证的页面.    selenium 模块让 Python 直接控制浏览器，实际点击链接，填写登录信息，几乎 就像是有一个人类用户在与页面交互. 与 Requests 和 Beautiful Soup 相比，Selenium 允许你用高级得多的方式与网页交互。  selenium 对 firefox 浏览器比较友好, 建议使用 selenium 操作 firefox  需要提前安装 selenium 并下载 mozilla/geckodriver 驱动( 否则会报 selenium. common. exceptions. WebDriverException: Message: ‘geckodriver’ executable needs to be in PATH 错误 )  需要先通过 pip 安装 selenium 模块1pip3. 7 install selenium 下载mozilla/geckodriver 驱动 下载 geckodriverckod  地址： mozilla/geckodriver解压后将geckodriverckod 存放至 /usr/local/bin/ 路径下即可sudo mv ～/Downloads/geckodriver /usr/local/bin/启动 selenium 控制的浏览器: 1234from selenium import webdriverbrowser = webdriver. Firefox()print(type(browser))browser. get('http://inventwithpython. com')寻找html 元素:  WebDriver 对象有好几种方法，用于在页面中寻找元素。它们被分成 find_element_*和 find_elements_*方法。find_element_*方法返回一个 WebElement 对象，代表页面中 匹配查询的第一个元素。find_elements_*方法返回 WebElement_*对象的列表，包含 页面中所有匹配的元素。 表 11-3 selenium 的 WebDriver 方法，用于寻找元素       方法名   返回的 WebElement 对象/列表         browser. find_element_by_class_name(name)   使用 CSS 类 name 的元素       browser. find_elements_by_class_name(name)           browser. find_element_by_css_selector(selector)   匹配 CSS selector 的元素       browser. find_elements_by_css_selector(selector)           browser. find_element_by_id(id)   匹配 id 属性值的元素       browser. find_elements_by_id(id)           browser. find_element_by_link_text(text)   完全匹配提供的 text 的&lt;a&gt;元素       browser. find_elements_by_link_text(text)           browser. find_element_by_partial_link_text(text)   包含提供的 text 的&lt;a&gt;元素       browser. find_elements_by_partial_link_text(text)           browser. find_element_by_name(name)   匹配 name 属性值的元素       browser. find_elements_by_name(name)           browser. find_element_by_tag_name(name)   匹配标签 name 的元素 (大小写无关，&lt;a&gt;元素匹配'a'和'A')       browser. find_elements_by_tag_name(name)          除了*_by_tag_name()方法，所有方法的参数都是区分大小写的。如果页面上没 有元素匹配该方法要查找的元素，selenium 模块就会抛出 NoSuchElement 异常。如 果你不希望这个异常让程序崩溃，就在代码中添加 try 和 except 语句。  一旦有了 WebElement 对象，就可以读取表 11-4 中的属性，或调用其中的方法， 了解它的更多功能。 表 11-4 WebElement 的属性和方法       属性或方法   描述         tag_name   标签名，例如 ‘a’表示&lt;a&gt;元素       get_attribute(name)   该元素 name 属性的值       text   该元素内的文本，例如&lt;span&gt;hello&lt;/span&gt;中的’hello’       clear()   对于文本字段或文本区域元素，清除其中输入的文本       is_displayed()   如果该元素可见，返回 True，否则返回 False       is_enabled()   对于输入元素，如果该元素启用，返回 True，否则返回 False       is_selected()   对于复选框或单选框元素，如果该元素被选中，选择 True，否则返回 False       location   一个字典，包含键’x’和’y’，表示该元素在页面上的位置   在网页中找到链接, 并点击链接 1234567891011from selenium import webdriverbrowser = webdriver. Firefox()browser. get('https://book. douban. com/latest?icn=index-latesbook-all')try:  elem = browser. find_element_by_link_text('梁光正的光') # 根据一个链接文本,获取到链接元素  print(elem. tag_name)  elem. click()  # 找到书后点击这本书except:  print('没有找到需要找的 html 元素')填写 form 表单并提交 123456from selenium import webdriverbrowser = webdriver. Firefox()browser. get('http://cn. bing. com')searchbox = browser. find_element_by_id('sb_form_q')searchbox. send_keys('python')searchbox. submit()2. 5 线程 进程 协程: 参考文档 官方线程文档 进程和线程的对比总结 多任务 操作系统都可以执行多任务 单核 CPU如何执行多任务? 一秒=1000毫秒, 一毫秒=1000微妙  操作系统轮流让各个任务交替执行, 但任务量远远超过 CPU 的核心数量, 所以操作系统会让每个任务轮流执行. 因为 cpu调度速度快, 所以看起来是同时执行的 多核 CPU执行任务 还是 CPU 轮训调度  并发: 看上去是一起执行的, 但其实当任务多于核心数时,还是轮训执行的 并行: 只有当任务数少于核心数时,才是真正的并行执行, 同时执行任务.  并发是逻辑上的同时执行, 并行是物理上的同时执行. 进程的概念 对于操作系统而言, 一个任务就是一个进程 (1). 单任务 ​	需要等待当前代码执行完毕后,再执行下一段代码 (2). 多任务(多进程) ​ 线程: 123help()threading线程概念:    如果是需要大量请求网络的工作, 尽量用多线程. 你完全可以放心的创建几千个 Python 线程，现代操作系统运行这么多线程没有任何压力，没啥可担心的.   依赖于 CPU 进行大量计算的工作, 尽量用多进程来分散计算量, 并行计算后可加快输出结果.   在一个进程的内部想干多件事情, 就需要同时运行多个子任务, 我们把每个子任务成为线程  一个进程的多个线程是共享内存空间的  线程是应用程序中工作的最小单元，它被包含在进程之中，是进程中的实际运作单位。  一条线程指的是进程中一个单一顺序的控制流，一个进程中可以并发多个线程，每条线程并行执行不同的任务。  主线程概念:       任何进程都有一个默认的主线程, 如果主线程死掉, 子线程也同时死掉, 所以子线程依赖于主线程    对比看, 主进程死掉,子进程不会死掉       线程模块       _thread : 老版本的低级模块    threading : 高级模块, 对_thread 进行了封装, 用的较多      python 使用线程  需要导入 threading 模块 123456789101112131415161718192021222324import threading&gt;&gt;&gt; threading. threading. active_count(    threading. Lock(       threading. activeCount(    threading. main_thread(    threading. Barrier(      threading. RLock(       threading. BoundedSemaphore(  threading. Semaphore(     threading. BrokenBarrierError( threading. setprofile(    threading. Condition(     threading. settrace(     threading. current_thread(   threading. stack_size(    threading. currentThread(   threading. Thread(      threading. enumerate(     threading. ThreadError(    threading. Event(       threading. TIMEOUT_MAX    threading. get_ident(     threading. Timer(       threading. local(       threading. WeakSet(   &gt;&gt;&gt; threading. Thread. threading. Thread. daemon   threading. Thread. mro(   threading. Thread. getName(  threading. Thread. name   threading. Thread. ident   threading. Thread. run(   threading. Thread. is_alive( threading. Thread. setDaemon(threading. Thread. isAlive(  threading. Thread. setName( threading. Thread. isDaemon( threading. Thread. start(  threading. Thread. join(  thread方法：  t. start() : 激活线程 t. getName() : 获取线程的名称 t. setName() ： 设置线程的名称 t. name : 获取或设置线程的名称 t. is_alive() ： 判断线程是否为激活状态 t. isAlive() ：判断线程是否为激活状态 t. setDaemon() 设置为后台线程或前台线程（默认：False）;通过一个布尔值设置线程是否为守护线程，必须在执行start()方法之前才可以使用。如果是后台线程，主线程执行过程中，后台线程也在进行，主线程执行完毕后，后台线程不论成功与否，均停止；如果是前台线程，主线程执行过程中，前台线程也在进行，主线程执行完毕后，等待前台线程也执行完成后，程序停止 t. isDaemon() ： 判断是否为守护线程 t. ident ：获取线程的标识符。线程标识符是一个非零整数，只有在调用了start()方法之后该属性才有效，否则它只返回None t. join() ：逐个执行每个线程，执行完毕后继续往下执行，该方法使得多线程变得无意义 t. run() ：线程被cpu调度后自动执行线程对象的run方法threading 创建线程: 有三种方法, (1) 老版本方法 , (2) 函数调用法 , (3) 类继承法 线程总结 12345678910111213141516171819202122232425262728293031#=====================# 总结创建线程的方法#=====================# 比较原始的线程创建方法import _thread_thread. start_new_thread(函数名,(参数,))#注意:就算不传递参数,也需要写一个空 tuple 占位#=====================# 通过类的继承创建线程import threadingclass MyThread(threading. Thread):  def __init__(self,arg):    threading. Thread. __init__(self)    self. arg = arg  # 对于父类的方法的重写 当start() 调用run() 方法  def run(self):    print(self. name, self. arg) # 输入传进来的参数, 当前线程名称    # 线程实例化mythread = MyThread('参数')mythread. start()# mythread. join() # 让线程是否等待, 如果不等待,为并发 . 等待就是依次执行#=====================# 通过实例化 threading. Thread 类来进行线程开启mythread = threading. Thread(target=函数名, args=(参数,), name='线程名')mythread. start()mythread. join()_thread 模块创建线程:  是老版本的低级模块 import _thread : 导入模块 _thread _thread. start_new_thread(函数名, 参数) : 开启线程.      参数为元祖类型   如果线程不需要传递参数, 也需要传递一个空元祖   如果主线程执行完毕, 子线程就会死掉   1234567891011121314import _threadimport timedef fun():  for i in range(10): # 循环执行10秒    print(i)    time. sleep(1)for i in range(5): # 做5次循环, 创建5个线程, 共10秒结束(5个 fun() 同时执行)  _thread. start_new_thread(fun,())for i in range(5): # 截停10秒, 这个循环是为了让主线程不结束, 这样子线程才能继续执行完毕  time. sleep(2) 因为线程共享数据,所以同时对同一个变量进行操作会出现意想不到的问题12345678910111213141516import _threadnum = 0def mySum():  # print(num)  global num  for i in range(1,100001):    num+=i  print(num)for i in range(5): # 循环5次累加1到100 (5050 * 5 = 25250)  _thread. start_new_thread(mySum, ())while True:  passthreading. Thread创建线程 函数调用法(基本法):  threading 库可以在单独的线程中执行任何的在 Python 中可以调用的对象。 当你创建好一个线程对象后，该对象并不会立即执行，除非你调用它的 start() 方法 当你调用 start() 方法时，它会调用你传递进来的函数，并把你传递进来的参 数传递给该函数 Python 中的线程会在一个单独的系统级线程中执行(比如说一个 POSIX 线程或者一个 Windows 线程)，这些线程将由操作系统来全权管理。线程一旦 启动，将独立执行直到目标函数返回1234567891011121314import threadingimport timedef worker(num): # 睡一秒, 打印传递进来的数字  time. sleep(1)  print(num)  returnfor i in range(10): # 分出10个线程  t = threading. Thread(target=worker, args=(i,), name= t. %d  % i)  # 调用 worker, 把 i 作为参数传递给 worker, 为线程起名为 t. 1, t. 2 . . . . t. 10. 返回线程对象t   print(t)  # 循环打印线程对象, 输出 &lt;Thread(t. 0, initial)&gt;  t. start() # 启动线程 t. is_alive你可以查询一个线程对象的状态，看它是否还 在执行:1234if t. is_alive():   print('Still running')else:   print('Completed') daemon=True Python 解释器在所有线程都终止后才继续执行代码剩余的部分。对于需要长时间运行的线程或者需要一直运行的后台任务，你应当考虑使用后台线程。例如:12t = Thread(target=countdown, args=(10,), daemon=True)t. start()threading. Semaphore(2) 信号量: 信号量 : 限制了线程的并发数, 超过规定个数, 就让其他的线程阻塞住  sem. acquire() : 子线程每次执行到 acquire() 会把当前的计数器的值减一 sem. release() : 子线程每次执行到 release() 会把当前的计数器的值加一123456789101112131415# 信号量# 信号量import threadingimport timesem = threading. Semaphore(2) # 创建两个并发线程通道对象def fun(num):  print('当前的{}线程在等待'. format(threading. Thread. name)) # 获取在等待的线程  sem. acquire() # 并发通道计数器减2 (两个线程分别执行到这一行,所以通道数减为0)  time. sleep(2)  print(num)  sem. release() # 睡2秒后两个通道被释放, 恢复到2个for i in range(5): # 创建5个并发线程  threading. Thread(target=fun, args=(i,)). start()with sem: 信号量简写(和上面的逻辑一致) 12345678910111213# 信号量import threadingimport timesem = threading. Semaphore(2) # 创建两个并发线程通道对象def fun(num):  print('当前的{}线程在等待'. format(threading. Thread. name)) # 获取在等待的线程  with sem: # 信号量简写的方法    time. sleep(2)    print(num)for i in range(5): # 创建5个并发线程  threading. Thread(target=fun, args=(i,)). start()123456789101112131415# 信号量import threadingimport timesem = threading. Semaphore(2) # 表示只有两个线程可以并发执行. def fun():  with sem:    print(threading. current_thread(). name)    time. sleep(2)for i in range(5): # 开启五个子线程  mythread = threading. Thread(target=fun, name='mythread='+str(i))  mythread. start()创建线程 继承法: 通过父类 run() 方法开启线程 对象. start  获取当前线程名称 self. name 注意: 不需要在代码结束处添加截停代码, 主线程会在子线程执行完毕后再继续执行123456789101112import threadingclass MyThread(threading. Thread):  def run(self): # run方法必须重写    print('我是 threading 模块创建的线程')for i in range(5):  mythread = MyThread() # 初始化一个线程对象  mythread. start() # 开启线程, start() 会自动调用 类的 run() 方法print('xxx')为线程传递参数: 123456789101112131415161718# 为线程传递参数import threadingclass MyThread(threading. Thread):  def __init__(self,i):    threading. Thread. __init__(self) # 调用父类初始化方法    self. i = i  def run(self):    print(self. name, '参数是' , self. i) # 使用参数mythread = MyThread(1) # 为线程传递一个参数mythread. start()print(mythread)# 输出如下# Thread-1 参数是 1# &lt;MyThread(Thread-1, stopped 123145414057984)&gt;线程等待: 线程对象. join(): 让每个子线程执行完毕后再让下一个线程执行. 如果没有join(),5个线程瞬间执行完毕 1234567891011121314151617# 线程等待import threadingimport timeclass MyThread(threading. Thread):  def run(self):    print(self. name,self)    time. sleep(2)for i in range(5):  my = MyThread()  my. start()  my. join() # , 让每个子线程等待主线程. 如果没有这一行,5个线程瞬间执行完毕print('执行完毕')threading. RLock() 和 threading. Lock() 线程锁:    我们使用线程对数据进行操作的时候，如果多个线程同时修改某个数据，可能会出现不可预料的结果，为了保证数据的准确性，引入了锁的概念。  在一个函数的开始和结束的位置加线程锁, 那么线程就不会同时执行这个函数 threading. RLock和threading. Lock 的区别    从 lock. acquire() 到 lock. release() 之间的代码在同一时刻只能有一个线程执行通过, 其他线程需要排队.   RLock允许在同一线程中被多次 acquire()。而Lock却不允许这种情况。 如果使用RLock，那么 acquire() 和 release() 必须成对出现，即调用了n次acquire，必须调用n次的release才能真正释放所占用的锁。  所以尽量用 Rlock  Luck 线程锁 为了避免线程间对数据的处理冲突,需要加入线程锁 一旦线程到了线程锁位置, 需要等待前面的线程执行完,解锁后才能进入.  加锁 lock. acquire() 和 解锁 lock. release() 必须成对出现. 如果没有成对出现, 那么就会造成多加一次锁或者多解一次锁的意想不到的问题1234567891011121314151617import threadingnum = 0lock = threading. Lock() # 创建一个锁对象class MyThread(threading. Thread):  def run(self):    global num    print(num)    # print(lock. acquire()) # 打印 True    lock. acquire() # 加锁    for i in range(1000000):      num+=i    print(num)    lock. release()     # 解锁for i in range(5): # 循环5次进行累加, 会返现  mythread = MyThread() 两把锁的锁定问题123456789import threadinglock = threading. Lock()print(lock. acquire())print(lock. acquire()) # 前一个锁没有被解锁, 那么执行第二次锁定就会线程阻塞, 不会执行后面的打印代码print('执行完毕')线程锁的 with 写法  with lock:1234567891011121314151617import threadingnum = 0lock = threading. Lock() # 创建一个锁对象class MyThread(threading. Thread):  def run(self):    global num    # print(num)    # print(lock. acquire()) # 打印 True    with lock: # 加锁      for i in range(1000000):        num+=i      print(num)for x in range(5): # 循环5次进行累加, 会返现  mythread = MyThread()  mythread. start()Lock 的例子 12345678910111213141516# import threading## lock = threading. Lock()# lock. acquire()# lock. acquire() # 产生死锁# lock. release()# lock. release()import threadingrlock = threading. RLock()rlock. acquire()rlock. acquire() # 在同一线程内，程序不会堵塞。rlock. release()rlock. release()print( end.  ) 死锁: 加了锁之后不解锁,就会卡住1234567891011121314151617181920212223242526272829import threadingimport timeboylock = threading. Lock()girllock = threading. Lock()class Boy(threading. Thread):  def run(self):    boylock. acquire()    print('男孩生气了')    time. sleep(1)    girllock. acquire()    print('气死了')    girllock. release()    boylock. release()class Girl(threading. Thread):  def run(self):    girllock. acquire()    print('女孩也生气了')    time. sleep(1)    boylock. acquire()    print('好像进不来了')    boylock. release()    girllock. release()boy = Boy()girl = Girl()boy. start()girl. start() Rlock, 可以解决死锁问题     RLock允许在同一线程中被多次acquire。而Lock却不允许这种情况。注意：如果使用RLock，那么acquire和release必须成对出现，即调用了n次acquire，必须调用n次的release才能真正释放所占用的琐。   简写 with RLock:   12345678910111213141516171819202122232425import threadingimport time# lock = threading. Lock() # 用 Luck, 在这个代码中会造成死锁rlock = threading. RLock() # 用 RLock 代替 Luck 可以避免解锁# Rlock 单线程可以重复锁定, 但是锁几次就要开几次num = 0class MyThread(threading. Thread):  def run(self):    global num    rlock. acquire()    num+=1    print(num,self. name)    rlock. acquire()    num+=100    print(num)    # rlock. release()for i in range(5):  mythread = MyThread()  mythread. start()  time. sleep(1)  # mythread. join()print('程序执行完成')Rlock 的例子 12345678910111213141516171819import threadingimport timenum = 0rlock = threading. RLock() # 实例化锁类def work():  with rlock: # 在一个函数的开始和结束的位置加线程锁, 那么线程就不会同时执行这个函数    global num    num += 1    time. sleep(1)    # 如果没有 lock有这行 sleep, 10个线程就会打印出10个10    print(num)  # 解锁. 从 lock. acquire() 到 lock. release() 之间的代码在同一时刻只能有一个线程执行, 其他线程需要排队. for i in range(10):  t = threading. Thread(target=work)  t. start()threading. Event()线程间通信: 线程的一个关键特性是每个线程都是独立运行且状态不可预测。如果程序中的其 他线程需要通过判断某个线程的状态来确定自己下一步的操作，这时线程同步问题就 会变得非常棘手。为了解决这些问题，我们需要使用 threading 库中的 Event 对象。 Event 对象包含一个可由线程设置的信号标志，它允许线程等待某些事件的发生。在 初始情况下，event 对象中的信号标志被设置为假。如果有线程等待一个 event 对象， 而这个 event 对象的标志为假，那么这个线程将会被一直阻塞直至该标志为真。一个 线程如果将一个 event 对象的信号标志设置为真，它将唤醒所有等待这个 event 对象的 线程。如果一个线程等待一个已经被设置为真的 event 对象，那么它将忽略这个事件， 继续执行。  一个线程发送一个event信号，其他的线程则等待这个信号, 用于主线程控制其他线程的执行。 Events 管理一个flag，     这个flag可以使用 set()设置成True或者使用 clear()重置为False，   wait()则用于阻塞 Block until the internal flag is true.    在 flag为 True之前。flag默认为 False。    Event. wait([timeout]) ： 堵塞线程，直到Event对象内部标识位被设为True或超时（如果提供了参数timeout） Event. set() ：将标识位设为Ture Event. clear() ： 将标识伴设为False Event. isSet() ：判断标识位是否为Ture12345678910import threadinge = threading. Event() # 创建一个 Event 对象def go():  e. wait() # wait() 只能被 set 解开  print(threading. current_thread(). name)threading. Thread(target=go). start()print(e)e. set() # 使用 set 方法取消所有线程的wait() 等待.  e. clear() Reset the internal flag to false12345678910111213141516171819import threadingimport timenum = 0rlock = threading. RLock() # 实例化锁类import threadinge = threading. Event() # 创建一个 Event 对象def go():  e. wait() # wait() 只能被 set 解开, set 设置 flag 为 true  e. clear() # 再次将 flag 设置为 false  print(threading. current_thread(). name)threading. Thread(target=go). start()for i in range(4):  e. set() # 使用 set 方法取消所有线程的wait() 等待.   time. sleep(1)12345678910111213141516171819import threadingdef do(event):  print('start')  event. wait()  # 堵塞线程，直到Event对象内部标识位被设为True或超时（如果提供了参数timeout）, 才会执行后面的代码.   print('execute')event_obj = threading. Event() # 创建一个 event 对象for i in range(10):  t = threading. Thread(target=do, args=(event_obj,)) # 建立十个线程分别执行 do(), 每个线程都会被阻塞, 直到 input 给出 true 的信号.   t. start()event_obj. clear()inp = input('input:')if inp == 'true': # 输入 ture 后,event 接收到 信号,设置flag 为 true  event_obj. set()threading. Condition() 线程同步 / 线程的切换:    Python提供的Condition对象提供了对复杂线程同步问题的支持。  Condition被称为条件变量，除了提供与Lock类似的acquire和release方法外，还提供了wait和notify方法。  线程首先acquire一个条件变量，然后判断一些条件。如果条件不满足则wait；如果条件满足，进行一些处理改变条件后，通过notify方法通知其他线程，其他处于wait状态的线程接到通知后会重新判断条件。不断的重复这一过程，从而解决复杂的同步问题。  notify: Wake up one or more threads waiting on this condition, if any  notifyAll: Wake up all threads waiting on this condition 1234&gt;&gt;&gt; threading. Condition. threading. Condition. mro(    threading. Condition. notifyAll( threading. Condition. notify(   threading. Condition. wait(   threading. Condition. notify_all( threading. Condition. wait_for( 两个线程交替执行 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647import threadingimport timedef fun1():  with cond:   # 1     for i in range(10):      time. sleep(1)      print(threading. current_thread(). name, i)      if i == 4:        cond. wait() # 2 def fun2():  with cond: # 1    for i in range(10):      time. sleep(1)      print(threading. current_thread(). name,i)    cond. notify() # 3 cond = threading. Condition()threading. Thread(target=fun1). start()threading. Thread(target=fun2). start()# 1有了 with condition, 那么两个线程就不会并行执行,而是会有先后顺序的一个一个执行每个线程# 2当执行完四个循环后, 让线程1的任务开始等待, 后面的线程开始启动# 3当第二个线程执行完毕, 通知所有等待线程继续工作. # 输出# Thread-1 0# Thread-1 1# Thread-1 2# Thread-1 3# Thread-1 4# Thread-2 0# Thread-2 1# Thread-2 2# Thread-2 3# Thread-2 4# Thread-2 5# Thread-2 6# Thread-2 7# Thread-2 8# Thread-2 9# Thread-1 5# Thread-1 6# Thread-1 7# Thread-1 8# Thread-1 9 三个线程交替执行123456789101112131415161718192021222324252627282930313233343536373839import threadingimport timedef fun1():  with cond:    for i in range(3):      print(threading. current_thread(). name, i)      cond. wait() # 1      time. sleep(1)      cond. notify() # 5def fun2():  with cond:    for i in range(3):      print(threading. current_thread(). name, i)      cond. wait() # 2      time. sleep(1)      cond. notify() # def fun3():  with cond:    for i in range(3):      print(threading. current_thread(). name,i)      cond. notify() # 3      time. sleep(1)      cond. wait() # 4cond = threading. Condition()threading. Thread(target=fun1). start()threading. Thread(target=fun2). start()threading. Thread(target=fun3). start()# 1 第一个线程执行一个循环开始等待, 后续线程自动开始继续# 2 第二个线程执行一个循环开始等待, 后续线程自动开始继续# 3 第三个线程开始工作,并通知其他暂停的线程开始工作# 4 第三个线程开始等待# 5 第一个线程由暂停状态恢复, 通知其他线程暂停的线程继续工作,并继续自己下一个循环 多个线程等一个线程的指令123456789101112131415161718192021222324252627import threadingdef consumer(cond):  with cond:    print( consumer before wait )    cond. wait()    print( consumer after wait )def producer(cond):  with cond:    print( producer before notifyAll )    cond. notifyAll()    print( producer after notifyAll )condition = threading. Condition() # 获取 Condition 对象c1 = threading. Thread(name= c1 , target=consumer, args=(condition,))  # 创建一个c1 线程对象, 执行 consumer 函数并将 condition 对象作为参数传递进去c1. start()c2 = threading. Thread(name= c2 , target=consumer, args=(condition,))  # 创建一个c2 线程对象. c2. start()# 两个 consumer 线程都已经被阻塞住了, 等待 notifyAll() 通知大家继续. p = threading. Thread(name= p , target=producer, args=(condition,))   # 创建另一个 p 线程对象, 执行 notifyAll() 方法p. start()# consumer()线程要等待producer()设置了Condition之后才能继续。threading. Timer 定时执行:  对线程定时执行1234567import threadingdef go():  print('我走了')t = threading. Timer(3,go)t. start()print('主线程结束了')queue. Queue 线程间通信:  适用于多线程编程的先进先出数据结构，可以用来安全的在多个线程间传递信息。  创建一个被多个线程共享的 Queue 对象，这些线程通过使用 put() 和 get() 操作 来向队列中添加或者删除元素 12345678910&gt;&gt;&gt; queue. queue. deque(     queue. heappush(   queue. threading   queue. Empty(     queue. LifoQueue(   queue. time(     queue. Full(     queue. PriorityQueue(queue. heappop(    queue. Queue(    &gt;&gt;&gt; queue. Queue. queue. Queue. empty(   queue. Queue. join(    queue. Queue. qsize(   queue. Queue. full(    queue. Queue. mro(    queue. Queue. task_done( queue. Queue. get(    queue. Queue. put(    queue. Queue. get_nowait( queue. Queue. put_nowait(queue. Queue 方法：  q = queue. Queue(maxsize=0) # 构造一个先进先出队列，maxsize指定队列长度，为0 时，表示队列长度无限制。 q. join() 　　# 等到队列为空的时候，在执行别的操作 q. qsize() 　 # 返回队列的大小 （不可靠） q. empty()  # 当队列为空的时候，返回True 否则返回False （不可靠） q. full()   # 当队列满的时候，返回True，否则返回False （不可靠） q. put(item, block=True, timeout=None) #     将item放入Queue尾部，item必须存在   可选参数block默认为True,表示当队列满时，会等待队列给出可用位置，为False时为非阻塞，此时如果队列已满，会引发queue. Full 异常。   可选参数timeout，表示 设置阻塞超时的时间，如果队列无法在规定时间内给出放入item的位置，则引发 queue. Full 异常    q. get(block=True, timeout=None) #     移除并返回队列头部的一个值   可选参数block默认为True，表示获取值的时候，如果队列为空，则阻塞，为False时，不阻塞，若此时队列为空，则引发 queue. Empty异常。   可选参数timeout，设置阻塞的超时时间，超时后，如果队列仍然为空，则引发Empty异常。    q. put_nowait(item) # 等效于 put(item,block=False) q. get_nowait()   # 等效于 get(item,block=False)多个线程从一个 Queue中读写数据 1234567891011121314151617181920212223242526272829303132333435import queueimport threadingque = queue. Queue(10) # 创建一个10个位置的队列def s(i):  que. put(i)  # print( size: , que. qsize())def x(i):  g = que. get(i)  print( get: , g)for i in range(1, 13): # 循环创建13个线程 , 每个线程都将一个数字放入到 queue 中  t = threading. Thread(target=s, args=(i,))  t. start()for i in range(1, 11): # 循环创建11个线程, 每个线程都从将数字从 queue 中取出一个数字  t = threading. Thread(target=x, args=(i,))  t. start()print( size: , que. qsize()) # 打印 queue 中还剩下的 2个数字# 输出结果：# get: 1# get: 2# get: 3# get: 4# get: 5# get: 6# get: 7# get: 8# get: 9# get: 10# size: 2两个进程单向收发信息 1234567891011121314151617181920212223from queue import Queuefrom threading import Threadimport time# A thread that produces datadef producer(out_q):  while True:    # Produce some data. . .     data = 'hello there. . . '    out_q. put(data)    time. sleep(1)# A thread that consumes datadef consumer(in_q):  while True:  # Get some data    data = in_q. get()  # Process the data . . .     print(data)# Create the shared queue and launch both threadsq = Queue()t1 = Thread(target=consumer, args=(q,))t2 = Thread(target=producer, args=(q,))t1. start()t2. start()停止信号: 当使用队列时，协调生产者和消费者的关闭问题可能会有一些麻烦。一个通用的 解决方法是在队列中放置一个特殊的只，当消费者读到这个值的时候，终止执行。 本例中有一个特殊的地方:消费者在读到这个特殊值之后立即又把它放回到队列 中，将之传递下去。这样，所有监听这个队列的消费者线程就可以全部关闭了。 1234567891011121314151617181920212223242526272829303132333435from queue import Queuefrom threading import Threadimport time# Object that signals shutdown 创建一个对象作为特殊信号用来停止监听线程_sentinel = object()# A thread that produces datadef producer(out_q):  for i in range(10):    # Produce some data. . .     data = 'hello there. . . '    out_q. put(data)    time. sleep(1)  # Put the sentinel on the queue to indicate completion  out_q. put(_sentinel) # 当工作结束了,就发送特殊信号给监听这,告诉他们可以结束了# A thread that consumes datadef consumer(in_q):  while True:  # Get some data    data = in_q. get()  # Process the data . . .     print(data)    # Check for termination    if data is _sentinel: # 一旦接到停止信号, 退出监听循环      in_q. put(_sentinel)      break# Create the shared queue and launch both threadsq = Queue()t1 = Thread(target=consumer, args=(q,))t2 = Thread(target=producer, args=(q,))t1. start()t2. start()线程池: 1pip3. 7 install threadpool线程池使用方式: 引入线程池模块: import threadpool 定义线程函数: def 任务函数(): 创建线程池:threadpool. ThreadPool(线程并发数) 创建线程池需要的任务: tasks = threadpool. makeRequests(func, nameList) 把创建的多个任务放入线程池: [pool. putRequest(i) for i in tasks] 等待所有线程: pool. wait()123456789101112import threadpooldef func(myStr):  print('hello{}'. format(myStr))pool = threadpool. ThreadPool(5)nameList = ['宋辉', '李红强' , '凤姐']reqs = threadpool. makeRequests(func, nameList) # 创建三个任务对象# print(request) # [&lt;threadpool. WorkRequest object at 0x106092e80&gt;, &lt;threadpool. WorkRequest object at 0x106092eb8&gt;, &lt;threadpool. WorkRequest object at 0x106092ef0&gt;][pool. putRequest(req) for req in reqs]pool. wait()   # wait 必须等待所有线程完成. 和其他方法的 p. join() 一个效果print('执行完毕')当函数有多个参数的情况，函数调用时第一个解包list，第二个解包dict，所以可以这样 1234567891011121314151617181920import threadpooldef hello(m, n, o):    print( m = %s, n = %s, o = %s  % (m, n, o))if __name__ == '__main__':  # 方法1  lst_vars_1 = ['1', '2', '3']  lst_vars_2 = ['4', '5', '6']  func_var = [(lst_vars_1, None), (lst_vars_2, None)]  # 方法2  dict_vars_1 = {'m': '1', 'n': '2', 'o': '3'}  dict_vars_2 = {'m': '4', 'n': '5', 'o': '6'}  func_var = [(None, dict_vars_1), (None, dict_vars_2)]  pool = threadpool. ThreadPool(2)  requests = threadpool. makeRequests(hello, func_var)  [pool. putRequest(req) for req in requests]  pool. wait()Queue 和 用Queue 设计 生产者/消费者模式:概述:  queue 是 python 线程模块中线程安全的队列, 提供了一个多线程先进先出的数据结构. 用来在生产者和消费者线程之间的信息传递.  创建两个线程, 一个线程用于产生线程往里塞, 另一个线程用于将工作结果往外取使用方法:  导入队列模块: import queue 创建队列: que=queue. Queue(maxsize=0)     如果 maxsize 是一个正整数, 代表当前对列存储对象的最大值. 如果超出最大值, 会造成队列阻塞(卡了)   0 代表没有大小限制   函数:  put : 压入数据, 如果压入数据超出队列最大数, 会阻塞 get: 从队列中获取数据,如果没有数据了会阻塞 empty: 判断队列是否为空, 空 True, 非空 FalseQueue 的使用 1234567891011121314151617181920212223import queue# 创建队列que = queue. Queue(4)# print(que)que. put('a')que. put('b')que. put('c')que. put('d')# que. put('e') # 超出 Queue 最大值, 队列阻塞, 程序卡住了# 获取数据print(que. get())print(que. get())print(que. get())print(que. get())# print(que. get())# emptyprint(que. empty()) # 判断queue 是否为空print('执行完毕')无穷大队列的 Queue 1234567import queueque = queue. Queue()for i in range(1000):  que. put(i)  print('执行完毕')用 Queue 创建生产者消费者模式 12345678910111213141516171819202122232425262728293031323334353637383940import time, threading, queue, random# 生产者class Product(threading. Thread):  def __init__(self, user, que):    threading. Thread. __init__(self)    self. user = user # 当前生产的用户    self. que = que  # 将生产出来的数据存放到队列中  def run(self):    while True:      num = random. randint(1,100000) # 返回一个随机整数      mystr = '当前生产者{} 生产了一个娃娃, 编号为 {}'. format(self. user, num)      self. que. put(mystr)      print(mystr)class Consumer(threading. Thread):  def __init__(self,buyer, que):    threading. Thread. __init__(self)    self. buyer = buyer    self. que = que  def run(self):    while True:      time. sleep(1)      data = self. que. get()      print(self. buyer, '买到了',data)que = queue. Queue(10) #for i in range(1,4):  p = Product(i, que). start()for i in range(1,8):  Consumer(i, que). start()另一种写法 123456789101112131415161718192021222324252627import time, threading, queue, randomdef seller(num):  global q  prod =  苹果 +str(num)  while True:    q. put(prod)    time. sleep(1)def buyer(num):  global q  while True:    prod = q. get()    print(prod)    time. sleep(1)q = queue. Queue(5)for i in range(1):  s = threading. Thread(target=seller, args=(i,))  s. start()for j in range(1):  b = threading. Thread(target=buyer, args=(j,))  b. start()协程: - 使用生成器代替线程: 定义: 协作完成:    从技术的角度来说，“协程就是你可以暂停执行的函数”  协程 是基于生成器的。  线程和进程的操作是由程序触发系统接口，最后的执行者是系统；  协程的操作则是程序员。 协程存在的意义：       对于多线程应用，CPU通过切片的方式来切换线程间的执行，线程切换时需要耗时（保存状态，下次继续）。        协程，则只使用一个线程，在一个线程中规定某个代码块执行顺序。    协程的适用场景：  当程序中存在大量不需要CPU的操作时（IO），适用于协程。 复习生成器的概念  有 yield 返回的函数的实例化对象是生成器 &lt;generator object a at 0x1041a0f68&gt; next 相当于 event 事件的 set触发, yield 相当于 wait123456789101112131415161718192021def fun():  yield 'abc'a = fun()print(next(a))print(a)print(a. send. __doc__)# 输出如下:# abc# &lt;generator object fun at 0x10418b1a8&gt;# send(arg) -&gt; send 'arg' into generator,# return next yielded value or raise StopIteration. #next 的概念如下next(. . . )  next(iterator[, default])    Return the next item from the iterator. If default is given and the iterator  is exhausted, it is returned instead of raising StopIteration. 协程的买家卖家模式 - 生产者消费者模式:  买家卖家模式(生产者和消费者), 使用协程更好 不需要使用多线程 可以达到和 Condition 一样的效果 通过生成器 yield 和 send 在两个任务中进行切换123456789101112131415161718192021# 生产者和消费者的雏形def run():  data = '苹果'  r = yield data  print('传递过来的指令是:',r)  r = yield data  print('传递过来的指令是:', r)  r = yield datax = run()# print(r)# print(next(x))# print(next(x))x. send(None) # 第一次必须传递 Noneprint(x. send('a')) # 相当于把a 给了 变量 rprint(x. send('b'))# 输出# 传递过来的指令是: a# 苹果# 传递过来的指令是: b# 苹果1234567891011121314151617181920212223242526# 一个生成器雏形, 可以返回 datadef cus():  data = ''  while True:    n = yield data    print('走到我了')    data = 'OK 200'def product(c):  print(c. send(None))  print(c. send('a'))  print(c. send('b'))  print(c. send('c'))# print(cus())c = cus() # 先生成一个卖家product(c) # 再将这个卖家传给买家, 让买家向卖家发送购买指令# 输出如下:# 走到我了# OK 200# 走到我了# OK 200# 走到我了# OK 200最终的代码 123456789101112131415161718192021222324252627282930313233def seller(): # 生产者 - 生成器 - 卖家  prod = '' # 第一个生产出来的东西要提前定义, 可以是任何字符串  while True:    i = yield prod # 1     print('卖家收到购买编号 ',i)    prod = '西红柿'def buyer(s): # 消费者 - 买家  s. send(None) # 要买的第一个东西必须是None  for i in range(3):    print('消费者发送购买编号: ',i)    resp = s. send(i)  # 2     print('消费者买到了产品:',resp)s = seller()buyer(s)# 1 yield 挂起,直到收到 send()指令后, 将send()的参数赋给 i, 并将 yield 后面的 value 让 send() 带回去. 然后继续 yield 下面的代码,直到再次碰到 yield 挂起. # 2 向卖家发送了指令i, 并获取到返回购买的产品# 输出如下:# 消费者发送购买编号: 0# 卖家收到购买编号 0# 消费者买到了产品: 西红柿# 消费者发送购买编号: 1# 卖家收到购买编号 1# 消费者买到了产品: 西红柿# 消费者发送购买编号: 2# 卖家收到购买编号 2# 消费者买到了产品: 西红柿进程:  依赖于 CPU 进行大量计算的工作, 尽量用多进程来分散计算量, 并行计算后可加快输出结果. 如果是需要大量请求网络的工作, 尽量用多线程. 你完全可以放心的创建几千个 Python 线程，现代操作系统运行这么多线程没有任何压力，没啥可担心的  进程是正在运行的程序的一个实例.  在多核处理器的计算机上,多进程要比多线程获得更好的效率  进程可以通过文件或者数据库进行通信  可以通过系统监视器查看系统上多了两个 python 的进程,每个进程都有自己的 PID 导入进程模块 from multiprocessing import Process 创建子进程 p = Process(target=函数名, args = (参数,) 参数是元祖类型 启动子进程 p. start() 默认情况下主进程不等待子进程完成,会继续执行 让主进程等待子进程完成: p. join(), 这样可以让主进程的每个子进程轮流干活1234567891011121314from multiprocessing import Processimport timedef func():  while True:    print('两个人相亲, 女的问男的你有车吗, 有房吗? 男的说没有, 你见过狗喝水吗?')    time. sleep(2)if __name__ == '__main__':  # func()  p = Process(target=func) # 如果没有开启子进程,  我是下面代码  是不会显示的  p. start()  p. join() # p. join() 会让主进程等待子进程执行完毕后再往下走.   print('我是下面的代码')  获取当前进程和父进程编号123import osos. getpid() # 获取当前进程编号os. getppid() # 获取当前进程的父进程编号123456789101112131415161718192021# 进程from multiprocessing import Processdef work(name):  print( Hello, %s  % name)if __name__ ==  __main__ :  p1 = Process(target=work, args=( One ,)) # 创建新的进程调用了wrok 函数  p2 = Process(target=work, args=( Two ,)) # 创建新的进程调用了wrok 函数  print(p1)    # p1 进程对象创建了,但是还没启动,没有 pid  print(p1. pid)  print(p2)  print(p2. pid)  p1. start()  print(p1)    # p1 进程启动,创建 pid  print(p1. pid)  p2. start()  print(p2)  print(p2. pid)  p1. join()  p2. join()用多进程拷贝文件的例子 12345678910111213141516171819202122232425262728from multiprocessing import Processimport osimport timedef mycopy(src, desc):  print('当前进程{}, 父进程{}'. format(os. getpid(), os. getppid()))  with open(src, 'rb') as rf:    data = rf. read()    with open(desc, 'wb') as wf:      wf. write(data)  time. sleep(1)if __name__ == '__main__':  srcpath = '. /testfolder'  descpath = '. /testfolder2'  files = [name for name in os. listdir(srcpath) if os. path. isfile(os. path. join(srcpath, name))]  myList = []  for file in files:    oldfile = os. path. join(srcpath,file)    newfile = os. path. join(descpath,file)    p = Process(target=mycopy , args=(oldfile,newfile))    p. start()    myList. append(p)  for i in myList:    i. join()print('拷贝完成')进程间通信概念: 进程变量共享问题  子进程在启动之后就无法再拿到主进程的数据了 主进程也拿不到子进程的变量,所以无法共享数据 只有通过管道 Pipe(), Queue(), Manager() 共享列表/字典, Pool()进程池的方式才能实现共享无法共享的例子 12345678910111213141516171819202122# 进程的变量共享from multiprocessing import Process# num = 10def func():  global num  print('子进程获取主进程的变量', num)  num = 2  print('子进程尝试更新主进程的 变量',num)if __name__ == '__main__':  # func()  p = Process(target=func)  num = 10  p. start() # 子进程只有在 start() 之前可以读取到主进程的 global 变量  num = 9 # 启动后就无法获取主进程的任何变量了  p. join()  print('主进程变量还是',num)  # 输出# 2 # 子进程尝试将2赋值到追进程的 global 变量中# 10 # 但主进程还是10Pipe 管道方式通信: 性能: Pipe 的性能要强于 Queue 使用场景: 有通信速度和性能要求, 并且只需要两个口进行通信的情况下推荐使用 Pipe from multiprocessing import Process, Pipe 一个管道分两头, 每头都需要定义一个对象. 创建管道: con_a, con_b = Pipe() # Pipe() 返回两个管道口对象, 两个对象一模一样 (&lt;multiprocessing. connection. Connection object at 0x10423df60&gt;, &lt;multiprocessing. connection. Connection object at 0x10423df98&gt;). 创建进程: p = Process(target=func , args=(con_b,)) 将一头传递给子进程, 主进程自己留另外一头 从管道向子进程发送数据: con_a. send([1,2,3,4]) 从管道接收数据: con_b. recv() 在子进程接受主进程的数据 123456789101112131415# 通过管道让进程间进行通信from multiprocessing import Process, Pipedef func(con):  print('在子进程接受主进程的数据',con. recv())  passif __name__ == '__main__':  print (Pipe())  con_a, con_b = Pipe() # 创建管道, Pipe() 返回管道的两个口  p = Process(target=func , args=(con_b,))  p. start()  con_a. send([1,2,3,4]) # 向管道中发送一个数组  p. join()  print('执行完毕')循环向子进程发送数据 1234567891011121314151617# 通过管道让进程间进行通信from multiprocessing import Process, Pipedef func(con):  while True:    print('在子进程接受主进程的数据',con. recv())if __name__ == '__main__':  print (Pipe())  con_a, con_b = Pipe() # 创建管道, Pipe() 返回管道的两个口  p = Process(target=func , args=(con_b,))  p. start()  while True:    msg = input( 请输入向管道传送的数据:  )    con_a. send(msg) # 向管道中发送数据  p. join()  print('执行完毕')在主进程接受子进程的数据 12345678910111213# 通过管道让进程间进行通信from multiprocessing import Process, Pipedef func(con):  con. send(['a','b','c'])if __name__ == '__main__':  con_a, con_b = Pipe() # 创建管道, Pipe() 返回管道的两个口  p = Process(target=func , args=(con_b,))  p. start()  print('在主进程接受子进程的数据',con_a. recv())  p. join()  print('执行完毕')循环向主进程发送数据 123456789101112131415from multiprocessing import Process, Pipeimport timedef func(con):  while True:    con. send('aaa')    time. sleep(1)if __name__ == '__main__':  con_a, con_b = Pipe() # 创建管道, Pipe() 返回管道的两个口  p = Process(target=func , args=(con_b,))  p. start()  while True:    print('在主进程接受子进程的数据',con_a. recv())  p. join()  print('执行完毕')双向循环传输数据 123456789101112131415161718from multiprocessing import Process, Pipeimport timedef func(con):  while True:    con. send('aaa')    print('在主进程接受子进程的数据', con. recv())    time. sleep(1)if __name__ == '__main__':  con_a, con_b = Pipe() # 创建管道, Pipe() 返回管道的两个口  p = Process(target=func , args=(con_b,))  p. start()  while True:    print('在主进程接受子进程的数据',con_a. recv())    con_a. send('bbb')    time. sleep(1)  p. join()  print('执行完毕')Queue 队列方式通信: 性能: 没有 Pipe 好 使用场景: 可以在需要多于两个口进行通信的情况下使用. Queue 不需要像 Pipe 一样创建两个口子对象. from multiprocessing import Process, Queue queue = Queue() 创建队列 queue. put(['a','b','c']) 压入数据, 每次向管道一次压入一个对象 q. get() 获取数据 Queue 特点: 先进先出 作业: 两个子进程, 一个进程压入数据, 另一个进程 get数据. 意义:多个子进程是否可以通信 从主进程向子进程传递数据 123456789101112from multiprocessing import Process, Queuedef func(q):  print(q. get())if __name__ == '__main__':  q = Queue() # 创建队列  q. put(['a','b','c']) # 压入数据  p = Process(target=func, args=(q,))  p. start()  p. join()从子进程向主进程传递数据 123456789101112from multiprocessing import Process, Queuedef func(q):  q. put(['a','b','c']) # 压入数据if __name__ == '__main__':  queue = Queue() # 创建队列  # queue. put(['a','b','c']) # 压入数据  p = Process(target=func, args=(queue,))  p. start()  print(queue. get())  p. join()子进程间的数据通信 12345678910111213141516171819202122232425from multiprocessing import Process, Queueimport osdef func(q,num):  if num == '1':    print('子进程{}: 开始压入数据'. format(os. getpid()))    q. put('a')    q. put('b')    q. put('c')  elif num == '2':    print('子进程{}: 开始接收数据'. format(os. getpid()))    print(q. get())    print(q. get())    print(q. get())if __name__ == '__main__':  q = Queue()  p1 = Process(target=func, args=(q,'1'))  p1. start()  p2 = Process(target=func, args=(q,'2'))  p2. start()  p1. join()  p2. join()  print('子进程间通信结束')一个主进程和多个子进程间的多点通信 123456789101112131415161718192021from multiprocessing import Process, Queueimport timedef func(q,num):  while True:    print('子进程{}获得数据:{}'. format(num, q. get())) # 接收其他子进程发送的数据    q. put('子进程{}在呼叫'. format(num)) # 向其他子进程发送自己的数据    time. sleep(10)if __name__ == '__main__':  queue = Queue() # 创建queue 对象  p1 = Process(target=func, args=(queue,1))  p2 = Process(target=func, args=(queue,2))  # p3 = Process(target=func, args=(queue,3))  p1. start()  p2. start()  # p3. start()  while True:    queue. put('主进程在呼叫所有子进程') # 压入数据    print('主进程获得子进程数据:{}'. format(queue. get()))    time. sleep(10)进程间字典列表共享: 1234567891011121314151617181920212223242526272829303132import multiprocessingdef setList(myList):  myList. append('x')  myList. append('y')  myList. append('z')  print('当前子进程的方法里面的值为',myList)def setDict(myDict):  myDict[3] = 3  myDict[4] = 4  print('当前子进程的方法里面的值为', myDict)if __name__ == '__main__':  # Manager 是一个进程间高级通信的方法, 支持 python 的字典和列表的数据类型  myList = multiprocessing. Manager(). list(range(5)) # 创建一个列表对象,并提供一个初始列表  myDict = multiprocessing. Manager(). dict({1:1,2:2}) # 创建一个字典对象, 并提供一个初始字典  print(myList)  print(myDict)  pl = multiprocessing. Process(target=setList, args=(myList,))  pl. start()  pl. join()  pd = multiprocessing. Process(target=setDict, args=(myDict,))  pd. start()  pd. join()# 输出为:# [0, 1, 2, 3, 4]# {1: 1, 2: 2}# 当前子进程的方法里面的值为 [0, 1, 2, 3, 4, 'x', 'y', 'z']# 当前子进程的方法里面的值为 {1: 1, 2: 2, 3: 3, 4: 4}进程池:  创建进程池: p=Pool(进程池可容纳进程的个数) Pool 的参数设定进程池可同时执行的进程数. 不填参数默认使用系统的 cpu个数 将进程放入进程池(异步): p. apply_async(fun, args=(i,)) 关闭进程池: p. close()123456789101112131415from multiprocessing import Pool# 进程池方便多个进程统一管理. import timedef fun(arg):  print('进程',arg)  time. sleep(2)if __name__ == '__main__':  p = Pool() # 参数设定进程池可同时执行的进程数. 不填参数默认使用系统的 cpu个数  for i in range(1,11):    p. apply_async(fun, args=(i,)) # 为进程池添加一个进程  p. close() # 关闭进程池  p. join()  print('执行完毕') 多进程抓取数据 12345678910111213141516171819202122from multiprocessing import Poolimport refrom urllib import requestdef getData(path):  data = request. urlopen(path). read(). decode('utf-8')  dataList = re. findall(r'[1-9]\d{5,10}',data)  print(path)  print(dataList)if __name__ == '__main__':  path = 'https://www. douban. com/group/topic/41011762/?start=0'  p = Pool()  page = 20  nowPage = 20*100  for i in range(0,500,100):    newPath = path+str(i)    # print(newPath)    p. apply_async(getData,args = (newPath,))  p. close()  p. join()多进程统计 csv 中的数据并进行汇总计算 12345678910111213141516171819202122232425262728293031323334import csvfrom multiprocessing import Poolimport multiprocessingimport osdef func(path,myList):  f = open(path, 'r')  reader = csv. reader(f)  num = 0  total = 0  for i in reader:    if num==0:      pass    else:      total+=eval(i[-2])    num+=1  print(path)  print( 平均值 ,total/(num-1))  print(myList. append(total/(num-1)))if __name__== __main__ :  p = Pool()  #创建列表  myList = multiprocessing. Manager(). list()  path = r . /csvfiles   fileList = os. listdir(path)  for file in fileList:    newPath = os. path. join(path,file)    p. apply_async(func,args=(newPath,myList))  p. close()  p. join()  print(myList)  print( 结束 )多进程在文件中搜索数据  Pool 比 Process 好用, 尽量用 Pool12345678910111213141516171819202122232425from multiprocessing import Pool, Process, Managerimport osdef search(i,mlist,uname):  print('子进程{}: 开始搜索数据'. format(os. getpid()))  for x in range(i, i+100):    if uname in mlist[x]:      m. append(mlist[x])if __name__ == '__main__':  path = '. /kaifanglist. txt'  uname = input('请输入你要搜索的名字')  with open(path, 'r', encoding='utf8') as f:    mlist = f. readlines()    print('\n')  # print(mlist)  m = Manager(). list()  p = Pool()  for i in range(0,len(mlist),100):    # print(i)    p. apply_async(search, args=(i,mlist,uname))  p. close()  p. join()  print('搜索结果,{}'. format(m))并行计算 - 将多个工作平均分配给每个 CPU, 做 map-reduce 计算: 你有个程序要执行 CPU 密集型工作，你想让他利用多核 CPU 的优势来运行的快 一点。 concurrent. futures 库提供了一个 ProcessPoolExecutor 类，可被用来在一个单 独的 Python 解释器中执行计算密集型函数。不过，要使用它，你首先要有一些计算密 集型的任务。我们通过一个简单而实际的例子来演示它。假定你有个 Apache web 服务 器日志目录的 gzip 压缩包 1234567891011121314151617181920212223242526272829303132# findrobots. pyimport gzipimport ioimport glob # 用来找文件的模块, 可以使用正则from concurrent import futuresdef find_robots(filename):  '''  Find all of the hosts that access robots. txt in a single log file  '''  robots = set()  with gzip. open(filename) as f:    for line in io. TextIOWrapper(f, encoding='ascii'):      fields = line. split()      if fields[6] == '/robots. txt':        robots. add(fields[0])  return robotsdef find_all_robots(logdir):  '''  Find all hosts across and entire sequence of files  '''  files = glob. glob(logdir+'/*. log. gz') # Return a list of paths matching a pathname pattern.   all_robots = set()  with futures. ProcessPoolExecutor() as pool: # 创建一个并发池    for robots in pool. map(find_robots, files): # 给池中放置需要并发的任务, 以及任务要执行的一系列工作. 原理相当于 map() 函数      all_robots. update(robots)  return all_robotsif __name__ == '__main__':  robots = find_all_robots('logs')  for ipaddr in robots:    print(ipaddr)2. 6 Socket: TCP 协议  有请求有相应, 是面向链接的协议.  在收数据和发数据的时候必须建立一个可靠的连接  一个 TCP连接必须经过三次对话才能建立起来, 俗称三次握手 UDP 协议  UDP 协议是一个非链接的协议.  传输数据是不需要在两端建立起链接 Socket  Socket 俗称套接字, 是计算机最底层的通信模块  一台计算机多个进程之间的通信, 或者是和服务器之间的通信 使用 Socket  导入 Socket 模块: import socket 创建 Socket: s = socket. socket() 端口号范围: 0 - 65535 . 我们在自定义端口号时, 最好是定义在1024以上的号码      参数   说明         AF_INET   使用 IPV4协议(IP地址)       SOCK_STREAM   TCP 连接       SOCK_DGRAM   UDP   建立 UDP 服务端课客户端: 服务端 用到的函数:  s. bind(('127. 0. 0. 1',8088)) # 绑定你当前服务端的 IP地址和端口号. 客户端可以通过这个地址进行访问 s. recvfrom(1024) # 接收从客户端传递的数据. 一次最多接受1024字节1234567891011import sockets = socket. socket(socket. AF_INET, socket. SOCK_DGRAM) # 创建 UDP 服务端# print(s) # &lt;socket. socket fd=3, family=AddressFamily. AF_INET, type=SocketKind. SOCK_DGRAM, proto=0, laddr=('0. 0. 0. 0', 0)&gt;s. bind(('127. 0. 0. 1',9058)) # 设置 IP地址和端口号data, addr = s. recvfrom(1024) # recvfrom 是 udp 接受客户端请求的数据的函数, 一次最多能接收1024 字节# print(date, addr)while True:  print('来自{}的消息是:'. format(addr[0]))  print('消息为: ', data)客户端  c. sendto(data. encode('utf-8'),('127. 0. 0. 1',9058)) # 发送信息给服务端12345678import socketc = socket. socket(socket. AF_INET,socket. SOCK_DGRAM)while True:  data = input( 说你想对UDP服务器端说的话 )  c. sendto(data. encode('utf-8'),('127. 0. 0. 1',9058))# 发送 udp 数据, 第一个参数是数据, 第二个参数是服务端地址(ip+端口)建立 TCP 连接, 服务端和客户端: 服务端 1234567891011121314import socket,time# 创建 TCP服务端s = socket. socket(socket. AF_INET, socket. SOCK_STREAM)# print(s) # &lt;socket. socket fd=3, family=AddressFamily. AF_INET, type=SocketKind. SOCK_STREAM, proto=0, laddr=('0. 0. 0. 0', 0)&gt;s. bind(('127. 0. 0. 1', 9069))s. listen(5) # 设置最大连接数, 超出5个等待cSocket, addr = s. accept() # 被动接收 tcp 客户端的连接, 阻塞式等待while True:  data = cSocket. recv(1024) # 最大接受数为1024字节  print(time. strftime('%Y-%m-%d %H:%M:%S'), '接收到的消息为: ', data. decode('utf-8'))  mydata = input('输入回复信息:')  cSocket. send(mydata. encode('utf-8'))客户端 1234567891011121314import socket, time# 建立 socket tcp 连接c = socket. socket(socket. AF_INET, socket. SOCK_STREAM)# 连接服务端地址c. connect(('127. 0. 0. 1', 9069)) # 连接 tcp 服务端while True:  data = input('输入内容')  c. send(data. encode('utf-8'))  mydata = c. recv(1024)  print(time. strftime('%Y-%m-%d %H:%M:%S'),' 收到服务端的信息是', mydata. decode('utf-8'))简单的TCP 客户端认证连接 服务端和客户端通过一个预先定义好的字符串来进行认证,认证通过才能进行数据的连接和交互 hmac 认证算法基于哈希函数如 MD5 和 SHA-1 还有一点需要强调的是连接认证和加密是两码事。认证成功之后的通信消息是以明 文形式发送的，任何人只要想监听这个连接线路都能看到消息(尽管双方的密钥不会 被传输) 基本原理是当连接建立后，服务器给客户端发送一个随机的字节消息(这里例子中 使用了 os. urandom() 返回值)。客户端和服务器同时利用 hmac 和一个只有双方知道 的密钥来计算出一个加密哈希值。然后客户端将它计算出的摘要发送给服务器，服务 器通过比较这个值和自己计算的是否一致来决定接受或拒绝连接。摘要的比较需要使 用 hmac. compare digest() 函数。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556import hmacimport osfrom socket import socket, AF_INET, SOCK_STREAMdef client_authenticate(connection, secret_key):  '''  Authenticate client to a remote service.   connection represents a network connection. secret_key is a key known only to both client/server.   '''  message = connection. recv(32)  hash = hmac. new(secret_key, message)  digest = hash. digest()  connection. send(digest)def server_authenticate(connection, secret_key):  '''  Request client authentication.   '''  message = os. urandom(32)  connection. send(message)  hash = hmac. new(secret_key, message)  digest = hash. digest()  response = connection. recv(len(digest))  return hmac. compare_digest(digest, response)secret_key = b'peekaboo'def echo_handler(client_sock):  if not server_authenticate(client_sock, secret_key):    client_sock. close()    return  while True:    msg = client_sock. recv(8192)     if not msg:      break    client_sock. sendall(msg)        def echo_server(address):  s = socket(AF_INET, SOCK_STREAM) s. bind(address)  s. listen(5)  while True:    c,a = s. accept()    echo_handler(c)    echo_server(('', 18000))# ===========================================# Within a client, you would do this:from socket import socket, AF_INET, SOCK_STREAM secret_key = b'peekaboo's = socket(AF_INET, SOCK_STREAM) s. connect(('localhost', 18000)) client_authenticate(s, secret_key)s. send(b'Hello World')resp = s. recv(1024)通过 Socket 远程访问网站并获取源代码: 获取百度首页代码,并写成 html文件 123456789101112131415161718import socketc = socket. socket(socket. AF_INET, socket. SOCK_STREAM)c. connect(('www. baidu. com', 80))c. send(b'GET / HTTP/1. 1\r\nHost:www. baidu. com\r\nConnection: close\r\n\r\n')myList = []while True:  data = c. recv(1024)  print(data)  myList. append(data)  if not data: breakdata = b''. join(myList) header, html = data. split(b'\r\n\r\n',1)print(html)with open('. /xxx. html', 'w') as f:  f. write(html)Socket 发送飞秋消息: 123456789import socketc = socket. socket(socket. AF_INET,socket. SOCK_DGRAM) # 用 UDP 协议发送mystr = '你好妹子'for i in range(256):  c. connect(('10. 11. 56. '+str(i),2425))  c. send(mystr. encode('gbk'))  print(i)用 Python 玩微信: 用PYTHON玩微信 "
    }, {
    "id": 32,
    "url": "http://localhost:4000/Beginning-Python-From-Novice-to-Professional-Part-1/",
    "title": "Learning Python From Scratch (Part1)",
    "body": "2017/10/22 - Python编程语言可能是使用最广泛的语言之一，因此不断发展。如果一个人想充分发挥其潜力，同时又想在专业上保持竞争力，那么学习 Python 是必不可少的。本文是我很早之前学习 Python 基础时的笔记, 希望可以为那些不了解该Python的人提供了快速入门的必要信息. Python 基础预安装软件: dreamweaver , PS, Pycharm [TOC] 1. 0 Python 环境安装: 安装 pycharm Python IDE www. macx. cn 找到下载 1. 0. 1 pycharm 快捷方式:  cmd + d 复制当前行cmd + del 删除当前行shift + option + 上/下 移动当前行shift + cmd + 8 列选择模式fn + 上/下 到代码页头部/尾部option + 上/下 扩大/缩小选择范围fn + F1 快速查看一个函数fn + shift 运行代码/运行调试cmd + b go to declination 跳到定义的出处cmd + option + b go to implementation 跳到实现的地方fn + option + F7 查看定义的变量或者方法在项目中哪些地方被调用cmd + / 多行 注释双击 shift 在多文件中查找关键字cmd + shift + enter 在当前行下面创建新行cmd + option + enter 在当前行上面添加新行cmd + option + L 格式化美化代码 1. 0. 2**安装 Python **: https://www. python. org/downloads/release/python-363/ python 之禅 1import this1. 0. 3安装 pip 和使用 pip: https://pip. pypa. io/en/stable/ 第三方包的安装 pip  pip 是从国外的 pip 源下载的, 如果网络不好, 可能无法下载安装成功国内有很多家机构有 pip 镜像源豆瓣源 , 清华源, 阿里源, 中科大源修改 源的方法cd ~/mkdir . pipcd . pipvi pip. conf  12345[global]timeout = 6000index-url = http://pypi. douban. com/simpletrusted-host = pypi. douban. comformat=columns 命令行中直接用镜像安装 1pip3. 7 install -i https://pypi. douban. com/simple/ django如何使用 pippip 使用教程Python包索引查看已经安装了的 python 扩展包 1pip list #查看已安装的包的版本和该包的最新版本 12pip list --outdatedpip freeze 更新 pip 1pip install -U pip安装包 1pip3. 6 install pygame反安装包 1pip3. 6 uninstall pygame查看某个已经安装了的包的详细信息 1pip3. 6 show pygame在 PYPI搜索包 1pip3. 6 search  pygame 飞机大战  pycharm 创建一个新的 python 空项目copy 飞机大战代码到新项目的目录用 pip 安装飞机大战依赖的 pygame 包  1pip3. 6 install pygame  pycharm 中运行老师给的飞机大战脚本 control + r 1. 0. 4 查看 python 的系统 PATH: *技巧: 查看 python 的系统 PATH : *  进入 python console 12import sysprint(sys. path) 输出的 PATH 如下:[‘/Applications/PyCharm. app/Contents/helpers/pydev’, ‘/Applications/PyCharm. app/Contents/helpers/pydev’, ‘/Library/Frameworks/Python. framework/Versions/3. 6/lib/python36. zip’, ‘/Library/Frameworks/Python. framework/Versions/3. 6/lib/python3. 6’, ‘/Library/Frameworks/Python. framework/Versions/3. 6/lib/python3. 6/lib-dynload’, ‘/Library/Frameworks/Python. framework/Versions/3. 6/lib/python3. 6/site-packages’, ‘/Library/Frameworks/Python. framework/Versions/3. 6/lib/python3. 6/site-packages/IPython/extensions’, ‘/Users/dalong/code/python1702/fighting’]其中包含当前路径, /Users/dalong/code/python1702/fighting , 第三方扩展包 /Library/Frameworks/Python. framework/Versions/3. 6/lib/python3. 6/site-packages , 和 python 官方包 /Library/Frameworks/Python. framework/Versions/3. 6/lib/python3. 6 查看python自带标准库 / 第三方库:  通过 sys. path 知道了标准库 的目录 , cd 到目录中去看/Library/Frameworks/Python. framework/Versions/3. 6/lib/python3. 6  通过 sys. path 找到了第三方库的目录/Library/Frameworks/Python. framework/Versions/3. 6/lib/python3. 6/site-packages 查看 Python 内置函数: 在 python 命令行输入: 1dir(__builtins__)###查看当前会话的变量域中的所有定义过的变量  vars() 查看 python 的帮助: 进入 python 命令行模式 123help()  # 命令行中输入 help() 回车后进入 help 界面print  # 输入想要了解的关键词 ,比如 print , threading 等. 按上下可以看更多, 按 fn + 上下进行翻页, 输入 '/搜索词' 直接在文档中搜索, 按 n 可以找搜索词下一个位置ipython 使用交互的 python 命令行: ipython 和 python 命令一样，只是有更丰富的交互提示。 有点类似于 Jupyter Notebook 12pip3 install ipythonipython用 pyenv 在不同 python 版本间自由切换: pjyenv 和 Anaconda 有冲突，只能保留一个 123456brew install pyenvpyenv # 查看命令使用方法pyenv versions  # 查看所有版本pyenv install 3. 8. 0 # 安装另一个版本pyenv global 3. 8. 0 # 切换当前机器默认的 python 版本Some useful pyenv commands are:  –version  Display the version of pyenv  commands  List all available pyenv commands  exec    Run an executable with the selected Python version  global   Set or show the global Python version(s)  help    Display help for a command  hooks    List hook scripts for a given pyenv command  init    Configure the shell environment for pyenv  install   Install a Python version using python-build  local    Set or show the local application-specific Python version(s)  prefix   Display prefix for a Python versionsed: RE error: illegal byte sequence  rehash   Rehash pyenv shims (run this after installing executables)  root    Display the root directory where versions and shims are kept  shell    Set or show the shell-specific Python version  shims    List existing pyenv shims  uninstall  Uninstall a specific Python version  version   Show the current Python version(s) and its origin  version-file  Detect the file that sets the current pyenv version  version-name  Show the current Python version  version-origin  Explain how the current Python version is set  versions  List all Python versions available to pyenv  whence   List all Python versions that contain the given executable  which    Display the full path to an executable See `pyenv help ’ for information on a specific command. For full documentation, see: https://github. com/pyenv/pyenv#readme 创建一个本地服务器: 用于在某个目录下快速查看 html 类型文件， 比如文档等 123python3 -m http. server # 访问 http://0. 0. 0. 0:8000python3 -m http. server 8089  # 指定端口号1. 0. 5 利用虚拟环境创建项目:  方法（0）12345678910python3 -m venv . venv  # 该命令创建一个名字叫 . venv 的虚拟环境目录source . venv/bin/activate# 该命令启动并进入该环境pip freeze# 查看环境已经安装的包pip freeze &gt; requirements. txt  # 将所有的Pip 环境安装的依赖包导出到文件中pip install -r requirements. txt # 读取安装文件来安装 pip 包deactivate # 推出环境 方法 （1） pipenv1234567891011121314151617181920212223242526272829303132333435363738394041424344454647pip3 install pipenvpip3 show pipenvpipenv shell # 进入虚拟环境# control + D 快捷键推出环境pipenv --helpUsage Examples:  Create a new project using Python 3. 7, specifically:  $ pipenv --python 3. 7  Remove project virtualenv (inferred from current directory):  $ pipenv --rm  Install all dependencies for a project (including dev):  $ pipenv install --dev  Create a lockfile containing pre-releases:  $ pipenv lock --pre  Show a graph of your installed dependencies:  $ pipenv graph  Check your installed dependencies for security vulnerabilities:  $ pipenv check  Install a local setup. py into your virtual environment/Pipfile:  $ pipenv install -e .  Use a lower-level pip command:  $ pipenv run pip freezeCommands: check   Checks for PyUp Safety security vulnerabilities and against PEP       508 markers provided in Pipfile.  clean   Uninstalls all packages not specified in Pipfile. lock.  graph   Displays currently-installed dependency graph information.  install  Installs provided packages and adds them to Pipfile, or (if no       packages are given), installs all packages from Pipfile.  lock    Generates Pipfile. lock.  open    View a given module in your editor.  run    Spawns a command installed into the virtualenv.  scripts  Lists scripts in current environment config.  shell   Spawns a shell within the virtualenv.  sync    Installs all packages specified in Pipfile. lock.  uninstall Uninstalls a provided package and removes it from Pipfile.  update   Runs lock, then sync.  方法(2)     创建虚拟环境 : terminal 中进行下面的命令   12345pip3. 7 install virtualenv # 安装# 进入项目目录,然后执行下面命令virtualenv venv # 创建source . /venv/bin/activate # 激活deactivate # 退出虚拟环境如果要指定虚拟环境的python版本,就需要用 -p 1virtualenv -p /Library/Frameworks/Python. framework/Versions/3. 7/bin/python3. 7 env为虚拟环境做依赖的配置信息文件: 123pip3. 7 freeze     # 查看 所有的Pip 环境安装的依赖包列表pip3. 7 freeze &gt; requirements. txt  # 将所有的Pip 环境安装的依赖包导出到文件中pip3. 7 install -r requirements. txt # 读取安装文件来安装 pip 包1. 1 Python基础语法: 1. 1. 1 python概述: Python 是一个非常简单的编程语言  python 可以用来做很多事情，比如科学计算等等 还可以用来作为人工智能的基础文件名:  字母数字下划线组成 不能以数字开头 不能是系统关键字看系统关键字的方法: 12import keywordprint(keyword. kwlist)输出的关键字如下:  `[‘False’, ‘None’, ‘True’, ‘and’, ‘as’, ‘assert’, ‘async’, ‘await’, ‘break’, ‘class’, ‘continue’, ‘def’, ‘del’, ‘elif’, ‘else’, ‘except’, ‘finally’, ‘for’, ‘from’, ‘global’, ‘if’, ‘import’, ‘in’, ‘is’, ‘lambda’, ‘nonlocal’, ‘not’, ‘or’, ‘pass’, ‘raise’, ‘return’, ‘try’, ‘while’, ‘with’, ‘yield’]` 注释: 12345#''''''      一行声明多个变量 / 删除多个变量: 123456a=1 ; b=2 ; c=3a, b, c, d = 1,2,3,4a = b = c = 1a,b,c = '123'del a,b,c,d数据大小单位换算: 字节换算    bit (比特 , 小 b): 一个bit 或者说一个比特就是二级制的两位 , 比如 1000 1110 的最后两位就是一个 b  bytes(字节, 大 B) : 一个字节等于八个比特  k : 1k 等于1024B (字节)  m : 1m 等于1024k  g : 1g 等于1024m  t : 1t 等于1024g 一个字节(byte)数的范围:    计算机存储数字的最小单位就是一个字节  因为一个 字节对应了 00000000 ~ 11111111 的二进制数范围,转化成十进制就是 0~255  无符号: 一个大 B可控的十进制数的范围是0~255 既 0000 0000 ~ 1111 1111  有符号: -127 ~ 127 , 用8位二进制表示就是 1111 1111 ~ 0111 1111  一个字节表示成负数就是在8位二进制的第一位用1表示.   0表示正 , 1表示负.   0000 0111 是 3  1000 0101 是 -3 有符号一字节的数的范围  1111 1111 = -1270111 1111 = +127 无符号一字节的范围  0000 0000 = 01111 1111 = 256 原码反码补码: 详细文档原码    原码就是一个带符号的8位二进制数(表示一个字节) .   原码的范围就是-127~127 , 既1111 1111 ~ 0111 1111  原码是人脑最容易理解和计算的表示方式.  反码和补码    正数的反码是其本身  正数的原码反码补码都是一样的  负数的反码是在其原码的基础上, 符号位不变，其余各个位取反.   对于负数, 人脑无法直观的看出反码的数值. 通常要将其转换成原码再计算  对于负数, 补码表示方式也是人脑无法直观看出其数值的. 通常也需要转换成原码再计算其数值.   负数的补码是在其原码的基础上, 符号位不变, 其余各位取反, 最后+1. (即在反码的基础上+1)  计算机存储的都是补码  对于负数       -3 作为例子 1000 0011  原码 1111 1100  反码 1111 1101  补码(末位加1)      为何要使用原码反码补码  人类可以理解原码但用原码直接进行计算(比如 1-1 = 0 ) 是不行的 11 - 1 = 1 + (-1) = [00000001]原 + [10000001]原 = [10000010]原 = -2 如果用反码进行计算, 部分正确,但符号位不对 11 - 1 = 1 + (-1) = [0000 0001]原 + [1000 0001]原= [0000 0001]反 + [1111 1110]反 = [1111 1111]反 = [1000 0000]原 = -0 用补码进行计算可以得到正确结果 11-1 = 1 + (-1) = [0000 0001]原 + [1000 0001]原 = [0000 0001]补 + [1111 1111]补 = [0000 0000]补=[0000 0000]原 这样0用[0000 0000]表示, 而以前出现问题的-0则不存在了. 而且可以用[1000 0000]表示-128: 1(-1) + (-127) = [1000 0001]原 + [1111 1111]原 = [1111 1111]补 + [1000 0001]补 = [1000 0000]补 使用补码, 不仅仅修复了0的符号以及存在两个编码的问题, 而且还能够多表示一个最低数. 这就是为什么8位二进制, 使用原码或反码表示的范围为[-127, +127], 而使用补码表示的范围为[-128, 127]. 数据类型: Python 支持8种标准数据类型  Numbers（数字）     int（有符号整型） a = 100   long（长整型[也可以代表八进制和十六进制]）   float（浮点型） b = 3. 1415926 #不要尝试判断两个浮点数相等,因为计算机存储浮点数时会有精度差异    complex（复数）    String（字符串） List（列表） Tuple（元组） Dictionary（字典） Set (集合) Bool (布尔) a = True, b = False # 注意头字母大写 None (空)  a = None # 注意头字母大写用 type 函数判断变量的类型 12a = 1type(a)进制转换: 手工方法十转十 1124 = 1*10^2 + 2*10^1 + 4*10^0手工方法八转十 1144 = 1*8^2 + 4*8^1 + 4*8^0 手工方法十六转十 11af = 1*16^2 + 10*16^1 + 15*16^0手工方法二转十  $ 156 = 110^2 + 510^1 + 610^0 = 100 + 50 + 6 = 156$$ 1010 = 12^3 + 02^2 + 12^1 + 0*2^0 = 8 + 2 = 10 $ 手工十转二 (辗转相除法/倒除法)  100 = 1100100100/2的模0 50/2 		025/2			112/2			06/2			03/2			11/2			1 手工十转十六 (辗转相除法/倒除法)    同十转二类似,就是每次除16 取余数  也可以先转二再转十六 手工十转八 (辗转相除法/倒除法)  同十转二类似,就是每次除8 取余数    也可以先转二再转八 手工二转八 ( 从末尾往前,每三位一组, 不够位数左侧填0 , 分别计算二转十 )  10011101001 = 2351010 011 101 001 2 3 5 1 手工二转十六 ( 从末尾往前,每四位一组, 不够位数左侧填0 , 分别计算二转十 )  10011101001 = 4e90100 1110 10014 e 9 15 (1111)以内的二进制       1-5   6-10   11-15         0001   0110   1011       0010   0111   1100       0011   1000   1101       0100   1001   1110       0101   1010   1111   互转举例(以十进制数字1257为例)           进制互转    公式 /例子    描述              二进制十进制互转    int('10011101001',2) / bin(1257)    10011101001 转成 1257          二进制十六进制互转    hex(int('10011101001',2)) / bin(int('4e9',16))    10011101001 转成 0x4e9          二进制八进制互转    oct(int('10011101001',2)) / bin(int('2351',8))    10011101001转0o2351          八进制十六进制互转    hex(int('2351',8)) / oct(int('4e9',16))    0o2351 转0x4e9          八进制十进制互转    int('2351',8) / oct(1257)    0o2351转1257          十六进制十进制互转    int('4e9',16) / hex(1257)    0x4e9 转 1257      format() 函数也可以用来进行进制转换 好处是不会出现 Ox, Oo, Ob 之类的前缀 123456format(100,'b')format(100,'o')format(100,'x')format(0b10011101001,'d') # 2转10format(0o2351,'d') # 8 转10format(0x4e9,'d') # 16 转10int 和 evalint 和 eval 都可以将二进制,八进制,十六进制转成十进制 123456int('10011101001',2) # 二进制转十进制 1257eval('0b10011101001') # 二进制转十进制 1257int('2351',8) # 八进制转十进制 1257eval('0o2351') # 八进制转十进制 1257int('4e9',16) # 十六进制转十进制 1257eval('0x4e9') # 十六进制转十进制 1257Python数据类型转换有时候，我们需要对数据内置的类型进行转换，数据类型的转换，你只需要将数据类型作为函数名即可。以下几个内置的函数可以执行数据类型之间的转换。这些函数返回一个新的对象，表示转换的值。       函数   描述         int(x [,base])   将x转换为一个整数 ; base参数可以是 2,8,16 分别代表2进制,8进制,16进制 ; x 必须是整型字符串       long(x [,base] )   将x转换为一个长整数 ; x 必须是整型字符串或者 int 型       float(x)   将x转换到一个浮点数       complex(real [,imag])   创建一个复数       str(x)   将对象 x 转换为字符串 ; x 可以是任意类型数据       repr(x)   将对象 x 转换为表达式字符串       eval(str)   用来计算在字符串中的有效Python表达式,并返回一个对象       tuple(s)   将序列 s 转换为一个元组       list(s)   将序列 s 转换为一个列表 ; s 可以是tuple, string, set, dict       set(s)   转换为可变集合 , s 可以是 list, tuple 转化后直接去重. s也可以是 dictionary, 转化后只是得到了所有的 keys       dict(d)   创建一个字典。d 必须是一个列表 (key,value)元组 。dict([(1,'a'),(2,'b'),(3,'c')])       frozenset(s)   转换为不可变集合, 相当于一个 set 增加了 tuple 的特点. a = frozenset(range(1,10))       chr(x)   将一个ascii 值的整数转换为一个字符       unichr(x)   将一个ascii值的整数转换为Unicode字符       ord(x)   将一个字符转换为它的整数值       hex(x)   将一个整数转换为一个十六进制字符串,转换的十六进制以 0x开头       oct(x)   将一个整数转换为一个八进制字符串, 转换的八进制以 0o开头       bin(x)   将一个整数转换成一个二进制数字串, 转换的二进制以0b 开头       eval(x)   将二进制,八进制和十六进制的数字串转成十进制数字   运算符: 算数运算符基本运算符: + - * / % ** //除法的结果是浮点型 123a = 100b = 50a / b # 结果是2. 0 取余 (取模)  a%b == 0 表示可以 a 可以整除 b 对2 取余得到两种可能(0,1) 既可以知道这个数的奇偶性 对3 取余得到三种可能(0~2) 对4取余得到四种可能(0~3) 如果 a比 b小, a%b 得到的余数就是a 对10取余就可以拿到这个数的个位数, 对100取余得到个位和十位数1234567a = 63b = 7c = 'yes' if a%b == 0 else 'no' # 可以被整除13 % 100 # 得到1317 % 10 # 得到7321 % 100 # 得到21次方(幂) **  a**b 就是 求a 的 b 次方110**2 求商(取整) //  除法结果的整数部分 (取整) 任何一个超过一位的数字, 如果想获取它的最左边的整数,就用对比他小一位的100… 求商1321 // 100 # 得到31100//33 # 结果是商 3 取绝对值 用 abs() 函数可以计算一个数的绝对值 赋值运算符 = 将等号右边的值赋给左边, 左边必须是变量 复合运算符= -= *= /= %= **= //= 123a=100a+=1 # 等同于 a = a + 1a **=3 # a 等于 a 的三次方比较运算符&gt; &gt;= &lt; &lt;= == !=   由变量,运算符, 常亮 组成的式子叫做表达式 由比较运算符组成的表达式得到的值是 bool 类型, True, False ==比较操作符：用来比较两个对象是否相等，value做为判断因素； is同一性运算符：比较判断两个对象是否相同，id做为判断因素。```python100 &gt; 200‘a’ == ‘a’x = y = [4,5,6]z = [4,5,6]x == y # 返回 Truex == z # 返回 Truex is y # 返回 Truex is z # 返回 Falseprint (id(x))print (id(y))print (id(z)) 12345678910111213141516##### 逻辑运算符(双目运算符)假: 代表假的有 -- 空字符串, 空列表, 空元组, 空字典, 空集合, 0, None, False, 真: 非零的数字 , True `and or not` - 表达式1 and 表达式2 得到新的一个表达式- 最终的表达式得到的结果也是 bool 型, True False- `and` ( 一假 都假) - 真 and 真 -&gt; 真 - 真 and 假 -&gt; 假 - 假 and 假 -&gt; 假```pythona = 100print(80 &lt; a &lt; 200)print(a &gt; 80 and a &lt; 200) or (一真 斗阵)     真 or 真 -&gt; 真   真 or 假 -&gt; 真   假 or 假 -&gt; 假    12a = 10print(a &lt; 80 or a &gt; 200)           not     真 -&gt; 假   假 -&gt; 真   12a = 10print( not ( a &gt; 5 and a &lt; 15))短路expre1 and expre2 # 如果表达式expre1 为假, 就不会执行 expre2表达式 expre1 or expre2 # 如果表达式expre1 为真, 就不会执行 expre2表达式 逻辑与算符与 if else 1234a = True and 'abc' or '123' # 结果: 'abc' 理解为: 如果是 True, 那么 a = 'abc', 否则 a = '123'b = False and 'abc' or '123' # 结果: '123' 理解为: 如果是 True, 那么 a = 'abc', 否则 a = '123'linenos = self. linenos and 'table' or Falseoptions = self. title and {'title': self. title} or {}成员运算符in not in  判断一个值是否在一个序列中 返回结果是 bool , True False list, set, tuple, string 都是按照 value 找 dictionary 则按照 key 进行查找```pythona = [‘a’,’b’,’c’,’d’]b=’a’b in ac=’e’c not in ad = {‘name’: ‘goudan’ , ‘age’: ‘21’}‘name’ in d # 返回 Truee = ‘abcdefg’f = ‘f’f in e # 返回True 123456789101112131415161718192021222324252627282930313233##### 位运算符[更多说明](http://blog. csdn. net/zhongjling/article/details/8004103)`&amp; | ^ ~ &lt;&lt; &gt;&gt;````bash# &amp; : 按位与 (and) 0000 11111101 01010000 0101# | : 按位或 (or)0000 11111101 01011101 1111# ^ : 按位异或 (不同为1, 相同为0)0000 11111101 01011101 1010# ~ : 按位取反 (~ 运算符只能用于数字操作)0000 11111111 0000# &lt;&lt; : 左移 (左移一位，则其数值变为 a*2)0010 01010100 1010# &gt;&gt; : 右移 (右移1位即除以2；，并且取整)0010 01000001 0010位运算符可以用于比较 set ( 集合)类型的 数据,比如比较两个集合的共同元素, 非共同元素, 并集 123456a = {'a','b','c','d'}b = {'e','a','b','f'}a &amp; b # 返回共有元素{'a', 'b'}a | b # 返回并集 {'b', 'f', 'a', 'd', 'e', 'c'}a ^ b # 返回非共有元素{'f', 'd', 'e', 'c'}运算符优先级详细文档  不需要背优先级, 用小括号可以自定义优先级####流程控制代码执行时从上向下一条条执行pass 可以作为流程控制语句块中的占位符, 保证代码不报错,后续再加逻辑代码代码块以冒号 开始四个缩进在代码块前 if elif else   依次判断每个表达式的逻辑的真假. 是真,退出结构, 是假, 继续执行下面的表达式逻辑  if 结构可以无限嵌套  else 可以省略 : 如果 if , elif 后面 省略 else 也不会报错,   1234567a = 7if a &lt; 0 : print( 数字小于0 )elif a &lt; 5 :  print( 数字大于等于0,小于5 ) # 因为前一个逻辑表达式 a &lt; 0 已经是假了, 所以必然是大于等于0else: print( 数字大于等于5 )      交换两个变量的值a = 200b = 300a,b = b,a[a,b] = [b,a](a,b) = (b,a)一行写打印print() 和 if else 写在一起 123456789101112a, b, c = 1, 2, 3#普通写法if a&gt;b:  c = aelse:  c = b#一行 if else 写法c = a if a&gt;b else b# 一行打印print(a if a&gt;b else b)#一行 二维列表 写法c = [a,b][a&gt;b] # 这种写法会同时将 a和b的值进行交换 快速打印序列 list, set, tuple通过 *, 配合 print() 的 sep 参数 就可以快速打印序列而且不用考虑字符串和数字不能混打的问题 123456789l = ['a','b','c',2,1. 234]print(*l, sep=',')t = ('a','b','c',2,1. 234)print(*t, sep=',')s = {'a','b','c',2,1. 234}print(*s, sep=',')while 循环有两种方式循环:  while for in当程序执行到 while 时, 判断表达式是否为真, 如果为真,执行循环体. 然后循环判断表达式为真并执行循环体,直到表达式为假, 结束循环 12while 表达式 :	循环体 列出100内的偶数 12345i = 0while i&lt;=100:	if(i%2 ==0):		print(i, end= , )	i+=1用 while 转移 列表 1234a , b= [1,2,3,4,5] , []while a:	b. append(a. pop())print(b)用 while 批量删除列表中的特定元素 1234567a = [1,2,3,2,1,2,3,4,1,2,1,2,3,2,1]while 1 in a:	a. remove(1)print(a)下面写法相当于一行 while, 不会报错, 和上面的一样while 1 in a:a. remove(1)break &amp; continuebreak 结束当前层的循环continue 停止执行剩下的代码块, 进入下次循环使用 continue 时需要把 i+= 放在 continue 的上面,否则进入死循环 12345678i = 0while i &lt; 10:	i+=1	if i == 7:		break	if i == 4:		continue	print(i, end=',')死循环当用 print() 在死循环中打印时, 如果加上了 end=',' 参数时,不会打印任何东西,因为end 会在循环结束后才会批量打印内容 1234import timewhile True:	print( a )	time. sleep(1)while 和 input 的配合使用 12345import randomnum = random. randint(1,100)while True:	input_num = int(input( 猜一个数字 ))for in 循环类型   for in 可以遍历string, list, tuple, set, dictionary  for in 能实现的循环, while 也可以, 反之亦然  for 可以用 step for x in range(start, end ,step) 遍历列表 1234567891011121314151617l = ['a','b','c','d','e']S =  abcde t = ('a','b','c','d','e')s = {'a','b','c','d','e'}d = {'a':1,'b':2,'c':3,'d':4,'e':5}for x in l:	print(x)for x in S:	print(x)for x in t:	print(x)for x in s:	print(x)for k,v in d. items():	print(str(k) +  ====&gt;  + str(v))print(*S,sep=',') # 也可以反向循环 , 用 reversed() 函数 1234567l = ['a','b','c','d','e']for x in reversed(l):  print(x)a = 'abcdefg' print(*reversed(a), sep='') # 翻转字符串print(a[::-1]) # 翻转字符串遍历列表的同时想拿到每个元素的索引值, 可以用 enumerate() 函数按行读取文件并处理的时候,可以使用这种方法打印处来出问题的行号 12345678my_list = ['a', 'b', 'c']for idx, val in enumerate(my_list):  print(idx, val)i = 0for val in my_list:  print(i, val)  i +=1for else / while else 结构    for 后面跟 else 的含义是: 如果 for in 循环正常结束, 会执行 else 段. 如果 for in 通过 break 结束, 那么就不会执行 else 的代码段  这种用法可以用于判断一个循环是否是正常结束 123456789101112131415l = [1,2,3,4,5]for i in l:  if i == 4:    break  print(i)else:  print( for 循环正常结束, 进入 else )i=0while i &lt; 5:  if i == 3:    break  print(i)else:  print(  while 循环正常结束, 进入 else )双重循环1234567891011121314151617181920212223#打印九九表for x in range(1,10):  for y in range(1,10):    if(y&gt;x):      break    print( %d x %d = %d  %(y,x,x*y) , end= \t|\t )  print(  )# 杨辉三角l = [[0,1,0]]for x in range(1, 10):  newlist=list()  newlist. append(0)  for y in range(1, 10):    if y&gt;x:      break    print ( %d-%d:  %(x,y), end=   )    newlist. append(l[x-1][y-1]+l[x-1][y])  l. append(newlist)  newlist. append(0)  print(   )print(str(l))1. 1. 2 LIST: list 的doc 1help(list)列表内嵌方法: 就是一个list 对象的内部方法 12a = []a. append( a. clear(  a. copy(  a. count(  a. extend( a. index(  a. insert( a. pop(   a. remove( a. reverse( a. sort( ####列表操作函数就是能够对列表对象进行操作的函数(不是列表对象的内部方法)       序号   函数   说明                         2   len(list)   列表元素个数       3   max(list)   返回列表元素最大值       4   min(list)   返回列表元素最小值       5   list(seq)   将元组, 字符串, 集合和字典(字典的 keys)转换为列表   *技巧: *    看一个变量有哪些扩展函数 比如 定义了变量 a = [1,2,3] , 想知道啊有哪些内置方法可以用 (比如 a. pop() …) 可以进入terminal 的 python 命令行, 定义一个变量,输入 a. 然后双击 tab 键, 终端会给出所有的和这个变量相关的函数    看一个变量的扩展函数的用法比如: 不知道 a. pop() 如何使用可以进入terminal 的 python 命令行,输入a. pop. __doc__ , 然后回车 **列表定义和修改 *: 1234567a = list()a = []a = ['a','b',1,2]a = [[1,2,3],['a','b','c'],{1:1,2:2,3:3},(1,2,3,4),{'a','b','c','d'}]a[-1][-2] # 输出ca[-1][-2] = 'x'给列表添加元素: 123a[0] =  abc  # 如果 a[0] 没有被定义过, 报越界错误, 列表不能以这种形式添加 a. append( rere )a. insert(0,'ducati')通过列表进行批量赋值: 123456[a,b,c] = [1,2,3] # 左右两边的赋值个数必须相等 a,b,c = [1,2,3] # 和上一个效果相同, 右侧可以是其他类型序列或者迭代对象, 包括字符串,文件对象,迭代器,生成器a,b = (x for x in range(2)) # 这里是生成器的例子_,name = ['234','dalong']  # 不用的元素可以赋值给一个不会被用到的字符串,之后就扔掉了.  在赋值过程中可以用 * 号进行赋值, 当不确定有多少个元素需要迭代时, * 就很有用了. 1234567*trailing, current = [10, 8, 7, 1, 9, 5, 10, 3]name, email, *phone_numbers = ('Dave', 'dave@example. com', '773-555-1212', '847-555-1212')line = 'nobody:*:-2:-2:Unprivileged User:/var/empty:/usr/bin/false'uname, *fields, homedir, sh = line. split(':')访问列表 列表切片: *注: 切片的语法和结果与字符串切片完全一样 , 可以参考字符串切片笔记 *使用方括号和索引访问list. 方括号中可以用冒号来指定先要获取的索引起点和终点 12345678list1 = ['physics', 'chemistry', 1997, 2000]list2 = [1, 2, 3, 4, 5, 6, 7 ]print ( list1[0]:  , list1[0])print ( list2[1:5]:  , list2[1:5])print ( 倒数第二个元素 , list1[-2])print ( 从第二个索引开始的所有元素 , list1[1:])print ( 从头开始取取到第五个元素 , list1[:5])print ( 每隔一个元素取一个元素 ,list2[::2])删除列表元素 , 清空列表: 12345del a[0] #删掉并不再使用a. pop(0) # 指定要删除的元素,并返回该元素. 如果不指定元素,默认删最后一个a. remove('abc') # 删除value 是 'abc'的第一个元素. 在不知道索引号的时候可以用 a[::] = [] # 清空一个列表a[0:2] = [] # 清空一个列表的前两个元素排序: 12a. sort(reverse=True) # 倒序排序,并对 a 产生永久变化. a. reverse() # 对 a 进行翻转,并不做排序动作####统计元素在列表中出现的频次count() 函数可以统计某元素在列表中出现的次数 12345words = ['look', 'into', 'my', 'eyes', 'look', 'into', 'my', 'eyes','the', 'eyes', 'the', 'eyes', 'the', 'eyes', 'not', 'around', 'the', 'eyes',  don't , 'look', 'around', 'the', 'eyes', 'look', 'into', 'my', 'eyes',  you're , 'under']words. count('look')如果需要知道 top3 频次最高的词, 就需要用collections 的 Counter 函数了 123456from collections import Counterword_counts = Counter(words)# 出现频率最高的 3 个单词top_three = word_counts. most_common(3) print(top_three)# Outputs [('eyes', 8), ('the', 5), ('look', 4)] 列表的复制和副本:  用 a=b 复制一个列表的变量只是将新的列表变量和原来的变量指向同一个列表用 a=b[:] 可以克隆一个新的列表 123b = [1,2,3]a = b # 这行不通,只是把 a 和b 指向了同一个 list 内存地址a = b[:] # 新的列表, 新的内存地址列表的操作: 可以用+, * , in 来操作列表: 注: 以下操作同样可以用于 dictionary, tuple, seta           Python 表达式    结果    描述              len([1, 2, 3])    3    长度          [1, 2, 3] + [4, 5, 6]    [1, 2, 3, 4, 5, 6]    组合          [‘Hi!’] * 4    [‘Hi!’, ‘Hi!’, ‘Hi!’, ‘Hi!’]    重复          3 in [1, 2, 3]    True    元素是否存在于列表中          for x in [1, 2, 3]: print x,    1 2 3    迭代      1. 1. 3 一行 生成式 / 生成器 / 过滤器 / 一行 for in: 生成式的方法 doc  用 for 循环可以快速生成数字型 list, tuple, set, dict 用 %d 等格式化符号可以对生成内容进行格式化 生成内容可以通过函数进行变化 注意, 一行 for 用来做嵌套时, 如果是嵌套循环, 那么for 逻辑是从右往左( 先看右边的 for , 再依次往左看) 如果是一行 for 是 顺序执行, 那么就是从左往右看1234567l = [x for x in range(1,11)] l = [x*2 for x in range(1,11)]l = [x*x for x in range(1,11)]l = [str(x) for x in range(1,11)]l = [str(x) for x in range(1,11)]l = [x for x in range(1,11) if x%2 == 0] 12345list(range(5))tuple(range(5))set(range(5))dict. fromkeys(range(5), 'x')dict(zip(range(100),range(100)))生成二维或者多维空 list 1234list_1d = [0 for i in range(5)]list_2d = [ [0 for x in range(5)] for y in range(5)]list3d = [[[x for x in range(3)] for y in range(3)] for z in range(3)]l = [[x for x in range(65,91)],[y for y in range(97,123)]]生成 dictionary: 123dict_1d = {i:i*10 for i in range(1,10) if(i%2==0)}dict_2d = {i:{i:1*2 for i in range(3)} for i in range(2)}dict_3d ={i:{i:{i:1*2 for i in range(3)} for i in range(3)} for i in range(3)}一行语句过滤:可以用一行语句来对列表,字典, tuple, set 等集合进行过滤 123456789L=['Hello',10,'World',None]print ([s. lower() for s in L if isinstance(s,str)])d2d = {i:{j:j+1 for j in range(3)}for i in range(3)} # 创建一个2d的字典m2d = {k:{kk:vv+1 for kk,vv in v. items()} for k,v in d2d. items()} # 递归遍历2d字典,给每个值+1a = [0, 3, 4, 5, 6, 7, 8, 9]l = [x for x in a if x&lt;6] # 过滤出 a中所有小于6的元素将多维数据降为一维数据 123456L2=[(1,3,5),(2,4,6),(100,200)] # 降二维到一维print ([value for t in L2 for value in t])l = [[[1, 2], [1, 2]], [[1, 2], [1, 2]]]v = [value for t in l for d in t for value in d] # 降三维到一维同时遍历两个列表并进行操作一行 for in 有两种:  [对(x)的表达式 for x in 集合 if 条件] [对(x,y)的表达式 for x in 集合1 for y in 集合2 if 条件]12345# 将两个列表中的偶数分别相加x=[1,2,3,4]y=[5,6,7,8][a + b for a in x for b in y if a%2 == 0 and b%2 ==0] # 结果就是[8, 10, 10, 12] , 分别表示 2+6, 2+8, 4+6, 4+8将两个列表生成一个字典 1234keys = ['age', 'name', 'role']values = [30, 'dalong', 'teacher']person = {key:value for key, value in zip(keys, values) if key!='role'}# {'age': 30, 'name': 'dalong'}一行生成九九乘法表  注意, 一行 for 是嵌套, 如果是嵌套循环, 那么for 逻辑是从右往左( 先看右边的 for , 再依次往左看) 如果是一行 for 是 顺序执行, 那么就是从左往右看1print('\n'. join([''. join([ %d*%d=%2s  %(y,x,y*x) for y in range(1,x+1)]) for x in range(1,10)]))1. 1. 4 Tuple: Tuple 就是一组其元素值不允许被变化的 list用小括号定义 1help(tuple)123456tup1 = ('physics', 'chemistry', 1997, 2000)tup2 = (1, 2, 3, 4, 5 )tup3 =  a ,  b ,  c ,  d  # 没有括号并用逗号分隔的数据定义的变量被认为是 Tupletup4 = ( a ,) # 创建只有一个元素的 tuple 时,需要带一个逗号结尾,否则会被创建成和定义的字符相近的类型(int, str 等)tup5 = tuple()tup6 = ()Tuple 的元素不能修改,但是 tuple 的变量可以被重新定义 12tup1 = ('physics', 'chemistry', 1997, 2000)tup1 = ('a','b')如果 tuple 中的某个元素是列表或者是字典,那么这个元素中的元素是可以被修改的 12tup1 = ('a',[1,2,3])tup1[1][0] =  a Tuple 的访问方式和 list 相同, 元组切片方式和列表一样 Tuple 的删除不能只删除Tuple的某个元素, Tuple 只能被整个删除, 用 del 123tup = ('physics', 'chemistry', 1997, 2000)print (tup)del (tup)Tuple 的操作Tuple 可以用 + , * 进行操作+ 是组合多个 tuple 成为一个新的 tuple* 是 将一个 tuple 重复多次成为一个新的 tuple tuple 的内置方法 1x. count( x. index(tuple 的 操作函数| 序号  | 方法     | 描述     || :— | :——— | :———- || 2  | len(tuple) | 计算元组元素个数。  || 3  | max(tuple) | 返回元组中元素最大值。 || 4  | min(tuple) | 返回元组中元素最小值。 || 5  | tuple(seq) | 将列表转换为元组。  | 1. 1. 5 Dictionary 字典: 1help(dict)   由健值对组成, 必须成对出现   key必须不可变, 字典的 key 可以是字符串,数字,也可以是 tuple (key 不可以是 dict, list, set) 字典值可以是任意数值类型, 包括str, int, bool , set, dict, tuple 等 不允许同一个键出现两次。创建时如果同一个键被赋值两次，后一个值会被记住 值可以重复 如果用 a[‘key’] 访问的键不存在, 会报错创建字典 12dict1 = { 'abc': 456 }dict2 = { 'abc': 123, 98. 6: 37 }创建一键多值的字典 123456d={'a' : [1, 2, 3],'b' : [4, 5] }e={'a' : {1, 2, 3},'b' : {4, 5} }可以用 collections 的 defaultdict 来快速创建一键多值的字典 1234567from collections import defaultdictd = defaultdict(list)d['a']. append(1)d['a']. append(2)d['a']. append(3)d['a']. append(4)print(d)访问字典可用 a[‘key’] 的形式或者 a. get(‘key’) 的形式访问字典的某个 key 的 value. 12345dict = {'Name': 'Zara', 'Age': 7, 'Class': 'First'}print ( dict['Name']:  , dict['Name'])print ( dict['Age']:  , dict['Age'])dict. get(Name) 用 . get() 的好处是: 就算 get 的 key 不存在, 也不会报错,只是返回 None 字典的添加和修改 12d1 = {}d1['name'] = 'maodan' # 如果d1['name'] 已经被定义, 那么就对原健值进行修改, 如果未被定义,就新建一个键值对删除字典用del 或者 内置的 clear 都可以 字典内置方法 12myd. clear(   myd. fromkeys(  myd. items(   myd. pop(    myd. setdefault( myd. values(  myd. copy(    myd. get(    myd. keys(    myd. popitem(  myd. update(  字典的内置函数如下       序号   函数   描述                         2   len(dict)   计算字典元素个数，即键的总数。       3   str(dict)   输出字典可打印的字符串表示。       4   type(variable)   返回输入的变量类型，如果变量是字典就返回字典类型。   让字典输出时保持插入时的顺序可以使用 cllections 的 OrderdDict 函数 1234567from collections import OrderedDictd = OrderedDict()d['dalong'] = 1d['xiaomi'] = 2d['wanzi'] = 3for x in d:  print(x,d[x])利用 zip() 快速找出字典的最大值 最小值 和 排序关键是用 zip() 分别处理 字典的 values 和 keys zip(prices. values(), prices. keys()) 123456789101112131415161718prices = { 'ACME': 45. 23,'AAPL': 612. 78, 'IBM': 205. 55, 'HPQ': 37. 20, 'FB': 10. 75}# 找最大最小值min_price = min(zip(prices. values(), prices. keys())) # min_price is (10. 75, 'FB')max_price = max(zip(prices. values(), prices. keys())) # max_price is (612. 78, 'AAPL')#排序prices_sorted = sorted(zip(prices. values(), prices. keys())) # prices_sorted is [(10. 75, 'FB'), (37. 2, 'HPQ'),# (45. 23, 'ACME'), (205. 55, 'IBM'),# (612. 78, 'AAPL')]利用 zip() 和 dict() 函数快速创建字典 123456# 方法一dict(zip(list(range(100)),range(100))) # 方法二a = [1,2,3]b = [4,5,6]dict(zip(a,b))找出两个字典的共同点  keys() 函数可以获取字典的所有 key 值,并返回一个列表. 但是很少有人知道 返回的 列表是支持集合操作的items() 函数也同样支持集合操作 123456789a={'x' : 1,'y' : 2,'z' : 3 }b={'w' : 10,'x' : 11,'y' : 2 }# Find keys in commona. keys() &amp; b. keys() # { 'x', 'y' } # Find keys in a that are not in b a. keys() - b. keys() # { 'z' }# Find (key,value) pairs in common a. items() &amp; b. items() # { ('y', 2) }快速删掉字典中的某些key 的元素 1234# Make a new dictionary with certain keys removeda={'x' : 1,'y' : 2,'z' : 3 }c = {key:a[key] for key in a. keys() - {'z', 'w'}} # c is {'x': 1, 'y': 2}1. 1. 6 Set 集合: 更全的doc集合两大特点: 无序性 , 唯一性无序: 内部元素没有顺序唯一: 内部元素不会重复 集合的定义 12s1 = set() # 如果用 s1 = {} 会把 s1定义成空字典s1 = {'李白', '杜甫', '白居易'}交集 差集 并集 对称差分 父级 子集set 会需要计算两个集合的交集,差集,并集,对称差分 父级 子集 123456789101112131415161718s1 = {'a','b','c','d'}s2 = {'a','b','e','f'}s3 = {'a'}print(s1 &amp; s2) # 交集 abprint(s1. intersection(s2)) # 交集 abprint(s1 | s2) # 并集 abcdef print(s1. union(s2)) # 并集 abcdef print(s1 - s2) # 差集 cdprint(s1. difference(s2)) # 差集 cdprint(s2 - s1) # 差集 efprint(s2. difference(s1)) # 差集 efprint(s1 ^ s2) # 对称差分 cdefprint(s1. symmetric_difference(s2)) # 对称差分 cdefprint(s3. issubset(s1))  # 子集 Trueprint(s3&lt;s1) # 子集 Trueprint(s1. issuperset(s3))  # 父集 Trueprint(s1&gt;s3) # 父集 True1. 1. 7字符串: 字符串的 doc 1help(str)字符串的定义:  用单引号或者双引号括起来, 单双引号无区别 \可以用于换行 三个单引号或者三个双引号允许多行, 三引号允许一个字符串跨多行，字符串中可以包含换行符、制表符以及其他特殊字符。```pythonvar1 = ‘Hello World!’var2 = “Python Runoob”var3 = ‘&lt;div class= abc &gt;adfad&lt;/div&gt;’var4 = ‘abcde’‘fghijk’var5 = ‘abcdefg’var6 = ‘’’ asbcdefg‘’’ var7 = ( “Hello “ “My Name is “ “Dalong “)12345**用 input 输入的内容定义字符串变量**通过 input 输入进来的数据都是字符串类型```pythonvar = input( input some text here:  )字符串长度计算: 不论中英文, 字符串长度都是由字数决定的,不需要关心底层是如何存储的 1234j = 'I love you baby'k = '我喜欢的明星是李小龙'len(j)len(k)**访问字符串中的值 **:  和访问 list 方式基本相同,会把每个字母看做一个 list 中的元素 从左边开始,下标从0开始 从右边开始,下标从-1开始 字符串的变量名不要起名叫 str123456var1 = 'Hello World!'var2 =  Python Runoob print ( var1[0]:  , var1[0])print ( var2[1:5]:  , var2[1:5])print(var1[::2])print(var[-4]) # 字符串切片:    a[索引]     a[开始下标:结束下标:步长下标]     ‘0123456789’ 的每个字符下标可以分别表示成 0123456789 或者 -10-9-8-7-6-5-4-3-2-1  开始字符下标包含开始字符  结束字符下标不包含结束字符  步长下标是正数是正序, 负数是逆序 12345678910111213a = '0123456789'a[:] # 全部提取. 和直接用 a 相同a[0:3] # 0到3a[:3] # 0到3a[2:] # 2到尾a[2:-2] # 2到-2 不包含倒数第二个a[-4:] # 取最后4个a[::2] # 从第一个元素开始,隔一个取a[::-1] # 字符串逆序 9876543210a[::-2] # 字符串从后往前,隔一个取 97531a[-1:-5:-1] # or a[:-5:-1] 先逆序,然后从最后开始往前取到第-5个位置 输出9876a[-5::-1] # 逆序,从第-5位置的数字取到头 543210a[5:1:-1] # or a[-5:-9:-1] 逆序, 从第五个取到第二个 5432slice() 函数 和切片  代码中如果有大量的切片会让代码可读性下降, 因为切片本身不具有含义. 可以使用 slice() 函数定义一个切片对象. 变量名是具有含义的. 123456record = '. . . . . . . . . . . . . . . . . . . . 100 . . . . . . . 513. 25 . . . . . . . . . . 'cost = int(record[20:23]) * float(record[31:37]) SHARES = slice(20, 23)PRICE = slice(31, 37)cost = int(record[SHARES]) * float(record[PRICE])字符串转义:  将字符串中的字符原有意义去掉 用 \ 来转义 \n 换行符 \t 制表符 (用来让文本进行缩进或者对齐) r 用来取消转义字符的意义1234567j = 'I love\' you baby' # 用转义让a = 'I love y\nou b\taby' # 使用用制表符,换行符print(a) b = 'I love \\tyou b\\naby' # 通过转义字符让制表符换行符失去意义print(b) c = r'abcd \n \t efg'print(c)** + * in 字符串运算符操作 **:       : 两个拼接的变量都必须是字符串类型才能拼接 a+b         : 原来的字符串重复拼接 n 次 a*10    当数字类型的变量需要和字符串拼接时,需要用 str() 转成字符串``` pythona = ‘abc’b = ‘bcd’c = 1d = a+b+str(c) # 当数字类型的变量需要和字符串拼接时,需要用 str() 转成字符串a = “Hello”b = “Python” print (“a + b 输出结果：”, a + b )print ( “a * 2 输出结果：”, a * 2 )print (“a[1] 输出结果：”, a[1] ) print (“a[1:4] 输出结果：”, a[1:4] ) if( “H” in a) :  print ( “H 在变量 a 中” )else :  print ( “H 不在变量 a 中” ) if( “M” not in a) :  print (“M 不在变量 a 中” )else :  print ( “M 在变量 a 中”) 12345也可以用 join 代替```python''. join([a,b,c])字符串的不可变性**: 字符串一旦赋值给一个变量, 字符串本身的每个字符不可以直接改变但字符串变量可以从新赋值 12a = 'abcde'a[0] = 1 # 报错 TypeError: 'str' object does not support item assignment字符串的格式化: 1help('FORMATTING')#####传统格式化方式 %       符号   占位符号描述         %s   str 类型       %d   int 类型 %10d 表示共10位,前面补空格 ; %010d 表示共10位,前面补0       %f   float 类型, 默认保留6位小数, 可以用 %. 2f 限制2位小数; %010. 2f 表示共10位,保留两位小数,前面补0       %c   打印一个字符 , 需要了解 ascii 碼 print('%c' % 97)       %o   将十进制数字按照八进制打印出来 print('%o' %97)       %x   将十进制数字按照十六进制打印出来 print('%x' %97)   123456789101112131415name = '狗蛋'like = '二丫'age = 18heigh = 183. 33#格式化占位符, 前后顺序需要对应print('我叫 %s, 我喜欢%s, 我今年 %d 岁了, 我身高 %. 2f cm ' %(name, like, age, heigh))print print('%c' % 97) print('%o' % 100) # 十进制转八进制 输出144print('%x' % 64) # 十进制转十六进制 输出40print(0o144) # 八进制转十进制 输出100print(0x40) # 十六进制转十进制 输出64第二种格式化方式 . format()更多说明官方文档 1help('FORMATTING') # 查看 format 的文档1234567891011name = '狗蛋'like = '二丫'age = 18heigh = 183. 33#格式化占位符, 前后顺序需要对应'我叫 {}, 我喜欢{}, 我今年 {} 岁了, 我身高 {} cm '. format(name, like, age, heigh)'我叫 {3}, 我喜欢{2}, 我今年 {1} 岁了, 我身高 {0} cm '. format(name, like, age, heigh)'我叫 {name}, 我喜欢{like}, 我今年 {age} 岁了, 我身高 {heigh} cm '. format(name='aa', like='bb', age='cc', heigh='dd')通过列表设置 format()参数, 把*放在实参前面,说明这是一个列表 . (字符串也可以通过这种方式进行切分) 123string = ['a','b','c']'{2}, {1}, {0}'. format(*string)     #1 '{0[2]}, {0[1]}, {0[0]}'. format(string) #2 如果不用星号, 就需要在字符串中给每个位置增加一个0的标志 通过字典设置 format() 参数, 把**放在实参前面说明这是一个字典 12345# 通过字典设置参数site = { name :  菜鸟教程 ,  url :  www. runoob. com }string = '网站名：{name}, 地址 {url}'print(string. format(**site))str. format_map() 函数这个函数可以直接将变量域中找到的变量作为 format 的参数 , vars() 返回当前变量域中的所有定义过的变量. 1234s = '{name} has {n} messages. 'name = 'Guido'n = 37print(s. format_map(vars())) # vars() 是 python 内置函数, 可以查看所有当前的变量域字符串的对齐 12345678#左对齐'{:&lt;30}'. format('left aligned')#右对齐'{:&gt;30}'. format('right aligned')#居中对齐'{:^30}'. format('centered')#居中对齐,两边填充星号'{:*^30}'. format('centered') # use '*' as a fill charprint 的格式化pring 函数可以定义打印字符串用什么结尾, 默认是换行符 \n 12print( aab )print('aab', end= ** )ascii 码: ascii 对应表  计算机只认识二进制的01, 所以需要有一套对应关系表把不同字符用二进制来表示. 这套编码就是 ascii 码 小写字母 a-z 的 ascii 值是97-122 大写字母 A-Z 的 ascii 值是 65 - 90 数字0-9 的 ascii 值是 48 - 57  12ord('@') # 将字符转化为 ascii 值 64chr(33) # 将 ascii 值转化成为字符 '!'   快速查询 askii 码表的方法 12345data = b'abcdefg'print(data[0]) # 查看 a 的 ascii 码, 输出97print(data[1]) # 查看 b 的 ascii 码, 输出98# or 直接 print 输出 ascii 码print(b'%'[0]) # 查看 百分号的 ascii 码, 输出37字符串内嵌方法: 1234567a =  abc a. capitalize(  a. expandtabs(  a. isalpha(   a. isprintable( a. lower(    a. rindex(    a. splitlines(  a. upper(    a. casefold(   a. find(     a. isdecimal(  a. isspace(   a. lstrip(    a. rjust(    a. startswith(  a. zfill(    a. center(    a. format(    a. isdigit(   a. istitle(   a. maketrans(  a. rpartition(  a. strip(    a. count(    a. format_map(  a. isidentifier( a. isupper(   a. partition(  a. rsplit(    a. swapcase(  a. encode(    a. index(    a. islower(   a. join(     a. replace(   a. rstrip(    a. title(    a. endswith(   a. isalnum(   a. isnumeric(  a. ljust(    a. rfind(    a. split(    a. translate(  1. 1. 8 Dict, Tuple, List, Set, String 的 差异分析: 12345678l = [1,2,3]t = (1,2,3)s = {1,2,3}d = {1:1, 2:2, 3:3}i = 10f = 1. 23b = TrueS =  abc 相同点:    访问: 都是使用 a[] 方式访问内部元素, a 是变量名  删除变量: 都可以用 del(a) 来删除变量  长度: 都可以用 len() 来计算长度  深度拷贝: 可以用如下方法进行克隆 import copy; a = [[1,2],3,4]; b=copy. deepcopy(a); 差异点:    创建: d = {} ; t = () ; s = set() ; l =[]  访问: List, Set, Tuple, String 用 a[0] 访问元素, 访问字典用 a[‘name’]  切片: list, tuple, stringS 用 a[::] 进行切片, 语法和结果相同 ; dict 没有切片  添加元素: dict 直接用 d['x']='def' 添加 ; List 用 . insert() or . append() ; set 用 . add() ; Tuple 不能添加  删除一个元素 .  List, dict, set 都可以用 . pop(index) 来删掉一个元素. set 和 list 可以用 . remove('abc') 来删除第一个找到的指定的元素值,  清空: List, Dict, Set 都可以用 . clear() 来清空. Tuple 不可变  排序: 只有 list 可以排序 l. sort() ; set 的元素可以通过 list() 来进行排序. a = {4,3,2,1} ; b = list(a); b. sort()  最大值 max()/最小值 min(): string 可以直接取出对应 ascii 最大的值返回; list, set, tuple, dict, 如果 元素内容都是同一类型, 那么返回最大的值, 如果不是相同类型,则报错 ;  复制:  a = b 会对目标进行复制,但是只是新的变量和老的变量指向同一个内存块,所以如果一个变量中的元素发生变化,那么另一个变量中的元素也变了.  tuple 除外  克隆: str, list, tuple 可以通过切片进行克隆 a = b[:] ; set 是无序的,不能用切片进行克隆.  还可以用 1. 2 函数: 自定义函数:  定义: 被封装起来并反复使用的代码块, 叫做函数使用代码块时调用函数就可以了    不要在function中用import 优点:  提高代码利用率 提高代码可读性 便于调试 debug     print()   exit() 程序执行到 exit() 后就结束程序   断点 调试   1234567def fb(word:int = ''):	print('a')	pass	print('b')	exit()	print('c')fb() # 只会打印 a 和 b分类  库函数: print , input, abs 等python 直接提供的函数 自定义函数: 自己封装的函数命名  和变量命名规则相同 字母数字下划线 不能数字开头 不能是关键字 多个单词时,用下划线或者小驼峰方式命名 () 里是参数 后面跟冒号12def abc(a:str =  abc , b:int = 0, c:list =[]) -&gt; dict:	pass参数  形参: 定义函数中的参数名 实参: 调用函数时提供的参数值 实参和形参必须一一对应出现 , 除非形参已经指定了默认值 实参可以指定形参的变量名. 这样就可以调换实参的顺序.  但不能在实参中指定不存在的变量名 实参可以是任意数据类型12345678def love_yao(n , m): # 形参	passlove_yao(m=3,n=2)  # 实参 , 实参可以指定形参的变量名def love_yao(n , m=0):	passlove_yao(2)  系统命令行参数  系统变量中可以解析到命令行中传给文件的参数 变量 sys. argv 是一个list, 存储了文件名本身和传递给文件的变量12import sysprint(sys. argv)参数的默认值  有了默认参数，如果不输入该参数，就取默认值    没有默认参数的情况：def search4vowels(word:str) -&gt; set:  有了默认参数的情况：def search4vowels(word:str = 'aeiou') -&gt; set:  带默认值得参数要放在所有参数的最后边: def a(c, b=  ): 否则会报错 SyntaxError: non-default argument follows default argument 任意参数  形参中用 * 表示可以接收任意个参数, 经常使用的参数名是 *args 传进来的参数是以 tuple 形式保存在函数中 如果要让函数接受不同类型的实参，必须在函数定义中将接纳任意数量实参的形参放在最后。Python先匹配位置实参和关键字实参，再将余下的实参都收集到最后一个形参中。 形参中用 ** 表示可接受任意形式的健值参数, 既在实参中制定了 参数名和参数值得参数.  经常使用 **kwargs 形参**kwargs中的两个星号让Python创建一个名为 a空字典，并将收到的所有名称—值对都封装到这个字典中。1234567891011121314151617181920212223242526#def f(*a:'anything') -&gt; None:def f(*a):	print(a)	return ai = f(1, a ,[1,2,3],{1,2,3}) # 返回 (1, 'a', [1, 2, 3], {1, 2, 3})def f(b, *a):	print(b)	print(a)f( hello ,1,2,[1,2],('a','b'))''' 会打印hello(1, 2, [1, 2], ('a', 'b'))'''def f(a,b,**c):	print(a)	print(b)	print(c)f( a , b ,c= c ,d= d )''' 会打印ab{'c': 'c', 'd': 'd'}''' 将**传递进函数里的参数进行批量赋值,12345class cls:  def __init__(self, name=None, **opts):    self. name = name    for key, value in opts. items():      setattr(self, key, value) 当传递实参时,可以通过 list 传递, 但是list 前面要加上 * , 俗称拆包 . list 中的元素个数和形参个数需要相同 当传递实参时,可以通过 字典 传递, 但是dict 前面要加上 ** , 俗称拆包 . dict的 key 必须和形参中的名字一一匹配 可以将形参的* 和 实参的 * 结合使用12345678910111213141516171819202122232425def f(a,b,c):	passl = ['a','b','c']f(*l)def f(a,b,c):	passd = {'a':1,'b':2 ,'c':3}f(**d)def f(a,b,c,*d):	print(a)	print(b)	print(c)	print(d)l = [1, 2, 3, 4, 5, 6, 7]f(*l) # 会打印如下信息''' 123(4, 5, 6, 7)'''函数的生命周期  函数的生命周期就是函数被调用开始,调用完成后就结束了 函数内部的所有变量都会在函数被执行结束后自动销毁 如果函数内有内嵌函数, 那么内嵌函数也会在外部函数执行完后被自动销毁, 除非外部函数把内部函数返回返回值  return 关键字用来传递返回值 如果函数没有返回值 (无 return关键字 ) 默认返回 None return 一旦被执行, 后面的函数体不会被执行 return 可以是任何数据类型 return 后面用逗号隔开的多个值 return a, b,  c , 1 其实是一个元组 return 不是一个方法，所以 return 后面跟的value 不需要用括号括起来1234567891011121314def abc():	passa = abc();print(a) # 输出 Nonereturn [1,2,3,4]return {1,2,3,4}return 1,2,3,4def myfun():	return 1, 2, 3a, b, c = myfun() #当函数返回多个值时, 直接对多个返回值进行赋值. Annotation  是 python3的新功能，用于标注funcion传递的参数类型和返回值的类型    没有annotation的情况：def search4vowels(word):  有了annotation的情况：def search4vowels(word:str) -&gt; set:       上面的代码说明了传递的参数word 是字符串，返回的值是set       annotation 并不是让python 强制输入参数和返回的类型，而是让程序员更容易的了解function的用法  如果要说明一个function 没有返回值, 可以写成 -&gt; None  由于python 的编译器是不对annotation进行实质性的操作的，所以程序员可以把annotation 加上两个单引号，当做自己的注释，比如 12345def log_request(req: 'the request from flask request' , res: str) -&gt; None:	passdef search4vowels(word:str =   ) -&gt; set:	pass用法 定义函数和调用 1234567891011def first_blood(a= 猥琐发育,别浪  ):	'''函数体'''	'''函数说明'''	print(a)	second_blood()def second_blood(b= 我喜欢球星库里 ):	print(b)first_blood(a= 集合,准备团战 )	函数重载如果定义了两个同名函数 (参数设置可以不同) ,那么后面的函数会覆盖前面的函数 1234567def f(a=1,b=2):	passdef f():	passf() # 会执行 def f() 这个函数全局变量和局部变量  局部变量: 函数体内定义的变量就是局部变量 全局变量: 在函数体外定义的变量是全局变量 查看全局变量和局部变量的方法:     全局变量: 在 文件中 print(globals()) 或者 print(vars())   局部 变量: 在函数内 print(locals()) 然后再调用这个函数    作用域:     局部变量作用域只在定义它的函数体内,   如果局部变量中有全局变量同名变量, 那么函数体内默认使用局部的同名变量值   全局变量在函数体内只能读取,不能修改 ( 直接修改等于定义了一个新的局部变量)   如果想在函数体内修改全局变量的值, 需要用 global 关键字声明要修改的内容是全局变量   如果全局变量是一个列表,集合或者字典,可以直接在函数体内直接修改   1234567891011121314151617a =  a l = [1,2,3]def f():	b = 'b'	print(b) # 输出b	print(a) # 输出a	a = 'aaaaa' 	print(a)# 输出 aaaaa	global a	a = 'global a'	l[0] = 100print(a) # 输出 aprint(b) # 报错f()print(a) # 输出 global aprint(l) # 输出 [100,2,3]嵌套函数  嵌套函数就是函数体内再定义子函数 子函数的作用域只能在子函数中调用 子函数如果想修改父函数的变量, 需要用 nonlocal 关键字 ( nonlocal 只能修改子函数对应的一层父函数 ) 子函数 如果想修改全局变量,需要用 global 关键字1234567891011121314151617181920212223242526272829303132333435363738394041424344def f():  s =  这是 外层 函数   print(s)  def g():    s =  这是 内层 函数     print(s)  g()f()def f():  s =  这是 外层 函数   print(s)  def g():    s =  这是 内层 函数     print(s)    nonlocal s    s =  修改外层函数     print(s)  g()f()s =  这是全局变量 def x():  s =  这是x 层   print(s)  def f():    s =  这是 f 层     print(s)    def g():      # s =  这是子函数       nonlocal s      s =  修改外层函数       print(s)      # global s      # s =  修改全局变量       # print(s)    g()    print(s)  f()  print(s)x()将内部嵌套函数返回  如果希望将内部函数返回， 内部函数也需要 return 关键字12345678910111213def parent(name= henry ):  print( this is parent function )  def child():    return 159  return childc = parent()print(c())print(type(c))print(type(c()))# this is parent function# 159# &lt;class 'function'&gt;# &lt;class 'int'&gt;内置函数: python 内置函数: 所有内置函数  技巧 	&gt; - python terminal 中输入 a , 然后 双击 tab 键查看所有以 a开头的内置函数dir(__builtins__) 可以查看所有内置函数       函数名   说明   示例代码         round   四舍五入, 第二个参数是要保留的小数位数   round(3. 1415926, 2) 第二个参数可以zhi’ding 保留几位小数, 不指定的化就只保留整数       range   返回一个序列, 三个参数分别是 start, stop, step           abs   绝对值   abs(-100)       bool   将内容转换成 bool 类型的值. 参数非零为真,零为假   bool(100)       max   取序列中的最大值, 参数可以是 str, list, tuple, set, dict. 字符串的话, 返回字符串中最大的 ascii 对应的字符   max(‘李大龙’)       min   取系列中的最小值           hex   将十进制转化为十六进制   hex(100)       oct   将十进制转化为八进制   oct(100)       bin   将十进制转化为二进制   bin(100)       sum   求和, 参数需要是一个数字列表   sum([1,2,3,4])       pow   求次方(幂)   pow(2,5)       divmod   求商和余数, 给两个数字, 返回商和余数的元组 (//, %)   divmod(33,2)   Python 模块函数:       函数名   说明   示例代码         random   import random           random. random()   返回一个随机数, 范围是0-1           random. randint(start, stop)   返回start - stop 之间的一个随机数           random. uniform(a,b)   返回 a-b之间的一个浮点数           random. choice(string)   在字符串string / 列表中随机抽取一个字符           random. shuffle(list)   将列表本身的顺序打乱           random. sample(list, num)   从列表中随机抽取 num 个元素, 返回一个新的列表中,并返回该列表           time   import time           time. sleep(1)   代码暂停1秒           math   import math math 所有内置函数 官方 math函数 说明           math. e               math. pi   获取圆周率           math. ceil()   向上取整           math. floor()   向下取整           math. degrees   给一个弧度,转化成度           math. radians()   给一个度,转化成弧度           sys   import sys 系统信息           sys. argv   文件被执行时收到的参数被放在 argv 参数中, 是一个列表, 列表的第一个元素是程序的名字,后面的都是参数   sys. argv       sys. cls   清屏       字符串函数:       函数名   说明   示例代码         join()   将给出的字符串或者列表按指定要求进行拼接. 如果拼接的是列表,那么列表中的 int 型必须先转化成字符串    - . join( Iloveyou )       ljust() / rjust() / center()   讲过指定字符填充到字符串指定宽度右侧, 第一个参数是宽度,第二个参数是要填充的字符    黄小明 . ljust(10,'-')       zfill()   给数字左边补充0到指定宽度           strip() / lstrip() / rstrip()   去首尾空白 , 空白包括 空格, \t, \n 等. 如果 增加参数,可去掉指定字符    ***** 黄晓明 ----- . strip( * - ) 可以一起去掉*, 空格, -       replace( oldstr, newstr, num)   将字符串中的指定内容替换为新指定的字符, 默认全部替换 , 可以替换前 num 个    ***** 黄晓明 ----- . replace( 黄晓明 , 黄渤 )       split()   将字符串按字符串中出现的指定字符进行分割, 默认是按空格切割. 和 join 正好相反, 返回一个 list    黄*晓*明 . split( * )       splitlines()   按照换行来切割, 和 split(‘\n’) 一个效果           find() / rfind()   在字符串中查找指定字符串 , 返回找到的第一个匹配到的字符串的下标 key, 找不到会返回 -1 ( 在用 find做逻辑判断时, 需要写成 if  ab-cd . find('-') == -1)           upper() / lower() / capitalize() /title() / swapcase()   将字符串全部大写/ 全部小写 / 字符串首字母大写 /把每个单词的首字母大写 / 将字符串的大小写互换           len()   求字符串长度           count()   查看一个字符串在另一个字符串中出现的次数           startswith() / endswith()   是否是以某字符串开头/结尾           isalnum() / isalpha() …   字符串中是否只包含字符和数字       列表函数:       函数名   说明   示例代码         append()   在列表中追加一个新的元素(是按整体追加), 新元素可以是任意类型的数据           extend()   和 append 类似, 但是是把追加的内容拆分成更小的元素进行追加   a = [1,2,3]; a. extend('abc')       index()   返回指定元素在列表中的位置, 找到返回 index, 找不到报错           insert(index, obj)   在指定索引位置插入新元素           pop()   默认弹出列表中的最后一个元素, 如果指定 index, 弹出指定 index 的元素           remove()   删除指定元素值的元素, 如果有重复值在列表中, 删除最前面的第一个元素           clear()   清空整个列表           reverse()   翻转列表元素位置           copy()   拷贝一个列表, 这里的 copy 是复制一个新的副本, 两个版本之间不互相影响           sort()   按元素的 value 进行排序, 默认是从小到大排, 使用参数 reverse=True 从大到小排.  如果排序内容是单词, 按照单词的首字母排序. abcd &gt; abc       字典函数:       函数名   说明   示例代码         items()   返回字典的列表元组模式           keys()   返回所有的 keys, 列表模式返回           values()   返回所有的 值, 列表返回           pop(key)   根据 key 返回对应的 value           d1. update(d2)   将d2 的键值对更新到 d1中           copy()   拷贝, 这里的 copy 是复制一个新的副本, 两个版本之间不互相影响           popitems()   弹出最后一个键值对       update 与 ChainMap 操作字典合并update 和 collections 的 ChainMap 都可以和并字典update 直接将b 中的元素更新到 a 中, 同时a 发生变化. 但是 a中之后又发生了变化,那么d 是不会变化的. ChianMap 的好处是它并不创建一个新的字典,而是保持 a和b, 如果a,b,发生变化, ChainMap 也会随之变化 1234567from collections import ChainMapa={'x':1, 'z':3}b={'y':2, 'z':4}c = ChainMap(a,b)d = dict(a)d. update(b)set 集合的函数:       函数名   说明   示例代码         add()   向集合中添加新元素           clear()   清空集合           pop()   随机弹出集合中的一个元素 (因为集合是无序的); 如果集合的元素是数字组合,默认从 前往后弹, 如果内容是字符串, 随机弹           remove()   按照指定 value 删除集合的元素           discard()   和删除功能一样, 只是如果删除不存在的值时,不会报错           issubset()/ issuperset()   是否是子集, 是否是父集           isdisjoint()   是否无交集       可迭代对象/迭代器:  __iter__ 返回迭代器自身 __next__返回容器中的下一个值 如果容器中没有更多的元素, 赞抛出 StopIteration 异常. 123456789101112131415# 自定义一个迭代器函数class Range:  def __init__(self, start, end):    self. start=start    self. end = end  def __iter__(self):    return self  def __next__(self):    if self. start &lt; self. end:      current = self. start      self. start+1      return current    else:      raise StopIteration()用迭代器生成一个斐波那契额数列 1234567891011121314151617181920212223242526class Fib:  def __init__(self, start, step):    self. step = step    self. ls = [0,start]  def __iter__(self):    return self  def __next__(self):    if self. step &gt; 0:      self. step -= 1      x = self. ls[-1] + self. ls[-2]      self. ls. append(x)      return x    else:      raise StopIteration()a = Fib(1,10)print(next(a))print(next(a))print(next(a))print(next(a))print(next(a))print(next(a))print(next(a))print(next(a))print(next(a))print(next(a))生成器: 用生成器实现斐波那契数列 1234567891011def fib(max_value):  ls =[0,1]  while max_value &gt;= ls[-1]:    ls. append(ls[-1]+ls[-2])    yield ls[-1]x = fib(100)print(next(x))print(next(x))print(next(x))print(next(x))程序调试:  打印调试: 多利用 print(), break, exit() 来中断程序 pycharm IDE调试 pdb 调试 linux命令行下的一款 debug 调试工具  python -m pdb 1. py     l : 查看执行处的代码   n : 看下一步代码执行的位置   p a : 用 print 看变量 a 的值   b : 直接输入b 看所有已经设置的断点   b 42: b 42 是在42行设置断点 , b 和 r 一起用   r : 直接跑到下一个断点处   s : step into 就是进入函数内   a : 可以查看函数能接收到的形参的值 s 和 a 一起用   cl 1: 清除断点1   quit : 退出调试   递归函数:  反复调用自己的函数叫做递归函数 递归有时候会很方便的实现业务逻辑,比如递归的对嵌套文件夹进行操作 递归比较耗内存, 超过100层就会比较慢了 写递归必须要有终止条件 todo: 汉诺塔的递归实现 案例场景: 递归函数求阶乘, 递归函数翻转字符串, 汉诺塔 技巧:     对字符串和列表类型进行处理的递归函数, 最好的方法就是list 作为参数进来, str 作为 返回出去 .    如果是对数字的一些计算然后出序列,   递归打印 N遍某字符串 1234567def test(n):  if n == 1:    print( 我喜欢高圆圆  ,n)    return  test(n-1)  print('我喜欢高圆圆 last ',n)test(4)输出如下  我喜欢高圆圆 1我喜欢高圆圆 last  2我喜欢高圆圆 last  3我喜欢高圆圆 last  4 *计算 n 的阶乘 n * (n-1) ** 传统方法 123456789# 阶乘def factorial(n):  if n &lt; 1:    exit()  result = 1  for i in range(1,n+1):    result *= i  return resultprint(factorial(5))递归方法 1234567891011121314# 阶乘 递归def jiecheng(n):  if n == 1:       #1    return 1  return n * jiecheng(n-1)     #2   #3print(jiecheng(5))#技巧: 先找到可以公式化的表达式 (n*(n-1)) 的循环, 直到 n=1 退出#1 创建退出条件#2 创建表达式, 包含函数名自身#3 返回最终结果123456789# 翻转字符串def convertStr(x):  if x == -1:    return ''  else:    return string[x] + convertStr(x - 1)string = '请输入若干字符'print(convertStr(len(string) - 1))递归选择排序和冒泡排序 1234567891011121314151617181920212223242526272829#递归选择排序def myOrderXuanze(lt:list)-&gt;str:  count = len(lt)  if count &lt; 2:    return lt[0]  for i in range(0,count-1):     #1 容易忘记 count 需要减1    # print (lt ,     , lt[i] ,     ,lt[i+1])    if lt[0] &lt; lt[i+1]:      lt[0],lt[i+1] = lt[i+1],lt[0]  first = lt. pop(0)  return first + myOrderXuanze(lt)l = ['f','b','a','c']print(myOrderXuanze(l))#递归冒泡排序def myOrderMaopao(lt:list)-&gt;str:  count = len(lt)  if count &lt; 2:    return lt[-1]  for i in range(0,count-1):    if lt[i] &gt; lt[i+1]:      lt[i] , lt[i + 1] = lt[i+1] , lt[i]  last = lt. pop(-1)  return myOrderMaopao(lt) + lastl = ['c','a','d','b','e']print (myOrderMaopao(l))用列表代替递归遍历目录中的所有文件 12345678910# 遍历一个目录, 输出所有的文件(包含子目录的文件)import ospathList = ['. /']while pathList:  thePath = pathList. pop()  files = [name for name in os. listdir(thePath) if os. path. isfile(os. path. join(thePath, name))]  print(files)  dirs = [os. path. join(thePath,name) for name in os. listdir(thePath) if os. path. isdir(os. path. join(thePath, name))]  pathList. extend(dirs)匿名函数 / 高阶函数 lambda 一行函数:  高阶函数就是可以使用函数作为参数进行传递的函数. 匿名函数 lambda的使用 函数也是一个对象 将一个函数名当做参数传递给另一个函数时可以用到匿名函数 常用的场景: 做一个匿名函数的小功能, 然后将匿名函数和匿名函数需要的其他参数一起传给一个工厂函数 匿名函数以 lambda 开头, lambda x,y: x+y     后面跟的是匿名函数的参数,   可以有多个参数. 参数后面跟冒号.    冒号后面跟表达式即可.    注意不要写 return,   表达式就是这个匿名函数的返回值   lambda 的表达式只能是一句话   用法: 将匿名函数赋值给一个变量, 直接去使用变量即可 .    1234567891011def add(a,b):  return a+bprint(add) #1a = addprint(a)  #2 print(add(4,5)) # 输出9print(a(4,5))	 # 输出9#1 &lt;function add at 0x101c62e18&gt; 不带括号的函数名本身也是 python 对象#2 &lt;function add at 0x101c62e18&gt; a 和 add 指向相同的内存地址实现一个计算器 12345678910111213def add(a,b):  return a+bdef sub(a,b):  return a-bdef mul(a,b):  return a*bdef div(a,b):  return a/bdef cal(a,b,fn):  # 这里的 fn 可以是 一个预定义的函数, 也可以是 lambda  return fn(a,b)print(cal(3,5,add))一个匿名函数和对应的普通函数 123456lala = lambda x,y: x+y # 定义了一个lala 的函数 , 匿名函数不写 returnprint(lala(3,5))def mylala(x, y): #   return x + yprint(mylala(3,5))直接把匿名函数作为参数直接写在实参中. 123def cal(a,b,fn):  return fn(a,b)print(cal(3,5, lambda x,y: x%y))  lambda 在 列表的 sort() 中的使用 1234567891011lt = [  {'name': 'lucy', 'age':18, 'height': 166},  {'name': 'zoe', 'age':16, 'height': 162},  {'name': 'rose', 'age':19, 'height': 176},  {'name': 'marry', 'age':28, 'height': 178}]lt. sort(key=lambda x: x['age'], reverse=1) #1print(lt)#1 按年龄排序降序, 这里的 x 代表的就是 lt 中的每个字典元素, x['age'] 代表返回给 key的 valuelambda 在 max() 中的使用 123456789l=[3,2,100,999,213,1111,31121,333]print(max(l))                #1dic={'k1':10,'k2':100,'k3':30}print(max(dic))               #2print(max(dic,key=lambda k:dic[k]))     #3#1: 列表的最大值#2: 字典最大值默认取的是 key 中的最大值 k3#3: 但是给 max 的 key设置匿名函数, 就会返回最大 value 对应的 key. 输入的是 k, 返回的是 k 对应的 valuenlargest() 和 nsmallest() 与 lambda  heapq 模块有两个函数:nlargest() 和 nsmallest() 可以获取一个列表中的 N个最大值,或者 N个最小值 123456import heapqportfolio = [{'name': 'IBM', 'shares': 100, 'price': 91. 1}, {'name': 'AAPL', 'shares': 50, 'price': 543. 22}, {'name': 'FB', 'shares': 200, 'price': 21. 09}, {'name': 'HPQ', 'shares': 35, 'price': 31. 75}, {'name': 'YHOO', 'shares': 45, 'price': 16. 35}, {'name': 'ACME', 'shares': 75, 'price': 115. 65}]cheap = heapq. nsmallest(3, portfolio, key=lambda s: s['price']) expensive = heapq. nlargest(3, portfolio, key=lambda s: s['price'])lambda 在 map() 中的使用    map() 会根据提供的函数对指定列表做遍历任意操作。  map() 本质是一个生成器  所以map() 是一个专门对已有列表进行操作的函数  第一个参数 function 以参数序列中的每一个元素调用 function 函数，返回包含每次 function 函数返回值的新列表。  第二个参数是一个可迭代对象, 一般都是用列表 1234567891011ge = map(str, [1,2,3,4]) # 将列表中的数字转成字符串. 这里的参数 str 是内置函数, 不需要写 str()ge = map(len, ['dalong', 'hello', 'hallo']) # 产生一个新列表, 对应原 list 每个元素的长度. l = map(lambda x: x ** 2, [1, 2, 3, 4, 5]) #1 list(l) # 返回 [1, 4, 9, 16, 25]m = map(lambda x, y: x + y, [1, 3, 5, 7, 9], [2, 4, 6, 8, 10])list(m) # 返回 [3, 7, 11, 15, 19]#1 使用 lambda 匿名函数, x 代表得个参数(列表)中的每个元素lambda 在 filter() 中的使用  filter() 函数用于过滤序列，过滤掉不符合条件的元素，返回由符合条件元素组成的新列表。filter() 本质是个生成器该接收两个参数，第一个为函数，第二个为序列，序列的每个元素作为参数传递给函数进行判，然后返回 True 或 False，最后将返回 True 的元素放到新列表中。 12345res = filter(lambda x: x%2 ==0, [1,2,3,4,5,6,7,8,9,10]) # 获取1~10的偶数res = filter(lambda x:x&gt;10,[5,8,11,9,15])for i in res:  print(i)reduce() 和 lambda 的使用  reduce 是 两两按照 fn 进行操作, 然后将结果再和后面的元素一一进行同样的操作 fn(fn(0,1),2)…. . 12345from functools import reduceret = reduce(lambda x,y: x+y , [1,2,3,4]) # ((1+2)+3 ) +4print(ret) # 得到 10ret = reduce(lambda x,y: x*y+y , [1,2,3,4]) # (((1*2+2)*3+3)*4+4)用 filter() 和 map() 快速过滤掉空格和 None, ‘’ 12345lt = ['abc ',' def ', '', None, ' lalala '][x. strip() for x in lt if x]list(map(lambda x:x. strip(),(filter(lambda x:x , lt))))练习  现有两元组((‘a’),(‘b’)),((‘c’),(‘d’)),请使用python中匿名函数生成列表[{‘a’:’c’},{‘b’:’d’}]12l = lambda tp1,tp2: [{x,y} for x in tp1 for y in tp2 ][::3]print(l((('a'),('b')),(('c'),('d'))))闭包:  lambda (匿名函数) 玩的是参数 闭包玩的是返回值 , 直接return 一个函数 制造闭包的函数( outer ) 可以想象成一个工厂, 而 这个工厂将返回一个函数和这个函数用过到的 outer 里面的变量 ( outer 中的变量可以包含调用 outer 时传递进去的参数) 如果定义了一个内部函数, 并且内部函数使用了外部函数的变量, 且外部函数将内部函数作为返回值返回了,那么这就是一个闭包 每次函数outer被调用的时候，函数inner都会被重新定义，如果它不被当做变量返回的话，每次执行过后它将不复存在。 闭包的好处是: 正常情况下,一个函数被使用完, 这个函数内的内部变量and 内部函数将被销毁; 但是使用闭包可以保存内部函数和外部函数的变量12345678def hello():  def world():    print('这是 world 函数')  return worlda = hello()print(a) # 显示这是一个对象print(a()) # 执行 world() 函数 创建一个闭包 12345678def outer():  x = 1 # 正常情况下, outer 在被执行完后会被自动销毁  def inner():     print(x) # 1  return innerfoo = outer()foo() # 一旦内部函数被作为闭包返回, 那么 outer 中的 x 就不会被销毁了. 这是因为闭包可以记住它所对应的命名空间所用到的所有变量print(foo. __closure__[0]. cell_contents) # 查看闭包中用到的外部变量(可以用 index 来获取多个变量)为 outer 传递一个参数 12345678def outer(x):  def inner():    print(x) # 1  return innerprint1 = outer(1)print2 = outer(2)print1()print2()123456789101112def mySum(*args):  # 将所有传递的参数求和  def inner():    sum = 0    for x in args:      sum += x    return sum  return innermysum = mySum(100,200,300)print(mysum)print(mysum())小例子: 通过闭包生成很多不同的二元一次方程 12345678910111213#通过闭包生成很多不同的二元一次方程def linearQuation(a,b):  def inner(x):    return a*x+b  return innerfunc1 = linearQuation(3,2) # 就是3x+2func2 = linearQuation(5,3) # 就是5x+3print(func1(1))   # 3*1+2print(func2(2))   # 5*2+3print(linearQuation(3,4)(5)) # 3*5+4装饰器: 12步轻松搞定装饰器浅谈Python装饰器装饰器更多练习  装饰器其实就是一个闭包，把一个函数当做参数然后返回一个替代版函数 装饰器在不修改原来函数的情况下对函数进行拓展 装饰器的函数名不能和其他变量同名,否则会相互覆盖 装饰器比较有用场景: 如果我们碰到一个功能有限的函数,但又不能直接修改, 我们就需要用装饰器来帮助原来的函数实现一些我们想要的功能 变量名和装饰器名不能同名,否则变量名会覆盖装饰器名     写一个装饰器,其实就是写一个函数, 该函数有一个参数是函数名, 返回值是也是函数名(闭包)   被装饰的函数在定义的行的上面加上 @ 符号来指定用哪个装饰器 , 本质等于 foo = bar(foo)   调用装饰函数时 foo = bar(foo), 就会从装饰器返回一个被装饰的闭包.     @将装饰器应用在函数上，只需要在函数的定义前加上@和装饰器的名称。 @符号是装饰器的语法糖，在定义函数的时候使用，避免再一次赋值操作 @foo 只是一个语法糖，其本质是 foo = bar(foo) @ 符号可以理解为把下面那个函数放到@指定的发廊中去做个美发 TODO : 用两个装饰器装饰1234567891011121314151617181920#简单的装饰器def outer(some_func):  def inner():    print( before some_func )    ret = some_func() # 1    return ret + 1  return innerdef foo():  return 1decorated = outer(foo) # 2print(decorated())#1 处调用了原始的 函数,并取得返回值, 后续进行加工#2 通过装饰器生成了 decorated 函数,并对 新生成的函数进行打印执行. 我们可以认为 decorated 是 foo 的一个装饰版本or 加强版本 . 可以用@ 代替直接声明 outer 函数 #2 可以写成 foo = outer(foo) , 理解成某人把自己放到发廊中改造型, 出来后还是自己,只是发型变帅了 '''输出 : before some_func2'''发廊的例子( 打印版 ) 123456789101112def falang(fn):  def meifa(*args, **kvargs):    print( 刘海减掉 )    fn(*args, **kvargs)    print( 两边剃秃 )   return meifa@falangdef dalong(type):  print(type)dalong('西瓜头')发廊的例子( return 版) 123456789101112131415def falang(fn):  def meifa(*args, **kvargs):    hirCute =  刘海减掉     hirCute += fn(*args, **kvargs)    hirCute +=  两边剃秃     return fn(hirCute)    # 1  return meifa@falangdef dalong(type):  return typeprint (dalong('西瓜头'))#1 这里 return fn(hirCute) 是为了把 fn 原来的参数返回出来.  参数可以写成*args, **kvargs, 但是如果对参数进行了操作, 那么就要返回新的参数变量了. 一个能够记录下传递给函数参数的装饰器, 简单地把日志输出到界面的例子 1234567891011121314151617181920212223# 一个能够记录下传递给函数参数的装饰器, 简单地把日志输出到界面的例子def logger(func):  def inner(*args, **kwargs): # 1    print( Arguments were: %s, %s  % (args, kwargs))    return func(*args, **kwargs) # 2  return inner@logger #1def foo1(x, y=1):  return x * y@loggerdef foo2():  return 2#foo1 = logger(fool(5,4))  #2foo1(5, 4) # Arguments were: (5, 4), {}foo1(1)   # Arguments were: (1,), {}foo2()   # Arguments were: (), {}#1 @将装饰器应用在函数上，只需要在函数的定义前加上@和装饰器的名称。@符号是装饰器的语法糖，在定义函数的时候使用，避免再一次赋值操作 (可以省去 foo1 = logger(fool(5,4)) 这行)#2 如果不在#1处写 @logger, 那么就需要在#2处加上这一行赋值带参数的双层装饰器 123456789101112131415161718192021222324252627import timedef function_performance_statistics(trace_this=True):  if trace_this:    def performace_statistics_delegate(func):      def counter(*args, **kwargs):        start = time. clock()        func(*args, **kwargs)        end =time. clock()        print ('used time: %d' % (end - start, ))      return counter  else:    def performace_statistics_delegate(func):      return func  return performace_statistics_delegate@function_performance_statistics(True)def add(x, y):  time. sleep(3)  print ('add result: %d' % (x + y,))@function_performance_statistics(False)def mul(x, y=1):  print ('mul result: %d' % (x * y,))add(1, 1)mul(10)生成器: 介绍  快速生成可迭代的生成东西的容器 普通的列表生成式 l = [x for x in range(1,101)] 生成式: 生成了可迭代的元素集合,但是不能通过 next() 进行迭代 但是这样的生成式会占用内存, 生成器是按需供给数据, 解决了瞬间占用过大内存的问题.  生成器就是通过预先写好的算法, 快速批量生成数据 生成器第一种方法: 把列表生成式的中括号改成小括号 就是生成器 生成器第二种方法: 通过函数和yield关键字生成 next(l) 可以操作生成器, 每调用一次,返回一个 迭代元素 , 返回最后一个元素后会报错 程序每次执行到 yield 都会返回 i 然后暂定,直到 next() 调用时,才再返回下一个 i list(l) 可以将 一个生成器转化成列表 生成器可以直接放在 for in 中直接被用于循环, 不用 i 进行循环, 循环结束后自动停止 生成器每次被调用都会生成一个新的生成器 一个实例化的生成器只能被遍历一次123456# 第一种方法l = (x for x in range(1,101))print(next(l))list(l)for i in l:	print(i)1234567891011121314151617181920212223242526272829303132#第二种方法def l(n):		#普通的列表生成函数  l = []  for i in range(1,n+1):    l. append(i)  return ldef g(n):		#生成器函数  for i in range(1,n+1):    yield i  #1 print(l(5))		# 这里是一个列表print(g(5))		# 这里是一个生成器myg = g(5)		# 实例化一个生成器对象print(next(g(5))) #2 print(next(g(5)))print(next(g(5)))print(next(myg))  #3 print(next(myg))print(next(myg))print(next(myg))print(next(myg))print(next(myg)) #4 #1程序每次执行到 yield 都会返回 i 然后暂停,直到 next() 调用时,才再返回下一个 i #2每次打印的都是一个新的生成器的下一个元素#3每次打印的是 实例化对象myg 的下一个元素#4当第六次调用 next(myg)时,因为没有元素可以返回了,所以会报 StopIteration 错斐波那契额数列的生成器实现 123456789101112131415161718192021222324252627def myfeibo(n):		# 我自己写的斐波那契数列实现  l = [0,1]  for i in range(1,n):    x = l[i-1]+l[i]    l. append(x)  l. pop(0)  return ldef feibo(n):		#老师的实现  l = []  a,b = 0,1  for i in range(1,n+1):    l. append(b)    a, b = b, a + b  return ldef feiboG(n):		#改成斐波那契生成器  a,b = 0,1  for i in range(1,n+1):    a, b = b, a + b  yield bprint(feibo(6))print(myfeibo(6))print(feiboG(6))迭代器 / 可迭代对象: 介绍  迭代器 (iterator): 指可以用 for in 进行遍历,并可以用 next() 获取下一个元素的对象是迭代器 迭代器可以理解为: 通过生成器生成出来的对象是迭代器 迭代器的创建方法      通过生成器实例化的对象是迭代器         通过 iter() 函数将可迭代对象转化成迭代器 , 比如讲列表, tuple, set 等    生成器是一种特殊迭代器 判断一个对象是否是迭代器 from collections import Iterator ; print(isinstance(lt,Iterator)) 迭代对象(iterable) : 能用 for in , 但不能用 next 进行遍历的对象是迭代对象 list, tuple, set, dict, string 都不是迭代器, 只是迭代对象 判断一个对象是否是迭代对象 from collections import Iterable ; print(isinstance(lt,Iterable)) 通过 lt1 = iter(lt) 函数可以将迭代对象转化成迭代器1234567891011121314from collections import Iteratorfrom collections import Iterablelt = [x for x in range(10)]lt1 = (x for x in range(10))print(isinstance(lt,Iterator))print(isinstance(lt1,Iterator))print(isinstance(lt,Iterable))print(isinstance(lt1,Iterable))lt2 = iter(lt)next(lt2)循环输出一个迭代器 / 生成器通过捕获 StopIteration 异常来结束循环 12345678910lst = range(5)it = iter(lst)try:  while True:    val = next(it)    print(val)except StopIteration: #1   pass#1 逐条输出所有迭代器返回的元素, 直到报 StopIteration 错, 退出循环通过读取 None 来结束循环 1234567lst = range(5)it = iter(lst)while True:  val = next(it, None)  if val is None:    break  print(val)排序算法: 选择排序将需要排序的元素中最左的元素依次和它右面的每个元素进行比较, 如果满足条件(a&gt;b or a&lt;b)就交换位置, 然后在用左边第二个进行同样的比较, 直到所有元素比较完毕, 这就是选择排序法. 1234567891011def order_xuanze(*args):  args = list(args)  count = len(args)  for i in range(count-1):    for j in range(i+1, count): #1      if args[i] &lt; args[j]:        args[i], args[j] = args[j], args[i]  return list(args)print(order_xuanze(2,1,3,4,5,6))#1 关键是要想到 i+1 和 count 冒泡排序一组元素, 下标0和1的元素比较, 大的交换到右边, 然后下标1和2的元素比较, 大的交换到右边, 直到所有元素比较完毕 12345678910111213# 冒泡排序法def order_maopao(*args):  args = list(args)  count = len(args)  for i in range(count-1):    for j in range(0, count - 1 - i):      if args[j] &gt; args[j+1]:        args[j] , args[j+1] = args[j+1] , args[j]  return argsprint(order_maopao(1,3,2,4,6,5))自己实现的 sort() 函数 , 可以处理把函数作为参数传递给 Key 12345678910111213141516171819202122# 自己写一个 sortdef mysort(lt, key=None, reverse=False):  args = lt  count = len(args)  for i in range(count-1):    for j in range(i+1, count):      if key == None:        operator = (args[i] &lt; args[j]) if reverse else (args[i] &gt; args[j])      else:        operator = (key(args[i]) &lt; key(args[j])) if reverse else (key(args[i]) &gt; key(args[j]))      if operator:        args[i], args[j] = args[j], args[i]  return list(args)# lt = [2,3,6,4,3,5]lt = [  {'name':'王晓梅','age':17,'height':165},  {'name':'郭志摩','age':13,'height':153},  {'name':'四方宇','age':16,'height':184},  {'name':'梅花鹿','age':14,'height':134}]print(mysort(lt, reverse=True, key= lambda x: x['age']))模块 Mudule:    模块就相当于工具包, 想使用需要先导入 import  如果导入两个模块中有同名的函数相互覆盖, 后面的会覆盖前面的  如果想在模块文件中(被引用前)写测试代码, 一定要先写 判断语句 if __name__ = '__main__' 以防止测试代码在导入模块的的文件中被错误执行  当模块当做主程序被调用时, name 就是 main  当模块被其他程序调用时, name 就是 模块分为三种:  python 标准库模块 : 官方提供的       random, time, os 等       第三方库模块 : 别人写的,你要安装       pygame 等, 需要用 pip (第三方包管理工具) 进行安装       自定义模块 查看模块 / 函数 的使用帮助 123&gt;&gt;&gt;import module&gt;&gt;&gt;help(module)&gt;&gt;&gt;help(fn1)模块制作 和 模块导入:  import 关键字要写在文件的开头 将定义好的一类函数放在一个文件中,起一个 . py 结尾的文件名, 这个文件名就可以作为一个模块. 被 import a 文件 import time, b 文件 import a, b文件不能直接用 time 中的函数.  格式如下:12import &lt;文件名&gt;  // 文件名就是模块名&lt;文件名&gt;. 函数名()创建一个文件 momo. py 123456789101112131415#momo. pydef hello():  print('hello')def world():  print( world )       if __name__ == '__main__': #1 	print(__name__)     #2   hello()         #3 测试代码块#1 判断这行代码对应的文件目前是源文件还是被导入的文件, 如果是自己,显示'__main__', 如果是被引入的, 会显示'&lt;模块名&gt;' #1 __name__ 这个是一个内置变量， 如果直接执行. py 文件， 这个__name__ 的值就是‘__main__’ . 如果当前文件是通过 module 调用， __name__ 就是模块名。#1 if __name__ == '__main__': 这段用来方便开发人员在文件内进行 debug 调试#2 会打印 __main__在另一个文件中调用自定义模块 123#调用 momoimport momomomo. hello() from 也可以从一个模块中指定要导入的函数, 如果要导入多个指定的函数, 可以用逗号分隔,12from module_name import fun1, fun2, fun3fun1()   # 就可以直接使用函数名了,给函数/ 模块 起别名:  as 如果要导入的函数可能会和自己定义的函数重名或者函数名太长, 可以用 as 关键字给函数起别名 , 在代码中只能使用别名,不能使用原名了.  as 也可以给模块起别名 , 在代码中只能使用别名,不能使用原名了. 123456from module_name import function1 as f1f1()import ninenine as nn. nine()模糊导入  导入模块中所有的函数 用 * 关键字 这种方法可以直接使用模块中的函数, 不需要写模块名.  但这种方式并不推荐, 因为会容易发生导入的 函数和自己定义的函数重名造成函数重载的问题 , 所以建议只导入自己需要用的函数. 1from module_name import *12from ramdom import *randint(1,10)__all__ 的用法 1234567891011121314import time__all__=['doudou']  # 这里会限制 from momo import * 只能使用 doudou 函数LALA =  ABC def hello():  print('hello')def world():  print( world )def doudou():  print('老夫聊发少年狂')print(__name__)       if __name__ == '__main__':   hello()         另一个文件 12import momomomo. hello()__pycache__ 文件夹当第一次导入一个模块时, 系统会在当前 python 文件所在文件夹中创建 一个缓存文件夹, 里面存放的是缓存文件字节码, 下次执行该文件时,如果模块没有改变, 那么文件会直接使用缓存文件 包 / 子包 Package:  多个模块功能类似, 那么我们将这些模块放在同一个目录下, 方便管理, 这个目录就称为包包也可以嵌套, 用法就是import bao. subbao包里面一般都有一个文件叫做 __init__. py, 这个额文件必须有, 一般情况下这个文件的内容是空的, 只是告诉编译器这个目录是一个包, 而不是普通的目录 12345from bao. subbao. subhello import subhello_test #1subhello_test()#1 这里 bao, subbao 是目录, subhello 是模块名(也就是文件名) , subhello_test 是模块中的函数Python 标准库: 官方标准类库3. 7版       库名   说明         time   时间库 import time       time. sleep()   休眠几秒       time. time()   unix 时间戳. 转化成1970年1月1日0时0分0秒到现在的秒       time. strftime('%Y-%m-%d %H:%M:%S')   格式化时间: %y 两位年 %Y 四位年 %m 月 %d 日(1-31) %H 24时 %I 12时 %M 分 %S 秒 %w 周几 %w 当年的第几周       time. mktime((2017,11,16,15,18,00,3,321,0))   根据元组序列生成 unix 时间戳, 元组元素个数不能少       time. gmtime(1510816680. 0)   根据时间戳生成时间元组, 默认返回格林威治(格林尼治)时间.        time. localtime(1510816680. 0)   生成本地时间, 北京时间       time. asctime((2017,11,16,15,18,00,3,321,0))   根据时间元组返回 人类可读时间       time. timezone   0时区减去当前时区的秒数. 一个小时是3600 秒, 我们应该返回 -28800 正好是 8 小时       datetime   日期时间 import datetime       dt = datetime. datetime. now()   获取当前时间的对象       dt. date()   当前的日期       dt. strftime('%Y-%m-%d %H:%M:%S')   当前的       dt. timestamp()   获取日期对象时间戳       dt. time()   当前时间       datetime. datetime. fromtimestamp(1510816680. 0)   根据时间戳获取日期和时间       datetime. timedelta(days=10)   创建一个时间段, 比如10天 . days = -10 十天前的日期. hours = 10 十小时后的时间. minutes, seconds       calendar   导入日期 import calendar       calendar. calendar(2017, w=2,l=1, c=6)   打印日历       calendar. isleap(year)   闰年       calendar. leapdays(y1, y2)   y1 ~ y2 闰年个数       calendar. month(year, month, w=2, l=1)   打印指定年指定月的日历       hashlib   加密库 import hashlib       hash = hashlib. md5() ;hash. update('12345'. encode('utf-8')) ; tmp = hash. hexdigest()   生成12345 的 md5       turtle   海龟制图 import turtle       turtle. screensize(100,80)   获取屏幕尺寸       turtle. done   画图       turtle. pensize(width)   画笔粗细       turtle. color(‘red’)   画笔颜色 , 可以用 rgp 画, 也可以用英文单词       turtle. forward(200)   向前画 200个像素       turtle. backward(200)   向后画       turtle. right(角度)   画笔向右拐多少度       turtle. goto(200,200)   到指定坐标点       turtle. setx(100)   将坐标x的原点设置到 100 的位置       turtle. circle(100)   画半径为100的圆       turtle. dot(100,’red’)   画一个直径为100的点       turtle. clear()   清除画, 画笔保留原来位置   对日期进行计算  计算某个日期指定 n 天是几月几日 计算两个日期中间差了多少天  1234567from datetime import timedeltafrom datetime import datetimea = datetime(2017, 10, 1)print(a + timedelta(days=10))b = datetime(2017, 11, 19)c = b - aprint(c)   Map reduce: python 之 map reduce ** 将’12345’ 互转 12345 , 实现类似 int(), str() 的函数 ** 1234567891011121314151617181920212223242526# 思路 '1' 的 ascii 值是 48, 减去 48就是整型 1from functools import reducedef my_int(string):  lt = map(lambda x: ord(x) - 48, string)  return reduce(lambda x,y: x*10 + y, lt)if __name__ == '__main__':  print(my_int('12345') + 5)r =reduce(lambda x,y: x*10 + y,map(lambda x:ord(x)-48,'12345')) #1print(r)def my_string(number):  lt = []  while number:    number, yushu = divmod(number,10)    lt. append(yushu)  lt. reverse()  # print(lt)  ge = map(lambda x: chr(x + 48), lt)  return ''. join(ge)if __name__ == '__main__':  print(my_string(12345) + 'abc')#1 更简单的方式就是用一行来完成 , 其中 m = map(lambda x:ord(x)-48,'12345') 这段返回的是一个 map 的序列对象, 可以用 print(list(m)) 来转换成数组看利用map和reduce编写一个str2float函数，把字符串’123. 456’转换成浮点数123. 456： 12345678910111213141516171819202122232425262728# 方法一s = '12345. 456789'zheng, xiao = s. split('. ')l_zheng = map(lambda x:ord(x)-48,zheng)l_xiao = map(lambda x:ord(x)-48,xiao)firstpart = reduce(lambda x,y : x*10+y , l_zheng)i = 1lastpart = 0for x in list(l_xiao):  lastpart += x*pow(0. 1,i)  i+=1result = firstpart+lastpartprint(type(result))print(result)#方法二s = '12345. 456789'def str2num(s):  return reduce(lambda x, y: x * 10 + y, map(lambda x: ord(x) - 48, s))def str2float(s):  num = s. split('. ')  return reduce(lambda x, y: x * 10 + y, map(str2num, num[0])) + reduce(lambda x, y: x * 10 + y, map(str2num, num[1]))/(10 ** len(num[1]))print(str2float(s))编码规范: 编码规范 isort: 可以通过 isort 对一个python 文件的多行 import 进行优化排序 12pip3 install isortisort abc. pyflake8: 可以帮我们检查代码，是否按照 pep8 规范进行书写，并会返回不符合的代码行并指出问题所在，需要我们手动修改 12pip3 install flake8flake8 abc. pyblack: black 可以帮我们把一些不好看不规范的代码变得更好看，比如超长的 list 按照每行一条来书写。 把单引号双引号混合的书写统一成双引号等等。black 直接完成修改 12pip3 install blackblack abc. py内存管理: 内存: 操作系统启动后, 所有的程序都是在内存中运行的. 16G	手机: 6g, 8g, 3g, 1g, ios内存地址: 指针: 存放指向某个内存地址的变量python 中用到的所有变量, 都是指针 , 包括元组, 字典, 字符串. 内存地址一般是用16进制表示 12345678a , b = 100, 200print(hex(id(a)), hex(id(b))) # 输出 0x1009e9780 0x1009ea400c = d = 100print(hex(id(c)), hex(id(d))) # 0x1009e9780 0x1009e9780lt = [100,200,300]# 先创建三个内存地址分配给100, 200, 300. 然后在创建一个内存地址放三个指针(lt[0],lt[1],lt[2]), 然后再创建一个 lt 指针,指向放三个指针的地址12345a = 100	lt = [a, 200]print(lt) # 显示 [100, 200]a = 1000  # a 指针不再指向 100, 但是 lt[0] 仍然指向100print(lt) # 显示 [100, 200]字典的键和值分别指向指针 Python 内存管理  python 引入了一个聊天室的概念    什么时候创建聊天室: 程序开始执行, 遇到声明变量, 就会创建一个内存地址.   什么时候销毁聊天室: 程序结束, 释放内存  引用计数:    负责对聊天室里的人进行统计, 记录聊天室里面有多少个指针在引用指针. 多一个指针就多一个计数  当引用计数为0时, 这块内存就会释放掉  当程序结束时, 程序变量等用到的内存都会被销毁掉 判断两个指针是否指向同一个内存地址  is 可以用于判断两个指针是否指向同一个地址 两个相同值得字符串是同一个对象 两个相同的 list, 不是同一个对象 两个相同的数字变量, 数字小于-5 就是不同的 ID, 否则相同123456lt1 = [100,200,300]lt2 = [100,200,300]print(lt1 == lt2)  # True  判断值是否相等print(lt1 is lt2)  # False 判断是否引用同一个内存地址print(id(lt1))  print(id(lt2)) 字符串会有些不同因为字符串是不可变对象, 所以两个变量定义了同样的字符串, 那么这两个变量会指向同一个内存地址 1234str1 = 'hello'str2 = 'hello'print (str1 == str2)  # Trueprint (str1 is str2)	# True1234num1 = 4num2 = 4print (num1 == num2)  # Trueprint (num1 is num2)	# True1. 3目录管理: os 模块的基本函数: 目录管理 1234567891011121314151617181920212223242526import osprint(os. name) # nt: windows. posix: linuxprint(os. environ) # 当前系统的环境变量print(os. environ. get( /Users/dalong/code/python1702/homework )) # 获取指定的环境变量print(os. getcwd()) # 获取当前程序所在目录print(os. listdir('/Users/dalong/code/python1702/homework'))  # 相当于 dir / ls 命令print(os. mkdir('hehe')) # 可以换成绝对路径, 如果文件夹存在会报错. 代码中需要提前判断.  分相对路径和绝对路径. . / . . / 是相对路径print(os. makedirs('hehe/momo/xixi')) # 可以递归创建多层目录 , 该函数无返回值print(os. rmdir('hehe')) # 只能删除空目录, 非空目录删除会报错print(os. stat('. /'))  # 获取目录 or 文件的信息, 包括文件大小, 创建时间, 修改时间, 访问时间print(os. rename('haha' , 'hehe')) # 修改文件目录或者文件名print(os. remove('hehe. txt'))  # 只能删除文件os. system('clear') #路径处理os. path. join(path, file) # 拼接路径和文件, 自动处理中间的斜线os. path. splitext(path) # 获取到含有两个元素的列表, 最后一个元素是文件类型, 比如 . txtos. path. isdir(path)  # 判断路径是否是文件夹os. path. isfile(path) # 判断路径是否是文件os. path. exists(path) # 判断文件和文件夹是否存在os. path. getsize(path) # 获取文件的大小os. path. dirname(path) # 返回最后一个斜线前面的部分os. path. basename(path) # 获取最后斜线后面的部分 , path 不存在不会报错os. path. split(path) # 通过最后一个斜线分割成一个元组 ,一个是路径,一个是文件名一行代码获取某路径下所有的文件 or 文件夹 12345678910111213# Get all regular filesdef getAllFiles(somedir):  return [name for name in os. listdir(somedir) if os. path. isfile(os. path. join(somedir, name))]# Get all dirsdef getAllDirs(somedir):  return [name for name in os. listdir(somedir) if os. path. isdir(os. path. join(somedir, name))]# 获取路径下所有 python 文件pyfiles = [name for name in os. listdir('somedir') if name. endswith('. py')]# 获取路径下所有 Python, c , html 文件.  这里的 endwith or startwith 必须传一个元组列表进去pyfiles = [name for name in os. listdir('somedir') if name. endswith(('. py','. c','. html'))]批量修改文件名 123456789101112131415import osfiles = os. listdir('hehe')print(files)for file in files:  new_file =  狗蛋  + file  print(new_file)  os. rename('hehe/'+file, 'hehe/'+new_file)files = os. listdir('hehe')print(files)for file in files:  new_file = file. lstrip('狗蛋')  print(new_file)  os. rename('hehe/'+file, 'hehe/'+new_file)glob 模块 帮助 搜索 文件, 并获取文件的详细信息:  可以查找符合特定规则的文件路径名。查找文件只用到三个匹配符：*, ?, []。*匹配0个或多个字符；?匹配单个字符；[]匹配指定范围内的字符，如：[0-9]匹配数字。 1234567891011121314151617181920212223import osimport os. pathimport glob# 获取指定目录下的所有图片print(glob. glob( /Users/dalong/code/python1702/homework/*. py )) # 绝对路径# 获取上级目录的所有. py文件print(glob. glob('. /*. py')) # 相对路径作为参数, 返回一个列表 ['. /homework17110902. py', '. /momo. py', '. /test. py', '. /temp. py', '. /main. py']print(list(glob. iglob('. /*. py'))) # 相对路径作为参数, 返回的是一个可迭代对象# 遍历获取所有文件和每个文件的 文件大小, 最后修改时间pyfiles = glob. glob('. /*. py')name_sz_date = [(name, os. path. getsize(name), os. path. getmtime(name)) for name in pyfiles]for name, size, mtime in name_sz_date:  print(name, size, mtime)# 另一种方法获取文件详细信息 Alternative: Get file metadatafile_metadata = [(name, os. stat(name)) for name in pyfiles]print(file_metadata)for name, meta in file_metadata:  print(name, meta. st_size, meta. st_mtime)  字符串类型文件和字节类型文件: 123- 字符串类型想字节类型转化称为编码: 人能看懂的文字转向二进制 `- encode()`- 字节类型想字符串类型转化称为解码: 二进制转向人看懂的文字   `- decode()`- 编解码要保持一致,用一样的编解码类型, 否则乱码12345678string = '窗外's_utf8 = string. encode('utf-8')s_gbk = string. encode('gbk')print(s_utf8) # utf-8: 一个中文三个字节print(s_gbk)  # gbk : 一个中文两个字节print(s_utf8. decode('utf-8'))print(s_gbk. decode('gbk'))常用字符集 encoding  ascii : 美国的标准, 一个字节, 无符号0~255, 有符号-177 到 177 , 其中 0~127 表示了老美用的东西.  但是后来不够用了 ansi : 是扩展的 ascii , 用 两个字节, 从 128 ~ 0xffff 可用. 但后来乱套了, 中国搞了一套自己的(gb2312),韩国日本也有自己的一套. 都用 128~0xfffff      gbk: 是 gb2312的扩展, 成为扩展的国标, 在 gb2312基础上扩展了很多的繁体字等. gbk 向下兼容 gb2314      unicode : 一套国际标准(为了解决 ansi), 称之为 万国码, 也是两个字节, 为每个国家的每个文字指定了编码. 可惜的是后来没有被广泛执行, 也没有规定如何存储和读取.     utf-8 : 后来互联网兴起, 迫切需要一套统一的标准, 在 unicode 的基础上规定了如何存储和读取的方式. utf-8 是变化的字节. 如果存储英文字母, 使用的是个字节, 如果存储汉字,使用的是二~三个字节. 最大是四个字节(日韩文字).  以后开发都需要 utf-8 无 BOM 编码格式   乱码的问题大多数是因为编码时和解码时使用了不同的比编码和解码字符集获取当前系统的默认编码默认情况下，所有的文件名都会根据 sys. getfilesystemencoding() 返回的文本编码来编码或解码 12import syssys. getfilesystemencoding()  # 获取当前系统的默认编码, 返回 'utf-8'文件管理:  fp = open(path, w , encoding = 'utf-8') # w 是 文件的打开模式 fp = open(path, wb ) # 带 b 的模式后面都不需要加 encoding ,因为通过二进制模式打开时, 不需要指定字符集.  如果将字符串进行了 encode() 操作, 那么就必须用 带 b 的模式打开文件进行读写.  否则报错.  记住: 写文件的时候, 如果不指定 encoding = 'utf8' , 文件会默认用系统的 encoding 格式打开, 中文系统一般默认 gbk . 这样会造成读取时乱码 读写字节数据(图片, 声音, 视频等) 时必须使用 rb, wb1234567path = '. /hehe/test. txt'# fp = open(path, 'r', encoding='utf-8') # 如果不指定 utf8 , 文件会默认用系统的 encoding 格式打开, 系统默认 gbk# print(fp. read())string =  中国你好haohao s1 = string. encode('utf-8')fp = open(path, 'wb')fp. write(s1)文件打开模式  r : 读取文件 w : 普通写, 会把原内容删掉 a : 就是追加写 b : 是以二进制形式打开or 写入. 如果要使用 wb, 那么要写入文件的字符串必须是先要 encode() 过的 ; 如果 使用 rb 读取文件, 那么文件读出来后是二进制形式, print 时候打印的就是把文字转成二进制的样子了.  b 与 copy: 如果想要原样不差的拷贝内容(包括文本), 那就需要用rb, wb 来拷贝      : 就是多一项功能(读, 写中的一个)的意思, w+ 多一个读, r+ 多一个写| 简写  |                    描述 || :— | —————————————: || w  | 以只写方式打开, 如果文件不存在会创建文件, 如果文件存在, 会清空文件 , 文件指针在文件开头 || r | 以只读方式打开文件, 文件指针从文件头开始读取, 如果文件不存在,报错| a | 和 w 类似, 但是打开后文件指针在文件的末尾, 以追加的方式添加文件内容| w+ | 以读写方式打开, 其他和 w 一样| r+ | 以读写方式打开, 和 r 一模一样, 比 r 多了一个写的功能| a+ | 和a一样,多了一个读的功能|wb | 以二进制形式写文件|rb | 以二进制形式读文件|ab | 以二进制形式写文件追加|wb+ ||rb+ ||ab+ ||rt | 读取文件,并将 \r 转化成 \n , 应该是在 windows 系统中比较有用吧?|wt | 写文件,并将 \r 转化成 \n|at | 追加文件,并将 \r 转化成 \n|x | 和 w 类似, 但是只有在文件不存在才可以写入, 如果文件存在, 会报一个文件已存在的错误. 用 x 可以防止误将文件覆盖掉. |xb | 如果是二进制的文件,用 xb   encoding:  如果打开的时候带 b, 这个参数不能加 如果打开的时候用普通方式打开, 有时候会用到 encoding文件读写:fp. read(字节数) , 读取文件字节数, 不写默认全部读取fp. readlines(字节数) 读进来按照换行形式切割,返回列表fp. write(写入内容) , 要写入的内容 关闭文件fp. close() 拷贝文件的三种方法  read() readline() readlines()123456789101112131415161718192021222324252627282930313233343536373839import os#方法一 read(1024)def myCopy(src: str, dis: str) -&gt; str:  fp_src = open(file=src, mode='r', encoding='utf-8')  fp_dis = open(file=dis, mode='w+', encoding='utf-8')  fp_dis. truncate()  while True:    block = fp_src. read(1024)    if not block:      fp_dis. close() # 关闭文件      break    fp_dis. write(block)#方法二 readline()def myCopy(src: str, dis: str) -&gt; str:  fp_src = open(file=src, mode='r', encoding='utf-8')  fp_dis = open(file=dis, mode='w+', encoding='utf-8')  fp_dis. truncate()  while True:    line = fp_src. readline()    if not line:      fp_dis. close() # 关闭文件      break    fp_dis. write(line)#方法三 readlines()def myCopy(src: str, dis: str) -&gt; str:  fp_src = open(file=src, mode='r', encoding='utf-8')  fp_dis = open(file=dis, mode='w+', encoding='utf-8')  fp_dis. truncate()  lines = fp_src. readlines()  for line in lines:    fp_dis. write(line)  fp_dis. close() # 关闭文件if __name__ == '__main__':  src = '. /testfolder/hahaha. txt'  dis = '. /testfolder/hahaha_copy. txt'  myCopy(src, dis)print() 打印内容追加到文件中  因为是追加存储,所以可以考虑作为日志分析. 12with open('. /test. txt', 'at') as f:	print('Hello World!', file=f)递归删除文件夹 1234567891011121314151617import os# 2、递归删除文件夹def rmFolderIter(dist):  ls = os. listdir(dist)  if not ls:    os. rmdir(dist)    return  print(ls)  for l in ls:    if os. path. isdir(dist):      sub_path = os. path. join(dist, l)      rmFolderIter(sub_path)if __name__ == '__main__':  path = '. /testfolder/tobedelete'  rmFolderIter(path)递归统计目录中的文件大小 123456789101112131415161718192021# # 3、递归统计文件夹大小import osdef sumDirSize(path):  if not os. path. isdir(path):    print('提供的路径不合法')    return  size = 0  all_files = [name for name in os. listdir(path) if os. path. isfile(os. path. join(path, name))]  all_folders = [name for name in os. listdir(path) if os. path. isdir(os. path. join(path, name))]  if len(all_files) == len(all_folders) == 0:    return 0  size += sum(map(lambda x: os. path. getsize(os. path. join(path,x)) , all_files)) + sum(map(lambda x:sumDirSize(os. path. join(path,x)),all_folders))  return sizeif __name__ == '__main__':  path = '. /'  print(sumDirSize(path))忽略文件名编码当需要打开一个乱码的文件名时(文件名不知道被什么编码进行命名导致乱码) , 那么需要有个办法可以忽略这个文件名的乱码 在 os. listdir() , open()函数中给路径名前面加上 b 就可以按照Unicode 字符进行处理 12345import osos. listdir(b'. ') # Note: byte stringwith open(b'jalapen\xcc\x83o. txt') as f:	print(f. read())处理文件中的空格 1234with open('. /test. txt', 'r', encoding='utf-8') as f:  lines = (line. strip() for line in f)  for line in lines:    print(line)文件指针  读取文件时, 文件的指针会向后移动, fp. tell() 函数会告诉我们文件指针的位置. tell() 是以字节为单位返回的位置(一个汉字三个字节). 用 r+ 参数打开 的指针是开头, 用 a+ 打开的文件的指针在末尾 用 r+ 打开文件写入的时候, 要注意将文件的指针指到文件末尾, fp. seek(0,2) ,然后在写入fp. seek() 函数可以确定从文件指针指定的位置开始读取, seek(4,1) 的第二个参数代表偏移, 有两个值0 , 1, 和 2 .  如果想使用第二个参数, 必须用 b 的形式打开文件.  如果不写第二个参数, 默认从文件开头读取0: 从文件开头位置偏移1: 从文件当前位置偏移2: 从文件末尾进行偏移 12345fp = open('ceshi. txt', 'r', encoding='utf-8') #对于中文print(fp. read(1))print(fp. tell()) # 获取当前文件指针的位置fp. seek(3)     # 从第n个指针的位置开始读print(fp. read(1))深浅拷贝  深拷贝: 拷贝了内存的(在内存创建新的空间,并复制了内容), 称为深拷贝浅拷贝: 只是创建了新的对内存块的引用. 浅拷贝只能拷贝最外层列表, 列表中的子列表不会被拷贝. 使用 import copy 模块可以实现深拷贝深拷贝大量内容,会造成性能问题浅拷贝l2 = copy. copy(l1)l2 = l1. copy(l2) 深拷贝l2 = copy. deepcopy(l1) 1234567891011121314151617181920212223242526s1 = 'hello's2 = 'hello'print(s1 is s2) # Truel1 = [1,2,3]l2 = [1,2,3]print(l1 is l2) #Falsel3 = l1   # 通过等号赋值, 只是浅拷贝l3[2] = 1000print(l1,l3)  # [1, 2, 1000] [1, 2, 1000]l3 = l1. copy()   # 通过copy()赋值, 是深拷贝l3[2] = 1000print(l1,l3)    # [1, 2, 1000] [1, 2, 2000]l1 = [1,2,[1,2,3]] # 如果深背拷贝元素中有子列表,那么子列表并不会被深拷贝(子列表被浅拷贝)l3 = l1. copy()l3[2][0] = 1000print(l1,l3)  # [1, 2, [1000, 2, 3]] [1, 2, [1000, 2, 3]]import copyl3 = copy. deepcopy(l1)l3[2][0] = 1000print(l1,l3)  # [1, 2, [1, 2, 3]] [1, 2, [1000, 2, 3]]邮件发送:  从服务器端发送邮件给指定邮箱通过运行 python 代码, 收到邮件 几种邮件的协议  smtp : 邮件发送协议pop3 : 邮件接收协议imap : 邮件接收协议python 需要引入 smtplib, 登录自己的 smtp 服务器(邮件服务器), 然后设置邮件发送的标题, 内容, 接收人地址, 然后即可发送. 发邮件之前要先注册邮箱并设置开启 SMTP 服务  163邮箱: dalong_coo@163. com密码: 授权码: 12345678910111213141516171819202122232425262728293031323334import smtplib# 将你写的字符串转化为邮件的文本形式from email. mime. text import MIMEText# smtp服务器地址smtp_server = 'smtp. 163. com'# 发送者是谁sender = 'dalong_coo@163. com'# 客户端授权码 , 注意, 不是邮箱密码,而是开启 SMTP 服务时输入的授权码password = '自己的密码'# 发给谁 多个用户中间使用 逗号 隔开to = '37016175@qq. com,dalong_co@hotmail. com'# 发送的消息message = '今晚预习面向对象'# 以后发送激活链接，就是定制一下内容即可# message = '&lt;a href= http://www. sb. com/login &gt;点击激活&lt;/a&gt;'# 转化为邮件文本message = MIMEText(message)# 定制邮件标题message['Subject'] = 'python 预习内容'# 定制发送者message['From'] = sender# 绑定服务器和端口号mail_server = smtplib. SMTP(smtp_server, 25)# 登录mail_server. login(sender, password)# 发送邮件mail_server. sendmail(sender, to, message. as_string())# 退出mail_server. quit()短信发送:  服务端发送短信给指定手机号码通过运行 python 代码收到短信秒嘀, 云之讯, 等短信平台注册后,可以送10元钱, 免费发100条秒嘀接口文档用户名 , 密码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657# 短信发送 (秒嘀科技)import timeimport hashlibimport urllib. parseimport http. client# 请求的urlurl = 'https://api. miaodiyun. com/20150822/industrySMS/sendSMS'# 请求的http包头headers = {	'Content-type': 'application/x-www-form-urlencoded'}# 注册时候给你的账户id和token玩意accountSid = 'fba49f9a5d904ed8ab037b0c48780ca8'token = '76308a6f7b334a30910c621ef2d77c4e'# 发送给谁# to = '18616362174'to = '13061611151'# 获取指定个是的时间戳timestamp = time. strftime('%Y%m%d%H%M%S')# print(timestamp)# 获取指定的md5码sig = accountSid + token + timestamphash = hashlib. md5()hash. update(sig. encode('utf-8'))sig = hash. hexdigest()# 指定发送短信模板templateid = '101933509'code = '987123'fenzhong = 5param = code + ',' + str(fenzhong)# 处理post请求时发送的数据formdata = {	'accountSid': accountSid,	'templateid': templateid,	'param': param,	'to': to,	'timestamp': timestamp,	'sig': sig}# 将formdata进行编码formdata = urllib. parse. urlencode(formdata)# 创建模拟浏览器对象conn = http. client. HTTPConnection('api. miaodiyun. com', port=80)# 通过模拟浏览器发送请求conn. request(method='POST', url=url, headers=headers, body=formdata)# 获取该请求响应response = conn. getresponse()print(response. read(). decode('utf-8'))# 将链接关闭conn. close()"
    }, {
    "id": 33,
    "url": "http://localhost:4000/Linux-Command-Basic/",
    "title": "Linux Command Basic",
    "body": "2015/06/20 - Linux®是一个开源操作系统（OS）。操作系统是直接管理系统硬件和资源（如 CPU、内存和存储）的软件。操作系统位于应用程序和硬件之间，并在所有软件和执行工作的物理资源之间建立连接。 本文我将详细介绍常用的 Linux 命令行, 帮助大家快速掌握 Linux 的基本服务命令和用法. 11 Linux 基础[TOC] redhat7 系统管理员手册 linux环境安装: 官方文档 (1)安装 VirtualBox 虚拟环境 5. 1. 18 (2)安装 CentOS mini发行版 CentOS-6. 8-x86_64-minimal. iso  用户名:root 密码:@moyu**I(3)配置VirtualBox 网络:  进入 setting-&gt;Network Attached to: 设置Bridge Adapter Name : en5 (这里面要看你现在用的电脑选的是哪个网络上网, 这里就选哪个网络)(4)配置 CentOS 网络并重启    vi /etc/sysconfig/network-scripts/ifcfg-enp0s3   1ONBOOT=yes      systemctl restart network     ping www. baidu. com  (5)ssh 从 Mac 远程连接CentOS    先查看 CentOS 服务器的内网 IP地址   1ip addr    可以看到 enp0s3 的 inet 是 10. 11. 56. 71     Mac 上运行 ssh 命令连接   1ssh root@10. 11. 56. 71   技巧: 快速向文件中追加一行信息: 1echo  This_is_an_errorline  &gt;&gt; /root/tmp/test. vim快速替换文件中的字符串: 1sed -i 's/This_is_an_errorline//g' /root/tmp/test. vim常用命令: redhat 7,6,5 常用命令对比 info 命令: 用法 就内容来说，info页面比man page编写得要更好、更容易理解，也更友好，但man page使用起来确实要更容易得多。一个man page只有一页，而info页面几乎总是将它们的内容组织成多个区段（称为节点），每个区段也可能包含子区段（称为子节点）。 快捷键: 12345678910?键：它就会显示info的常用快捷键。N键：显示（相对于本节点的）下一节点的文档内容。P键：显示（相对于本节点的）前一节点的文档内容。U键：进入当前命令所在的主题。M键：敲M键后输入命令的名称就可以查看该命令的帮助文档了。G键：敲G键后输入主题名称，进入该主题。L键：回到上一个访问的页面。SPACE键：向前滚动一页。BACKUP或DEL键：向后滚动一页。Q：退出info。命令: 12345678910111213141516171819202122？   显示帮助窗口在帮助窗口中：Ctrl-x 0     关闭帮助窗口Ctrl-x Ctrl-c  关闭整个 Info    q   退出 infon   打开与本 Node 关联的下一个 Nodep   打开与本 Node 关联的前一个 Nodeu   打开与本 Node 关联的上一个 Nodel   回到上一次访问的 Nodem或g  选择一个菜单项（Node 的名字）    输入指定菜单的名字后按回车，打开指定菜单项关联的 Node空格键 下一页（PageDown 也可以，下一页从当前页的最后两行开始算起）    下一个 Node （若当前页在 Node 文档的末尾）Del 键 上一页（PageUp 也可以，上一页从当前页的开始两行开始算起）    上一个 Node （若当前页 Node 文档的开始）b 或 t 或 Home  文档的开始（b 是 begining 的意思）e 或 End     文档的末尾（b 是 ending 的意思）Ctrl-l  刷新当前页，若当前文档显示情况有问题时Ctrl-g  取消所键入的指令whatis 命令: 可以用于处理任何不懂的命令, 配置文件等. 会返回简单的描述和常看详细解释的途径. 在 man 一个命令之前, 最好先 whatis 一下 相当于 man -f 搜索关键字 查看什么是 systemctl 123whatis systemctl# systemctl (1)    - Control the systemd system and service manager# 然后就可以用 man 1 syxtemctl 去查看更详细信息了help --help命令: help命令用于显示shell内部命令(就是 shell 编程语言中用的命令)的帮助信息。    help 不加参数可以查看所有shell 内部命令     而对于外部命令的帮助信息只能使用man或者info命令查看。   ​  --help  任何命令后面接 –help 可以获得该命令的基本使用方法的帮助信息man 命令详解: man命令详解 因为centos 7 的 man 命令补全, 很多 man 5 都没有, 需要先安装扩展包 1yum install man-pagesman 最有用的两个参数  -a: 在所有手册中查找相关内容  -k: 模糊查找想要找的内容 1man -a system. cong # 在所有帮助文档中查找这个关键字man [数字] 目标内容 数字的含义       章节   功能         1   标准用户命令（Executable programs or shell commands）       2   系统调用（System calls）functions provided by the kernel       3   库调用（Library call）functions within program libraries       4   特殊文件（设备文件）的访问入口（/dev）Special files (usually found in /dev)       5   文件格式（配置文件的语法），指定程序的运行特性 File formats and conventions       6   游戏（Games）       7   杂项（Miscellaneous）including macro packages and conventions       8   管理命令 System administration commands       9   跟kernel有关的文件 Kernel routines   history 命令: 详细的说明 history 查看历史命令 !745 执行历史的第745条命令 !-1 执行上一条命令 , 数字可以是更多 !$ 上条命令的参数 . !$ 符号可以将上一条命令的参数传递给下一条命令参数： 12mkdir /home/testcd !$ # 进入/home/test 目录!cp:1 可以获取上一个 cp 命令的第一个参数 , 2 就是第二个参数了 Linux 系统介绍: 系统启动流程:  (1)基本输入输出系统(BIOS)     上电之后系统基本硬件自检    (2)主引导分区(MBR)     启动引导分区, 系统从哪里驱动的代码(boot loader)    (3)启动引导代码(boot loader)     在操作系统运行之前执行的一段代码   负责将系统的软硬件带入到合适的运行环境中.    因为操作系统是运行在内存中的, 这段代码就是为了将早做系统放入内存之前做准备的.     (4)操作系统(OS)     操作系统启动后执行的第一个进程-守护进程 /sbin/init   这个文件做了很多初始化的工作         /etc/inittab 该文件标识了系统启动运行的级别. 根据运行级别完成后续相关初始化操作.      centOS7 只有两个运行级别             multi-user. target : 相当于 runlevel 3       graphical. target: 向东与 runlevel 5       systemctl get-default 是获取当前运行级别       systemctl set-default graphical. target 设置当前默认运行级别为 图形化.                 centOS6运行级别如下:             #  0 - halt (Do NOT set initdefault to this) 关机       #  1 - Single user mode       #  2 - Multiuser, without NFS (The same as 3, if you do not have networking)       #  3 - Full multiuser mode       #  4 - unused       #  5 - X11       #  6 - reboot (Do NOT set initdefault to this)       id:3:initdefault: 默认                      (5)执行系统执行初始化脚本     /etc/rc. d/ 有多个脚本文件, rc0. d ~ rc6. d 分别代表了不同即启动级别的运行脚本. 默认执行 rc. sysinit   /etc/rc. d/rc. sysinit 系统初始化脚本文件   /etc/rc. d/rc. local 是最后留给用户的自定义初始化脚本. 用户可以在文件中编辑自己要执行的初始化操作    (6)用户登录界面     桌面环境: GNOME, KDE   终端: bash   目录结构: 文件系统介绍:  文件系统是操作系统用来管理存储设备或分区上的文件的方法和数据结构  操作系统中管理和存储文件信息的阮籍机构叫做文件管理系统. 简称文件系统  fat16(MS-DOS 6. x) 分区最大2G fat32(windows 95) 分区最大4G, 性能弱,容易产生碎片.  ntfs(windows NT) 提升 fat 32稳定性, 最大单个文件无限制 ext4(linux) 扩展型日志文件系统 hfs+(Mac) 苹果文件系统 exfat(win/mac). 可以支持4G 以上的单个文件, 适合闪存.  xfs (linux) centOS 7 采用 xfs 文件系统.      Ext4受限制于磁盘结构和兼容问题，可扩展性和scalability确实不如XFS，另外XFS经过很多年发展，各种锁的细化做的也比较好.    XFS使用64位管理空间，文件系统规模可以达到EB级别，可以说未来几年XFS彻底取代Ext4是早晚的事情！   linux 文件类型:  LINUX有四种基本文件系统类型：普通文件、目录文件、连接文件和特殊文件，可用file命令来识别。  普通文件(-)：如文本文件、C语言元代码、SHELL脚本、二进制的可执行文件等，可用cat、less、more、vi、emacs来察看内容，用mv来改名。 目录文件(d)：包括文件名、子目录名及其指针。它是LINUX储存文件名的唯一地方，可用ls列出目录文件。 连接文件(l)：是指向同一索引节点的那些目录条目。用ls来查看是，连接文件的标志用l开头，而文件面后以”-&gt;”指向所连接的文件。 特殊文件(d,c,)：LINUX的一些设备如磁盘、终端、打印机等都在文件系统中表示出来，则一类文件就是特殊文件，常放在/dev目录内。例如，软驱A称为/dev/fd0。LINUX无C：的概念，而是用/dev/had来自第一硬盘。目录结构:  详细介绍 cd / &amp;&amp; ls -lo       目录   描述         /   根目录。对你的电脑来说，有且只有一个根目录。所有的东西，我是说所有的东西都是从这里开始。举个例子：当你在终端里输入“/home”，你其实是在告诉电脑，先从/(根目录)开始，再进入到home目录。       /bin   这里存放了标准的(或者说是缺省的)linux的工具，比如像“ls”、“vi”还有“more”等等。通常来说，这个目录已经包含在你的“path”系 统变量里面了。什么意思呢?就是：当你在终端里输入ls，系统就会去/bin目录下面查找是不是有ls这个程序。       /sbin   大多是涉及系统管理的命令的存放，是超级权限用户root的可执行命令存放地，普通用户无权限执行这个目录下的命令，这个目录和/usr/sbin; /usr/X11R6/sbin或/usr/local/sbin目录是相似的；我们记住就行了，凡是目录sbin中包含的都是root权限才能执行的。       /boot   Linux的内核及引导系统程序所需要的文件目录，比如 vmlinuz initrd. img 文件都位于这个目录中。在一般情况下，GRUB或LILO系统引导管理器也位于这个目录       /dev   这里主要存放与设备(包括外设)有关的文件(unix和linux系统均把设备当成文件)。想连线打印机吗?系统就是从这个目录开始工作的。另外还有一些包括磁盘驱动、USB驱动等都放在这个目录       /etc   这里主要存放了系统配置方面的文件。举个例子：你安装了samba这个套件，当你想要修改samba配置文件的时候，你会发现它们(配置文件)就在/etc/samba目录下。       /home   这里主要存放你的个人数据。具体每个用户的设置文件，用户的桌面文件夹，还有用户的数据都放在这里。每个用户都有自己的用户目录，位置为：/home/用户名。当然，root用户除外。       /root   这是系统管理员(root user)的目录。对于系统来说，系统管理员就好比是上帝，它能对系统做任何事情，甚至包括删除你的文件。因此，请小心使用root帐号。       /run   运行时应用使用的临时目录       /lib   库文件       /lib64   64位库文件       /lost+found   出现异常时保存信息,以便恢复时使用       /media   有些linux的发行版使用这个目录来挂载那些usb接口的移动硬盘(包括U盘)、CD/DVD驱动器等等。       /mnt   mount.  这个目录一般是用于存放挂载储存设备的挂载目录的，比如有cdrom 等目录。可以参看/etc/fstab的定义。有时我们可以把让系统开机自动挂载文件系统，把挂载点放在这里也是可以的。主要看/etc/fstab中怎 么定义了；比如光驱可以挂载到/mnt/cdrom 。       /opt   option.  这里主要存放那些可选的程序。你想尝试最新的firefox测试版吗?那就装到/opt目录下吧，这样，当你尝试完，想删掉firefox的时候，你就可 以直接删除它，而不影响系统其他任何设置。安装到/opt目录下的程序，它所有的数据、库文件等等都是放在同个目录下面。       /proc   进程信息, 是一个虚拟的文件系统.  操作系统运行时，进程信息及内核信息（比如cpu、硬盘分区、内存信息等）存放在这里。/proc目录伪装的文件系统proc的挂载目录，proc并不是真正的文件系统       /srv   service srv代表服务。包含服务器特定服务相关的数据。例如，/srv/cvs包含cvs相关的数据。       /sys   system 类似 proc, 也是虚拟文件系统, 用于映射内核信息       /tmp   这是临时目录。对于某些程序来说，有些文件被用了一次两次之后，就不会再被用到，像这样的文件就放在这里。有些linux系统会定期自动对这个目录进行清理，因此，千万不要把重要的数据放在这里       /usr   unix system resource 的缩写.  在这个目录下，你可以找到那些不适合放在/bin或/etc目录下的额外的工具。比如像游戏阿，一些打印工具拉等等。/usr目录包含了许多子目录： /usr/bin目录用于存放程序;/usr/share用于存放一些共享的数据，比如音乐文件或者图标等等;/usr/lib目录用于存放那些不能直接 运行的，但却是许多程序运行所必需的一些函数库文件。       /usr/local   这里主要存放那些手动安装的软件，即不是通过“新立得”或apt-get安装的软件。它和/usr目录具有相类似的目录结构。让软件包管理器来管理/usr目录，而把自定义的脚本(scripts)放到/usr/local目录下面，我想这应该是个不错的主意。       /var   系统自动产生的,不会自动销毁的文件, 比如日志, 邮件等.  这个目录的内容是经常变动的，看名字就知道，我们可以理解为vary的缩写，/var下有/var/log 这是用来存放系统日志的目录。/var/www目录是定义Apache服务器站点存放目录；/var/lib 用来存放一些库文件，比如MySQL的，以及MySQL数据库的的存放地   ls 命令: -lah 列表形式显示所有文件, 文件大小显示单位 文件类型: 12345678910111213-: 普通文件d: 目录l: 软链接c: 字符设备b: 块设备s: socket 套接字文件p: 管道文件mv 模糊批量移动/改名: 批量将当前目录下的以file 开头,一位数字,结尾是. py . sh . pl 的文件移动到 subdir中  注意 要用 egrep 才好用. 1mv `ls |egrep '^file[0-9]\. (py|sh|pl)'` . /subdir/其他 命令 touch, rm, cp, mkdir, rmdir:    相关命令               命令     说明                   touch     创建新文件(可以一次性创建多个)             rm     删除文件或目录(目录需要添加’-r’)             cp     拷贝文件或目录(目录需要添加’-r’)             mkdir     创建目录(可以一次性创建多个)             rmdir     删除空目录,             选项说明      -f：表示强制操作   -r：表示递归操作   *：表示模糊匹配，如：rm -f *. c，删除所有的. c文件   -p：创建中间目录时需要添加   文件管理: 相关命令 cat, tac, head, tail, nl, wc, more, less:       命令   说明         cat   从上到下，将文件所有内容全部展示出来       tac   从下到上，将文件所有内容全部展示出来       head   显示文件开头的几行内容，默认10行，可以指定，如：head -3 1. txt       tail   显示文件末尾的几行内容，默认10行，可以指定，如：tail -3 1. txt       nl   功能同cat，多显示了行号       wc   统计显示，内容：行数 单词数 字符数 文件名       more   一点一点查看内容       less   一点一点查看内容   more 和 less使用说明:  内容显示一屏就会停止 q可以退出查看 enter向下翻一行 空格向下翻一页 more查看到末尾会自动退出，less不会 less可以上下按钮上下翻看，more不可以 经常结合管道使用，如：ls /etc | morevim 命令详解: 简介: vi 是默认终端编辑器, 后来有了vim, 是加强版的vi. yum install vim 先安装 vim vimtutor 查看 vim 手册 vim ~/. vimrc 按喜好配置 123:set nu:set tabstop=4:set mouse=aresource ~/vimrc 让编辑的配置立即生效 分屏模式  更详细的解释  :vsp 左右分屏  :sp 上下分屏  :vsp 文件名 在现有文件基础上增加一个文件的分屏  control + w + w 在分屏间切换  :only 关闭其他分屏 编辑模式       操作________   描述         h j k l   移动光标 h j k l       fn + 上/下 或者 control + f/b   翻页       ESC   结束编辑 ESC       x X 5x 5X   向左/向右删除字符 , 设定删除n个字符                   i a A   光标处插入字符 i , 光标下一个位置插入字符 a , 行尾插入 A       O o   光标上面插入空行 O, 光标下面插入空行 o       dw d2w de   删除单词 dw 删除两个单词 d2w 删除到当前单词末尾 de       d^ d$ dd 10dd   删除到行位 d$ 删除到行头 d^ 删除整行 dd 删除多行 2dd       u U control+r   取消修改 u , 回复本行原始状态 U .  重做撤销 control + r       w e   向后移动到下一个单词 w 向后移动到单词尾部 e       0 ^ $   移动到行头 0 ^, 移动到行尾 $       y yw yy 10yy p   复制字母或赋值高亮选中y ,复制到单词尾yw , 复制整行yy, 复制10行10yy, 黏贴 p       p P dd   粘贴 p 将最后一次删除的内容黏贴到当前光标的下一行; P 是放在光标上一行 p 和 dd 经常联合使用       r   替换字母 r 然后输入想要替换的字母; 只能替换英文字母,而且一次 r 只能替换一次字母.        R   进入替换模式 R , 直到 ESC 位置       ce c$ de   更改到当前单词末尾 ce 更改到行尾 c$ ; ce 比 de 多了一个 插入模式       gg 100gg G   跳到文章头 gg , 跳到文章尾 G , 跳到指定行 数字+G       /key ?key n N   搜索 /字符串 逆向搜索 ?字符串 下一个 n , 上一个 N       /\cmysql   找mysql , 忽略大小写       \Cmysql   找mysql , 大小写敏感       shift + 3   快速查找, 把焦点对应到要找的单词上, 按下 shift + 3 就可以在全文中找对应的单词.        control+o control+i   回到上一个操作的位置 control + o, 回到新的位置 control + i       %   移动光标到配对括号 % , 可以是(){}[]       v   高亮选取部分文本 v 然后进行删除 d 或者另存 :w , 或者复制 y 在黏贴 p ; 按 v 键使 Vim 进入可视模式进行选取。您可以四处移动光标使选取区域变大或变小。接着您可以使用一个操作符对选中文本进行操作。例如，按 d 键会删除选中的文本内容   命令模式       命令   描述         :x   保存退出.  shift + zz 快捷保存退出       :q! :w :wq :wq!   退出, 保存, 保存退出q! w wq wq!;       :w   另存为 :w+文件名       :!   执行系统命令 :!+命令 , 比如 :!ls :!pwd       :set nu[mber]   显示行号 :set number       :set nonu[mber]   取消行号 :set nonu       :e!   恢复到文件刚打开时的状态(恢复到修改之前的状态).        :set tabstop=4   设置一个tab 可以有4个空格       :s/old/new/g :%s/old/new/g   一行中替换单词 :s/old/new/g , 全文中替换单词 :%s/old/new/g       :set mouse=a   启动光标可以点击. 是高级的 v 启动后可以配合 d, y, p 命令使用 . 更多鼠标操作       :set ignorecase :set noignorecase   忽略大小写, 取消忽略大小写       :100   到第100行.  和 100gg一样   命令行模式(在shell 模式下)       命令   描述         !v   打开最近vim 编辑的最后一个文件       vim filename + 5   直接进入第5行       vim filename +   进入到文件最后一行   链接文件 ln: 命令: ln ln [-s] oldfile newfile 硬链: 使用ln 创建时不加-s 选项, 相当于给一个文件又起了一个名字. 极少用到  不能给目录创建硬链接 不能跨文件系统软链接: 使用 -s 参数的ln, 相当于一个快捷方式. 可以给目录创建软链接, 也可以跨操作系统  创建软链接, 源文件和新文件的引用数不变.  软链接的文件属性变成 l lrwxrwxrwx.  软链接既可以对文件,也可以对目录创建硬链接创建. 创建后引用数由1 变成了 2. 1234ln city. html city2. htmlll#-rw-r--r--. 2 root root 1443 12月 25 22:05 city2. html#-rw-r--r--. 2 root root 1443 12月 25 22:05 city. html软链接的例子 1234ll /etc/rc. d/rc1. d/ln -s tcpClient. py tcpClient2. py ll# lrwxrwxrwx. 1 root root  12 12月 27 16:03 tcpClient2. py -&gt; tcpClient. pyfind 文件查找: find正则使用方法 作用: 功能非常强大, 可搜索任意类型. 默认递归目标目录 前提: 必须知道文件名全名 格式: find [目录] 条件       参数________   标书         -name   文件名关键字       -type   指定类型 (d/l/s/p/c/b) (目录/连接/)       -size   指定大小, 单位 k/m/g. +表示大于 - 表示小于       -mtime/atime/ctime   指定 修改/访问/创建 的文件的天数 +表示几天前 -表示几天内       -mmin/amin/cmin   指定 修改/访问/创建 的文件的分钟 +表示几分钟前 -表示几分钟内       -user   指定用户       -group   指定组       -maxdepth   因为find 默认无限递归目录进行搜索, 该参数指定搜索目录最大深度.        -mindepth   最小深度   123find / -name passwdfind /root/ -5kfind / -maxdepth 2 -name passwd1234567find /home/test/ -regextype  posix-egrep  -regex  /home/test/. *\. (sh|pl|py)$  # 查找/home/test/目录下的所有. sh, . pl, . py结尾的文件（脚本）。#/home/test/file1. py#/home/test/file2. py#/home/test/file3. sh#/home/test/file4. plwhich whereis 命令查找: 用于查找命令. 该命令必须是在 $PATH 对应目录下的命令 1234567which ls#alias ll='ls -l --color=auto'#	/bin/lswhereis ls #ls: /usr/bin/ls /usr/share/man/man1/ls. 1. gzgrep, egrep 正则查找: 详细文章  作用: 正则搜索  格式: grep [选项] pattern [文件名]  选项:    i:忽略大小写  n:显示行号 实例: 12345grep -n /bin/bash /etc/passwd # 搜索可以登录的用户grep -i abc 1. py # 忽略大小写找abcls /bin/ |grep '^m' #查找bin 下面以 m 开头的文件tree /etc |grep 'um' #模糊查找 /etc 目录下包含'um' 文件的所有文件rm -f `ls |grep '[0-9]'` # 正则删除文件注意:  普通的文字匹配用grep, 如果需要用正则来匹配复杂内容, 用 egrep . grep 过滤多个条件 1cp --help |grep -E  \-a,|\-d,|\-f,|\-i,|^\-p,|\-r, locate 命令: 是centos7默认没有安装该命令，在联网状态运行“yum install mlocate”命令即可安装”locate”命令。 123yum install mlocateupdatedb # 更新文件索引locate inittab # 初始化 locate 1. html # 恭喜, 可以开始查找文件了. 文件/目录的拥有者 chown: -v 输出每个操作的情况 -R 递归子目录和文件 123 chown root /u			# 将 /u 的属主更改为 root 。 chown root:staff /u	# 和上面类似，但同时也将其属组更改为 staff 。 chown -R root /u		# 将 /u 及其子目录下所有文件的属主更改为 root 。改变文件/目录的权限 chown chmod: 说明: 在linux中所有文件都会涉及权限. 分为拥有者(u),所属组(g), 其他人(o). 三组总成all (a) 权限: 所有文件分为 可读(r) 可写(w) 可执行(x) 用户标识: # 标识超级用户 $ 表示普通用户 操作: 添加权限(+) , 去掉权限(-) , 设置权限(=) ls 显示: ls -l 的结果-rw-r--r-- 分为三组, 除了第一个-之外还有九个-    前三个 对应 拥有者权限     中间三个 对应 所属组权限     后面三个 对应 其他人权限     第一个表示文件类型   12345678910111213-: 普通文件d: 目录l: 软链接c: 字符设备b: 块设备s: socket 套接字文件p: 管道文件    ​  权限本质: 是一组3位八进制的数字表示的权限. 如: 0755 解析如下 12340755转成二进制: 111 101 101 # 每一位最大是7 , 是由4+2+1 组成, 既二进制的 100+10+1 对应三个身份:  u  g  o每组三个权限:  rwx rwx rwx实例 123chmod a+w 1. py # 为所有用于 添加可写权限chmod g+x 1. py # 为所属组用户添加可执行权限. chmod 777 1. py umask  作用: 限定新建文件的默认权限. 权限与该值相反  当用户登录时, 会读取 /etc/profile 中的配置信息获取该用户的 umask 值, 然后给出该用户默认的文件读取权限 root 用户 123umask 输出 : 022 =&gt; 000 010 010新建文件权限: 644 =&gt; 110 100 100新建目录权限: 755 =&gt; 111 101 101用户, 用户组 , 用户权限管理: Linux 分为三类用户  root 用户(超级用户):有最高权限,可操作任何文件和命令 虚拟用户: 这类用户不具有系统登录能力, 但是系统运行需要. 比如 bin, daemon, adm, ftp ,mail 普通真实用户: 能登录系统, 但只能操作其根目录的内容, 权限受到限制. 这类用户都是系统管理员添加的. 相关命令 whoami, useradd, userdel, passwd, su - , sudo, visudo, groupadd, groupdel, gpasswd, groups, chsh, chown, chgrp, chage, usermod:       命令_______   说明         whoami   查看当前登录的用户名       useradd   添加用户，会在/home下创建一个同名的目录. 常用参数 -G 组, -e 过期时间, 更多解释看表格下面的解释.        userdel   删除用户，’-r’参数可以一并删除用户相关目录.        passwd   设置指定用户的密码，不指定时设置当前用户       su -   切换用户，一定要加’-‘，若不指定用户，切换到root用户       sudo   临时切换到指定用户(root)执行命令，如：sudo service iptables stop       visudo   修改配置文件/etc/sudoers，如给dalong sudo 权限：dalong ALL=(ALL) ALL 不要直接编辑这个文件; 如果是给一个用户组sudo 权限,: %tgroup ALL=(ALL) ALL       groupadd   添加用户组       groupdel   删除用户组       gpasswd   将指定的用户组添加/删除指定组，gpasswd -a/-d 用户 用户组.  比如将dalong 放到       groups   查看用户属于哪个组.        chsh   修改指定用户的shell . 比如: chsh xiaoming -s /sbin/nologin 让某用户无法       chown   改变文件的拥有者[:组] chown xiaoming . /myFile. txt chown root:root . /myfile. txt 常用参数 -R 递归更改       chgrp   改变文件的拥有组 chgrp xiaoming . /myfile. txt       chage -l dalong   查看用户的账号有效期限, 去掉 -l 就是修改过期时间.        usermod   修改用户信息 , 比如修改用户账号的到期时间 usermod -e  Dec 28,2017  t1 修改t1用户的到期日期. 修改后/etc/shadow最后   涉及文件 /etc/passwd /etc/shadow /etc/group 123/etc/passwd：系统中的所有用户信息/etc/shadow：用户的密码/etc/group：系统中的所有用户组信息创建用户的注意事项  -u: 创建用户时指定的UID , 但必须这个id不存在,并且必须要大于499, 因为小于500的已经被系统占用.  -g/G: g 创建用的的同时,只为用户分配指定的组. G 除了用户本身的组, 同时添加到指定的组 -c: 给用户增加一些注释. usermod 命令 修改用户信息的最常用命令. 包括主目录, 所属组, 登录shell: 参数  -d: 登录目录 -e: 账号有效期, 单位为天 -g: 所属组 -l: 修改登录名 -p: 修改密码.  -s: 修改默认shell. 注意:  不要用 usermod 修改密码,因为这样修改的密码是明文显示在 shadow文件中.  不能用usermod 修改已登录用户的用户名passwd 命令: 12345passwd -l: This option is to lock the password for the user's accountpasswd -u: This option is to unlock the password for the user's accountpasswd -e: This option is to expire the password for the userpasswd -x: This option is to define the maximum days for the password lifetimepasswd -n: This option is to define the minimum days for the password lifetime可以用 chage 做更多用户过期相关的命令操作. /etc/passwd 文件: 详细解释 格式如下: 1注册名：口令(x)：用户标识号：组标识号：用户注释：用户主目录：命令行解释程序(shell) 设置密码 有效期 和 密码长度 /etc/login. defs: 密码有效期和密码长度在 /etc/login. defs 文件中设置 对该文件的修改不会影响现有用户, 只会影响修改后创建的新用户 1234#  PASS_MAX_DAYS  Maximum number of days a password may be used. #  PASS_MIN_DAYS  Minimum number of days allowed between password changes. #  PASS_MIN_LEN  Minimum acceptable password length. #  PASS_WARN_AGE  Number of days warning given before a password expires. 设置密码 复杂度 和 密码重设要求 /etc/pam. d/system-auth: 设置密码复杂度和重设要求的配置文件在 /etc/pam. d/system-auth 文件中设置 12345678password requisite pam_cracklib. so try_first_pass retry=3 type= ucredit=-2 lcredit=-2 dcredit=-2 ocredit=-2 #ucredit=-2 大写字母最少2个 #lcredit=-2 小写字母最少2个#dcredit=-2 数字最少2个#ocredit=-2 字符最少2个 password sufficient pam_unix. so sha512 shadow nullok try_first_pass use_authtok remember=5#remember=5 重设密码不能使用过去5次内使用过的密码密码输入错误的相关设置 /etc/pam. d/password-auth /etc/pam. d/login: 设置密码如果连续输错3次,那么需要等5分钟后再输入. 相关的3个配置文件:  /etc/pam. d/password-auth /etc/pam. d/system-auth /etc/pam. d/login 下面代码分别放入password-auth 和 system-auth ; 只有第一行放入 login12auth    required  pam_tally2. so file=/var/log/tallylog deny=3 no_magic_root unlock_time=300account   required  pam_tally2. so让用户登录失败的命令 pam_tally2 --user user1 重置用户,让用户可以继续登录的命令 pam_tally2 --user user1 --reset /etc/shadow 文件: 详细解释 修改密码和有效性等信息:  passwd t1 chage t1格式如下: 1234567891011121314151617登录名:加密口令(!!):最后一次修改时间:最小时间间隔:最大时间间隔:警告时间:不活动时间:失效时间:标志1）“登录名”是与/etc/passwd文件中的登录名相一致的用户账号2）“口令”字段存放的是加密后的用户口令字，如果为空，则对应用户没有口令，登录时不需要口令；  *星号代表帐号被锁定；!!双叹号表示这个密码已经过期了。$6$开头的，表明是用SHA-512加密的，$1$ 表明是用MD5加密的$2$ 是用Blowfish加密的$5$ 是用 SHA-256加密的。 3）“最后一次修改时间”表示的是从某个时刻起，到用户最后一次修改口令时的天数。时间起点对不同的系统可能不一样。例如在SCOLinux中，这个时间起点是1970年1月1日。4）“最小时间间隔”指的是两次修改口令之间所需的最小天数。5）“最大时间间隔”指的是口令保持有效的最大天数。6）“警告时间”字段表示的是从系统开始警告用户到用户密码正式失效之间的天数。7）“不活动时间”表示的是用户没有登录活动但账号仍能保持有效的最大天数。8）“失效时间”字段给出的是一个绝对的天数，如果使用了这个字段，那么就给出相应账号的生存期。期满后，该账号就不再是一个合法的账号，也就不能再用来登录了。磁盘/文件系统管理服务: 磁盘使用步骤: 分区 =&gt; 格式化 =&gt; 挂载 /dev/sda 相关配置文件:    /etc/fstab      记录了计算机上硬盘分区的相关信息，启动 Linux 的时候，检查分区的 fsck 命令，和挂载分区的 mount 命令，都需要 fstab 中的信息，来正确的检查和挂载硬盘。      /etc/mtab      记载的是现在系统已经装载的文件系统，包括操作系统建立的虚拟文件等；而/etc/fstab是系统准备装载的。   每当 mount 挂载分区、umount 卸载分区，都会动态更新 mtab，mtab 总是保持着当前系统中已挂载的分区信息，fdisk、df 这类程序，必须要读取 mtab 文件，才能获得当前系统中的分区挂载情况。当然我们自己还可以通过读取/proc/mount也可以来获取当前挂载信息    ​  fdisk -l # 查看系统磁盘分布:  多个磁盘命名: sda, sdb, sdc 分区命名: sda1, sda2 表示这块磁盘的第一个分区和第二个分区     一个磁盘最多分4个主分区. 其中一个可以是扩展分区.    主分区可以安装操作系统, 扩展分区不能直接使用(可以分多个逻辑分区)   mount 挂载的查看和设置命令 用命令:  mount 配置文件决定了开机需要自动挂载的磁盘.      /etc/fstab 格式如下:         磁盘分区名 挂载点 文件系统类型 选项(iocharset=utf8) 备份 检查     选项的内容:             rw 可读可写       auto ?       async 异步?       iocharset=utf8       default 包含上面几个?                     mount -a # 重新解析/etc/fstab 文件, 完成挂载. 用于在修改完配置信息后, 再按照配置文件重新挂一遍    mount #不带参数查看当前挂载情况 语法: mount [选项] 磁盘分区 挂载点     -o 指定字符集: iocharset=utf8   -t 指定文件系统类型:         msdos(fat16),     vfat(fat32),     auto(自动识别)     ntfs-3g*(ntfs). linux默认不支持, 需要安装一个扩展          12mount -o iocharset=utf8 /dev/sdc1 /mnt/umount /dev/sdc1 分区: 对一个已有的磁盘进行分区, 比如 fdisk /dev/sdb: 一块磁盘可以最多有四个分区(Linux), 最多三个主分区,剩下一个是扩展分区. 一块磁盘只能有一个扩展分区  fdisk /dev/sdb # 对磁盘进行分区, m for help12345678910111213141516171819202122232425262728293031323334353637383940命令(输入 m 获取帮助)：m    命令操作  a  toggle a bootable flag  b  edit bsd disklabel  c  toggle the dos compatibility flag  d  delete a partition 删除分区  g  create a new empty GPT partition table  G  create an IRIX (SGI) partition table  l  list known partition types  m  print this menu  n  add a new partition 新建分区  o  create a new empty DOS partition table  p  print the partition table 打印分区信息  q  quit without saving changes 不保存退出  s  create a new empty Sun disklabel  t  change a partition's system id  u  change display/entry units  v  verify the partition table  w  write table to disk and exit 保存分区修改,退出  x  extra functionality (experts only)格式化 mkfs: centos 7 不带 vfat 格式的格式化, 需要安装 mkfs. vfat 1yum install dosfstools命令:  mkfs #查看可可视化的分区 mkfs. ext2 mkfs. ext3 mkfs. ext4 mke2fs12mkfs. ext4 /dev/sdb1 # 用 ext4格式化sdb1mke2fs -t ext3 /dev/sdb5 # 这个命令需要指定文件系统类型查看磁盘使用情况的命令 df -Th du: df 查看磁盘分区使用情况, 包括已用空间, mount点, 文件系统类型  -T: print file system type -h: 人能看懂的单位格式du 递归统计指定目录的使用情况  -h: 人能看懂的单位格式 -d: 递归最大深度实验: 为virtual box 虚拟机 dalongCentOS7 添加新磁盘,分区,挂载并格式化: 进入vbox中的配置-&gt;storage-&gt;controller 创建一个新的虚拟硬盘  add hard disk重新启动dalongCentOS7 虚拟机 为新硬盘分区 123fdisk -l # 查看是否找到/dev/sdbfdisk /dev/sdb # 开始分区 p # 查看现有分区情况, 这是还是什么都没有 n # 创建主分区(Linux), 分配空间+200M; 创建扩展分区(Extended) 分配所有空间. 创建3个逻辑分区(Linux) , 分配剩下所有空间.  w # 保存退出格式化新建的分区为 1mkfs. ext4 /dev/sdb1 # 格式化新建的分区为 xfs格式. 同时把sdb1 ~ sdb 7 都格式化掉为新格式化的分区建立挂载点 12mkdir /mnt/1 # 为新格式化的分区建立挂载点. 同时把 1~7 的目录都创建出来mnt /dev/sdb1 /mnt/1 # 将每个分区挂载到对应的目录挂载点上. 为了让系统重启后制动挂载, 需要编辑 /etc/fatab 文件 12/dev/sdb1 /mnt/b1                xfs   defaults 0 0/dev/sdc5 /mnt/c5                ext4  defaults 0 0磁盘配额 qutas: 简单的说就是限制用户对磁盘空间的使用量。  因为Linux是多用户多任务的操作系统，许多人共用磁盘空间，为了合理的分配磁盘空间，于是就有了quota的出现。 用途:  针对WWW server 针对mail server 针对file server创建磁盘镜像文件 virtual block device dd losetup: 设想一个场景：你要用U盘在两台Linux主机之间移动文件，如果U盘是exfat格式的，那么这个文件移动到U盘之后，文件的所有者以及权限信息将会全部丢失，移动到新的主机上又要重新设置一遍。在文件多而且权限复杂的时候真的非常令人崩溃！假设U盘不能格式化，那有什么方法解决这个问题呢？那就是磁盘映像啦～ macOS上的磁盘工具可以轻松地创建磁盘映像(dmg文件)，而且功能非常丰富。Linux当然也可以啦～只要你是真的喜欢Linux！ 用 dd 创建一个1G 的空磁盘镜像文件 /tmp/test. img 123# 创建一个1G 的空虚拟文件 /tmp/test. imgdd if=/dev/zero of=/tmp/test. img bs=1M count=100 # 如果存在 /dev/zero 就创建/tmp/test. img#1048576000字节(1. 0 GB)已复制，1. 62677 秒，645 MB/秒losetup将镜像文件模拟成一个回环磁盘 123456losetup -fP /tmp/test. img # 创建磁盘# -f, --find          查找第一个未使用的设# -P, --partscan        创建分区的回环设备losetup -a # 查看已经生成的磁盘# /dev/loop0: [64768]:6123119 (/tmp/test. img) 默认会将磁盘放在 dev/loopN的地方对磁盘进行分区, 并进行格式化 12345678# 为这个磁盘创建一个扩展分区, 为扩展分区再创建一个逻辑分区fdisk /dev/loop0#   设备 Boot   Start     End   Blocks  Id System#/dev/loop0p1      2048   204799   101376  5 Extended#/dev/loop0p5      4096   204799   100352  83 Linux#格式化成功 xfs 格式mkfs. xfs /dev/loop0p5mount 到创建的挂载点上 123mkdir /mnt/myImgmount /dev/loop0p5 /mnt/myImg/df -Th网络服务: 相关命令: ip addr , ping, ifup, ifdown, systemctl:       命令   说明         ip addr   查看网卡信息       ping   检查网络连通性       ifup   启动网卡       ifdown   关闭网卡       systemctl start/stop/status network. service   开启关闭,查看网络服务   网卡配置文件: /etc/sysconfig/network-scripts/ifcfg-enp0s3: 服务监测:  netstat (centos7没有了)     centos7中用 ss 代替了. 查看补充信息   说明: 查看网络端口使用说明   参数:         -l 监听     -p 显示pid     -t     -u     -n           ip -s link     说明: 网络接口统计信息                  ip route     column -t              说明: 显示路由表   添加静态路由         ip route add 10. 15. 150. 0/24 via 192. 168. 150. 253 dev enp0s3          删除静态旅游         ip route del 10. 15. 150. 0/24          系统监控: free:  说明:-h 查看内存使用情况 w     说明: 当前用户正在做的事情,包含了用户,终端, 从哪里连接, 登录时间, 空闲时间, 占用cpu情况, 正在做什么.    top:  说明: 查看本机正在运行的系统服务的详细信息. 相当于 详细版本的w. 每三秒刷新一次.  第一行字段包含: 登录时间, 登录人数 负载情况.  第二行字段 - 进程汇总: 运行中的进程, 睡眠中的进程 第三行字段 - CPU 使用情况: 用户占比, 系统占比, …, 第四行字段 - 内存使用情况: 第五行字段 - 交换分区: 动态部分 - 系统进程信息:   q 退出   vmstat     说明: 虚拟内存使用情况   字段说明: 进程, 内存, 交换分区, io, 系统, cpu   进程管理: ps aux , ps -ef:  说明: 查看进程情况 -e all processes -f full a 所有 u 显示用户信息   x 显示没有终端的进程     kill      说明: 杀掉进程   选项: -9 强制杀死 一个进程ID    ​  服务管理: 老版本的是 service Linux Systemctl是一个系统管理守护进程(后台运行程序)的管理命令，初始进程主要负责控制systemd系统和服务管理器。 所有开启自动启动的服务都会Created symlink from /etc/systemd/system/multi-user. target. wants/服务名 to /usr/lib/systemd/system/服务名. systemctl 命令 , 老版本是 service 命令:    列出所有已安装的可用单元      systemctl list-unit-files      列出所有运行中单元      systemctl list-units      检查某个单元（如 crond. service）是否启用      systemctl is-enabled crond. service      列出所有服务 / 查找某项服务是否启动                            systemctl -t service -a –state running       grep chrony                        Linux中如何启动、重启、停止、重载服务以及检查服务（如 httpd. service）状态          systemctl start httpd. service           systemctl restart httpd. service           systemctl stop httpd. service           systemctl reload httpd. service           systemctl status httpd. service     注意：当我们使用systemctl的start，restart，stop和reload命令时，终端不会输出任何内容，只有status命令可以打印输出。          如何激活服务并在开机时启用或禁用服务（即系统启动时自动启动mysql. service服务）      systemctl is-active mysql. service   systemctl enable mysql. service   systemctl disable mysql. service      使用systemctl命令杀死服务      systemctl kill crond      列出所有系统挂载点      systemctl list-unit-files –type=mount      挂载、卸载、重新挂载、重载系统挂载点并检查系统中挂载点状态      systemctl start tmp. mount   systemctl stop tmp. mount   systemctl restart tmp. mount   systemctl reload tmp. mount   systemctl status tmp. mount      在启动时激活、启用或禁用挂载点（系统启动时自动挂载）   # systemctl is-active tmp. mount   # systemctl enable tmp. mount   # systemctl disable tmp. mount   \13. 在Linux中屏蔽（让它不能启用）或可见挂载点   # systemctl mask tmp. mount   ln -s ‘/dev/null”/etc/systemd/system/tmp. mount’   # systemctl unmask tmp. mount   rm ‘/etc/systemd/system/tmp. mount’   \14. 列出所有可用系统套接口   # systemctl list-unit-files –type=socket   \15. 检查某个服务的所有配置细节   # systemctl showmysql   \16. 获取某个服务（httpd）的依赖性列表   # systemctl list-dependencies httpd. service   \17. 启动救援模式   # systemctl rescue   \18. 进入紧急模式   # systemctl emergency   \19. 列出当前使用的运行等级   # systemctl get-default   \20. 启动运行等级5，即图形模式   # systemctl isolate runlevel5. target   或   # systemctl isolate graphical. target   \21. 启动运行等级3，即多用户模式（命令行）   # systemctl isolate runlevel3. target   或   # systemctl isolate multiuser. target   \22. 设置多用户模式或图形模式为默认运行等级   # systemctl set-default runlevel3. target   # systemctl set-default runlevel5. target   \23. 重启、停止、挂起、休眠系统或使系统进入混合睡眠   # systemctl reboot   # systemctl halt   # systemctl suspend   # systemctl hibernate   # systemctl hybrid-sleep   对于不知运行等级为何物的人，说明如下。   Runlevel 0 : 关闭系统   Runlevel 1 : 救援，维护模式   Runlevel 3 : 多用户，无图形系统   Runlevel 4 : 多用户，无图形系统   Runlevel 5 : 多用户，图形化系统   Runlevel 6 : 关闭并重启机器  centos6的 chkconfig 配置服务启动项,: 作用: 查看/设置一个服务在不同的运行级别的状态, 修改后不会立即生效. 格式: chkconfig [选项][操作][参数] 选项 1234--list: 查看service中的信息--add : 新添加服务到service--del : 从service中删除服务--level: 指定级别. 默认是2345操作 12on: 开启off: 关闭举例 12345chkconfig iptables off # 关闭防火墙, 修改后重启才能生效. chkconfig --list # 查看修改结果service iptables status # 查看修改后对iptables 的影响chkconfig --del iptableschkconfig --add iptables添加服务到启动项中 12341. 创建一个服务控制脚本, 放在/etc/rc. d/init. d 目录下2. 给该脚本添加可执行权限 , 比如 7553. /etc/rc. d/rc3. d 下面有很多文件. 每当使用 chkconfig --add 某服务, 就会在该目录下生成一个新的软链接文件. 该文件下开启的服务是 S 开头, 关闭的服务时 K 开头系统安全: redhat7 安全指南 selinux 的使用: selinux 是一套linux 安全管理方案, 比较复杂, 一般都要关掉  getenforce     说明: 查看selinux 状态, Enabled 是开启, Disabled 是关闭    setenforce 0     说明: 设置selinux 关闭(Disabled)    vi /etc/selinux/config     设置 SELINUX=disabled 将selinux 永久关闭.    防火墙命令 firewall-cmd firewall-config: 网上教程 redhat 官方说明  墙后台程序 firewalld 提供了一个 动态管理的防火墙，用以支持网络 “zones” ，以分配对一个网络及其相关链接和界面一定程度的信任。它具备对 IPv4 和 IPv6 防火墙设置的支持。它支持以太网桥，并有分离运行时间和永久性配置选择。它还具备一个通向服务或者应用程序以直接增加防火墙规则的接口。 zone 区域  默认区配置文件都放在 /usr/lib/firewalld/zones/ 目录下, 该目录的文件不能直接编辑, 需要拷贝到 /etc/firewalld/zones/ 才能开始编辑 两个目录是覆盖关系, 既如果etc 下面的文件存在,则优先使用, 否则使用 usr 目录下的.  比如: cp /usr/lib/firewalld/zones/work. xml /etc/firewalld/zones/ drop（丢弃）  任何接收的网络数据包都被丢弃，没有任何回复。仅能有发送出去的网络连接。  block（限制）  任何接收的网络连接都被 IPv4 的 icmp-host-prohibited 信息和 IPv6 的 icmp6-adm-prohibited 信息所拒绝。  public（公共）  在公共区域内使用，不能相信网络内的其他计算机不会对您的计算机造成危害，只能接收经过选取的连接。  external（外部）  特别是为路由器启用了伪装功能的外部网。您不能信任来自网络的其他计算，不能相信它们不会对您的计算机造成危害，只能接收经过选择的连接。  dmz（非军事区）  用于您的非军事区内的电脑，此区域内可公开访问，可以有限地进入您的内部网络，仅仅接收经过选择的连接。  work（工作）  用于工作区。您可以基本相信网络内的其他电脑不会危害您的电脑。仅仅接收经过选择的连接。  home（家庭）  用于家庭网络。您可以基本信任网络内的其他计算机不会危害您的计算机。仅仅接收经过选择的连接。  internal（内部）  用于内部网络。您可以基本上信任网络内的其他计算机不会威胁您的计算机。仅仅接受经过选择的连接。  trusted（信任）  可接受所有的网络连接。 命令行工具 firewall-cmd 是默认安装的应用程序 firewalld 的一部分。  firewall-cmd 是 firewalld 服务的命令行工具. 注意:  为了设置一个永久或者可执行命令，除了 –direct 命令（它们本质上是暂时的）之外，要向所有命令添加 –permanent 选择。注意，这不只是意味着永久更改，而且更改将仅仅在防火墙重新加载、服务器重启或者系统重启之后生效 缺少 –permanent 选项的设定能立即生效，但是它仅仅在下次防火墙重新加载、系统启动或者 firewalld 服务重启之前可用启用/禁用防火墙:  注: iptable 和 firewall 不能同时使用1234systemctl disable firewalldsystemctl stop firewalldsystemctl enable firewalldsystemctl start firewalld1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374#=====================基本状态查看# 查看防火墙运行情况firewall-cmd --state # running# 查看本机活跃的网卡firewall-cmd --get-active-zones # public interfaces: enp0s9 enp0s3# 获取public zone 的 内容列表 , public 意思是所有公共可以访问本机的内容. firewall-cmd --zone=public --list-all # # 查看public zone 的网卡firewall-cmd --zone=public --list-interfaces # enp0s9 enp0s3# 查看所有public 下的开放端口firewall-cmd --zone=public --list-ports # 8081/tcp , 这个是之前安装nginx时打开的端口# 查看某张网卡的区域firewall-cmd --get-zone-of-interface=enp0s3 # public # 查看所有活跃的网络区域firewall-cmd --get-service#=====================网络开关# 终止所有数据包的输入和输出的数据包, 终止后一段时间, 所有连接将断掉firewall-cmd --panic-on# 恢复/启动 数据传输的输入和输出, firewall-cmd --panic-off# 查看panic 模式的状态, no 是关闭, yes 是开启. no 表示不禁止. yes 表示禁止firewall-cmd --query-panic#=====================重新加载防火墙# 不中断的重新加载防火墙firewall-cmd --reload# 中断的重新加载防火墙, 通常在防火墙出现严重问题的时候才会用到这个命令firewall-cmd -complete-reload#=====================添加网卡接口firewall-cmd --zone=public --add-interface=em1firewall-cmd --zone=public --list-interface#======================设置默认zone 区域vim /etc/firewalld/firewalld. conf 设置 DefaultZone=home#或者firewall-cmd --set-default-zone=public#====================开放一个端口#增加 --permanent 选项并重新加载防火墙，使之成为永久性设置firewall-cmd --zone=public --add-port=8080/tcp firewall-cmd --zone=public --add-port=5060-5061/udp # 增加一系列端口, 用 -firewall-cmd --zone=public --list-ports # 8081/tcp 8080/tcp 5060-5061/udp , 加进去了. firewall-cmd --reload#====================将一个服务添加/删除 zone 区firewall-cmd --zone=work --add-service=smtpfirewall-cmd --zone=work --remove-service=smtpfirewall-cmd --reload#====================配置端口转发# 要将进入网络的程序包从一个端口转发到一个替代端口或者地址, 首先, 在external zong 允许伪装IPfirewall-cmd --zone=external --add-masquerade # 将本来要送到 22 端口的程序包转发到 3753 端口。源目的端口用 port 选项指定。这个选项可以是一个端口，或者一组端口范围并加上协议。如果指定协议的话，这个协议必须是 tcp 或 udp。这个新的本地端口，即流量被转发过去的端口或者端口范围，需用 toport 选项指定。增加 --permanent 选项并重新加载防火墙，可以使设置永久保存。firewall-cmd --zone=external --add-forward-port=port=22:proto=tcp:toport=3753 # 原本发往22端口的程序包现在被转发到相同的端口，地址则由 toaddr 提供。firewall-cmd --zone=external --add-forward-port=port=22:proto=tcp:toaddr=192. 0. 2. 55# 转发既改变端口号, 也指定IPfirewall-cmd --zone=external --add-forward-port=port=22:proto=tcp:toport=2055:toaddr=192. 0. 2. 55#====================增加/删除一个自定义规则 # 使用 --direct 选项在运行时间里增加或者移除 规则# 创建一个叫做 IN_public_allow 的规则: firewall-cmd --direct --add-rule ipv4 filter IN_public_allow 0 -m tcp -p tcp --dport 666 -j ACCEPT# 删除一个叫做 IN_public_allow 的规则: firewall-cmd --direct --remove-rule ipv4 filter IN_public_allow 0 -m tcp -p tcp --dport 666 -j ACCEPT限制某IP 禁止访问ssh 服务 1firewall-cmd --permanent --add-rich-rule='rule family=ipv4 source address= 123. 44. 55. 66  drop'富规则: 概念  通过 “rich language” 语法，可以用比直接接口方式更易理解的方法建立复杂防火墙规则。此外，还能永久保留设置。这种语言使用关键词值，是 iptables 工具的抽象表示。这种语言可以用来配置分区，也仍然支持现行的配置方式。 语法: 1234567rule [family= &lt;rule family&gt; ] # {ipv4|ipv6|}  [ source address= &lt;address&gt;  [invert= True ] ] # ip 地址可以用 /线 来指定区间  [ destination address= &lt;address&gt;  [invert= True ] ] # ip 地址可以用 /线 来指定区间  [ &lt;element&gt; ] # 规则元素只能是 service ， port ， protocol ， masquerade ， icmp-block 和 forward-port 中的一个   [ log [prefix= &lt;prefix text&gt; ] [level= &lt;log level&gt; ] [limit value= rate/duration ] ]  [ audit ]  [ accept|reject|drop ] #只能是其中一个 上面的规则元素只能是 service ， port ， protocol ， masquerade ， icmp-block 和 forward-port 中的一个     firewall-cmd –get-services 获得所有被预定义的 service.    firewall-cmd –list-services 获得所有已经添加到zone 中的 services   vim /etc/protocols 可以查看所有支持的protocol   对特定ip 禁止访问shh服务 （ip shh可以自己改别的 最后面的reject是禁止的意思也可以换成允许的英文，） 12firewall-cmd --permanent --zone=public --add-rich-rule= rule family= ipv4  source address= 192. 168. 10. 0/24  service name= ssh  reject  firewall-cmd --reload # 让规则生效下面的是对特定ip允许访问8080端口（你也可以自己改） 12firewall-cmd --permanent --zone=public --add-rich-rule= rule family= ipv4  source address= 192. 168. 0. 4/24  port protocol= tcp  port= 8080  accept firewall-cmd --reload # 让规则生效列出已经添加的富规则 1firewall-cmd --list-rich-rules # 如果之前没有 firewall-cmd --reload, 那么这里可能看不到讲一个IP地址加入白名单 1rule family= ipv4  source address= 192. 168. 2. 2  accept删除已经添加的富规则  方法就是先 firewall-cmd --list-rich-rules 看输出的规则列表, 然后想删哪个就把这条拷贝出来黏贴到 –remove-rich-rule 的参数中 1234567firewall-cmd --remove-rich-rule= rule family= ipv4  source address= 192. 168. 10. 0/24  service name= ssh  reject firewall-cmd --remove-rich-rule= rule family= ipv4  source address= 192. 168. 0. 4/24  port port= 8080  protocol= tcp  accept firewall-cmd --list-rich-rulesfirewall-cmd --reloadiptables 是防火墙(centos6中使用):  操作: service iptables start|stop|restart|status 永久关闭: chkconfig iptables off 配置文件: /etc/sysconfig/iptables-config 规则管理:     单次生效: iptables 命令   永久生效: 修改配置文件 vim /etc/sysconfig/iptables-config   举例, 限制某IP ssh 登录本机 123456iptables -A INPUT -s 10. 11. 56. 93 -p tcp --dport 22 -j REJECT# A 决定# -s 是源头# -p protocal 类型# --deport 设置连接类型 22# -j 远程会话 screen: 需求: 系统管理员远程连接服务器. 出现耗时操作, 中间不能间断. screen 的概念是先创建会话, 会话中可以创建多个窗口. 每个窗口都是一个子会话. 离开窗口或会话后,程序不会断掉. 1yum install -y screen 创建新会话:     screen -S hello # 进入新的会话    查看已创建会话:     screen -ls    暂时离开会话:     control + a + d    返回一个会话     screen -r hello # 返回hello 这个会话.     退出并结束一个会话     exit    在会话外面结束一个会话     screen -S hello -X quit    在同一个会话中创建多个窗口     control + a + c   control + a + w  # 查看所有窗口, 其他中有* 的是当前窗口    在多个窗口中切换     control + a + a  # 两个窗口来回切换   control + a + n  # 下一个窗口   control + a + p  # 上一个窗口    关掉一个窗口     control + a 松手 k # 关掉当前窗口   服务配置/软件安装: yum 基本命令 list installed|grep , check-update, update, search kw, info 包名, install, remove, clean all, makecache:    YUM 缩写是 yellowdog updater, modified  redhat 系列linux 使用 yum 进行软件安装  yum 是redhat 系列的发行版的软件包管理工具. 可以自动解决包的依赖关系.   yum 包放在软件仓库中. 可以是redhat 官方的, 可以是阿里,腾讯的.   yum 的配置文件在 /etc/yum. repos. d/ 目录下  常用 yum 源.        网易: mirrors. 163. com    阿里: mirrors. aliyun. com       常用参数:       -y 安装过程中默认都选yes    –downloadonly 只下载不安装    –downloaddir: 指定下载目录    list installed: 已经安装的软件    groupinstall: 组件安装    groupremove: 组件删除    makecache: 生成缓存, 将源的所有包的信息缓存到本地,可以加快搜索速度    clean all: 清除缓存,      常用参数和用法:  yum list installed |grep python 查看已经安装过的包 yum check-update 检查已安装的包是否有更新 yum -y update 完整所有软件的更新. -y 是默认同意选项.      更新完后最好重启一下服务 reboot    yum search keyword 搜索软件名, 可能会列出一大堆 yum info package_name 查看某个搜出来的包的详细信息   yum deplist package_name 查看一个包的依赖关系   yum install 包名 安装软件     -y 用来默认让所有的问题都回答 yes   yum install -y 包1 包2 包3 一次安装多个包    yum remove 包名 删除安装的包     yum remove -y 包1 包2 包3 一次删除多个包   yum remove -y vim* 模糊删除vim 开头的软件包.    为 yum 添加阿里云镜像: 12345mv /etc/yum. repos. d/CentOS-Base. repo /etc/yum. repos. d/CentOS-Base. repo. backupcurl -o /etc/yum. repos. d/CentOS-Base. repo http://mirrors. aliyun. com/repo/Centos-7. repoyum clean all # 清理缓存yum makecache # 生成缓存yum -y update # 验证是否已经是aliyun为 yum 添加 网易 镜像: 网易的帮助 12345mv /etc/yum. repos. d/CentOS-Base. repo /etc/yum. repos. d/CentOS-Base. repo. backupcurl -o /etc/yum. repos. d/CentOS-Base. repo http://mirrors. 163. com/. help/CentOS7-Base-163. repoyum clean all # 清理缓存yum makecache # 生成缓存yum -y update # 验证是否已经是网易镜像为 yum 添加 epel 源 IUS 源: PEL (Extra Packages for Enterprise Linux)是基于Fedora的一个项目，为“红帽系”的操作系统提供额外的软件包，适用于RHEL、CentOS和Scientific Linux. IUS 源 12yum install epel-releaseyum install https://centos7. iuscommunity. org/ius-release. rpm为 yum 下载加速: yum-axelget 是 EPEL 提供的一个 yum 插件。使用该插件后用 yum 安装软件时可以并行下载，大大提高了软件的下载速度，减少了下载的等待时间: 12yum install yum-axelgetyum update用yum 安装基础开发环境: 12345678910yum install gcc           # C 编译器yum install gcc-c++         # C++ 编译器yum install gcc-gfortran      # Fortran 编译器yum install compat-gcc-44      # 兼容 gcc 4. 4yum install compat-gcc-44-c++    # 兼容 gcc-c++ 4. 4yum install makeyum install gdb   # 代码调试器yum install cmake  # Cmakeyum install git   # 版本控制yum install ntfs-3g # CentOS 下默认无法挂载 NTFS 格式的硬盘。需安装 nfts-3g 即可实现即插即用RPM 包的安装和使用 -ivh, -Uvh, -qlp, -qip, -ql, -qf, -qa|grep:  说明: RPM 是 redhat package manager 的缩写, 现在是 PRM Package Manager 的缩写.  也是redhat 系列发行版的软件包. 后缀 (. rpm) rpm 包通常都依赖其他软件包. 命令和参数: 12345678910rpm # 命令-i: 安装-e: 卸载-v: 显示安装信息-h: 显示安装进度-U: 更新-q: 查询-qa: 查询所有-ql: 显示rpm软件包的安装目录-qf: 查询该文件是哪个rpm软件包安装的常用参数组合 -ivh, -Uvh, -qlp, -qip, -ql, -qf, -qa|grep 1234567rpm -ivh remi-release-7. rpm : 安装rpm -Uvh remi-release-7. rpm : 升级rpm -qlp remi-release-7. rpm : 查看包会安装什么文件rpm -qip remi-release-7. rpm : 查看包的说明书rpm -ql vim-enhanced-7. 4. 160-2. el7. x86_64 : 查看已安装的包装了什么文件rpm -qf /usr/bin/vim		: 查看一个文件是什么包装的rpm -qa|grep vim 			: 搜索一个曾经装过的包实例: 12345678910111213141516yum remove -y vim* #删除yum install -y vim --downloadonly --downloaddir . / # 下载vim 三个软件包到当前目录rpm -ivh vim-filesystem-7. 4. 160-2. el7. x86_64. rpmrpm -ivh vim-common-7. 4. 160-2. el7. x86_64. rpmrpm -ivh vim-enhanced-7. 4. 160-2. el7. x86_64. rpmrpm -ql vim-enhanced-7. 4. 160-2. el7. x86_64 # 查看软件包将把文件安装到哪个目录. 包名不要带后缀#/etc/profile. d/vim. csh#/etc/profile. d/vim. sh#/usr/bin/rvim#/usr/bin/vim#/usr/bin/vimdiff#/usr/bin/vimtutorrpm -qf /etc/profile. d/vim. sh # 查看某个文件是哪个软件包安装的. #vim-enhanced-7. 4. 160-2. el7. x86_64原码安装 . configure, make, make install PHP7. 2 Python3. 7安装为例子: 基本步骤:  下载源码包 wget 解压: tar -zxvf 包 进入解压的目录 准备依赖包和编译工具(gcc) yum install -y gcc 配置: configure 一般包中都有一个配置文件 编译: make 预编译 =&gt; 编译 =&gt; 汇编 =&gt; 安装: make install实例演示php7. 2: 123456789101112131415161718cd /root/code/wget http://cn2. php. net/distributions/php-7. 2. 0. tar. gztar -zxvf php-7. 2. 0. tar. gzcd php-7. 2. 0vi . /configure # 可以看配置文件中有哪些参数和内容. /configure --prefix=/usr/local/php7# 报错 configure: error: no acceptable C compiler found in $PATHyum install -y gcc. /configure --prefix=/usr/local/php7# 又报错 configure: error: xml2-config not found. Please check your libxml2 installation. yum install libxml2yum install libxml2-devel -y. /configure --prefix=/usr/local/php7make &amp;&amp; make install# 为命令创建软链接到 PATH 对应的目录下ln -s /usr/local/php7/bin/php /root/bin/phpexport PATH=/usr/local/php7/bin/:$PATHecho $PATH实例演示Python 3. 7 12345678910wget https://www. python. org/ftp/python/3. 7. 0/Python-3. 7. 0a3. tgztar -zxvf Python-3. 7. 0a3. tgzcd Python-3. 7. 0a3. /configure --prefix=/usr/local/python3. 7make &amp;&amp; make install#报错了提示少了个 _ctype ,网上查说少了个 libffi-develyum install libffi-develmake &amp;&amp; make installln -s /usr/local/python3. 7/bin/python3. 7 /root/bin/python3. 7ln -s /usr/local/python3. 7/bin/pip3. 7 /root/bin/pip3. 7压缩解压缩软件 zip unzip: zip unzip    使用: linux 下没有自带,需要安装     yum install -y zip unzip  123# 压缩解压命令zip a. zip . /*. * # 压缩当前目录所有文件unzip a. zip压缩解压缩软件gzip/gunzip:  说明,只能对文件进行压缩,不能压缩目录, 压缩文件以. gz结尾 gzip -d 也可以解压缩 压缩文件不会保留原有文件. 解压缩也不会保留原有压缩包123gzip . /*. * gunzip . /*. *压缩解压缩软件bzip2/bunzip2:  yum install bzip2 bunzip2 说明: 使用方式和gzip/gunzip 一样. 默认后缀为. bz212bzip2 1. htmlbunzip2 -k 1. html. bz2 # 加 -k 参数可以保留压缩文件. 打包解包 tar -zxvf -zcvf:  可以打包,解包, 压缩, 解压, 包后缀是. tar 选项: (头三个参数互斥) , 最常用的组合 -zcvf -zxvf     -c 打包, 创建新包   -x 解包,   -t 看看包里有什么, 但不打开   -f 指定文件   -v 命令输出详细信息   -z 调用gzip/gunzip 压缩/解压   -j 调用bzip2/bunzip2 压缩/解压   -C 大C 指定压缩到的目录   –exclude: 排除   12345678910111213141516171819# ==========打包/拆包tar -cvf city. tar city2. html city3. html  # 将 2个文件达成一个包. 并不压缩tar -tf city. tar # 查看包中的文件tar -xvf city. tar # 拆包# =========打包压缩/拆包解压缩 gziptar -zcvf city. tar. gz city2. html city3. html  # 将 2个文件达成一个包. 并不压缩tar -tf city. tar. gz # 查看包中的文件tar -zxvf city. tar. gz # 拆包# =========打包压缩/拆包解压缩 bzip2tar -jcvf city. tar. bz2 city2. html city3. html  # 将 2个文件达成一个包. 并不压缩tar -tf city. tar. bz2 # 查看包中的文件tar -jxvf city. tar. bz2 # 拆包# =========排除一个, 其他进行压缩tar -zcvf city. tar. gz *. html --exclude city. html # 排除了city. html , 其他进行压缩. 文件名缩写: . tar. gz 缩写 tgz . tar. bz2 缩写 tbz2 安装桌面环境: 注释: 提前用virtual box 拍快照, 如果不想要, 还可以恢复回来 123yum groupinstall -y Desktop # 安装桌面yum groupinstall -u chinese-support # 安装中文支持init 5 # 启动桌面MariaDB 安装:  配置 MariaDB 的源, 创建新文件 vi /etc/yum. repos. d/MariaDB. repo12345[mariadb]name = MariaDBbaseurl = http://yum. mariadb. org/10. 1/centos7-amd64gpgkey=https://yum. mariadb. org/RPM-GPG-KEY-MariaDBgpgcheck=1123yum install -y MariaDB-server MariaDB-clientsystemctl start mariadbsystemctl enable marriadbmysql 安装 ( 安装了 MariaDB 就不需要装 mysql 了):  yum search mysql yum install mysql-server mysql service mysqld start vi /etc/rc. d/rc. local # 增加一行service mysqld startmongodb 安装:    vi /etc/yum. repos. d/mongodb-org-3. 4. repo # 配置MongoDB的yum源     123456[mongodb-org-3. 4] name=MongoDB Repository baseurl=https://repo. mongodb. org/yum/redhat/$releasever/mongodb-org/3. 4/x86_64/ gpgcheck=1 enabled=1 gpgkey=https://www. mongodb. org/static/pgp/server-3. 4. asc    123yum -y install mongodb-orgsystemctl start mongodsystemctl enable mongod    ​  redis 安装: 123yum install -y redissystemctl start redissystemctl enable redis. servicepython3. 6 和 pip3. 6 安装: 12yum install -y python36uyum install -y python36u-pipApache 服务安装: 网上文章 1234yum install -y httpdsystemctl start httpd. servicesystemctl enable httpd. servicefirewall-cmd --permanent --add-service=http   vim /var/www/html/index. html # 为apache 创建第一个测试html 程序     firewall-cmd –add-service=http # 打开防火墙的80端口.      此时, 局域网其他机器打开浏览器访问该主机的ip地址就应该可以看到网页内容了      firewall-cmd –permanent –add-service=http # 写入配置文件          此时，httpd这个服务添加到了/etc/firewalld/zones/public. xml这个zone配置文件中，所以firewalld才能够据此放行     123456789&lt;?xml version= 1. 0  encoding= utf-8 ?&gt;&lt;zone&gt; &lt;short&gt;Public&lt;/short&gt; &lt;description&gt;For use in public areas. You do not trust the other computers on networks to not harm your computer. Only selected incoming connections are accepted. &lt;/description&gt; &lt;service name= ssh /&gt; &lt;service name= dhcpv6-client /&gt; &lt;service name= http /&gt;&lt;/zone&gt;             vim /etc/httpd/conf/httpd. conf # 可以对apache 进行配置.  NTP 网络时间同步 chrony: Network Time Protocol (NTP) 是 网络时间同步服务 Chrony 是一个开源的自由软件，它能保持系统时钟与时钟服务器（NTP）同步，让时间保持精确。 它由两个程序组成：chronyd 和 chronyc。 chronyd是一个后台运行的守护进程，用于调整内核中运行的系统时钟和时钟服务器同步。它确定计算机增减时间的比率，并对此进行补偿。 chronyc提供了一个用户界面，用于监控性能并进行多样化的配置。它可以在chronyd实例控制的计算机上工作，也可以在一台不同的远程计算机上工作。 12345yum install -y chronysystemctl start chronydsystemctl status chronydsystemctl enable chronyd # 让chronyd 在开机的时候就自动启动timedatectl # 查看是否已经同步, yes 就是同步了. DNS 服务设置 和自己的host: vi /etc/resolve. conf # 设置dns 服务商, 可以使用google 的dns 服务. 123# use google for dnsnameserver 8. 8. 8. 8nameserver 8. 8. 4. 4设置自己的hosts 名字    hostnamectl set-hostname dalongCentOS7     hostnamectl status     ip addr list     vi /etc/hosts # 添加一行   1127. 0. 0. 1  dalongCentOS7    ​  ssh 登录欢迎信息: vi /etc/motd 12345678910111213141516171819202122232425262728293031323334353637383940                  . . . ~OOOOZO. . . .                          . $OOOOOOOOOOOOOOOO:.                       . OOOZ???IIIIIIIIIIIOOOO.                     . OOZ??. . I7777777777II?I7OO.                     ZOZ??.  . 7777777777777??7OO.                    OO??7.  . 7IIIIIIIIIIII7??OO.                    OO+?IIIIIIIIIIIIIIIIIII??OO.                    OO+??????????????IIIIII??OO.                 . OOOOOOOOOOOOOOOOOOOOO??IIIIII?IOO.               . ,OOOOOOOOOOOOOOOOOOOOOOO??IIII?$IIOO. . . . . . . . . .          . ,OO++++++++++++++++++++++++?7$$$$IIOO7777777777. .         . OO++???????????$$$$$$$$$$$$$$$$$$IIOO77:::::::$77.        . OO+++????+I7777777$$$$$$$$$$$$$$$$IIOO77::~~~~::77.        . OO+++++77777777777$$$$$$$$$$$$$$$II$OO77::~~~~~::77.       . OO==++7777777777777$$$$$$$$$$$$$$IIIOO77,,:~~~~~,:$7.       . OO==77777777777$IIIIIIIIIIIIIIIIIIOOO77I,,:::::::,:77       OO?I777777777$IIIIOOOOOOOOOOOOOOOOOO77=,,::::::::,,77.       . OOII77777777IIIOOOO?77777777777777777,,,:::::::::,:77.       . OOII$777777IIIOOO777777777777777$:,,,,::::::::::=::77.       ~OOII777777IIOO777,,,,,,,,,,,,,,,,,,::::::::~+===:~77       . OOII$7777IIOOI77,,,,,,,,,,,,,:===++++++========~:=77       . ZO7II7777IIOO77,,,,,,,,,~+=++++++++++++++++++++~~77.        . OOIIIIIIIIZO77,. ,,,,=++++++++++++++++++++++++~~I77         . OOOIIIIIIZO77. . ,,++++++++++++++++++++++++++~~~77.          . . OOOOOOOOO77. . ++++++~~~~~~~~~~~~~~~~~~~~~~~~77. .            . . . . . . . 77:~++++++~~77777777777777777777777.                . $7~~++++++~~~~~~~~~~~~~~~7$. . . . . . .              . . . ,,,$7~~++++++~~~~~~~~~~~~~~~77,. . .               . ,,::~~=77~~+++++++++++++++. . ++~~77~::,,.            . . ,,:~~==++$7~~++++++++++++++.  . +~~77==~~:,,. .          . . ,::~==++?I777~~=++++++++++++. . . . ~~77?++==~::,. .          . ,,::~=++??I7777~~~~~~++++++=~~~~~7777??+==~::,,.          . . ,,:~~==+??I7777777~~~~~~~~~~+$7777I??+==~~:,,.           . . ,::~~==++??II7777777777777777I??++==~~::,. .             . . ,,::~~====++++?????????+++==~~~::,,. .                . . . . ,,,:::::~~~~~~~~~:::::,,,. . . .                     . . . . . . . . . . . . . . .                                                                       Dalong. com12345678910111213141516171819202122232425262728293031323334353637383940      . . . . . ,~?III7IIIII?~. . . . . . :=7$ZZZZZZZZZZZZZ7~. . . .      . . :+IIIIIIIIIIIII?:. . . =$ZZZZZZZZZZZZZZZ$$ZZZZZ=. .    . . . . ~IIIIIIIIIIIIII+,. . ~$ZZZZZZZZZZZZZZZZZ:. . ~ZZZZO+.    . . =IIIIIIIIIIIIII+. . . +ZZZZZZZZZZZZZZZZZZZZ+. . ?ZZZZZZ.  . . . . ~IIIIIIIIIIIII7I~. . =ZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZOZ+  . . . ?7IIIIIIIIIIIII+. . :ZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZI.  . ,?7IIIIIIIIIIIII~. . ?OZZZZZZZZZZZZZZZZZZZZZZZZ+. . . . . . . . . . . . . . ?IIIIIIIIIIIIII~. . 7ZZZZZZZZZZZZZZZZZZZZZZZZZZZZ7+=~=??. . . . . ?IIIIIIIIIIIIII:. . $ZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZ$~. . . . . =IIIIIIIIIIIIII?. . $ZZZZZZZZZZZZZZZZZZZZ$7I++++++++=:. .   . ,IIIIIIIIIIIIIII. . 7ZZZZZZZZZZZZZZZOOZ?. . . . . . . . . . . . . . . . .   . =IIIIIIIIIIIIII:. :ZZZZZZZZZZZZZZZZZ+. . . :+IIIII?. . . . . . . .   ,?IIIIIIIIIIIII?. . $ZZZZZZZZZZZZZZZ$. . ,+IIIIIIIII,. . . .    ,IIIIIIIIIIIIII:. ,ZZZZZZZZZZZZZZZI. . =IIIIIIIIIII:. . . .    :IIIIIIIIIIII7I. . ?ZZZZZZZZZZZZZZ$. . ~IIIIIIIIIIII~. . . .    :IIIIIIIIIIIII?. . 7ZZZZZZZZZZZZZZ,. ,IIIIIIIIIIIII~. . . .    :IIIIIIIIIIII7I,. =ZZZZZZZZZZZZZ$. . +IIIIIIIIIIIII~. . . .    :IIIIIIIIIIIIII+. . $ZZZZZZZZZZZZ,. :IIIIIIIIIIIIII,. . . .    :IIIIIIIIIIIIIII~. . 7ZZZZZZZZZZ,. . IIIIIIIIIIIIIII. . . . .    :IIIIIIIIIIIIIIII+. . . +$ZZZ$I:. . ~IIIIIIIIIIIIIII?. .      :IIIIIIIIIIIIIIIIII+,. . . . . . . . ~IIIIIIIIIIIIIIIII,.      :IIIIIIIIIIIIIIIIIIIIII?++IIIIIIIIIIIIIIIIIIII=. . .      :IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII+. . .      :IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII?,. . . .      :IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII~. .        ,IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII,. . .        :IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII+,. . . . . . . . . . . . . . . . . . ,I7IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII~. . . $ZZZZZZZZZZZZI. . . . ,IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIII:. . . +ZZZZZZZZZZZZZZZZ+. . :IIIIIIIIIIIII?,~?IIIIIIIIII7II?=,. . . :7ZZZZZZZZZZZZZZZZZZOI. ,IIIIIIIIIIIII?. . . . . ,,:~~~::,. . . . . ,?ZZZZZZZZZZZZZZZZZZZZZZZ. ,IIIIIIIIIIIII?. . . .    . . ,=I$ZZZZZZZZZZZZZZZZZZZZZZZZZZZ+,IIIIIIIIIIIII?. . . .    . ,$ZZZZZZZZZZZZZZZZZZZZZZZZZZZZZZ,,IIIIIIIIIIIII?. . . .    . . . ,7$ZZZZZZZZZZZZZZZZZZZZZZZZZZ7. :IIIIIIIIIIIII?. . . .     . . . :IZZZZZZZZZZZZZZZZZZZZZZZI. . ,IIIIIIIIIIII7I. . . .      . . . . . ,=7ZZZZZZZZZZZZZZZZZ$:. . . . ,,,,,,,,,,,,. ,. . . .        . .  . . . . . ,,:::,,,,,. . . .  . . . . . . . . . . . . . . . . . . .          . . . . . . . . . . . . . . . . . . .                         Dalong. comvim /etc/profile. d/motd. sh 1234567891011#!/bin/bashecho -e  #################################### Welcome to `hostname`, you are logged in as `whoami`# This system is running `cat /etc/redhat-release`# kernel is `uname -r`# Uptime is `uptime | sed 's/. *up ([^,]*), . */1/'`# Mem total `cat /proc/meminfo | grep MemTotal | awk {'print $2'}` kB################################### 日志管理及服务 journald: centos7 自带了系统日志功能（journald），取代了syslog后台程序。journald的配置文件位于 /etc/systemd/journald. conf 基本命令    journalctl # 查看所有系统日志     journalctl –since “2017-12-26 15:00:00” –until “2017-12-26 16:00:00” # 查看指定时间段的log     journalctl -u httpd. service –since “yesterday” # 查看从昨天到现在的所有阿帕奇日志     journalctl -p err -b # 查看所有错误日志     journalctl -f 实时最终最新的日志输出     journalctl –boot=-1 # 查看刚刚启动的时候的错误信息.   ​  日志持久化  journald 产生的日志会在每次重启后被删除 , 默认存放目录是 /run/log/journal, 所以如果想要把日志持久的保留下来,需要做如下操作.  mkdir /var/log/journal # 为journald 创建存放日志的目录 systemd-tmpfiles –create –prefix /var/log/journal  # 更改日志目录的系统权限 systemctl restart systemd-journald     这时应该就可以在新的目录下看到日志文件了.     journalctl –boot=-1 # 查看重启后日志是否正常用户管理 useradd groupadd:  cat /etc/passwd # 列出所有系统用户 id root # 查看root 用户的 uid, gid 信息 useradd dalong passwd dalong groupadd py1702 usermod -G py1702 dalong id dalong userdel dalong groupdel py1702设置定时任务 cron:    crontab -e # 编辑当前用户的定时任务   1*/5 * * * * echo `date`  Hello world  &gt;&gt;$HOME/cron-helloworld. txt    保存后会输出: crontab: installing new crontab     cat ~/cron-helloworld. txt # 可以查看每过5分钟插入一条数据     cat /var/log/cron     五颗星          &lt;分钟&gt; &lt;小时&gt; &lt;日/月&gt; &lt;月&gt; &lt;日/周&gt; &lt;命令&gt;           一个或两个空格分割           分钟0-59 , 小时0-23, 日1-31, 月1-12, 日1-7.           /每 .          比如: */5 每5分钟 , */1 每1分钟              -跨度 .          比如: 1-5 周一到周五        10 20 * * 1-5 /full/path/to/your/php/script. php # 周一到周五每天晚上8点执行php          让系统界面支持中文: 运行以下命令到命令行 12345yum -y install kde-l10n-Chineseyum -y reinstall glibc-commonyum clean alllocaledef -c -f UTF-8 -i zh_CN zh_CN. utf8cat /dev/null &gt; /etc/locale. conf &amp;&amp; echo  LC_ALL=\ zh_CN. UTF-8\   &gt; /etc/locale. confgit 服务配置: 1yum install -y git git config –global user. email “37016175@qq. com” git config –global user. name “dalong” cd /etc/ git init git add * git commit -a -m “inital commit of the full /etc/ directory” vim . /yum. conf git status git commit -a -m “change yum. conf with a comment line” git log –pretty=oneline –abbrev-commit git diff a2a108a c6b8e6f git checkout c6b8e6f文件同步 rsync: centos7自带rsync 教学文章 配置文件: /etc/rsyncd. conf Nginx 安装: 123yum install -y nginxsystemctl start nginx. servicesystemctl enable nginx. servicevim /etc/nginx/nginx. conf # 由于之前安装了 httpd 的阿帕奇服务,已经占用了 80 端口, 所以需要修改端口为一个没用过的端口号. 123     listen    8081 default_server;     listen    [::]:8081 default_server;   firewall-cmd --zone=public --add-port=8081/tcp --permanent # 为防火墙添加8081端口.     启动服务 失败了: systemctl start nginx. service     systemctl status nginx. service -l      输出如下:    12345678912月 28 09:58:18 dalongcentos7 systemd[1]: Starting The nginx HTTP and reverse proxy server. . . 12月 28 09:58:18 dalongcentos7 nginx[2479]: nginx: the configuration file /etc/nginx/nginx. conf syntax is ok12月 28 09:58:18 dalongcentos7 nginx[2479]: nginx: [emerg] bind() to 0. 0. 0. 0:8081 failed (13: Permission denied)12月 28 09:58:18 dalongcentos7 nginx[2479]: nginx: configuration file /etc/nginx/nginx. conf test failed12月 28 09:58:18 dalongcentos7 systemd[1]: nginx. service: control process exited, code=exited status=112月 28 09:58:18 dalongcentos7 systemd[1]: Failed to start The nginx HTTP and reverse proxy server. 12月 28 09:58:18 dalongcentos7 systemd[1]: Unit nginx. service entered failed state. 12月 28 09:58:18 dalongcentos7 systemd[1]: nginx. service failed.        权限拒绝，经检查发现是开启selinux 导致的。 直接关闭   getenforce  这个命令可以查看当前是否开启了selinux 如果输出 disabled 或 permissive 那就是关闭了      getenforce      如果输出 enforcing 那就是开启了 selinux   setenforce 0  ##设置SELinux 成为permissive模式   setenforce 1  ##设置SELinux 成为enforcing模式      setenforce 0     systemctl start nginx. service # 开启服务     systemctl enable nginx. service # 设置开启启动     vi /etc/selinux/config # 将SELINUX=enforcing改为SELINUX=disabled 永久关闭 selinux     访问网址 http://192. 168. 31. 242:8081 ,如果不出错,就是可以看到 nginx首页了     如果出错,查看error log      tail -f /var/log/nginx/error. log   机器学习环境安装: 深度学习环境搭建 机器学习环境搭建 机器学习环境 shell 脚本编写: shell 脚本文档  shell 是外壳, 针对 kurnal 内核 shell 是一门编程语言, 将终端命令放在文件中进行执行 不同的shell , sh, bsh, csh, bash(linux 默认), ksh, zsh 执行命令的方式:     交互式: 在命令行执行,输入一条,解析器输出一个结果   脚本式: 多条命令放在一个脚本文件中, 然后运行脚本文件.    升级 bash4. 4: 123456789# 升级 bash4. 4cd /root/codewget http://ftp. gnu. org/gnu/bash/bash-4. 4. tar. gztar -zxvf bash-4. 4. tar. gzcd bash-4. 4. /configure --prefix=/usr/local/bash4. 4make &amp;&amp; make installmv /usr/bin/bash /usr/bin/bash. BAKln -s /usr/local/bash4. 4/bin/bash /usr/bin/bash多个命令写在一行的技巧: 12345cmd1 ; cmd2 # 执行前一个,执行后一个cmd1 || cmd2 # 执行前一个, 如果失败,执行第二个. 第一个成功了就结束了. cmd1 &amp;&amp; cmd2 # 执行前一个, 如果成功了,执行第二个. 第一个失败了就结束了make &amp;&amp; make install # 一种经常的执行方式, 一条条执行, 直到失败就停住. 不要用test 作为文件名: 因为已经有一个shell 内置命令叫 test 12345whatis test# test (1)       - check file types and compare values# test (1p)      - evaluate expression# Test (3pm)      - provides a simple framework for writing test scripts#! sha-bang 符号: 1234#!/bin/bash #!/usr/bin/env bash#!/bin/sh注释 #: 1234567# shell 中的单行注释:&lt;&lt;!多行注释e f g ha b c d !执行 Shell 脚本两种方式: cd到目录中执行: 123456bash test. sh#或者chmod a+x test. sh. /test. sh#或者放到 PATH 对应目录中#或者将脚本目录export 到 PATH 目录中为脚本创建软链接: 到/root/bin目录下 1ln -s /usr/local/php7/bin/php /root/bin/php$PATH 将脚本所在目录添加到环境变量中: export PATH=/usr/local/php7/bin/:$PATHecho $PATH变量: 在 bash 中所有的变量值都是以字符串形式保存的. 不管你给变量赋值时有没有使用引号, 都以字符串形式存储. 预定义变量 $#, $*, $@, $?, $$, $!, $0, $N:  $# ：命令行中位置参数的个数 $* , $@ ：所有位置参数的内容 $? ：上一条命令执行后返回的状态，当返回状态值为0时表示执行正常，非0表示执行异常或出错 $$ ：当前所在进程的进程号 $! ：后台运行的最后一个进程号 $0 ：当前执行的进程/程序名 $N : N 是大于0的数字, 表示第几个参数. 123456789101112131415#!/bin/bashecho '参数个数为' $#echo '参数内容为' $*echo '上一条命令返回的状态是: ' $?echo '当前进程号是: ' $$echo '当前运行的最后一个进程号' $!echo '当前执行的进程/程序名' $0# 输出如下#参数个数为 3#参数内容为 a b c#上一条命令返回的状态是: 0#当前进程号是: 2010#当前运行的最后一个进程号#当前执行的进程/程序名 . /my. sh位置变量: 表示为 $n (n 为 1-9之间的数字) $0 表示当前文件 123456789#!/bin/bashecho '当前文件名: ' $0echo '第一个变量是: ' $1echo '第二个变量是: ' $2echo '第三个变量是: ' $3#当前文件名: . /my. sh#第一个变量是: a#第二个变量是: b#第三个变量是: c环境变量: 环境变量配置文件  全局配置文件：/etc/profile 用户配置文件：~/. bash_profile查看环境变量 set, env命令可以查看所有的shell变量，其中包括环境变量 常见的环境变量  $USER 查看当前用户名 $UID 当前用户UID $SHELL 当前用户使用的shell $HOME 当前用户家目录 $PWD 当前目录 $PATH 用户所输入的命令是在哪些目录中查找 $PS1 就是用户平时的提示符。 $PS2 第一行没输完，等待第二行输入的提示符。 $RANDOM 随机数PS1 中的特殊符号所代表的意义 1234567891011121314151617181920212223　　\d ：代表日期，格式为weekday month date，例如： Mon Aug 1  　　\H ：完整的主机名称。例如：我的机器名称为：fc4. linux，则这个名称就是fc4. linux 　　\h ：仅取主机的第一个名字，如上例，则为fc4，. linux则被省略 　　\t ：显示时间为24小时格式，如：HH：MM：SS 　　\T ：显示时间为12小时格式 　　\A ：显示时间为24小时格式：HH：MM 　　\u ：当前用户的账号名称 　　\v ：BASH的版本信息 　　\w ：完整的工作目录名称。家目录会以 ~代替 　　\W ：利用basename取得工作目录名称，所以只会列出最后一个目录 　　\# ：下达的第几个命令 　　\$ ：提示字符，如果是root时，提示符为：# ，普通用户则为：$pgrep 查看某个进程(mysqld)在运行中所用到的环境变量 123pgrep mysqld # 获取mysqld 的进程号cat /proc/911/environ # 获取mysql 用到的所有环境变量# LANG=zh_CN. UTF-8PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin_WSREP_START_POSITION=NOTIFY_SOCKET=/run/systemd/notifyHOME=/var/lib/mysqlLOGNAME=mysqlUSER=mysqlSHELL=/sbin/nologin[root@MiWiFi-R2D-srv code]#引号对变量的作用: 结合不同的引导为变量赋值  双引号 “” ：允许通过$符号引用其他变量值 单引号 ‘’ ：禁止引用其他变量值，$视为普通字符 反撇号 `` ：将命令执行的结果输出给变量用户自定义变量:  变量名只能是字母数字下划线, 不能以数字开头 等号两边不能有空格.  值如果没有空格, 可以不用引号括起来 值有空格, 必须用引号括起来单行变量定义 123456789#!/bin/bash# 变量名只能是字母数字下划线, 不能以数字开头a=abc # 等号两边不能有空格. 值如果没有空格, 可以不用引号括起来echo $aecho ${a}b= ab c  # 值有空格, 必须用引号括起来echo $b多行文本变量定义 更多方法 123456789c=`cat &lt;&lt;EOF&lt;?xml version= 1. 0  encoding='UTF-8'?&gt;&lt;report&gt; &lt;img src= a-vs-b. jpg /&gt;&lt;/report&gt;EOF`echo $c#&lt;?xml version= 1. 0  encoding='UTF-8'?&gt; &lt;report&gt; &lt;img src= a-vs-b. jpg /&gt; &lt;caption&gt;Thus is a future post on Multi Line Strings in bash &lt;date&gt;1511&lt;/date&gt;-&lt;date&gt;1512&lt;/date&gt;. &lt;/caption&gt; &lt;/report&gt;unset 删除一个变量: 12c=(1 2 3 4)unset cread 命令获取用户输入变量值: 12345678#!/bin/bashread yourname ageecho  你输入的名字是:   $yournameecho  你年龄是:   $age#/bin/bash /Users/dalong/code/python1702/centos7/my. sh#大龙 41#你输入的名字是: 大龙#你年龄是: 41参数:  -a: 数组 read -a arrayname -p: 输入提示 read -p “text” -r: 允许反斜线 read -r line -t: 等待读取数值的时间(秒) -s: 输入的内容不显示在终端上, 输入密码的时候用. $REPLY 是默认的输入值保存变量, 如果不定义其他的保存变量, 就找REPLY 123#!/usr/bin/env bashread -p '请输入你的姓名:'echo $REPLY-a 获取数组 1234#!/usr/bin/env bashread -a friendsecho echo  They are ${friends[0]}, ${friends[1]} and ${friends[2]}.  echo $REPLY获取密码,但不显示密码的方法 stty -echo , stty echo 1234567891011121314# 方法一#!/usr/bin/env bashread -p  输入密码:   -s pwdechoecho  password is :   $pwd# 方法二#!/usr/bin/env bashstty -echo # 让终端输入的内容不可见read -p  请输入密码:   pwdstty echo  # 让终端输入的内容可见echo $pwdalias 定义别名变量: 别名就是提供一种便捷的方式来完成某些长串命令的操作。省去不必要的麻烦，提高效率。  一般是会把别名定义表达式放到 ~/. bashrc 文件中. 1echo 'alias nginxrestart= /usr/local/webserver/nginx/sbin/nginx -s reload ' &gt;&gt; ~/. bashrc 查看系统已经定义的别名就直接用命令 alias 12345678alias cp='cp -i'alias l. ='ls -d . * --color=tty'alias ll='ls -l --color=tty'alias ls='ls --color=tty'alias mv='mv -i'alias rm='rm -i'alias vi='vim'alias which='alias | /usr/bin/which --tty-only --read-alias --show-dot --show-tilde'文件的描述符和重定向 &gt;, &gt;&gt;, 2&gt;, &amp;&gt;, tee: 文件描述符:  常见的文件描述符是stdin、stdout和stderr  系统预留文件描述符    0 —— stdin（标准输入）  1 —— stdout（标准输出）  2 —— stderr（错误输出） 输出重定向 &gt; &gt;&gt;:  重定向将输入文本通过截取模式保存到文件 重定向和追加重定向 12echo  this is a text line one  &gt; test. txt #写入到文件之前，文件内容首先会被清空。echo  this is a text line one  &gt;&gt; test. txt #写入到文件之后，会追加到文件结尾。将标准输出和错误输出重定向到文件中/黑洞中 123cat linuxde. net 2&gt; out. txt # 只将错误信息重定向到文件中cat linuxde. net &amp;&gt; out. txt # 正确和错误信息都重定向cat linuxde. net 2&gt; /dev/null # 将错误信息重定向到黑洞tee 命令 和重定向: tee命令可以将数据重定向到文件，另一方面还可以提供一份重定向数据的副本作为后续命令的stdin 12345678ls | tee out. txt | cat -n # ls 结果通过 tee 放入 out. txt 中, 又将结果再次转给 cat 显示行号. # 输出# 	 1 a. repo#   2 b. repo#   3 c. repo#   4 d. txt#   5 out. txt多行文本重定向 cat &lt;&lt;EOF&gt; 文件名: 在cat &lt;&lt;EOF&gt;text. log与下一个EOF行之间的所有文本都会当作stdin数据输入到text. log中 123456#!/bin/bashcat &lt;&lt;EOF&gt;text. logthis is a text line1this is a text line2this is a text line3EOF下标数组,关联数组: 更全面的数组介绍 数组作为一种特殊的数据结构在任何一种编程语言中都有它的一席之地，数组在Shell脚本中也是非常重要的组成部分，它借助索引将多个独立的数据存储为一个集合    bash支持一维数组（不支持多维数组），并且没有限定数组的大小。   ​  下标数组:: 下标数组元素的下标由0开始编号。获取数组中的元素要利用下标，下标可以是整数或算术表达式，其值应大于或等于0。 定义数组和读取和删除 等号两边不能有空格 1234567891011121314a[0]='a'a[1]= b echo ${a[0]}echo ${a[1]}echo ${a[@]} # 输出所有数组元素b=(1 2 3 4 5)echo ${b[@]} # 输出所有元素echo ${b[*]} # 输出所有元素echo ${#b[@]} # 输出数组长度unset a[0] # 删除一个元素unset b  # 删除整个数组数组的分片 123array=( [0]=one [1]=two [2]=three [3]=four )echo ${array[@]:0:2} # one twoecho ${array[@]:1:2} # two three关联数组:: shell 提供了另外一种数组，其可以使用任意的字符串作为下标（不必是整数）来访问数组元素。这种数组叫做关联数组（associative array）。  关联数组的下标和值称为键值对，它们是一一对应的关系。 在关联数组中，键是唯一的，值可以不唯一。关联数组定义和使用 定义关联数组  shell 的关联数组和 shell 的下标数组在定义和使用上完全一样，只是在索引上有区别。 需要注意的是，在使用关联数组之前，需要使用命令 declare -A array 进行显示声明。      语法   描述         ${!array[*]}   取关联数组所有键       ${!array[@]}   取关联数组所有键       ${array[*]}   取关联数组所有值       ${array[@]}   取关联数组所有值       ${#array[*]}   关联数组的长度       ${#array[@]}   关联数组的长度   12345678910111213141516#!/usr/bin/bashdeclare -A phone              # 定义关联数组phone=([jim]=135 [tom]=136 [lucy]=158)   # 为关联数组添加元素# 或者单独赋值# phone[jim]=135# phone[tom]=136for key in ${!phone[*]}do  echo  $key -&gt; ${phone[$key]} done#输出如下:#tom -&gt; 136#jim -&gt; 135#lucy -&gt; 158函数定义,执行,传参数: Bash（Bourne Again shell）也跟其他编程语言一样也支持函数，一般在编写大型脚本中需要用到，它可以将shell脚本程序划分成一个个功能相对独立的代码块，使代码的模块化更好，结构更加清晰，并可以有效地减少程序的代码量。但是bash作为一种解释性语言，bash 在编程能力方面提供的支持并不像其他编译性的语言（例如 C 语言）那样完善，执行效率也会低很多。 定义和调用函数: 123456#!/usr/bin/bashfunction sayhello(){ # function 关键字可以忽略  echo  hello }sayhello # 调用函数时不要加小括号定义全局变量, 局部变量, 并获得返回值:  bash中定义局部变量要用关键字 local return 只能返回0~257之间的整数12345678910111213#!/usr/bin/bashaa= this is aa bb= this is bb function name() {         #定义函数name    local cc= this is cc    #定义局部变量$cc    local dd= this is dd    #定义局部变量$dd    echo $aa, $bb       #访问参数1和参数2    echo $cc          #打印局部变量    return 0          #shell函数返回值是整形，并且在0~257之间。}echo $dd              #这里将会打印不生效，因为dd是局部变量。name                #使用函数name为函数传递参数: 123456789101112131415161718#!/usr/bin/bashstr= This is a test! fun1(){  echo $str;  echo $1;  echo $2;}#调用fun1fun1  aa   bb   # 参数不要放在括号中fun1  cc   dd #输出如下#This is a test!#aa#bb#This is a test!#cc#dd条件测试 test 与 流程控制 if else: 在编写Shell脚本时候，经常需要判断两个字符串是否相等，检查文件状态或者是数字的测试等。Shell提供了对字符串、文件、数值等内容的条件测试以及逻辑流程控制。 关于真值 true false: 与其他语言不同，Bash（包括其他Shell）中，是用0表示真，非0表示假的。 1234567trueecho $? # 输出 0falseecho $? # 输出 1# if [ 0 ];then echo  真 ; else echo  假 ; fi# 输出为  真 常用测试操作 test [ 条件表达式 ]: test命令，测试特定的表达式是否成立，当条件成立时，命令执行后的返回值为0，否则为其他数值。  格式1    test 条件表达式  格式2    [ 条件表达式 ]  使用方括号时，要注意在条件两边加上空格。  如果条件中有 = 号, 注意在=前后各有一个空格，如果没有空格就是赋值的关系，不是比较的关系。  在[]中使用=，（尽管==也可以可以用的）  字符串的&gt; &lt;比较运算符，一般放在[[ ]]之中，而不是test ( [] )  字符串的&gt; &lt;比较的结果，与本地的locale有关，是按照其字典序列进行比较的  [] 中的变量都要加上双引号括起来, 比如 [  $str  =  hello  ] 常见测试种类  测试文件状态 字符串的比较 整数值的比较 逻辑测试测试目录是否存在 123456789101112#!/usr/bin/bash[ -d /Users/dalong/code/python1702/centos7 ]echo $?[ -d /Users/dalong/code/python1702/centos7/test. log ]echo $?[ -d /Users/dalong/code/python1702/centos7/abc ]echo $?#输出如下:#1  是目录#0  不是目录,是文件#0  目录不存在测试文件状态:  格式[ 操作符 文件或目录 ] 操作符：  -d：测试是否为目录，是则为真（Directory） -e：测试目录或文件是否存在，存在则为真（Exist） -f：测试是否为文件，是则为真（file) -r：测试当前用户是否有权限读取，是则为真（read） -w：测试当前用户是否有权限写入，是这为真（write]) -x：测试当前用户是否可执行该文件，可执行则为真（Excute） -L：测试是否为符号链接文件，是则为真（Link） -nt：file1 -nt file2 如果 file1 比 file2 新（修改时间），则为真 -ot：file1 -ot file2 如果 file1 比 file2 旧（修改时间），则为真判断文件或目录是否存在 1234if [ -d /root/code ];then echo  是目录 ; else echo  不是目录 ; fi # 判断目录是否存在# 输出为  是目录 if [ -e /root/code/my. sh ];then echo  文件或目录存在 ; else echo  文件或目录不存在 ; fi # 判断文件或目录是否存在# 输出为  文件或目录存在 判断两个文件的新旧 123test test. txt -ot test2. txtecho $?#输出为0判断两个文件是否相同 123456789101112#!/bin/bashstr1=`md5sum /root/code/test. txt|cut -f1 -d   ` # 将文件md5之后,取第一个字段str2=`md5sum /root/code/test2. txt|cut -f1 -d   `if [  $str1  =  $str2  ]; then  echo  Files have the same content  $str1, $str2else  echo  Files have NOT the same content  $str1, $str2fi# 输出如下:# Files have the same content 22eac1e6209922a4a62863644b833950, 22eac1e6209922a4a62863644b833950字符串的比较:  格式[ 字符串1 = 字符串2 ][ 字符串1 != 字符串2 ][ -z 字符串 ] 判断字符串长度是否为0. the length of STRING is zero 操作符：  =：字符串内容相同则为真，就是说包含的文本一摸一样。 !=：字符串内容不同，则为真（!号表示相反的意思） -z：字符串内容为空（长度为零）则为真 -n：字符串内容非空（长度非零）则为真 &lt;：string1 &lt; string2 如果string1在本地的字典序列中排在string2之前，则为真 &gt;：string2 如果string1在本地的字典序列中排在string2之后，则为真注意点：  字符串的 “等于” 比较，为了与POSIX一致，在[]中使用=，（尽管==也可以） 注意在=前后各有一个空格，如果没有空格就是赋值的关系，不是比较的关系。 字符串的&gt; &lt;比较运算符，一般放在[[ ]]之中，而不是test ( [] ) 字符串的&gt; &lt;比较的结果，与本地的locale有关，是按照其字典序列进行比较的123456789101112131415161718# 比较字符串str= Hello [  $str  =  Hello  ]echo $? # 输出0[  $str  =  hello  ]echo $? # 输出1# 判断是否为空str=  [ -z  $str  ]echo $? # 输出0[[  a  &lt;  b  ]]echo $? # 输出0[[  a  &gt;  b  ]]echo $? # 输出1整数值的比较:  格式[ 整数1 操作符 整数2 ] 操作符  -eq：等于（equal） -ne：不等于（not equal） -gt：大于（Greater than） -lt：小于（lesser than） -le：小于等于（lesser or equal） -ge：大于等于（Greater or equal）注意:  整数的test就是大小关系的比较，与其他语言不同，Bash中没有使用&lt;,&gt;来做大于等于号，而是使用了减号开头的选项来比较。 整数值的定义可以用引号,也可以不用 使用以上操作符，那么操作符两边一定要是整数。12345num= 15  # num=15 结果一样[  $num  -eq 15 ]echo $? # 输出0[  $num  -eq 20 ]echo $? # 输出11234a=15b=14test $a -gt $b &amp;&amp; echo Yes#输出 Yes逻辑运算符 -a, &amp;&amp;, -o, ||, !:  格式[ 表达式1 ] 操作符 [ 表达式2 ] … 操作符  -a 或 &amp;&amp; ：逻辑与，“而且”的意思，前后两个表达式都成立时整个测试结果才为真，否则为假               -o 或           ： 逻辑或，“或者”的意思，操作符两边至少一个为真时，结果为真，否为为假           ! ：逻辑否，当制定条件不成立时，返回结果为真if 条件语句: if 单分支：  [语法]  if [ 条件语句 ]; then  ​	代码  fi 当/boot分区的空间使用超过80%，就输出报警信息。 123456#!/bin/bash#当/boot分区的空间使用超过80%，就输出报警信息。use=`df -hT | grep  /boot  | awk '{print $6}' | cut -d  %  -f1`if [ $use -gt 80 ]; then  echo  Warning!!/boot disk is full fiif 双分支：  [语法]  if [ 条件语句 ]; then  ​	代码  else  ​	代码  fi 判断防火墙 firewalld 是否在运行，如果已经在运行提示信息，没就有开启它。 123456789#!/bin/bash#判断iptables是否在运行，如果已经在运行提示信息，如果没有开启它。systemctl status firewalld &amp;&gt; /dev/null if [ $? -eq 0 ]; then  echo  防火墙服务正在运行. . .   else  systemctl start firewalld fiif 多分支  [语法]  if [ 条件语句1 ]; then  ​	代码  elif [ 条件语句2 ]; then  ​	代码  else  ​	代码  fi for 循环:  [语法]  for 变量名 in 取值列表do 	命令序列done for循环总是接收in语句之后的某种类型的字列表。在本例中，指定了四个英语单词，但是字列表也可以引用磁盘上的文件，甚至文件通配符。 for循环中用seq产生循环次数 1for x in $(seq 1 5); do echo $x ; done传统方式: i=1;i&lt;5;i++ 1for (( i=1; i&lt;=5; i++ )); do echo $i; donefor in 循环一堆空格分割的单词 123456for y in one two three four; do echo number $y; done#输出如下:#number one#number two#number three#number four对目录中的文件做for循环 1234567# for x in /var/log/*; do echo `basename $x` ; done # 不带绝对路径的文件名for x in /var/log/*; do echo  $x  ; done # 带绝对路径的文件名#输出目录下每个文件#/var/log/anaconda#/var/log/audit#/var/log/boot. log#. . . 循环获得执行脚本的所有参数 123456789101112#!/bin/bashfor attr in  $@ do  echo you typed ${attr}. done # 命令执行: . /my. sh a b c d#输出#you typed a. #you typed b. #you typed c. #you typed d. while 循环: 重复测试指令的条件，只要条件为真则反复执行对应的命令操作，直到条件为假。如果使用true作为循环条件能够产生无限循环。  [语法]  while 命令表达式do 命令列表done 批量添加20个系统账户用户名依次为user1~20 12345678910#!/bin/bashi=1while [ $i -le 20 ]do  useradd user$i  echo  123456  | passwd --stdin user$i &amp;&gt; /dev/null   # --stdin从标准输入读取令牌(只有根用户才能进行此操作)  i=`expr $i + 1`doneuntil循环语句: 根据条件执行重复操作，直到条件成立为止。Until语句提供了与while语句相反的功能：只要特定条件为假，它们就重复循环，直到条件为真。  [语法]  until 条件测试命令do​	 命令序列done 12345678910111213#!/bin/bashmyvar=1until [ $myvar -gt 10 ]do	echo $myvar	myvar=$(( $myvar + 1 ))	# myvar=`expr $myvar + 1` 也可以done # 输出:#1#2#. . . #10循环控制语句 break, continue: break语句：在for、while、until等循环语句中，用于跳出当前所在的循环体, 循环体内剩下的操作不再执行. continue语句：在for、while、until等循环语句中，用于跳过本次循环体内余下的语句，重新判断条件以便执行下一次循环。 case 多重分支: 根据变量的不通取值，分别执行不同的命令操作  [语法]  case 变量值 in模式1)​	 命令序列1;;模式2)​	 命令序列2;; ……*)​	 默认执行的命令序列;;esac 通过用户输入的参数判断如何执行MySQL 的命令 123456789101112131415#!/bin/bashcase $1 instart)  echo  start mysql ;;stop)  echo  stop mysql ;;*)  echo  usage: $0 start|stop ;;esac# 执行 . /my. sh start# 输出: start mysqlshift 命令弹出第一个脚本参数: 脚本获得多个参数, shift 将第一个参数弹出,其他前移 例如：若当前脚本程序获得的位置变量如下：  $1=file1、$2=file2、$3=file3、$4=file4 shift # 执行shift 命令后, 参数变成如下 $1=file2、$2=file3、$3=file4 shift # 执行shift 命令后, 参数变成如下 $1=file3、$2=file4对多个参数做加法 1234567891011#!/bin/bashres=0while [ $# -gt 0 ]do  res=`expr $res + $1`  shiftdoneecho  the sum is:$res #执行: . /my. sh 1 2 3 4 5#输出: the sum is:15exit 无条件终止(退出)脚本: 用于无条件终止当前脚本的执行. 若用在交互式shell中, 它将logout当前shell. exit后经常带有一个数值参数, 表征退出状态: exit n  n == 0      脚本执行成功 n == 1~125  出错, 这些对应的错误值由用户在脚本中定义.  n == 126    文件不可执行 n == 127    不存在该命令 n &gt;= 128    产生信号注意n处于1到125的情况, 这些错误值可以由用户自定义, 这就不需要使用类似C中errno这样的全局变量. 如果不给定n的值, 而直接使用exit, 那么返回exit之前最后一条语句的状态. 等效于”exit $?”. 检查本机是否可以连接外网(百度) 12345678910#!/usr/bin/env bash# 检查是否可以连接外网pingres=`ping -c 1 baidu. com | sed -n '/64 bytes from/p'`if [ -z  $pingres  ];then  echo  网络连接失败   exit 1else  echo  网络连接成功 fiexit 0初始化 Centos7 的SHELL 脚本实例2. 0: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291#!/usr/bin/env bash# ===========================# = 作者: dalong# = 用途: 用于初始化 centos7 系统# = centos 版本: CentOS-7-x86_64-Minimal-1708. iso# = 脚本版本: v2. 0# = 使用方法: 将本文件拷贝到 服务器的/root/code/目录下以root 身份执行. # = 安装内容包括# = 1. # ===========================#define EXIT_SUCCESS  0    /* Successful exit status.  */#define EXIT_FAILURE  1    /* Failing exit status.  */#define EX_NETWORKERR  64   /* 外网连接错误 */#define EX__MAX 78   /* maximum listed value */function createDir(){  local flag=1  if ! [ -d $1 ];then    mkdir /var/log/install    local flag=1  fi  return $flag}function createFile(){  local flag=1  if ! [ -f $1$2 ];then    touch $1$2    local flag=0  fi  return $flag}function writeLog(){  local flag=1  if [ -f $1$2 ];then    echo $3 &gt;&gt; $1$2    local flag=0  fi  return $flag}# 文件按照日期为文件名存放在 /var/log/install/目录下# 先判断目录是否存在, 不存在就创建# 判断log 文件是否存在,不存在就创建logDir=/var/log/install/createDir $logDirfilename=`date + %Y%m%d%H%I `createFile $logDir $filename# 配置网卡配置文件并启动网络服务sed -i 's/ONBOOT=no/ONBOOT=yes/g' /etc/sysconfig/network-scripts/ifcfg-enp0s3systemctl restart network. serviceif [ `echo $?` = 0 ];then  writeLog $logDir $filename  网络重启成功 fi# 检查是否可以连接外网pingres=`ping -c 1 baidu. com | sed -n '/64 bytes from/p'`if [ -z  $pingres  ];then  writeLog $logDir $filename  外网连接失败   exit 64else  writeLog $logDir $filename  外网连接成功 fi# 更换 yum 源mv /etc/yum. repos. d/CentOS-Base. repo /etc/yum. repos. d/CentOS-Base. repo. backupcurl -o /etc/yum. repos. d/CentOS-Base. repo http://mirrors. aliyun. com/repo/Centos-7. repoyum clean all # 清理缓存yum makecache # 生成缓存yum -y update # 验证是否已经是aliyun# 为 yum 添加 epel 源 IUS 源yum install -y epel-releaseyum install -y https://centos7. iuscommunity. org/ius-release. rpm# 为 yum 下载加速yum install -y yum-axelget# 为 centos 安装基础开发环境yum install -y gcc           # C 编译器yum install -y gcc-c++         # C++ 编译器yum install -y gcc-gfortran      # Fortran 编译器yum install -y compat-gcc-44      # 兼容 gcc 4. 4yum install -y compat-gcc-44-c++    # 兼容 gcc-c++ 4. 4yum install -y makeyum install -y gdb           # 代码调试器yum install -y cmake          # Cmakeyum install -y git           # 版本控制yum install -y ntfs-3g         # CentOS 下默认无法挂载 NTFS 格式的硬盘。需安装 nfts-3g 即可实现即插即用yum install -y libxml2yum install -y libxml2-devel -yyum install -y libffi-develyum install -y man-pagesyum install -y vim-enhancedyum install -y treeyum install -y dosfstools # 支持vfat 格式的格式化yum install -y wgetyum install -y mlocateupdatedblocate inittabyum install -y screen# 设置登录欢迎信息(cat &lt;&lt;'EOF'               . . . ~OOOOZO. . . .              . $OOOOOOOOOOOOOOOO:.             . OOOZ???IIIIIIIIIIIOOOO.            . OOZ??. . I7777777777II?I7OO.            ZOZ??.  . 7777777777777??7OO.            OO??7.  . 7IIIIIIIIIIII7??OO.            OO+?IIIIIIIIIIIIIIIIIII??OO.            OO+??????????????IIIIII??OO.        . OOOOOOOOOOOOOOOOOOOOO??IIIIII?IOO.       . ,OOOOOOOOOOOOOOOOOOOOOOO??IIII?$IIOO. . . . . . . . . .      . ,OO++++++++++++++++++++++++?7$$$$IIOO7777777777. .      . OO++???????????$$$$$$$$$$$$$$$$$$IIOO77:::::::$77.      . OO+++????+I7777777$$$$$$$$$$$$$$$$IIOO77::~~~~::77.      . OO+++++77777777777$$$$$$$$$$$$$$$II$OO77::~~~~~::77.     . OO==++7777777777777$$$$$$$$$$$$$$IIIOO77,,:~~~~~,:$7.     . OO==77777777777$IIIIIIIIIIIIIIIIIIOOO77I,,:::::::,:77     OO?I777777777$IIIIOOOOOOOOOOOOOOOOOO77=,,::::::::,,77.     . OOII77777777IIIOOOO?77777777777777777,,,:::::::::,:77.     . OOII$777777IIIOOO777777777777777$:,,,,::::::::::=::77.      ~OOII777777IIOO777,,,,,,,,,,,,,,,,,,::::::::~+===:~77     . OOII$7777IIOOI77,,,,,,,,,,,,,:===++++++========~:=77     . ZO7II7777IIOO77,,,,,,,,,~+=++++++++++++++++++++~~77.      . OOIIIIIIIIZO77,. ,,,,=++++++++++++++++++++++++~~I77      . OOOIIIIIIZO77. . ,,++++++++++++++++++++++++++~~~77.       . . OOOOOOOOO77. . ++++++~~~~~~~~~~~~~~~~~~~~~~~~77. .         . . . . . . . 77:~++++++~~77777777777777777777777.            . $7~~++++++~~~~~~~~~~~~~~~7$. . . . . . .          . . . ,,,$7~~++++++~~~~~~~~~~~~~~~77,. . .         . ,,::~~=77~~+++++++++++++++. . ++~~77~::,,.       . . ,,:~~==++$7~~++++++++++++++.  . +~~77==~~:,,. .       . . ,::~==++?I777~~=++++++++++++. . . . ~~77?++==~::,. .       . ,,::~=++??I7777~~~~~~++++++=~~~~~7777??+==~::,,.       . . ,,:~~==+??I7777777~~~~~~~~~~+$7777I??+==~~:,,.        . . ,::~~==++??II7777777777777777I??++==~~::,. .         . . ,,::~~====++++?????????+++==~~~::,,. .          . . . . ,,,:::::~~~~~~~~~:::::,,,. . . .               . . . . . . . . . . . . . . .                           Dalong. comEOF) &gt; /etc/motd(cat &lt;&lt;'EOF'#!/bin/bashecho -e  #################################### Welcome to `hostname`, you are logged in as `whoami`# This system is running `cat /etc/redhat-release`# kernel is `uname -r`# Uptime is `uptime | sed 's/. *up ([^,]*), . */1/'`# Mem total `cat /proc/meminfo | grep MemTotal | awk {'print $2'}` kB################################### EOF) &gt; /etc/profile. d/motd. sh# MariaDB 安装(cat &lt;&lt;'EOF'[mariadb]name = MariaDBbaseurl = http://yum. mariadb. org/10. 1/centos7-amd64gpgkey=https://yum. mariadb. org/RPM-GPG-KEY-MariaDBgpgcheck=1EOF) &gt; /etc/yum. repos. d/MariaDB. repoyum install -y MariaDB-server MariaDB-clientsystemctl start mysql. servicesystemctl enable mysql. service# mongodb 安装(cat &lt;&lt;'EOF'[mongodb-org-3. 4]name=MongoDB Repositorybaseurl=https://repo. mongodb. org/yum/redhat/$releasever/mongodb-org/3. 4/x86_64/gpgcheck=1enabled=1gpgkey=https://www. mongodb. org/static/pgp/server-3. 4. ascEOF) &gt; /etc/yum. repos. d/mongodb-org-3. 4. repoyum -y install mongodb-orgsystemctl start mongodsystemctl enable mongod# 安装redisyum install -y redissystemctl start redissystemctl enable redis. service# 安装pythonyum install -y python36uyum install -y python36u-pip# Apache 服务安装yum install -y httpdsystemctl start httpd. servicesystemctl enable httpd. servicefirewall-cmd --permanent --add-service=http# 安装NTP 时间同步服务yum install -y chronysystemctl start chronydsystemctl status chronydsystemctl enable chronyd # 让chronyd 在开机的时候就自动启动timedatectl # 查看是否已经同步, yes 就是同步了. # 安装gityum install -y git# Nginx 安装yum install -y nginxsed -i 's/listen    80 default_server/listen    8081 default_server/g' /etc/nginx/nginx. confsed -i 's/listen    \[::\]:80 default_server/listen    \[::\]:8081 default_server/g' /etc/nginx/nginx. confsystemctl start nginx. servicesystemctl enable nginx. service# 源码安装php7. 2mkdir /root/codemkdir /root/bincd /root/codewget http://cn2. php. net/distributions/php-7. 2. 0. tar. gztar -zxvf php-7. 2. 0. tar. gzcd php-7. 2. 0. /configure --prefix=/usr/local/php7make &amp;&amp; make install# 为命令创建软链接到 PATH 对应的目录下ln -s /usr/local/php7/bin/php /root/bin/php# 安装python3. 7cd /root/codewget https://www. python. org/ftp/python/3. 7. 0/Python-3. 7. 0a3. tgztar -zxvf Python-3. 7. 0a3. tgzcd Python-3. 7. 0a3. /configure --prefix=/usr/local/python3. 7make &amp;&amp; make installln -s /usr/local/python3. 7/bin/python3. 7 /root/bin/python3. 7ln -s /usr/local/python3. 7/bin/pip3. 7 /root/bin/pip3. 7# 升级 bash4. 4cd /root/codewget http://ftp. gnu. org/gnu/bash/bash-4. 4. tar. gztar -zxvf bash-4. 4. tar. gzcd bash-4. 4. /configure --prefix=/usr/local/bash4. 4make &amp;&amp; make installmv /usr/bin/bash /usr/bin/bash. BAKln -s /usr/local/bash4. 4/bin/bash /usr/bin/bash# 让系统支持中文yum -y install kde-l10n-Chineseyum -y reinstall glibc-commonyum clean alllocaledef -c -f UTF-8 -i zh_CN zh_CN. utf8cat /dev/null &gt; /etc/locale. conf &amp;&amp; echo  LC_ALL=\ zh_CN. UTF-8\   &gt; /etc/locale. conf# 关闭安全设置setenforce 0sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/configsystemctl stop firewalldsystemctl disable firewalldexit 0初始化 Centos7 的SHELL 脚本实例1. 0 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227#!/usr/bin/env bash# ===========================# = 作者: dalong# = 用途: 用于初始化 centos7 系统# = centos 版本: CentOS-7-x86_64-Minimal-1708. iso# = 脚本版本: v1. 0# = 使用方法: 将本文件拷贝到 服务器的/root/code/目录下以root 身份执行. # = 安装内容包括# = 1. # ===========================# 配置网卡,能上网sed -i 's/ONBOOT=no/ONBOOT=yes/g' /etc/sysconfig/network-scripts/ifcfg-enp0s3systemctl restart network. service# 更换 yum 源mv /etc/yum. repos. d/CentOS-Base. repo /etc/yum. repos. d/CentOS-Base. repo. backupcurl -o /etc/yum. repos. d/CentOS-Base. repo http://mirrors. aliyun. com/repo/Centos-7. repoyum clean all # 清理缓存yum makecache # 生成缓存yum -y update # 验证是否已经是aliyun# 为 yum 添加 epel 源 IUS 源yum install -y epel-releaseyum install -y https://centos7. iuscommunity. org/ius-release. rpm# 为 yum 下载加速yum install -y yum-axelget# 为 centos 安装基础开发环境yum install -y gcc           # C 编译器yum install -y gcc-c++         # C++ 编译器yum install -y gcc-gfortran      # Fortran 编译器yum install -y compat-gcc-44      # 兼容 gcc 4. 4yum install -y compat-gcc-44-c++    # 兼容 gcc-c++ 4. 4yum install -y makeyum install -y gdb           # 代码调试器yum install -y cmake          # Cmakeyum install -y git           # 版本控制yum install -y ntfs-3g         # CentOS 下默认无法挂载 NTFS 格式的硬盘。需安装 nfts-3g 即可实现即插即用yum install -y libxml2yum install -y libxml2-devel -yyum install -y libffi-develyum install -y man-pagesyum install -y vim-enhancedyum install -y treeyum install -y dosfstools # 支持vfat 格式的格式化yum install -y wgetyum install -y mlocateupdatedblocate inittabyum install -y screen# 设置登录欢迎信息(cat &lt;&lt;'EOF'               . . . ~OOOOZO. . . .              . $OOOOOOOOOOOOOOOO:.             . OOOZ???IIIIIIIIIIIOOOO.            . OOZ??. . I7777777777II?I7OO.            ZOZ??.  . 7777777777777??7OO.            OO??7.  . 7IIIIIIIIIIII7??OO.            OO+?IIIIIIIIIIIIIIIIIII??OO.            OO+??????????????IIIIII??OO.        . OOOOOOOOOOOOOOOOOOOOO??IIIIII?IOO.       . ,OOOOOOOOOOOOOOOOOOOOOOO??IIII?$IIOO. . . . . . . . . .      . ,OO++++++++++++++++++++++++?7$$$$IIOO7777777777. .      . OO++???????????$$$$$$$$$$$$$$$$$$IIOO77:::::::$77.      . OO+++????+I7777777$$$$$$$$$$$$$$$$IIOO77::~~~~::77.      . OO+++++77777777777$$$$$$$$$$$$$$$II$OO77::~~~~~::77.     . OO==++7777777777777$$$$$$$$$$$$$$IIIOO77,,:~~~~~,:$7.     . OO==77777777777$IIIIIIIIIIIIIIIIIIOOO77I,,:::::::,:77     OO?I777777777$IIIIOOOOOOOOOOOOOOOOOO77=,,::::::::,,77.     . OOII77777777IIIOOOO?77777777777777777,,,:::::::::,:77.     . OOII$777777IIIOOO777777777777777$:,,,,::::::::::=::77.      ~OOII777777IIOO777,,,,,,,,,,,,,,,,,,::::::::~+===:~77     . OOII$7777IIOOI77,,,,,,,,,,,,,:===++++++========~:=77     . ZO7II7777IIOO77,,,,,,,,,~+=++++++++++++++++++++~~77.      . OOIIIIIIIIZO77,. ,,,,=++++++++++++++++++++++++~~I77      . OOOIIIIIIZO77. . ,,++++++++++++++++++++++++++~~~77.       . . OOOOOOOOO77. . ++++++~~~~~~~~~~~~~~~~~~~~~~~~77. .         . . . . . . . 77:~++++++~~77777777777777777777777.            . $7~~++++++~~~~~~~~~~~~~~~7$. . . . . . .          . . . ,,,$7~~++++++~~~~~~~~~~~~~~~77,. . .         . ,,::~~=77~~+++++++++++++++. . ++~~77~::,,.       . . ,,:~~==++$7~~++++++++++++++.  . +~~77==~~:,,. .       . . ,::~==++?I777~~=++++++++++++. . . . ~~77?++==~::,. .       . ,,::~=++??I7777~~~~~~++++++=~~~~~7777??+==~::,,.       . . ,,:~~==+??I7777777~~~~~~~~~~+$7777I??+==~~:,,.        . . ,::~~==++??II7777777777777777I??++==~~::,. .         . . ,,::~~====++++?????????+++==~~~::,,. .          . . . . ,,,:::::~~~~~~~~~:::::,,,. . . .               . . . . . . . . . . . . . . .                           Dalong. comEOF) &gt; /etc/motd(cat &lt;&lt;'EOF'#!/bin/bashecho -e  #################################### Welcome to `hostname`, you are logged in as `whoami`# This system is running `cat /etc/redhat-release`# kernel is `uname -r`# Uptime is `uptime | sed 's/. *up ([^,]*), . */1/'`# Mem total `cat /proc/meminfo | grep MemTotal | awk {'print $2'}` kB################################### EOF) &gt; /etc/profile. d/motd. sh# MariaDB 安装(cat &lt;&lt;'EOF'[mariadb]name = MariaDBbaseurl = http://yum. mariadb. org/10. 1/centos7-amd64gpgkey=https://yum. mariadb. org/RPM-GPG-KEY-MariaDBgpgcheck=1EOF) &gt; /etc/yum. repos. d/MariaDB. repoyum install -y MariaDB-server MariaDB-clientsystemctl start mysql. servicesystemctl enable mysql. service# mongodb 安装(cat &lt;&lt;'EOF'[mongodb-org-3. 4]name=MongoDB Repositorybaseurl=https://repo. mongodb. org/yum/redhat/$releasever/mongodb-org/3. 4/x86_64/gpgcheck=1enabled=1gpgkey=https://www. mongodb. org/static/pgp/server-3. 4. ascEOF) &gt; /etc/yum. repos. d/mongodb-org-3. 4. repoyum -y install mongodb-orgsystemctl start mongodsystemctl enable mongod# 安装redisyum install -y redissystemctl start redissystemctl enable redis. service# 安装pythonyum install -y python36uyum install -y python36u-pip# Apache 服务安装yum install -y httpdsystemctl start httpd. servicesystemctl enable httpd. servicefirewall-cmd --permanent --add-service=http# 安装NTP 时间同步服务yum install -y chronysystemctl start chronydsystemctl status chronydsystemctl enable chronyd # 让chronyd 在开机的时候就自动启动timedatectl # 查看是否已经同步, yes 就是同步了. # 安装gityum install -y git# Nginx 安装yum install -y nginxsed -i 's/listen    80 default_server/listen    8081 default_server/g' /etc/nginx/nginx. confsed -i 's/listen    \[::\]:80 default_server/listen    \[::\]:8081 default_server/g' /etc/nginx/nginx. confsystemctl start nginx. servicesystemctl enable nginx. service# 源码安装php7. 2mkdir /root/codemkdir /root/bincd /root/codewget http://cn2. php. net/distributions/php-7. 2. 0. tar. gztar -zxvf php-7. 2. 0. tar. gzcd php-7. 2. 0. /configure --prefix=/usr/local/php7make &amp;&amp; make install# 为命令创建软链接到 PATH 对应的目录下ln -s /usr/local/php7/bin/php /root/bin/php# 安装python3. 7cd /root/codewget https://www. python. org/ftp/python/3. 7. 0/Python-3. 7. 0a3. tgztar -zxvf Python-3. 7. 0a3. tgzcd Python-3. 7. 0a3. /configure --prefix=/usr/local/python3. 7make &amp;&amp; make installln -s /usr/local/python3. 7/bin/python3. 7 /root/bin/python3. 7ln -s /usr/local/python3. 7/bin/pip3. 7 /root/bin/pip3. 7# 升级 bash4. 4cd /root/codewget http://ftp. gnu. org/gnu/bash/bash-4. 4. tar. gztar -zxvf bash-4. 4. tar. gzcd bash-4. 4. /configure --prefix=/usr/local/bash4. 4make &amp;&amp; make installmv /usr/bin/bash /usr/bin/bash. BAKln -s /usr/local/bash4. 4/bin/bash /usr/bin/bash# 让系统支持中文yum -y install kde-l10n-Chineseyum -y reinstall glibc-commonyum clean alllocaledef -c -f UTF-8 -i zh_CN zh_CN. utf8cat /dev/null &gt; /etc/locale. conf &amp;&amp; echo  LC_ALL=\ zh_CN. UTF-8\   &gt; /etc/locale. conf# 关闭安全设置setenforce 0sed -i 's/SELINUX=enforcing/SELINUX=disabled/g' /etc/selinux/configsystemctl stop firewalldsystemctl disable firewalld"
    }];

var idx = lunr(function () {
    this.ref('id')
    this.field('title')
    this.field('body')

    documents.forEach(function (doc) {
        this.add(doc)
    }, this)
});
function lunr_search(term) {
    document.getElementById('lunrsearchresults').innerHTML = '<ul></ul>';
    if(term) {
        document.getElementById('lunrsearchresults').innerHTML = "<p>Search results for '" + term + "'</p>" + document.getElementById('lunrsearchresults').innerHTML;
        //put results on the screen.
        var results = idx.search(term);
        if(results.length>0){
            //console.log(idx.search(term));
            //if results
            for (var i = 0; i < results.length; i++) {
                // more statements
                var ref = results[i]['ref'];
                var url = documents[ref]['url'];
                var title = documents[ref]['title'];
                var body = documents[ref]['body'].substring(0,160)+'...';
                document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML + "<li class='lunrsearchresult'><a href='" + url + "'><span class='title'>" + title + "</span><br /><span class='body'>"+ body +"</span><br /><span class='url'>"+ url +"</span></a></li>";
            }
        } else {
            document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = "<li class='lunrsearchresult'>No results found...</li>";
        }
    }
    return false;
}

function lunr_search(term) {
    $('#lunrsearchresults').show( 400 );
    $( "body" ).addClass( "modal-open" );
    
    document.getElementById('lunrsearchresults').innerHTML = '<div id="resultsmodal" class="modal fade show d-block"  tabindex="-1" role="dialog" aria-labelledby="resultsmodal"> <div class="modal-dialog shadow-lg" role="document"> <div class="modal-content"> <div class="modal-header" id="modtit"> <button type="button" class="close" id="btnx" data-dismiss="modal" aria-label="Close"> &times; </button> </div> <div class="modal-body"> <ul class="mb-0"> </ul>    </div> <div class="modal-footer"><button id="btnx" type="button" class="btn btn-danger btn-sm" data-dismiss="modal">Close</button></div></div> </div></div>';
    if(term) {
        document.getElementById('modtit').innerHTML = "<h5 class='modal-title'>Search results for '" + term + "'</h5>" + document.getElementById('modtit').innerHTML;
        //put results on the screen.
        var results = idx.search(term);
        if(results.length>0){
            //console.log(idx.search(term));
            //if results
            for (var i = 0; i < results.length; i++) {
                // more statements
                var ref = results[i]['ref'];
                var url = documents[ref]['url'];
                var title = documents[ref]['title'];
                var body = documents[ref]['body'].substring(0,160)+'...';
                document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML + "<li class='lunrsearchresult'><a href='" + url + "'><span class='title'>" + title + "</span><br /><small><span class='body'>"+ body +"</span><br /><span class='url'>"+ url +"</span></small></a></li>";
            }
        } else {
            document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = "<li class='lunrsearchresult'>Sorry, no results found. Close & try a different search!</li>";
        }
    }
    return false;
}
    
$(function() {
    $("#lunrsearchresults").on('click', '#btnx', function () {
        $('#lunrsearchresults').hide( 5 );
        $( "body" ).removeClass( "modal-open" );
    });
});